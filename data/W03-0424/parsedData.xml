<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000305">
<title confidence="0.659714">
b&apos;Language Independent NER using a Maximum Entropy Tagger
</title>
<author confidence="0.910625">
James R. Curran and Stephen Clark
</author>
<affiliation confidence="0.9910465">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.878663">
2 Buccleuch Place, Edinburgh. EH8 9LW
</address>
<email confidence="0.99662">
{jamesc,stephenc}@cogsci.ed.ac.uk
</email>
<sectionHeader confidence="0.988984" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9884471">
Named Entity Recognition (NER) systems need
to integrate a wide variety of information for
optimal performance. This paper demonstrates
that a maximum entropy tagger can effectively
encode such information and identify named
entities with very high accuracy. The tagger
uses features which can be obtained for a vari-
ety of languages and works effectively not only
for English, but also for other languages such
as German and Dutch.
</bodyText>
<sectionHeader confidence="0.6850115" genericHeader="introduction">
1 Introduction
Named Entity Recognition1
</sectionHeader>
<bodyText confidence="0.996095809523809">
(NER) can be treated as a
tagging problem where each word in a sentence is as-
signed a label indicating whether it is part of a named
entity and the entity type. Thus methods used for part
of speech (POS) tagging and chunking can also be used
for NER. The papers from the CoNLL-2002 shared
task which used such methods (e.g. Malouf (2002),
Burger et al. (2002)) reported results significantly lower
than the best system (Carreras et al., 2002). However,
Zhou and Su (2002) have reported state of the art results
on the MUC-6 and MUC-7 data using a HMM-based tagger.
Zhou and Su (2002) used a wide variety of features,
which suggests that the relatively poor performance of the
taggers used in CoNLL-2002 was largely due to the fea-
ture sets used rather than the machine learning method.
We demonstrate this to be the case by improving on the
best Dutch results from CoNLL-2002 using a maximum
entropy (ME) tagger. We report reasonable precision and
recall (84.9 F-score) for the CoNLL-2003 English test
data, and an F-score of 68.4 for the CoNLL-2003 Ger-
man test data.
</bodyText>
<page confidence="0.919215">
1
</page>
<bodyText confidence="0.99974575">
We assume that NER involves assigning the correct label to
an entity as well as identifying its boundaries.
Incorporating a diverse set of overlapping features
in a HMM-based tagger is difficult and complicates the
smoothing typically used for such taggers. In contrast, a
ME tagger can easily deal with diverse, overlapping fea-
tures. We also use a Gaussian prior on the parameters for
effective smoothing over the large feature space.
</bodyText>
<sectionHeader confidence="0.988704" genericHeader="method">
2 The ME Tagger
</sectionHeader>
<bodyText confidence="0.995368">
The ME tagger is based on Ratnaparkhi (1996)s POS tag-
ger and is described in Curran and Clark (2003) . The
tagger uses models of the form:
</bodyText>
<equation confidence="0.9990611">
p(y|x) =
1
Z(x)
exp
n
X
i=1
ifi(x, y)
!
(1)
</equation>
<bodyText confidence="0.923028">
where y is the tag, x is the context and the fi(x, y) are
the features with associated weights i. The probability
of a tag sequence y1 . . . yn given a sentence w1 . . . wn is
approximated as follows:
</bodyText>
<equation confidence="0.998377">
p(y1 . . . yn|w1 . . . wn)
n
Y
i=1
p(yi|xi) (2)
</equation>
<bodyText confidence="0.9975984">
where xi is the context for word wi. The tagger uses
beam search to find the most probable sequence given the
sentence.
The features are binary valued functions which pair a
tag with various elements of the context; for example:
</bodyText>
<equation confidence="0.989013666666667">
fj(x, y) =
\x1a
1 if word(x) = Moody &amp; y = I-PER
0 otherwise
(3)
word(x) = Moody is an example of a contextual predi-
</equation>
<bodyText confidence="0.7296975">
cate.
Generalised Iterative Scaling (GIS) is used to estimate
the values of the weights. The tagger uses a Gaussian
prior over the weights (Chen et al., 1999) which allows a
large number of rare, but informative, features to be used
without overfitting.
\x0cCondition Contextual predicate
freq(wi) &lt; 5 X is prefix of wi, |X |4
X is suffix of wi, |X |4
wi contains a digit
wi contains uppercase character
wi contains a hyphen
</bodyText>
<equation confidence="0.999720125">
wi wi = X
wi1 = X, wi2 = X
wi+1 = X, wi+2 = X
wi POSi = X
POSi1 = X, POSi2 = X
POSi+1 = X, POSi+2 = X
wi NEi1 = X
NEi2NEi1 = XY
</equation>
<tableCaption confidence="0.998776">
Table 1: Contextual predicates in baseline system
</tableCaption>
<sectionHeader confidence="0.97587" genericHeader="method">
3 The Data
</sectionHeader>
<bodyText confidence="0.999736736842105">
We used three data sets: the English and German data
for the CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003) and the Dutch data for the CoNLL-
2002 shared task (Tjong Kim Sang, 2002). Each word in
the data sets is annotated with a named entity tag plus POS
tag, and the words in the German and English data also
have a chunk tag. Our system does not currently exploit
the chunk tags.
There are 4 types of entities to be recognised: persons,
locations, organisations, and miscellaneous entities not
belonging to the other three classes. The 2002 data uses
the IOB-2 format in which a B-XXX tag indicates the first
word of an entity of type XXX and I-XXX is used for sub-
sequent words in an entity of type XXX. The tag O in-
dicates words outside of a named entity. The 2003 data
uses a variant of IOB-2, IOB-1, in which I-XXX is used
for all words in an entity, including the first word, unless
the first word separates contiguous entities of the same
type, in which case B-XXX is used.
</bodyText>
<sectionHeader confidence="0.9868" genericHeader="method">
4 The Feature Set
</sectionHeader>
<bodyText confidence="0.994385461538462">
Table 1 lists the contextual predicates used in our base-
line system, which are based on those used in the
Curran and Clark (2003) CCG supertagger. The first set
of features apply to rare words, i.e. those which appear
less than 5 times in the training data. The first two kinds
of features encode prefixes and suffixes less than length 5,
and the remaining rare word features encode other mor-
phological characteristics. These features are important
for tagging unknown and rare words. The remaining fea-
tures are the word, POS tag, and NE tag history features,
using a window size of 2. Note that the NEi2NEi1
feature is a composite feature of both the previous and
previous-previous NE tags.
</bodyText>
<table confidence="0.922155">
Condition Contextual predicate
freq(wi) &lt; 5 wi contains period
wi contains punctuation
wi is only digits
wi is a number
wi is {upper,lower,title,mixed} case
wi is alphanumeric
length of wi
wi has only Roman numerals
wi is an initial (X.)
wi is an acronym (ABC, A.B.C.)
wi memory NE tag for wi
unigram tag of wi+1
unigram tag of wi+2
wi wi in a gazetteer
wi1 in a gazetteer
wi+1 in a gazetteer
wi wi not lowercase and flc &gt; fuc
wi unigrams of word type
bigrams of word types
trigrams of word types
</table>
<tableCaption confidence="0.9818695">
Table 2: Contextual predicates in final system
Table 2 lists the extra features used in our final
</tableCaption>
<bodyText confidence="0.999166918032787">
system. These features have been shown to be use-
ful in other NER systems. The additional ortho-
graphic features have proved useful in other systems,
for example Carreras et al. (2002), Borthwick (1999) and
Zhou and Su (2002). Some of the rows in Table 2 de-
scribe sets of contextual predicates. The wi is only digits
predicates apply to words consisting of all digits. They
encode the length of the digit string with separate pred-
icates for lengths 14 and a single predicate for lengths
greater than 4. Titlecase applies to words with an ini-
tial uppercase letter followed by all lowercase (e.g. Mr).
Mixedcase applies to words with mixed lower- and up-
percase (e.g. CityBank). The length predicates encode
the number of characters in the word from 1 to 15, with a
single predicate for lengths greater than 15.
The next set of contextual predicates encode extra in-
formation about NE tags in the current context. The
memory NE tag predicate (see e.g. Malouf (2002))
records the NE tag that was most recently assigned to
the current word. The use of beam-search tagging
means that tags can only be recorded from previous
sentences. This memory is cleared at the beginning
of each document. The unigram predicates (see e.g.
Tsukamoto et al. (2002)) encode the most probable tag
for the next words in the window. The unigram probabil-
ities are relative frequencies obtained from the training
data. This feature enables us to know something about
the likely NE tag of the next word before reaching it.
\x0cMost systems use gazetteers to encode information
about personal and organisation names, locations and
trigger words. There is considerable variation in the size
of the gazetteers used. Some studies found that gazetteers
did not improve performance (e.g. Malouf (2002)) whilst
others gained significant improvement using gazetteers
and triggers (e.g. Carreras et al. (2002)). Our system in-
corporates only English and Dutch first name and last
name gazetteers as shown in Table 6. These gazetteers
are used for predicates applied to the current, previous
and next word in the window.
Collins (2002) includes a number of interesting con-
textual predicates for NER. One feature we have adapted
encodes whether the current word is more frequently seen
lowercase than uppercase in a large external corpus. This
feature is useful for disambiguating beginning of sen-
tence capitalisation and tagging sentences which are all
capitalised. The frequency counts have been obtained
from 1 billion words of English newspaper text collected
by Curran and Osborne (2002).
Collins (2002) also describes a mapping from words to
word types which groups words with similar orthographic
forms into classes. This involves mapping characters to
classes and merging adjacent characters of the same type.
For example, Moody becomes Aa, A.B.C. becomes
A.A.A. and 1,345.05 becomes 0,0.0. The classes
are used to define unigram, bigram and trigram contex-
tual predicates over the window.
We have also defined additional composite features
which are a combination of atomic features; for exam-
ple, a feature which is active for mid-sentence titlecase
words seen more frequently as lowercase than uppercase
in a large external corpus.
</bodyText>
<sectionHeader confidence="0.999922" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.991414176470588">
The baseline development results for English using the
supertagger features only are given in Table 3. The full
system results for the English development data are given
in Table 7. Clearly the additional features have a signifi-
cant impact on both precision and recall scores across all
entities. We have found that the word type features are
particularly useful, as is the memory feature. The perfor-
mance of the final system drops by 1.97% if these fea-
tures are removed. The performance of the system if the
gazetteer features are removed is given in Table 4. The
sizes of our gazetteers are given in Table 6. We have
experimented with removing the other contextual pred-
icates but each time performance was reduced, except for
the next-next unigram tag feature which was switched off
for all final experiments.
The results for the Dutch test data are given in Table 5.
These improve upon the scores of the best performing
</bodyText>
<table confidence="0.988131285714286">
system at CoNLL-2002 (Carreras et al., 2002).
English DEV PRECISION RECALL F=1
LOCATION 90.78% 90.58% 90.68
MISC 85.80% 81.24% 83.45
ORGANISATION 82.24% 80.09% 81.15
PERSON 92.02% 92.67% 92.35
OVERALL 88.53% 87.41% 87.97
</table>
<tableCaption confidence="0.705182">
Table 3: Baseline C&amp;C results for English DEV data
</tableCaption>
<table confidence="0.998828">
English DEV PRECISION RECALL F=1
LOCATION 91.69% 93.14% 92.41
MISC 88.15% 83.08% 85.54
ORGANISATION 83.48% 85.53% 84.49
PERSON 94.40% 95.11% 94.75
OVERALL 90.13% 90.47% 90.30
</table>
<tableCaption confidence="0.949616">
Table 4: No external resources results for Eng. DEV data
</tableCaption>
<table confidence="0.959600272727273">
Dutch TEST PRECISION RECALL F=1
LOCATION 84.42% 81.91% 83.15
MISC 78.46% 74.89% 76.64
ORGANISATION 77.35% 68.93% 72.90
PERSON 80.13% 90.71% 85.09
OVERALL 79.91% 79.35% 79.63
Table 5: Results for the Dutch TEST data
Gazetteer ENTRIES
FIRST NAME 6,673
LAST NAME 89,836
freqLC &gt; freqUC LIST 778,791
</table>
<tableCaption confidence="0.794774333333333">
Table 6: Size of Gazetteers
The final results for the English test data are given in
Table 7. These are significantly lower than the results for
</tableCaption>
<bodyText confidence="0.922184">
the development data. The results for the German devel-
opment and test sets are given in Table 7. For the German
NER we removed the lowercase more frequent than up-
percase feature. Apart from this change, the system was
identical. We did not add any extra gazetteer information
for German.
</bodyText>
<sectionHeader confidence="0.990849" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998983583333333">
Our NER system demonstrates that using a large variety
of features produces good performance. These features
can be defined and extracted in a language independent
manner, as our results for German, Dutch and English
show. Maximum entropy models are an effective way
of incorporating diverse and overlapping features. Our
maximum entropy tagger employs Gaussian smoothing
which allows a large number of sparse, but informative,
\x0cfeatures to be used without overfitting.
Using a wider context window than 2 words may im-
prove performance; a reranking phase using global fea-
tures may also improve performance (Collins, 2002).
</bodyText>
<sectionHeader confidence="0.944105" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9942688">
We would like to thank Jochen Leidner for help collecting
the Gazetteers. This research was supported by a Com-
monwealth scholarship and a Sydney University Trav-
elling scholarship to the first author, and EPSRC grant
GR/M96889.
</bodyText>
<sectionHeader confidence="0.977271" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.952539171428571">
Andrew Borthwick. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. thesis,
New York University.
John D. Burger, John C. Henderson, and William T. Mor-
gan. 2002. Statistical Named Entity Recogizer Adap-
tation. In Proceedings of the 2002 CoNLL Workshop,
pages 163166, Taipei, Taiwan.
Xavier Carreras, Lluis Marquez, and Lluis Padro. 2002.
Named Entity Recognition using AdaBoost. In Pro-
ceedings of the 2002 CoNLL Workshop, pages 167
170, Taipei, Taiwan.
John Chen, Srinivas Bangalore, and K. Vijay-Shanker.
1999. New Models for Improving Supertag Disam-
biguation. In Proceedings of the 9th Meeting of EACL,
Bergen, Norway.
Michael Collins. 2002. Ranking Algorithms for Named-
Entity Extraction: Boosting and the Voted Perceptron.
In Proceedings of the 40th Meeting of the ACL, pages
489496, Philadelphia, PA.
James R. Curran and Stephen Clark. 2003. Investigating
GIS and Smoothing for Maximum Entropy Taggers.
In Proceedings of the 11th Meeting of the European
Chapter of the ACL, Budapest, Hungary.
James R. Curran and Miles Osborne. 2002. A very very
large corpus doesnt always yield reliable estimates. In
Proceedings of the 2002 CoNLL Workshop, pages 126
131, Taipei, Taiwan.
Robert Malouf. 2002. Markov models for language-
independent named entity recognition. In Proceedings
of the 2002 CoNLL Workshop, pages 187190, Taipei,
Taiwan.
Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-
Of-Speech Tagger. In Proceedings of the EMNLP
Conference, pages 133142, Philadelphia, PA.
English devel. Precision Recall F=1
</reference>
<table confidence="0.999342782608695">
LOC 91.75% 93.20% 92.47
MISC 88.34% 82.97% 85.57
ORG 83.54% 85.53% 84.52
PER 94.26% 95.39% 94.82
Overall 90.15% 90.56% 90.35
English test Precision Recall F=1
LOC 84.97% 90.53% 87.66
MISC 76.77% 75.78% 76.27
ORG 79.60% 79.41% 79.51
PER 91.64% 90.79% 91.21
Overall 84.29% 85.50% 84.89
German devel. Precision Recall F=1
LOC 67.59% 70.11% 68.83
MISC 71.87% 48.81% 58.14
ORG 71.85% 50.60% 59.39
PER 81.69% 64.03% 71.79
Overall 73.29% 58.89% 65.31
German test Precision Recall F=1
LOC 70.91% 71.11% 71.01
MISC 68.51% 46.12% 55.13
ORG 68.43% 50.19% 57.91
PER 88.04% 72.05% 79.25
Overall 75.61% 62.46% 68.41
</table>
<tableCaption confidence="0.749894">
Table 7: Full system results for the English and German
development and test data sets.
</tableCaption>
<reference confidence="0.995410777777778">
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition. In
Walter Daelemans and Miles Osborne, editors, Pro-
ceedings of CoNLL-2003, Edmonton, Canada.
Erik F. Tjong Kim Sang. 2002. Introduction to the
CoNLL-2002 Shared Task: Language-Independent
Named Entity Recognition. In Proceedings of CoNLL-
2002, Taipei, Taiwan.
Koji Tsukamoto, Yutaka Mitsuishi, and Manabu Sassano.
2002. Learning with Multiple Stacking for Named En-
tity Recognition. In Proceedings of the 2002 CoNLL
Workshop, pages 191194, Taipei, Taiwan.
GuoDong Zhou and Jian Su. 2002. Named Entity
Recognition using an HMM-based Chunk Tagger. In
Proceedings of the 40th Annual Meeting of the ACL,
pages 473480, Philadelphia, PA.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657211">
<title confidence="0.998959">b&apos;Language Independent NER using a Maximum Entropy Tagger</title>
<author confidence="0.999992">James R Curran</author>
<author confidence="0.999992">Stephen Clark</author>
<affiliation confidence="0.999657">School of Informatics University of Edinburgh</affiliation>
<address confidence="0.694298">2 Buccleuch Place, Edinburgh. EH8 9LW</address>
<email confidence="0.996103">jamesc@cogsci.ed.ac.uk</email>
<email confidence="0.996103">stephenc@cogsci.ed.ac.uk</email>
<abstract confidence="0.995372454545454">Named Entity Recognition (NER) systems need to integrate a wide variety of information for optimal performance. This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew Borthwick</author>
</authors>
<title>A Maximum Entropy Approach to Named Entity Recognition.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>New York University.</institution>
<contexts>
<context position="6103" citStr="Borthwick (1999)" startWordPosition="1098" endWordPosition="1099">gth of wi wi has only Roman numerals wi is an initial (X.) wi is an acronym (ABC, A.B.C.) wi memory NE tag for wi unigram tag of wi+1 unigram tag of wi+2 wi wi in a gazetteer wi1 in a gazetteer wi+1 in a gazetteer wi wi not lowercase and flc &gt; fuc wi unigrams of word type bigrams of word types trigrams of word types Table 2: Contextual predicates in final system Table 2 lists the extra features used in our final system. These features have been shown to be useful in other NER systems. The additional orthographic features have proved useful in other systems, for example Carreras et al. (2002), Borthwick (1999) and Zhou and Su (2002). Some of the rows in Table 2 describe sets of contextual predicates. The wi is only digits predicates apply to words consisting of all digits. They encode the length of the digit string with separate predicates for lengths 14 and a single predicate for lengths greater than 4. Titlecase applies to words with an initial uppercase letter followed by all lowercase (e.g. Mr). Mixedcase applies to words with mixed lower- and uppercase (e.g. CityBank). The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater th</context>
</contexts>
<marker>Borthwick, 1999</marker>
<rawString>Andrew Borthwick. 1999. A Maximum Entropy Approach to Named Entity Recognition. Ph.D. thesis, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Burger</author>
<author>John C Henderson</author>
<author>William T Morgan</author>
</authors>
<title>Statistical Named Entity Recogizer Adaptation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 CoNLL Workshop,</booktitle>
<pages>163166</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1051" citStr="Burger et al. (2002)" startWordPosition="164" endWordPosition="167">entify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable precision and recall (84.9</context>
</contexts>
<marker>Burger, Henderson, Morgan, 2002</marker>
<rawString>John D. Burger, John C. Henderson, and William T. Morgan. 2002. Statistical Named Entity Recogizer Adaptation. In Proceedings of the 2002 CoNLL Workshop, pages 163166, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
<author>Lluis Padro</author>
</authors>
<date>2002</date>
<contexts>
<context position="1134" citStr="Carreras et al., 2002" startWordPosition="176" endWordPosition="179"> be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable precision and recall (84.9 F-score) for the CoNLL-2003 English test data, and an F-score of 68.4 for the CoNL</context>
<context position="6085" citStr="Carreras et al. (2002)" startWordPosition="1094" endWordPosition="1097">e wi is alphanumeric length of wi wi has only Roman numerals wi is an initial (X.) wi is an acronym (ABC, A.B.C.) wi memory NE tag for wi unigram tag of wi+1 unigram tag of wi+2 wi wi in a gazetteer wi1 in a gazetteer wi+1 in a gazetteer wi wi not lowercase and flc &gt; fuc wi unigrams of word type bigrams of word types trigrams of word types Table 2: Contextual predicates in final system Table 2 lists the extra features used in our final system. These features have been shown to be useful in other NER systems. The additional orthographic features have proved useful in other systems, for example Carreras et al. (2002), Borthwick (1999) and Zhou and Su (2002). Some of the rows in Table 2 describe sets of contextual predicates. The wi is only digits predicates apply to words consisting of all digits. They encode the length of the digit string with separate predicates for lengths 14 and a single predicate for lengths greater than 4. Titlecase applies to words with an initial uppercase letter followed by all lowercase (e.g. Mr). Mixedcase applies to words with mixed lower- and uppercase (e.g. CityBank). The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for </context>
<context position="7772" citStr="Carreras et al. (2002)" startWordPosition="1372" endWordPosition="1375">e the most probable tag for the next words in the window. The unigram probabilities are relative frequencies obtained from the training data. This feature enables us to know something about the likely NE tag of the next word before reaching it. \x0cMost systems use gazetteers to encode information about personal and organisation names, locations and trigger words. There is considerable variation in the size of the gazetteers used. Some studies found that gazetteers did not improve performance (e.g. Malouf (2002)) whilst others gained significant improvement using gazetteers and triggers (e.g. Carreras et al. (2002)). Our system incorporates only English and Dutch first name and last name gazetteers as shown in Table 6. These gazetteers are used for predicates applied to the current, previous and next word in the window. Collins (2002) includes a number of interesting contextual predicates for NER. One feature we have adapted encodes whether the current word is more frequently seen lowercase than uppercase in a large external corpus. This feature is useful for disambiguating beginning of sentence capitalisation and tagging sentences which are all capitalised. The frequency counts have been obtained from </context>
<context position="10078" citStr="Carreras et al., 2002" startWordPosition="1747" endWordPosition="1750">rticularly useful, as is the memory feature. The performance of the final system drops by 1.97% if these features are removed. The performance of the system if the gazetteer features are removed is given in Table 4. The sizes of our gazetteers are given in Table 6. We have experimented with removing the other contextual predicates but each time performance was reduced, except for the next-next unigram tag feature which was switched off for all final experiments. The results for the Dutch test data are given in Table 5. These improve upon the scores of the best performing system at CoNLL-2002 (Carreras et al., 2002). English DEV PRECISION RECALL F=1 LOCATION 90.78% 90.58% 90.68 MISC 85.80% 81.24% 83.45 ORGANISATION 82.24% 80.09% 81.15 PERSON 92.02% 92.67% 92.35 OVERALL 88.53% 87.41% 87.97 Table 3: Baseline C&amp;C results for English DEV data English DEV PRECISION RECALL F=1 LOCATION 91.69% 93.14% 92.41 MISC 88.15% 83.08% 85.54 ORGANISATION 83.48% 85.53% 84.49 PERSON 94.40% 95.11% 94.75 OVERALL 90.13% 90.47% 90.30 Table 4: No external resources results for Eng. DEV data Dutch TEST PRECISION RECALL F=1 LOCATION 84.42% 81.91% 83.15 MISC 78.46% 74.89% 76.64 ORGANISATION 77.35% 68.93% 72.90 PERSON 80.13% 90.71% </context>
</contexts>
<marker>Carreras, Marquez, Padro, 2002</marker>
<rawString>Xavier Carreras, Lluis Marquez, and Lluis Padro. 2002.</rawString>
</citation>
<citation valid="false">
<title>Named Entity Recognition using AdaBoost.</title>
<booktitle>In Proceedings of the 2002 CoNLL Workshop,</booktitle>
<pages>167--170</pages>
<location>Taipei, Taiwan.</location>
<marker></marker>
<rawString>Named Entity Recognition using AdaBoost. In Proceedings of the 2002 CoNLL Workshop, pages 167 170, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Srinivas Bangalore</author>
<author>K Vijay-Shanker</author>
</authors>
<title>New Models for Improving Supertag Disambiguation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Meeting of EACL,</booktitle>
<location>Bergen,</location>
<contexts>
<context position="3148" citStr="Chen et al., 1999" startWordPosition="548" endWordPosition="551"> . . . yn given a sentence w1 . . . wn is approximated as follows: p(y1 . . . yn|w1 . . . wn) n Y i=1 p(yi|xi) (2) where xi is the context for word wi. The tagger uses beam search to find the most probable sequence given the sentence. The features are binary valued functions which pair a tag with various elements of the context; for example: fj(x, y) = \x1a 1 if word(x) = Moody &amp; y = I-PER 0 otherwise (3) word(x) = Moody is an example of a contextual predicate. Generalised Iterative Scaling (GIS) is used to estimate the values of the weights. The tagger uses a Gaussian prior over the weights (Chen et al., 1999) which allows a large number of rare, but informative, features to be used without overfitting. \x0cCondition Contextual predicate freq(wi) &lt; 5 X is prefix of wi, |X |4 X is suffix of wi, |X |4 wi contains a digit wi contains uppercase character wi contains a hyphen wi wi = X wi1 = X, wi2 = X wi+1 = X, wi+2 = X wi POSi = X POSi1 = X, POSi2 = X POSi+1 = X, POSi+2 = X wi NEi1 = X NEi2NEi1 = XY Table 1: Contextual predicates in baseline system 3 The Data We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for</context>
</contexts>
<marker>Chen, Bangalore, Vijay-Shanker, 1999</marker>
<rawString>John Chen, Srinivas Bangalore, and K. Vijay-Shanker. 1999. New Models for Improving Supertag Disambiguation. In Proceedings of the 9th Meeting of EACL, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking Algorithms for NamedEntity Extraction: Boosting and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>489496</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="7996" citStr="Collins (2002)" startWordPosition="1412" endWordPosition="1413">reaching it. \x0cMost systems use gazetteers to encode information about personal and organisation names, locations and trigger words. There is considerable variation in the size of the gazetteers used. Some studies found that gazetteers did not improve performance (e.g. Malouf (2002)) whilst others gained significant improvement using gazetteers and triggers (e.g. Carreras et al. (2002)). Our system incorporates only English and Dutch first name and last name gazetteers as shown in Table 6. These gazetteers are used for predicates applied to the current, previous and next word in the window. Collins (2002) includes a number of interesting contextual predicates for NER. One feature we have adapted encodes whether the current word is more frequently seen lowercase than uppercase in a large external corpus. This feature is useful for disambiguating beginning of sentence capitalisation and tagging sentences which are all capitalised. The frequency counts have been obtained from 1 billion words of English newspaper text collected by Curran and Osborne (2002). Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. This involve</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking Algorithms for NamedEntity Extraction: Boosting and the Voted Perceptron. In Proceedings of the 40th Meeting of the ACL, pages 489496, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Investigating GIS and Smoothing for Maximum Entropy Taggers.</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th Meeting of the European Chapter of the ACL,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2313" citStr="Curran and Clark (2003)" startWordPosition="378" endWordPosition="381">st data, and an F-score of 68.4 for the CoNLL-2003 German test data. 1 We assume that NER involves assigning the correct label to an entity as well as identifying its boundaries. Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers. In contrast, a ME tagger can easily deal with diverse, overlapping features. We also use a Gaussian prior on the parameters for effective smoothing over the large feature space. 2 The ME Tagger The ME tagger is based on Ratnaparkhi (1996)s POS tagger and is described in Curran and Clark (2003) . The tagger uses models of the form: p(y|x) = 1 Z(x) exp n X i=1 ifi(x, y) ! (1) where y is the tag, x is the context and the fi(x, y) are the features with associated weights i. The probability of a tag sequence y1 . . . yn given a sentence w1 . . . wn is approximated as follows: p(y1 . . . yn|w1 . . . wn) n Y i=1 p(yi|xi) (2) where xi is the context for word wi. The tagger uses beam search to find the most probable sequence given the sentence. The features are binary valued functions which pair a tag with various elements of the context; for example: fj(x, y) = \x1a 1 if word(x) = Moody &amp; </context>
<context position="4745" citStr="Curran and Clark (2003)" startWordPosition="857" endWordPosition="860"> to the other three classes. The 2002 data uses the IOB-2 format in which a B-XXX tag indicates the first word of an entity of type XXX and I-XXX is used for subsequent words in an entity of type XXX. The tag O indicates words outside of a named entity. The 2003 data uses a variant of IOB-2, IOB-1, in which I-XXX is used for all words in an entity, including the first word, unless the first word separates contiguous entities of the same type, in which case B-XXX is used. 4 The Feature Set Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger. The first set of features apply to rare words, i.e. those which appear less than 5 times in the training data. The first two kinds of features encode prefixes and suffixes less than length 5, and the remaining rare word features encode other morphological characteristics. These features are important for tagging unknown and rare words. The remaining features are the word, POS tag, and NE tag history features, using a window size of 2. Note that the NEi2NEi1 feature is a composite feature of both the previous and previous-previous NE tags. Condition Contextual predicate freq(w</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Investigating GIS and Smoothing for Maximum Entropy Taggers. In Proceedings of the 11th Meeting of the European Chapter of the ACL, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Miles Osborne</author>
</authors>
<title>A very very large corpus doesnt always yield reliable estimates.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 CoNLL Workshop,</booktitle>
<pages>126--131</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="8452" citStr="Curran and Osborne (2002)" startWordPosition="1480" endWordPosition="1483"> name and last name gazetteers as shown in Table 6. These gazetteers are used for predicates applied to the current, previous and next word in the window. Collins (2002) includes a number of interesting contextual predicates for NER. One feature we have adapted encodes whether the current word is more frequently seen lowercase than uppercase in a large external corpus. This feature is useful for disambiguating beginning of sentence capitalisation and tagging sentences which are all capitalised. The frequency counts have been obtained from 1 billion words of English newspaper text collected by Curran and Osborne (2002). Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. This involves mapping characters to classes and merging adjacent characters of the same type. For example, Moody becomes Aa, A.B.C. becomes A.A.A. and 1,345.05 becomes 0,0.0. The classes are used to define unigram, bigram and trigram contextual predicates over the window. We have also defined additional composite features which are a combination of atomic features; for example, a feature which is active for mid-sentence titlecase words seen more frequently as lowe</context>
</contexts>
<marker>Curran, Osborne, 2002</marker>
<rawString>James R. Curran and Miles Osborne. 2002. A very very large corpus doesnt always yield reliable estimates. In Proceedings of the 2002 CoNLL Workshop, pages 126 131, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>Markov models for languageindependent named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 CoNLL Workshop,</booktitle>
<pages>187190</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="1029" citStr="Malouf (2002)" startWordPosition="162" endWordPosition="163">ormation and identify named entities with very high accuracy. The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable prec</context>
<context position="6862" citStr="Malouf (2002)" startWordPosition="1231" endWordPosition="1232">sting of all digits. They encode the length of the digit string with separate predicates for lengths 14 and a single predicate for lengths greater than 4. Titlecase applies to words with an initial uppercase letter followed by all lowercase (e.g. Mr). Mixedcase applies to words with mixed lower- and uppercase (e.g. CityBank). The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater than 15. The next set of contextual predicates encode extra information about NE tags in the current context. The memory NE tag predicate (see e.g. Malouf (2002)) records the NE tag that was most recently assigned to the current word. The use of beam-search tagging means that tags can only be recorded from previous sentences. This memory is cleared at the beginning of each document. The unigram predicates (see e.g. Tsukamoto et al. (2002)) encode the most probable tag for the next words in the window. The unigram probabilities are relative frequencies obtained from the training data. This feature enables us to know something about the likely NE tag of the next word before reaching it. \x0cMost systems use gazetteers to encode information about persona</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. Markov models for languageindependent named entity recognition. In Proceedings of the 2002 CoNLL Workshop, pages 187190, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy PartOf-Speech Tagger.</title>
<date>1996</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>133142</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2257" citStr="Ratnaparkhi (1996)" startWordPosition="369" endWordPosition="370">recall (84.9 F-score) for the CoNLL-2003 English test data, and an F-score of 68.4 for the CoNLL-2003 German test data. 1 We assume that NER involves assigning the correct label to an entity as well as identifying its boundaries. Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers. In contrast, a ME tagger can easily deal with diverse, overlapping features. We also use a Gaussian prior on the parameters for effective smoothing over the large feature space. 2 The ME Tagger The ME tagger is based on Ratnaparkhi (1996)s POS tagger and is described in Curran and Clark (2003) . The tagger uses models of the form: p(y|x) = 1 Z(x) exp n X i=1 ifi(x, y) ! (1) where y is the tag, x is the context and the fi(x, y) are the features with associated weights i. The probability of a tag sequence y1 . . . yn given a sentence w1 . . . wn is approximated as follows: p(y1 . . . yn|w1 . . . wn) n Y i=1 p(yi|xi) (2) where xi is the context for word wi. The tagger uses beam search to find the most probable sequence given the sentence. The features are binary valued functions which pair a tag with various elements of the conte</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy PartOf-Speech Tagger. In Proceedings of the EMNLP Conference, pages 133142, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>English devel</author>
</authors>
<title>Precision Recall F=1 Erik F. Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Walter Daelemans and</booktitle>
<editor>Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>devel, 2003</marker>
<rawString>English devel. Precision Recall F=1 Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL2002,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3797" citStr="Sang, 2002" startWordPosition="682" endWordPosition="683">but informative, features to be used without overfitting. \x0cCondition Contextual predicate freq(wi) &lt; 5 X is prefix of wi, |X |4 X is suffix of wi, |X |4 wi contains a digit wi contains uppercase character wi contains a hyphen wi wi = X wi1 = X, wi2 = X wi+1 = X, wi+2 = X wi POSi = X POSi1 = X, POSi2 = X POSi+1 = X, POSi+2 = X wi NEi1 = X NEi2NEi1 = XY Table 1: Contextual predicates in baseline system 3 The Data We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL2002 shared task (Tjong Kim Sang, 2002). Each word in the data sets is annotated with a named entity tag plus POS tag, and the words in the German and English data also have a chunk tag. Our system does not currently exploit the chunk tags. There are 4 types of entities to be recognised: persons, locations, organisations, and miscellaneous entities not belonging to the other three classes. The 2002 data uses the IOB-2 format in which a B-XXX tag indicates the first word of an entity of type XXX and I-XXX is used for subsequent words in an entity of type XXX. The tag O indicates words outside of a named entity. The 2003 data uses a </context>
</contexts>
<marker>Sang, 2002</marker>
<rawString>Erik F. Tjong Kim Sang. 2002. Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of CoNLL2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koji Tsukamoto</author>
<author>Yutaka Mitsuishi</author>
<author>Manabu Sassano</author>
</authors>
<title>Learning with Multiple Stacking for Named Entity Recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 CoNLL Workshop,</booktitle>
<pages>191194</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="7143" citStr="Tsukamoto et al. (2002)" startWordPosition="1276" endWordPosition="1279">o words with mixed lower- and uppercase (e.g. CityBank). The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater than 15. The next set of contextual predicates encode extra information about NE tags in the current context. The memory NE tag predicate (see e.g. Malouf (2002)) records the NE tag that was most recently assigned to the current word. The use of beam-search tagging means that tags can only be recorded from previous sentences. This memory is cleared at the beginning of each document. The unigram predicates (see e.g. Tsukamoto et al. (2002)) encode the most probable tag for the next words in the window. The unigram probabilities are relative frequencies obtained from the training data. This feature enables us to know something about the likely NE tag of the next word before reaching it. \x0cMost systems use gazetteers to encode information about personal and organisation names, locations and trigger words. There is considerable variation in the size of the gazetteers used. Some studies found that gazetteers did not improve performance (e.g. Malouf (2002)) whilst others gained significant improvement using gazetteers and triggers</context>
</contexts>
<marker>Tsukamoto, Mitsuishi, Sassano, 2002</marker>
<rawString>Koji Tsukamoto, Yutaka Mitsuishi, and Manabu Sassano. 2002. Learning with Multiple Stacking for Named Entity Recognition. In Proceedings of the 2002 CoNLL Workshop, pages 191194, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GuoDong Zhou</author>
<author>Jian Su</author>
</authors>
<title>Named Entity Recognition using an HMM-based Chunk Tagger.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>473480</pages>
<location>Philadelphia, PA. \x0c&apos;</location>
<contexts>
<context position="1163" citStr="Zhou and Su (2002)" startWordPosition="181" endWordPosition="184">guages and works effectively not only for English, but also for other languages such as German and Dutch. 1 Introduction Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type. Thus methods used for part of speech (POS) tagging and chunking can also be used for NER. The papers from the CoNLL-2002 shared task which used such methods (e.g. Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002). However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger. We report reasonable precision and recall (84.9 F-score) for the CoNLL-2003 English test data, and an F-score of 68.4 for the CoNLL-2003 German test data. 1 We</context>
<context position="6126" citStr="Zhou and Su (2002)" startWordPosition="1101" endWordPosition="1104"> Roman numerals wi is an initial (X.) wi is an acronym (ABC, A.B.C.) wi memory NE tag for wi unigram tag of wi+1 unigram tag of wi+2 wi wi in a gazetteer wi1 in a gazetteer wi+1 in a gazetteer wi wi not lowercase and flc &gt; fuc wi unigrams of word type bigrams of word types trigrams of word types Table 2: Contextual predicates in final system Table 2 lists the extra features used in our final system. These features have been shown to be useful in other NER systems. The additional orthographic features have proved useful in other systems, for example Carreras et al. (2002), Borthwick (1999) and Zhou and Su (2002). Some of the rows in Table 2 describe sets of contextual predicates. The wi is only digits predicates apply to words consisting of all digits. They encode the length of the digit string with separate predicates for lengths 14 and a single predicate for lengths greater than 4. Titlecase applies to words with an initial uppercase letter followed by all lowercase (e.g. Mr). Mixedcase applies to words with mixed lower- and uppercase (e.g. CityBank). The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater than 15. The next set of </context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>GuoDong Zhou and Jian Su. 2002. Named Entity Recognition using an HMM-based Chunk Tagger. In Proceedings of the 40th Annual Meeting of the ACL, pages 473480, Philadelphia, PA. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>