<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<bodyText confidence="0.62961">
b&amp;apos;Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 328332,
Jeju, Republic of Korea, 8-14 July 2012. c
</bodyText>
<figure confidence="0.857842333333333">
2012 Association for Computational Linguistics
A Meta Learning Approach to Grammatical Error Correction
Hongsuck Seo1
, Jonghoon Lee1
, Seokhwan Kim2
, Kyusong Lee1
Sechun Kang1
, Gary Geunbae Lee1
1
</figure>
<affiliation confidence="0.942139">
Pohang University of Science and Technology
</affiliation>
<page confidence="0.899955">
2
</page>
<affiliation confidence="0.767576">
Institute for Infocomm Research
</affiliation>
<email confidence="0.9024645">
{hsseo, jh21983}@postech.ac.kr, kims@i2r.a-star.edu.sg
{kyusonglee, freshboy, gblee}@postech.ac.kr
</email>
<sectionHeader confidence="0.989941" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99774725">
We introduce a novel method for
grammatical error correction with a number
of small corpora. To make the best use of
several corpora with different
characteristics, we employ a meta-learning
with several base classifiers trained on
different corpora. This research focuses on
a grammatical error correction task for
article errors. A series of experiments is
presented to show the effectiveness of the
proposed approach on two different
grammatical error tagged corpora.
</bodyText>
<sectionHeader confidence="0.993845" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999149236363636">
As language learning has drawn significant
attention in the community, grammatical error
correction (GEC), consequently, has attracted a fair
amount of attention. Several organizations have
built diverse resources including grammatical error
(GE) tagged corpora.
Although there are some publicly released GE
tagged corpora, it is still challenging to train a
good GEC model due to the lack of large GE
tagged learner corpus. The available GE tagged
corpora are mostly small datasets having different
characteristics depending on the development
methods, e.g. spoken corpus vs. written corpus.
This situation forced researchers to utilize native
corpora rather than GE tagged learner corpora for
the GEC task.
The native corpus approach consists of learning
a model that predicts the correct form of an article
given the surrounding context. Some researchers
focused on mining better features from the
linguistic and pedagogic knowledge, whereas
others focused on testing different classification
methods (Knight and Chandler, 1994; Minnen et
al., 2000; Lee, 2004; Nagata et al., 2006; Han et al.,
2006; De Felice, 2008).
Recently, a group of researchers introduced
methods utilizing a GE tagged learner corpus to
derive more accurate results (Han et al., 2010;
Rozovskaya and Roth, 2010). Since the two
approaches are closely related to each other, they
can be informative to each other. For example,
Dahlmeier and Ng (2011) proposed a method that
combines a native corpus and a GE tagged learner
corpus and it outperformed models trained with
either a native or GE tagged learner corpus alone.
However, methods which train a GEC model from
various GE tagged corpora have received less
focus.
In this paper, we present a novel approach to the
GEC task using meta-learning. We focus mainly
on article errors for two reasons. First, articles are
one of the most significant sources of GE for the
learners with various L1 backgrounds. Second, the
effective features for article error correction are
already well engineered allowing for quick
analysis of the method. Our approach is
distinguished from others by integrating the
predictive models trained on several GE tagged
learner corpora, rather than just one GE tagged
corpus. Moreover, the framework is compatible to
any classification technique. In this study, we also
use a native corpus employing Dahlmeier and Ngs
approach. We demonstrate the effectiveness of the
proposed method against baseline models in article
error correction tasks.
</bodyText>
<page confidence="0.997663">
328
</page>
<bodyText confidence="0.9976305">
\x0cThe remainder of this paper is organized as
follows: Section 2 explains our proposed method.
The experiments are presented in Section 3. Finally,
Section 4 concludes the paper.
</bodyText>
<sectionHeader confidence="0.986122" genericHeader="introduction">
2. Method
</sectionHeader>
<bodyText confidence="0.999647681818182">
Our method predicts the type of article for a noun
phrase within three classes: null, definite, and
indefinite. A correction arises when the prediction
disagrees with the observed article. The
meta-learning technique is applied to this task to
deal with multiple corpora obtained from different
sources.
A meta-classifier decides the final output based
on the intermediate results obtained from several
base classifiers. Each base classifier is trained on a
different corpus than are the other classifiers. In
this work, the feature extraction processes used for
the base classifiers are identical to each other for
simplicity, although they need not necessarily be
identical. The meta-classifier takes the output
scores of the base classifiers as its input and is
trained on the held-out development data (Figure
1a). During run time, the trained classifiers are
organized in the same manner. For the given
features, the base classifiers independently
calculate the score, then the meta-classifier makes
the final decision based on the scores (Figure 1b).
</bodyText>
<subsectionHeader confidence="0.665598">
2.1. Meta-learning
</subsectionHeader>
<bodyText confidence="0.991031882352941">
Meta-learning is a sequential learning process
following the output of other base learners
(classifiers). Normally, different classifiers
successfully predict results on different parts of the
input space, so researchers have often tried to
combine different classifiers together (Breiman,
1996; Cohen et al., 2007; Zhang, 2007; Aydn,
2009; Menahem et al., 2009). To capitalize on the
strengths and compensate for the weaknesses of
each classifier, we build a meta-learner that takes
an input vector consisting of the outputs of the
base classifiers. The performance of meta-learning
can be improved using output probabilities for
every class label from the base classifiers.
The meta-classifier for the proposed method
consists of multiple linear classifiers. Each
classifier takes an input vector consisting of the
output scores of each base classifier and calculates
a score for each type of article. The meta-classifier
finally takes the class having the maximum score.
A common design of an ensemble is to train
different base classifiers with the same dataset, but
in this work one classification technique was used
with different datasets each having different
characteristics. Although only one classification
method was used in this work, different methods
each well-tuned to the individual corpora may be
used to improve the performance.
We employed the meta-learning method to
generate synergy among corpora with diverse
characteristics. More specifically, it is shown by
cross validation that meta-learning performs at a
level that is comparable to the best base classifier
(Dzeroski and Zenko, 2004).
</bodyText>
<subsectionHeader confidence="0.48924">
2.2. Base Classifiers
</subsectionHeader>
<bodyText confidence="0.946409666666667">
In the meta-learning framework, the performance
of the base classifiers is important because the
improvement in base classification generally enha-
</bodyText>
<figureCaption confidence="0.987401">
Figure 1: Overview of the proposed method
</figureCaption>
<page confidence="0.999453">
329
</page>
<bodyText confidence="0.981194129032258">
\x0cnces the overall performance. The base classifiers
can be expected to become more informative as
more data are provided. We followed the structural
learning approach (Ando and Zhang, 2005), which
trains a model from both a native corpus and a GE
tagged corpus (Dahlmeire and Ng, 2011), to
improve the base classifiers by the additional
information extracted from a native corpus.
Structural learning is a technique which trains
multiple classifiers with common structure. The
common structure chooses the hypothesis space of
each individual classifier and the individual
classifiers are trained separately once the
hypothesis space is determined. The common
structure can be obtained from auxiliary problems
which are closely related to the main problems.
A word selection problem is a task to predict the
appropriate word given the surrounding context in
a native corpus and is a closely related auxiliary
problem of the GEC task. We can obtain the
common structure from the article selection
problem and use it for the correction problem.
In this work, all the base classifiers used the
same least squares loss function for structural
learning. We adopted the feature set investigated
in De Felice (2008) for article error correction. We
use the Stanford coreNLP toolkit1
(Toutanova and
Manning, 2000; Klein and Manning, 2003a; Klein
and Manning, 2003b; Finkel et al, 2005) to extract
the features.
</bodyText>
<subsectionHeader confidence="0.841258">
2.3. Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.993168090909091">
The effectiveness of the proposed method is
evaluated in terms of accuracy, precision, recall,
and F1-score (Dahlmeire and Ng, 2011). Accuracy
is the number of correct predictions divided by the
total number of instances. Precision is the ratio of
the suggested corrections that agree with the
tagged answer to the total number of the suggested
corrections whereas recall is the ratio of the
suggested corrections that agree with the tagged
answer to the total number of corrections in the
corpus.
</bodyText>
<sectionHeader confidence="0.670636" genericHeader="method">
3. Experiments
3.1. Datasets
</sectionHeader>
<bodyText confidence="0.836482">
In this work we used a native corpus and two GE
tagged corpora. For the native corpus, we used
</bodyText>
<equation confidence="0.507634333333333">
1
http://nlp.stanford.edu/software/corenlp.shtml
news data2
</equation>
<bodyText confidence="0.993792658536585">
which is a large English text extracted
from news articles. The First Certificate in English
exams in the Cambridge Learner Corpus 3
(hereafter, CLC-FCE; Yannakoudakis et al., 2011)
and the Japanese Learner English corpus (Izumi et.
al., 2005) were used for the GE tagged corpora.
We extracted noun phrases from each corpus by
parsing the text of the respective corpora. (1) We
parsed the native corpus from the beginning until
approximately a million noun phrases are extracted.
(2) About 90k noun phrases containing ~3,300
mistakes in article usage were extracted from the
entire CLC-FCE corpus, and (3) about 30k noun
phrases containing ~2,500 mistakes were extracted
from the JLE corpus.
The extracted noun phrases were used for our
training and test data. We hold out 10% of the data
for the test. We applied 20% under-sampling to the
training instances that do not have any errors to
alleviate data imbalance in the training set.
We emphasize the fact that the two learner
corpora differ from each other in three aspects. The
first aspect is the styles of the texts: the CLC is
literary whereas the JLE is colloquial. The second
is the error rate: about 3.5% for CLC-FCE and
8.5% for JLE. Finally, the third is the distribution
of L1 languages of the learners: the learners of the
CLC corpus have various L1 backgrounds whereas
the learners of the JLE consist of only Japanese.
These experiments demonstrate the effectiveness
of the proposed method relying on the diversity of
the corpora.
The native corpus was used to find the common
structure using structural learning and two GE
tagged learner corpora are used to train the base
classifiers by structural learning with the common
structure obtained from the news corpus.
We trained three classifiers for comparison; (1)
the classifier (INTEG) trained with the integrated
training set of the two GE tagged corpora, and two
base classifiers used for the ensemble: (2) the base
</bodyText>
<listItem confidence="0.640111">
classifier (CB) trained only with the CLC-FCE and
(3) the other base classifier (JB) trained with the
</listItem>
<footnote confidence="0.49480875">
JLE.
3.2. Results
The accuracy obtained from the word selection
task with the news corpus was 76.10%. Upon
2
http://www.statmt.org/wmt09/translation-task.html
3
http://www.ilexir.com/
</footnote>
<page confidence="0.989934">
330
</page>
<bodyText confidence="0.998718730769231">
\x0cobtaining the parameters of the word selection task,
the structural parameter was calculated by
singular value decomposition and was used for the
structural learning of the main GEC task.
We used three different test data sets: the
CLC-FCE, the JLE and an integrated test set of the
two. The accuracy (Acc.) and the precision (Prec.)
of the INTEG was poorer than CB on the CLC-
FCE test set (Table 1), whereas INTEG
outperformed JB on the JLE test (Table 2).
Some instances extracted from the CLC-FCE
corpus have similar characteristics to the instances
from the JLE corpus. This overlap of instances
affected the performance in both positive and
negative ways. Prediction of instances similar to
those in the JLE was enhanced. Consequently,
INTEG model demonstrated better accuracy and
precision for the JLE test set. Unfortunately, for
the CLC test set, the instances resulted in lower
accuracy and precision.
The proposed model is able to alleviate this
model bias due to similar instances observed in the
INTEG model. The accuracy of the proposed
model consistently increased by over 10% for all
three data sets. The relative performance gain in
terms of F1-score (F1) was 15% on the integrated
set. This performance gain stems from the over
25% relative improvement of the precision (Table
1, 2 and 3).
We believe the improvement comes from the
contribution of reconfirming procedures performed
by the meta-classifier. When the prediction of the
two base classifiers conflicts with each other, the
meta-classifier tends to choose the one with a
higher confidence score; this choice improves the
accuracy and precision because known features
generate a higher confidence whereas unseen or
less-weighted features generate a lower score.
Although the proposed model introduced a
tradeoff between precision and recall (Rec.), this
tradeoff was tolerable in order to improve the
overall F1-score. Since GEC is a task where false
alarm is critical, obtaining high precision is very
important. The low precision on the whole
experiments is due to the data imbalance. Instances
in the dataset are mostly not erroneous, e.g., only
3.5% of erroneous instances for the CLC corpus.
The standard for correct prediction is also very
strict and does not allow multiple answers.
Performance can be evaluated in a more realistic
way by applying a softer standard, e.g., by
evaluating manually.
</bodyText>
<sectionHeader confidence="0.972023" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999278076923077">
We have presented a novel approach to
grammatical error correction by building a
meta-classifier using multiple GE tagged corpora
with different characteristics in various aspects.
The experiments showed that building a
meta-classifier overcomes the interference that
occurs when training with a set of heterogeneous
corpora. The proposed method also outperforms
the base classifier themselves tested on the same
class of test set as the training set with which the
base classifiers are trained. A better automatic
evaluation metric would be needed as further
research.
</bodyText>
<sectionHeader confidence="0.937762" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<subsectionHeader confidence="0.549897">
Industrial Strategic technology development
</subsectionHeader>
<bodyText confidence="0.936277333333333">
program, 10035252, development of dialog-based
spontaneous speech interface technology on
mobile platform, funded by the Ministry of
</bodyText>
<table confidence="0.9166228">
Knowledge Economy (MKE, Korea).
Model Acc. Prec. Rec. F1
INTEG 73.37 4.69 72.39 8.82
CB 77.20 5.39 71.17 10.03
Proposed 86.99 6.17 45.77 10.88
</table>
<tableCaption confidence="0.9165905">
Table 1: Best results for GEC task on CLC-FCE
test set.
</tableCaption>
<table confidence="0.918897625">
Model Acc. Prec. Rec. F1
INTEG 78.87 14.88 85.47 25.35
JB 78.02 14.49 86.32 24.82
Proposed 89.61 19.28 46.60 27.27
Table 2: Best results for GEC task on JLE test set.
Model Acc. Prec. Rec. F1
INTEG 74.64 6.84 77.86 12.58
Proposed 87.50 8.61 46.12 14.52
</table>
<tableCaption confidence="0.997028">
Table 3: Best results for GEC task on the
</tableCaption>
<bodyText confidence="0.480148">
integrated set of CLC-FCE and JLE test sets.
</bodyText>
<page confidence="0.989321">
331
</page>
<reference confidence="0.992519358024692">
\x0cReferences
R.K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. Journal of Machine Learning Research,
6, pp. 1817-1853.
U. Aydn, S. Murat, Olcay T Yldz, A. Ethem, 2009,
Incremental construction of classifier and
discriminant ensembles, Information Science, 179 (9),
pp. 144-152.
L. Breiman, 1996, Bagging predictors, Machine
Learning, pp. 123140.
S. Cohen, L. Rokach, O. Maimon, 2007, Decision tree
instance space decomposition with grouped gain-
ratio, Information Science, 177 (17), pp. 35923612.
D. Dahlmeier, H. T. Ng, 2011, Grammatical error
correction with alternating structure optimization, In
Proceedings of the 49th
Annual Meeting of the ACL-
HLT 2011, pp. 915-923.
R. De Felice. 2008. Automatic Error Detection in Non-
native English. Ph.D. thesis, University of Oxford.
S. Dzeroski, B. Zenko, 2004, Is combining classifiers
with stacking better than selecting the best one?,
Machine Learning, 54 (3), pp. 255273.
J. R. Finkel, T. Grenager, and C. Manning. 2005.
Incorporating Non-local Information into
Information Extraction Systems by Gibbs Sampling.
In Proceedings of the 43nd Annual Meeting of the
ACL, pp. 363-370.
N.R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12(02), pp.
115-129.
N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010.
Using an error-annotated learner corpus to develop
an ESL/EFL error correction system. In Proceedings
of LREC.
D. Klein and C.D. Manning. 2003a. Accurate unlexical-
ized parsing. In Proceedings of ACL, pp. 423-430.
D. Klein and C.D. Manning. 2003b. Fast exact inference
with a factored model for natural language
processing. Advances in Neural Information
Processing Systems (NIPS 2002), 15, pp. 3-10.
K. Knight and I. Chander. 1994. Automated postediting
of documents. In Proceedings of AAAI, pp. 779-784.
J. Lee. 2004. Automatic article restoration. In Proceed-
ings of HLT-NAACL, pp. 31-36.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A
feedback-augmented method for detecting errors in
the writing of learners of English. In Proceedings of
COLING-ACL, pp. 241--248.
A. Mariko, 2007, Grammatical errors across proficiency
levels in L2 spoken and written English, The
Economic Journal of Takasaki City University of
Economics, 49 (3, 4), pp. 117-129.
E. Menahem, L. Rokach, Y. Elovici, 2009, Troika-An
imporoved stacking schema for classification tasks,
Information Science, 179 (24), pp. 4097-4122.
G. Minnen, F. Bond, and A. Copestake. 2000. Memory-
based learning for article generation. In Proceedings
of CoNLL, pp. 43-48.
E. Izumi, K. Uchimoto, H. Isahara, 2005, Error
annotation for corpus of Japanese learner English, In
Proceedings of the 6th
International Workshop on
Linguistically Interpreted Corpora, pp. 71-80.
A. Rozovskaya and D. Roth. 2010. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of HLT-NAACL, pp. 154-162.
K. Toutanova and C. D. Manning. 2000. Enriching the
Knowledge Sources Used in a Maximum Entropy
Part-of-Speech Tagger. In Proceedings of the Joint
SIGDAT Conference on EMNLP/VLC-2000, pp. 63-
70.
H.Yannakoudakis, T. Briscoe, B. Medlock, 2011, A
new dataset and method for automatically grading
ESOL texts, In Proceedings of ACL, pp. 180-189.
G. P. Zhang, 2007, A neural network ensemble method
with jittered training data for time series forecasting,
Information Sciences: An International Journal, 177
(23), pp. 53295346.
</reference>
<page confidence="0.987771">
332
</page>
<figure confidence="0.246863">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239277">
<title>A Meta Learning Approach to Grammatical Error Correction</title>
<note confidence="0.888938">b&amp;apos;Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 328332, Jeju, Republic of Korea, 8-14 July 2012. c 2012 Association for Computational Linguistics A Meta Learning Approach to Grammatical Error Correction Hongsuck Seo1 , Jonghoon Lee1 , Seokhwan Kim2 , Kyusong Lee1 Sechun Kang1 , Gary Geunbae Lee1 1</note>
<affiliation confidence="0.960627333333333">Pohang University of Science and Technology 2 Institute for Infocomm Research</affiliation>
<address confidence="0.639466">{hsseo, jh21983}@postech.ac.kr, kims@i2r.a-star.edu.sg</address>
<email confidence="0.97756">kyusonglee@postech.ac.kr</email>
<email confidence="0.97756">freshboy@postech.ac.kr</email>
<email confidence="0.97756">gblee@postech.ac.kr</email>
<abstract confidence="0.992646692307692">We introduce a novel method for grammatical error correction with a number of small corpora. To make the best use of several corpora with different characteristics, we employ a meta-learning with several base classifiers trained on different corpora. This research focuses on a grammatical error correction task for article errors. A series of experiments is presented to show the effectiveness of the proposed approach on two different grammatical error tagged corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences R K Ando</author>
<author>T Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<pages>1817--1853</pages>
<contexts>
<context position="6806" citStr="Ando and Zhang, 2005" startWordPosition="1019" endWordPosition="1022">nergy among corpora with diverse characteristics. More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier (Dzeroski and Zenko, 2004). 2.2. Base Classifiers In the meta-learning framework, the performance of the base classifiers is important because the improvement in base classification generally enhaFigure 1: Overview of the proposed method 329 \x0cnces the overall performance. The base classifiers can be expected to become more informative as more data are provided. We followed the structural learning approach (Ando and Zhang, 2005), which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus. Structural learning is a technique which trains multiple classifiers with common structure. The common structure chooses the hypothesis space of each individual classifier and the individual classifiers are trained separately once the hypothesis space is determined. The common structure can be obtained from auxiliary problems which are closely related to the main problems. A word selection problem is a ta</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>\x0cReferences R.K. Ando and T. Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6, pp. 1817-1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Aydn</author>
<author>S Murat</author>
<author>Olcay T Yldz</author>
<author>A Ethem</author>
</authors>
<title>Incremental construction of classifier and discriminant ensembles,</title>
<date>2009</date>
<journal>Information Science,</journal>
<volume>179</volume>
<issue>9</issue>
<pages>144--152</pages>
<marker>Aydn, Murat, Yldz, Ethem, 2009</marker>
<rawString>U. Aydn, S. Murat, Olcay T Yldz, A. Ethem, 2009, Incremental construction of classifier and discriminant ensembles, Information Science, 179 (9), pp. 144-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Breiman</author>
</authors>
<title>Bagging predictors,</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>123140</pages>
<contexts>
<context position="5080" citStr="Breiman, 1996" startWordPosition="760" endWordPosition="761"> and is trained on the held-out development data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al., 2007; Zhang, 2007; Aydn, 2009; Menahem et al., 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>L. Breiman, 1996, Bagging predictors, Machine Learning, pp. 123140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cohen</author>
<author>L Rokach</author>
<author>O Maimon</author>
</authors>
<title>Decision tree instance space decomposition with grouped gainratio,</title>
<date>2007</date>
<journal>Information Science,</journal>
<volume>177</volume>
<issue>17</issue>
<pages>35923612</pages>
<contexts>
<context position="5100" citStr="Cohen et al., 2007" startWordPosition="762" endWordPosition="765"> on the held-out development data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al., 2007; Zhang, 2007; Aydn, 2009; Menahem et al., 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-c</context>
</contexts>
<marker>Cohen, Rokach, Maimon, 2007</marker>
<rawString>S. Cohen, L. Rokach, O. Maimon, 2007, Decision tree instance space decomposition with grouped gainratio, Information Science, 177 (17), pp. 35923612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H T Ng</author>
</authors>
<title>Grammatical error correction with alternating structure optimization,</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACLHLT</booktitle>
<pages>915--923</pages>
<contexts>
<context position="2445" citStr="Dahlmeier and Ng (2011)" startWordPosition="356" endWordPosition="359"> article given the surrounding context. Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most significant sources of GE for the learners with various L1 backgrounds. Second, the effective features for article error correction are already well engineered allo</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>D. Dahlmeier, H. T. Ng, 2011, Grammatical error correction with alternating structure optimization, In Proceedings of the 49th Annual Meeting of the ACLHLT 2011, pp. 915-923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
</authors>
<title>Automatic Error Detection in Nonnative English.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oxford.</institution>
<marker>De Felice, 2008</marker>
<rawString>R. De Felice. 2008. Automatic Error Detection in Nonnative English. Ph.D. thesis, University of Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dzeroski</author>
<author>B Zenko</author>
</authors>
<title>Is combining classifiers with stacking better than selecting the best one?,</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<volume>54</volume>
<issue>3</issue>
<pages>255273</pages>
<contexts>
<context position="6398" citStr="Dzeroski and Zenko, 2004" startWordPosition="959" endWordPosition="962">gn of an ensemble is to train different base classifiers with the same dataset, but in this work one classification technique was used with different datasets each having different characteristics. Although only one classification method was used in this work, different methods each well-tuned to the individual corpora may be used to improve the performance. We employed the meta-learning method to generate synergy among corpora with diverse characteristics. More specifically, it is shown by cross validation that meta-learning performs at a level that is comparable to the best base classifier (Dzeroski and Zenko, 2004). 2.2. Base Classifiers In the meta-learning framework, the performance of the base classifiers is important because the improvement in base classification generally enhaFigure 1: Overview of the proposed method 329 \x0cnces the overall performance. The base classifiers can be expected to become more informative as more data are provided. We followed the structural learning approach (Ando and Zhang, 2005), which trains a model from both a native corpus and a GE tagged corpus (Dahlmeire and Ng, 2011), to improve the base classifiers by the additional information extracted from a native corpus. </context>
</contexts>
<marker>Dzeroski, Zenko, 2004</marker>
<rawString>S. Dzeroski, B. Zenko, 2004, Is combining classifiers with stacking better than selecting the best one?, Machine Learning, 54 (3), pp. 255273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="7993" citStr="Finkel et al, 2005" startWordPosition="1206" endWordPosition="1209">A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning. We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Experiments 3.1. Datasets In this work we used a n</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the ACL, pp. 363-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N R Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<issue>02</issue>
<pages>115--129</pages>
<contexts>
<context position="2122" citStr="Han et al., 2006" startWordPosition="304" endWordPosition="307">ving different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this </context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N.R. Han, M. Chodorow, and C. Leacock. 2006. Detecting errors in English article usage by non-native speakers. Natural Language Engineering, 12(02), pp. 115-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N R Han</author>
<author>J Tetreault</author>
<author>S H Lee</author>
<author>J Y Ha</author>
</authors>
<title>Using an error-annotated learner corpus to develop an ESL/EFL error correction system.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="2280" citStr="Han et al., 2010" startWordPosition="329" endWordPosition="332">e corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most</context>
</contexts>
<marker>Han, Tetreault, Lee, Ha, 2010</marker>
<rawString>N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010. Using an error-annotated learner corpus to develop an ESL/EFL error correction system. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="7945" citStr="Klein and Manning, 2003" startWordPosition="1198" endWordPosition="1201">ems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning. We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Ex</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003a. Accurate unlexicalized parsing. In Proceedings of ACL, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language processing.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems (NIPS</booktitle>
<volume>15</volume>
<pages>3--10</pages>
<contexts>
<context position="7945" citStr="Klein and Manning, 2003" startWordPosition="1198" endWordPosition="1201">ems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning. We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of corrections in the corpus. 3. Ex</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003b. Fast exact inference with a factored model for natural language processing. Advances in Neural Information Processing Systems (NIPS 2002), 15, pp. 3-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated postediting of documents.</title>
<date>1994</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>779--784</pages>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of AAAI, pp. 779-784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lee</author>
</authors>
<title>Automatic article restoration.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>31--36</pages>
<contexts>
<context position="2083" citStr="Lee, 2004" startWordPosition="298" endWordPosition="299">ora are mostly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged cor</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>J. Lee. 2004. Automatic article restoration. In Proceedings of HLT-NAACL, pp. 31-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>A Kawai</author>
<author>K Morihiro</author>
<author>N Isu</author>
</authors>
<title>A feedback-augmented method for detecting errors in the writing of learners of English.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>241--248</pages>
<contexts>
<context position="2104" citStr="Nagata et al., 2006" startWordPosition="300" endWordPosition="303">tly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received le</context>
</contexts>
<marker>Nagata, Kawai, Morihiro, Isu, 2006</marker>
<rawString>R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A feedback-augmented method for detecting errors in the writing of learners of English. In Proceedings of COLING-ACL, pp. 241--248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mariko</author>
</authors>
<title>Grammatical errors across proficiency levels in L2 spoken and written English,</title>
<date>2007</date>
<journal>The Economic Journal of Takasaki City University of Economics,</journal>
<volume>49</volume>
<issue>3</issue>
<pages>117--129</pages>
<marker>Mariko, 2007</marker>
<rawString>A. Mariko, 2007, Grammatical errors across proficiency levels in L2 spoken and written English, The Economic Journal of Takasaki City University of Economics, 49 (3, 4), pp. 117-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Menahem</author>
<author>L Rokach</author>
<author>Y Elovici</author>
</authors>
<title>Troika-An imporoved stacking schema for classification tasks,</title>
<date>2009</date>
<journal>Information Science,</journal>
<volume>179</volume>
<issue>24</issue>
<pages>4097--4122</pages>
<contexts>
<context position="5148" citStr="Menahem et al., 2009" startWordPosition="770" endWordPosition="773">. During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al., 2007; Zhang, 2007; Aydn, 2009; Menahem et al., 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-classifier finally takes the class having the max</context>
</contexts>
<marker>Menahem, Rokach, Elovici, 2009</marker>
<rawString>E. Menahem, L. Rokach, Y. Elovici, 2009, Troika-An imporoved stacking schema for classification tasks, Information Science, 179 (24), pp. 4097-4122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Minnen</author>
<author>F Bond</author>
<author>A Copestake</author>
</authors>
<title>Memorybased learning for article generation.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="2072" citStr="Minnen et al., 2000" startWordPosition="294" endWordPosition="297">ilable GE tagged corpora are mostly small datasets having different characteristics depending on the development methods, e.g. spoken corpus vs. written corpus. This situation forced researchers to utilize native corpora rather than GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE</context>
</contexts>
<marker>Minnen, Bond, Copestake, 2000</marker>
<rawString>G. Minnen, F. Bond, and A. Copestake. 2000. Memorybased learning for article generation. In Proceedings of CoNLL, pp. 43-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>H Isahara</author>
</authors>
<title>Error annotation for corpus of Japanese learner English,</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora,</booktitle>
<pages>71--80</pages>
<marker>Izumi, Uchimoto, Isahara, 2005</marker>
<rawString>E. Izumi, K. Uchimoto, H. Isahara, 2005, Error annotation for corpus of Japanese learner English, In Proceedings of the 6th International Workshop on Linguistically Interpreted Corpora, pp. 71-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Training paradigms for correcting errors in grammar and usage.</title>
<date>2010</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>154--162</pages>
<contexts>
<context position="2308" citStr="Rozovskaya and Roth, 2010" startWordPosition="333" endWordPosition="336">han GE tagged learner corpora for the GEC task. The native corpus approach consists of learning a model that predicts the correct form of an article given the surrounding context. Some researchers focused on mining better features from the linguistic and pedagogic knowledge, whereas others focused on testing different classification methods (Knight and Chandler, 1994; Minnen et al., 2000; Lee, 2004; Nagata et al., 2006; Han et al., 2006; De Felice, 2008). Recently, a group of researchers introduced methods utilizing a GE tagged learner corpus to derive more accurate results (Han et al., 2010; Rozovskaya and Roth, 2010). Since the two approaches are closely related to each other, they can be informative to each other. For example, Dahlmeier and Ng (2011) proposed a method that combines a native corpus and a GE tagged learner corpus and it outperformed models trained with either a native or GE tagged learner corpus alone. However, methods which train a GEC model from various GE tagged corpora have received less focus. In this paper, we present a novel approach to the GEC task using meta-learning. We focus mainly on article errors for two reasons. First, articles are one of the most significant sources of GE f</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Proceedings of HLT-NAACL, pp. 154-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C D Manning</author>
</authors>
<title>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on EMNLP/VLC-2000,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="7920" citStr="Toutanova and Manning, 2000" startWordPosition="1194" endWordPosition="1197">obtained from auxiliary problems which are closely related to the main problems. A word selection problem is a task to predict the appropriate word given the surrounding context in a native corpus and is a closely related auxiliary problem of the GEC task. We can obtain the common structure from the article selection problem and use it for the correction problem. In this work, all the base classifiers used the same least squares loss function for structural learning. We adopted the feature set investigated in De Felice (2008) for article error correction. We use the Stanford coreNLP toolkit1 (Toutanova and Manning, 2000; Klein and Manning, 2003a; Klein and Manning, 2003b; Finkel et al, 2005) to extract the features. 2.3. Evaluation Metric The effectiveness of the proposed method is evaluated in terms of accuracy, precision, recall, and F1-score (Dahlmeire and Ng, 2011). Accuracy is the number of correct predictions divided by the total number of instances. Precision is the ratio of the suggested corrections that agree with the tagged answer to the total number of the suggested corrections whereas recall is the ratio of the suggested corrections that agree with the tagged answer to the total number of correct</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>K. Toutanova and C. D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of the Joint SIGDAT Conference on EMNLP/VLC-2000, pp. 63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe H Yannakoudakis</author>
<author>B Medlock</author>
</authors>
<title>A new dataset and method for automatically grading ESOL texts,</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>180--189</pages>
<marker>Yannakoudakis, Medlock, 2011</marker>
<rawString>H.Yannakoudakis, T. Briscoe, B. Medlock, 2011, A new dataset and method for automatically grading ESOL texts, In Proceedings of ACL, pp. 180-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G P Zhang</author>
</authors>
<title>A neural network ensemble method with jittered training data for time series forecasting,</title>
<date>2007</date>
<journal>Information Sciences: An International Journal,</journal>
<volume>177</volume>
<issue>23</issue>
<pages>53295346</pages>
<contexts>
<context position="5113" citStr="Zhang, 2007" startWordPosition="766" endWordPosition="767">elopment data (Figure 1a). During run time, the trained classifiers are organized in the same manner. For the given features, the base classifiers independently calculate the score, then the meta-classifier makes the final decision based on the scores (Figure 1b). 2.1. Meta-learning Meta-learning is a sequential learning process following the output of other base learners (classifiers). Normally, different classifiers successfully predict results on different parts of the input space, so researchers have often tried to combine different classifiers together (Breiman, 1996; Cohen et al., 2007; Zhang, 2007; Aydn, 2009; Menahem et al., 2009). To capitalize on the strengths and compensate for the weaknesses of each classifier, we build a meta-learner that takes an input vector consisting of the outputs of the base classifiers. The performance of meta-learning can be improved using output probabilities for every class label from the base classifiers. The meta-classifier for the proposed method consists of multiple linear classifiers. Each classifier takes an input vector consisting of the output scores of each base classifier and calculates a score for each type of article. The meta-classifier fin</context>
</contexts>
<marker>Zhang, 2007</marker>
<rawString>G. P. Zhang, 2007, A neural network ensemble method with jittered training data for time series forecasting, Information Sciences: An International Journal, 177 (23), pp. 53295346.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>