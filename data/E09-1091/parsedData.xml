<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.7000675">
b&amp;apos;Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799807,
Athens, Greece, 30 March 3 April 2009. c
</bodyText>
<sectionHeader confidence="0.51875" genericHeader="abstract">
2009 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.602578">
MINT: A Method for Effective and Scalable Mining of
Named Entity Transliterations from Large Comparable Corpora
</title>
<author confidence="0.656226">
Raghavendra Udupa K Saravanan A Kumaran Jagadeesh Jagarlamudi*
</author>
<affiliation confidence="0.848464">
Microsoft Research India
</affiliation>
<address confidence="0.867462">
Bangalore 560080 INDIA
</address>
<email confidence="0.99099">
[raghavu,v-sarak,kumarana,jags}@microsoft.com
</email>
<sectionHeader confidence="0.990472" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.962306625">
In this paper, we address the problem of min-
ing transliterations of Named Entities (NEs)
from large comparable corpora. We leverage
the empirical fact that multilingual news ar-
ticles with similar news content are rich in
Named Entity Transliteration Equivalents
(NETEs). Our mining algorithm, MINT, uses
a cross-language document similarity model to
align multilingual news articles and then
mines NETEs from the aligned articles using a
transliteration similarity model. We show that
our approach is highly effective on 6 different
comparable corpora between English and 4
languages from 3 different language families.
Furthermore, it performs substantially better
than a state-of-the-art competitor.
</bodyText>
<sectionHeader confidence="0.99674" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976870105263158">
Named Entities (NEs) play a critical role in many
Natural Language Processing and Information
Retrieval (IR) tasks. In Cross-Language Infor-
mation Retrieval (CLIR) systems, they play an
even more important role as the accuracy of their
transliterations is shown to correlate highly with
the performance of the CLIR systems (Mandl
and Womser-Hacker, 2005, Xu and Weischedel,
2005). Traditional methods for transliterations
have not proven to be very effective in CLIR.
Machine Transliteration systems (AbdulJaleel
and Larkey, 2003; Al-Onaizan and Knight, 2002;
Virga and Khudanpur, 2003) usually produce
incorrect transliterations and translation lexcions
such as hand-crafted or statistical dictionaries are
too static to have good coverage of NEs1
occur-
ring in the current news events. Hence, there is a
critical need for creating and continually updat-
</bodyText>
<page confidence="0.39031">
*
</page>
<affiliation confidence="0.629775">
Currently with University of Utah.
</affiliation>
<page confidence="0.900355">
1
</page>
<bodyText confidence="0.996640520833333">
New NEs are introduced to the vocabulary of a lan-
guage every day. On an average, 260 and 452 new
NEs appeared daily in the XIE and AFE segments of
the LDC English Gigaword corpora respectively.
ing multilingual Named Entity transliteration
lexicons.
The ubiquitous availability of comparable
news corpora in multiple languages suggests a
promising alternative to Machine Transliteration,
namely, the mining of Named Entity Translitera-
tion Equivalents (NETEs) from such corpora.
News stories are typically rich in NEs and there-
fore, comparable news corpora can be expected
to contain NETEs (Klementiev and Roth, 2006;
Tao et al., 2006). The large quantity and the per-
petual availability of news corpora in many of
the worlds languages, make mining of NETEs a
viable alternative to traditional approaches. It is
this opportunity that we address in our work.
In this paper, we detail an effective and scala-
ble mining method, called MINT (MIning
Named-entity Transliteration equivalents), for
mining of NETEs from large comparable corpo-
ra. MINT addresses several challenges in mining
NETEs from large comparable corpora: exhaus-
tiveness (in mining sparse NETEs), computa-
tional efficiency (in scaling on corpora size),
language independence (in being applicable to
many language pairs) and linguistic frugality (in
requiring minimal external linguistic resources).
Our contributions are as follows:
We give empirical evidence for the hypo-
thesis that news articles in different languages
with reasonably similar content are rich sources
of NETEs (Udupa, et al., 2008).
We demonstrate that the above insight can
be translated into an effective approach for min-
ing NETEs from large comparable corpora even
when similar articles are not known a priori.
We demonstrate MINTs effectiveness on
4 language pairs involving 5 languages (English,
Hindi, Kannada, Russian, and Tamil) from 3 dif-
ferent language families, and its scalability on
corpora of vastly different sizes (2,000 to
200,000 articles).
We show that MINTs performance is sig-
nificantly better than a state of the art method
(Klementiev and Roth, 2006).
</bodyText>
<page confidence="0.993719">
799
</page>
<bodyText confidence="0.993572833333333">
\x0cWe discuss the motivation behind our ap-
proach in Section 2 and present the details in
Section 3. In Section 4, we describe the evalua-
tion process and in Section 5, we present the re-
sults and analysis. We discuss related work in
Section 6.
</bodyText>
<sectionHeader confidence="0.994728" genericHeader="method">
2 Motivation
</sectionHeader>
<bodyText confidence="0.965433315789474">
MINT is based on the hypothesis that news ar-
ticles in different languages with similar content
contain highly overlapping set of NEs. News
articles are typically rich in NEs as news is about
events involving people, locations, organizations,
etc2
. It is reasonable to expect that multilingual
news articles reporting the same news event
mention the same NEs in the respective languag-
es. For instance, consider the English and Hindi
news reports from the New York Times and the
BBC on the second oath taking of President Ba-
rack Obama (Figure 1). The articles are not pa-
rallel but discuss the same event. Naturally, they
mention the same NEs (such as Barack Obama,
John Roberts, White House) in the respective
languages, and hence, are rich sources of NETEs.
Our empirical investigation of comparable
corpora confirmed the above insight. A study of
</bodyText>
<page confidence="0.966374">
2
</page>
<bodyText confidence="0.999019242424242">
News articles from the BBC corpus had, on an
average, 12.9 NEs and new articles from the The
New Indian Express, about 11.8 NEs.
200 pairs of similar news articles published by
The New Indian Express in 2007 in English and
Tamil showed that 87% of the single word NEs
in the English articles had at least one translitera-
tion equivalent in the conjugate Tamil articles.
The MINT method leverages this empirically
backed insight to mine NETEs from such compa-
rable corpora.
However, there are several challenges to the
mining process: firstly, vast majority of the NEs
in comparable corpora are very sparse; our anal-
ysis showed that 80% of the NEs in The New
Indian Express news corpora appear less than 5
times in the entire corpora. Hence, any mining
method that depends mainly on repeated occur-
rences of the NEs in the corpora is likely to miss
vast majority of the NETEs. Secondly, the min-
ing method must restrict the candidate NETEs
that need to be examined for match to a reasona-
bly small number, not only to minimize false
positives but also to be computationally efficient.
Thirdly, the use of linguistic tools and resources
must be kept to a minimum as resources are
available only in a handful of languages. Finally,
it is important to use as little language-specific
knowledge as possible in order to make the min-
ing method applicable across a vast majority of
languages of the world. The MINT method pro-
posed in this paper addresses all the above is-
sues.
</bodyText>
<page confidence="0.987465">
800
</page>
<sectionHeader confidence="0.818921" genericHeader="method">
\x0c3 The MINT Mining Method
</sectionHeader>
<bodyText confidence="0.9986605">
MINT has two stages. In the first stage, for
every document in the source language side, the
set of documents in the target language side with
similar news content are found using a cross-
language document similarity model. In the
second stage, the NEs in the source language
side are extracted using a Named Entity Recog-
nizer (NER) and, subsequently, for each NE in a
source language document, its transliterations are
mined from the corresponding target language
documents. We present the details of the two
stages of MINT in the remainder of this section.
</bodyText>
<subsectionHeader confidence="0.999888">
3.1 Finding Similar Document Pairs
</subsectionHeader>
<bodyText confidence="0.9783248">
The first stage of MINT method (Figure 2) works
on the documents from the comparable corpora
(CS, CT) in languages S and T and produces a col-
lection AS,T of similar article pairs (DS, DT). Each
article pair (DS, DT) in AS,T consists of an article
(DS) in language S and an article (DT) in language
T, that have similar content. The cross-language
similarity between DS and DT, as measured by the
cross-language similarity model MD, is at least
&amp;gt; 0.
</bodyText>
<subsectionHeader confidence="0.531414">
Cross-language Document Similarity Model:
</subsectionHeader>
<bodyText confidence="0.976931214285714">
The cross-language document similarity model
measures the degree of similarity between a pair
of documents in source and target languages.
We use the negative KL-divergence between
source and target document probability distribu-
tions as the similarity measure.
Given two documents DS, DT in source and tar-
get languages respectively, with T
S V
V , denoting
the vocabulary of source and target languages,
the similarity between the two documents is giv-
en by the KL-divergence measure, -KL(DS  ||DT),
as:
</bodyText>
<equation confidence="0.999122629629629">
T
T
w S
T
T
T
S
T
V D
w
p
D
w
p
D
w
p
)
|
(
)
|
(
log
)
|
(
</equation>
<bodyText confidence="0.916031">
where p(w  |D) is the likelihood of word w in D.
As we are interested in target documents which
are similar to a given source document, we can
ignore the numerator as it is independent of the
target document. Finally, expanding p(wT  |Ds)
as )
</bodyText>
<equation confidence="0.699884625">
|
(
)
|
( S
V
w
T
</equation>
<figure confidence="0.694869146341463">
S
S w
w
p
D
w
p
S
S
we specify the
cross-language similarity score as follows:
Cross-language similarity =
)
|
(
log
)
|
(
)
|
( T
T
S
T
w w
S
S D
w
p
w
w
p
D
w
p
T
V
T S
V
S
</figure>
<subsectionHeader confidence="0.987058">
3.2 Mining NETEs from Document Pairs
</subsectionHeader>
<bodyText confidence="0.9990724">
The second stage of the MINT method works on
each pair of articles (DS, DT) in the collection AS,T
and produces a set PS,T of NETEs. Each pair (S,
T) in PS,T consists of an NE S in language S, and
a token T in language T, that are transliteration
equivalents of each other. Furthermore, the
transliteration similarity between S and T, as
measured by the transliteration similarity model
MT, is at least &amp;gt; 0. Figure 3 outlines this algo-
rithm.
</bodyText>
<subsubsectionHeader confidence="0.385589">
Discriminative Transliteration Similarity
</subsubsectionHeader>
<bodyText confidence="0.9407312">
Model:
The transliteration similarity model MT measures
the degree of transliteration equivalence between
a source language and a target language term.
Input: Comparable news corpora (CS, CT) in languages (S,T)
</bodyText>
<figure confidence="0.996038428571429">
Crosslanguage Document Similarity Model MD for (S, T)
Threshold score .
Output: Set AS,T of pairs of similar articles (DS, DT) from (CS, CT).
1 AS,T ; // Set of Similar articles (DS, DT)
2 for each article DS in CS do
3 XS ; // Set of candidates for DS.
4 for each article dT in CT do
5 score = CrossLanguageDocumentSimilarity(DS,dT,MD);
6 if (score ) then XS XS (dT , score) ;
7 end
8 DT = BestScoringCandidate(XS);
9 if (DT = ) then AS,T AS,T (DS, DT) ;
10 end
CrossLanguageSimilarDocumentPairs
</figure>
<figureCaption confidence="0.810197">
Figure 2. Stage 1 of MINT
</figureCaption>
<figure confidence="0.995584473684211">
Input:
Set AS,T of similar documents (DS, DT) in languages
(S,T),
Transliteration Similarity Model MT for (S, T),
Threshold score .
Output: Set PS,T of NETEs (S, T) from AS,T ;
1 PS,T ;
2 for each pair of articles (DS, DT) in AS,T do
3 for each named entity S in DS do
4 YS ; // Set of candidates for S.
5 for each candidate eT in DT do
6 score = TransliterationSimilarity(S, eT, MT) ;
7 if (score ) then YS YS (eT , score) ;
8 end
9 T = BestScoringCandidate(YS) ;
10 if (T = null) then PS,T PS,T (S, T) ;
11 end
12 end
TransliterationEquivalents
</figure>
<figureCaption confidence="0.994544">
Figure 3. Stage 2 of MINT
</figureCaption>
<page confidence="0.993426">
801
</page>
<bodyText confidence="0.932617">
\x0cWe employ a logistic function as our translitera-
tion similarity model MT, as follows:
</bodyText>
<equation confidence="0.5739144">
TransliterationSimilarity (S,eT,MT) =
)
,
( T
S
1
1
e
wt
e
</equation>
<bodyText confidence="0.9926469375">
where (S, eT) is the feature vector for the pair
(S, eT) and w is the weights vector. Note that the
transliteration similarity takes a value in the
range [0..1]. The weights vector w is learnt dis-
criminatively over a training corpus of known
transliteration equivalents in the given pair of
languages.
Features: The features employed by the model
capture interesting cross-language associations
observed in (S, eT):
All unigrams and bigrams from the
source and target language strings.
Pairs of source string n-grams and target
string n-grams such that difference in the
start positions of the source and target n-
grams is at most 2. Here n
</bodyText>
<page confidence="0.540661">
2
</page>
<equation confidence="0.830638666666667">
,
1
.
</equation>
<bodyText confidence="0.969640692307692">
Difference in the lengths of the two
strings.
Generative Transliteration Similarity Model:
We also experimented with an extension of Hes
W-HMM model (He, 2007). The transition prob-
ability depends on both the jump width and the
previous source character as in the W-HMM
model. The emission probability depends on the
current source character and the previous target
character unlike the W-HMM model (Udupa et
al., 2009). Instead of using any single alignment
of characters in the pair (wS, wT), we marginalize
over all possible alignments:
</bodyText>
<figure confidence="0.85576418">
1
1
1
1
1 ,
|
,
|
 |1
j
a
j
a
j
j
A
m
j
n
m
t
s
t
p
s
a
a
p
s
t
P j
j
Here, j
t (and resp. i
s ) denotes the jth
(and resp.
ith
) character in wT (and resp. wS) and m
a
A 1
is
the hidden alignment between wT and wS where
j
t is aligned to j
a
s , ,m
,
j
1
. We estimate
</figure>
<bodyText confidence="0.97886425">
the parameters of the model using the EM algo-
rithm. The transliteration similarity score of a
pair (wS, wT) is log P(wT  |wS) appropriately trans-
formed.
</bodyText>
<sectionHeader confidence="0.996718" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99022675">
Our empirical investigation consists of experi-
ments in three data environments, with each en-
vironment providing answer to specific set of
questions, as listed below:
</bodyText>
<listItem confidence="0.894024">
1.Ideal Environment (IDEAL): Given a collec-
tion AS,T of oracle-aligned article pairs (DS, DT)
in S and T, how effective is Stage 2 of MINT in
mining NETE from AS,T?
2. Near Ideal Environment (NEAR-IDEAL):
Let AS,T be a collection of similar article pairs
(DS, DT) in S and T. Given comparable corpora
(CS, CT) consisting of only articles from AS,T, but
without the knowledge of pairings between the
articles,
a. How effective is Stage 1 of MINT in re-
covering AS,T from (CS, CT) ?
b. What is the effect of Stage 1 on the
overall effectiveness of MINT?
3. Real Environment (REAL): Given large
comparable corpora (CS, CT), how effective is
MINT, end-to-end?
</listItem>
<bodyText confidence="0.998733689655172">
The IDEAL environment is indeed ideal for
MINT since every article in the comparable cor-
pora is paired with exactly one similar article in
the other language and the pairing of articles in
the comparable corpora is known in advance.
We want to emphasize here that such corpora are
indeed available in many domains such as tech-
nical documents and interlinked multilingual
Wikipedia articles. In the IDEAL environment,
only Stage 2 of MINT is put to test, as article
alignments are given.
In the NEAR-IDEAL data environment, every
article in the comparable corpora is known to
have exactly one conjugate article in the other
language though the pairing itself is not known
in advance. In such a setting, MINT needs to
discover the article pairing before mining NETEs
and therefore, both stages of MINT are put to
test. The best performance possible in this envi-
ronment should ideally be the same as that of
IDEAL, and any degradation points to the short-
coming of the Stage 1 of MINT. These two en-
vironments quantify the stage-wise performance
of the MINT method.
Finally, in the data environment REAL, we
test MINT on large comparable corpora, where
even the existence of a conjugate article in the
target side for a given article in the source side of
the comparable corpora is not guaranteed, as in
</bodyText>
<page confidence="0.979874">
802
</page>
<bodyText confidence="0.99172375">
\x0cany normal large multilingual news corpora. In
this scenario both the stages of MINT are put to
test. This is the toughest, and perhaps the typical
setting in which MINT would be used.
</bodyText>
<subsectionHeader confidence="0.995533">
4.1 Comparable Corpora
</subsectionHeader>
<bodyText confidence="0.987047857142857">
In our experiments, the source language is Eng-
lish whereas the 4 target languages are from
three different language families (Hindi from the
Indo-Aryan family, Russian from the Slavic fam-
ily, Kannada and Tamil from the Dravidian fami-
ly). Note that none of the five languages use a
common script and hence identification of cog-
nates, spelling variations, suffix transformations,
and other techniques commonly used for closely
related languages that have a common script are
not applicable for mining NETEs. Table 1 sum-
marizes the 6 different comparable corpora that
were used for the empirical investigation; 4 for
the IDEAL and NEAR-IDEAL environments (in
</bodyText>
<figure confidence="0.973224956521739">
4 language pairs), and 2 for the REAL environ-
ment (in 2 language pairs).
Cor-
pus
Source -
Target
Data
Environ-
ment
Articles (in
Thousands)
Words (in
Millions)
Src Tgt Src Tgt
EK-S
English-
Kannada
IDEAL&amp;
NEAR-IDEAL
2.90 2.90 0.42 0.34
ET-S
English-
Tamil
IDEAL&amp;
NEAR-IDEAL
2.90 2.90 0.42 0.32
ER-S
English-
Russian
IDEAL&amp;
NEAR-IDEAL
2.30 2.30 1.03 0.40
EH-S
English-
Hindi
IDEAL&amp;
NEAR-IDEAL
11.9 11.9 3.77 3.57
EK-L
English-
Kannada
REAL 103.8 111.0 27.5 18.2
ET-L
English-
Tamil
REAL 103.8 144.3 27.5 19.4
</figure>
<tableCaption confidence="0.987126">
Table 1: Comparable Corpora
</tableCaption>
<bodyText confidence="0.9960054">
The corpora can be categorized into two sepa-
rate groups, group S (for Small) consisting of
EK-S, ET-S, ER-S, and EH-S and group L (for
Large) consisting of EK-L and ET-L. Corpora in
group S are relatively small in size, and contain
pairs of articles that have been judged by human
annotators as similar. Corpora in group L are two
orders of magnitude larger in size than those in
group S and contain a large number of articles
that may not have conjugates in the target side.
In addition the pairings are unknown even for the
articles that have conjugates. All comparable
corpora had publication dates, except EH-S,
which is known to have been published over the
same year.
The EK-S, ET-S, EK-L and ET-L corpora are
from The New Indian Express news paper, whe-
reas the EH-S corpora are from Web Dunia and
the ER-S corpora are from BBC/Lenta News
Agency respectively.
</bodyText>
<subsectionHeader confidence="0.997165">
4.2 Cross-language Similarity Model
</subsectionHeader>
<bodyText confidence="0.996189642857143">
The cross-language document similarity model
requires a bilingual dictionary in the appropriate
language pair. Therefore, we generated statistical
dictionaries for 3 language pairs (from parallel
corpora of the following sizes: 11K sentence
pairs in English-Kannada, 54K in English-Hindi,
and 14K in English-Tamil) using the GIZA++
statistical alignment tool (Och et al., 2003), with
5 iterations each of IBM Model 1 and HMM.
We did not have access to an English-Russian
parallel corpus and hence could not generate a
dictionary for this language pair. Hence, the
NEAR-IDEAL experiments were not run for the
English-Russian language pair.
Although the coverage of the dictionaries was
low, this turned out to be not a serious issue for
our cross-language document similarity model as
it might have for topic based CLIR (Ballesteros
and Croft, 1998). Unlike CLIR, where the query
is typically smaller in length compared to the
documents, in our case we are dealing with news
articles of comparable size in both source and
target languages.
When many translations were available for a
source word, we considered only the top-4 trans-
lations. Further, we smoothed the document
probability distributions with collection frequen-
cy as described in (Ponte and Croft, 1998).
</bodyText>
<subsectionHeader confidence="0.999599">
4.3 Transliteration Similarity Model
</subsectionHeader>
<bodyText confidence="0.991785952380952">
The transliteration similarity models for each of
the 4 language pairs were produced by learning
over a training corpus consisting of about 16,000
single word NETEs, in each pair of languages.
The training corpus in English-Hindi, English-
Kannada and English-Tamil were hand-crafted
by professionals, the English-Russian name pairs
were culled from Wikipedia interwiki links and
were cleaned heuristically. Equal number of
negative samples was used for training the mod-
els. To produce the negative samples, we paired
each source language NE with a random non-
matching target language NE. No language spe-
cific features were used and the same feature set
was used in each of the 4 language pairs making
MINT language neutral.
In all the experiments, our source side lan-
guage is English, and the Stanford Named Entity
Recognizer (Finkel et al, 2005) was used to ex-
tract NEs from the source side article. It should
be noted here that while the precision of the NER
</bodyText>
<page confidence="0.991603">
803
</page>
<bodyText confidence="0.9769482">
\x0cused was consistently high, its recall was low,
(~40%) especially in the New Indian Express
corpus, perhaps due to the differences in the data
used for training the NER and the data on which
we used it.
</bodyText>
<subsectionHeader confidence="0.978855">
4.4 Performance Measures
</subsectionHeader>
<bodyText confidence="0.995101466666666">
Our intention is to measure the effectiveness of
MINT by comparing its performance with the
oracular (human annotator) performance. As
transliteration equivalents must exist in the
paired articles to be found by MINT, we focus
only on those NEs that actually have at least one
transliteration equivalent in the conjugate article.
Three performance measures are of interest to
us: the fraction of distinct NEs from source lan-
guage for which we found at least one translitera-
tion in the target side (Recall on distinct NEs),
the fraction of distinct NETEs (Recall on distinct
NETEs) and the Mean Reciprocal Rank (MRR)
of the NETEs mined. Since we are interested in
mining not only the highly frequent but also the
infrequent NETEs, recall metrics measure how
effective our method is in mining NETEs ex-
haustively. The MRR score indicates how effec-
tive our method is in preferring the correct ones
among candidates.
To measure the performance of MINT, we
created a test bed for each of the language pairs.
The test beds are summarized in Table 2.
The test beds consist of pairs of similar ar-
ticles in each of the language pairs. It should be
noted here that as transliteration equivalents must
exist in the paired articles to be found by MINT,
we focus only on those NEs that actually have at
least one transliteration equivalent in the conju-
gate article.
</bodyText>
<sectionHeader confidence="0.997676" genericHeader="method">
5 Results &amp; Analysis
</sectionHeader>
<bodyText confidence="0.9998805">
In this section, we present qualitative and quan-
titative performance of the MINT algorithm, in
mining NETEs from comparable news corpora.
All the results in Sections 5.1 to 5.3 were ob-
tained using the discriminative transliteration
similarity model described in Section 3.2. The
results using the generative transliteration simi-
larity model are discussed in Section 5.4.
</bodyText>
<subsectionHeader confidence="0.952041">
5.1 IDEAL Environment
</subsectionHeader>
<bodyText confidence="0.980277363636364">
Our first set of experiments investigated the ef-
fectiveness of Stage 2 of MINT, namely the min-
ing of NETEs in an IDEAL environment. As
MINT is provided with paired articles in this ex-
periment, all experiments for this environment
were run on test beds created from group S cor-
pora (Table 2).
Results in the IDEAL Environment:
The recall measures for distinct NEs and distinct
NETEs for the IDEAL environment are reported
in Table 3.
</bodyText>
<table confidence="0.91106825">
Test
Bed
Recall (%)
Distinct NEs Distinct NETEs
EK-ST 97.30 95.07
ET-ST 99.11 98.06
EH-ST 98.55 98.66
ER-ST 93.33 85.88
</table>
<tableCaption confidence="0.998915">
Table 3: Recall of MINT in IDEAL
</tableCaption>
<bodyText confidence="0.992161090909091">
Note that in the first 3 language pairs MINT was
able to mine a transliteration equivalent for al-
most all the distinct NEs. The performance in
English-Russian pair was relatively worse, per-
haps due to the noisy training data.
In order to compare the effectiveness of
MINT with a state-of-the-art NETE mining ap-
proach, we implemented the time series based
Co-Ranking algorithm based on (Klementiev and
Roth, 2006).
Table 4 shows the MRR results in the IDEAL
environment both for MINT and the Co-
Ranking baseline: MINT outperformed Co-
Ranking on all the language pairs, despite not
using time series similarity in the mining
process. The high MRRs (@1 and @5) indicate
that in almost all the cases, the top-ranked candi-
date is a correct NETE. Note that Co-Ranking
could not be run on the EH-ST test bed as the
articles did not have a date stamp. Co-Ranking is
crucially dependent on time series and hence re-
quires date stamps for the articles.
</bodyText>
<figure confidence="0.946790555555556">
Test Bed
Comparable
Corpora
Article
Pairs
Distinct
NEs
Distinct
NETEs
</figure>
<table confidence="0.896801615384615">
EK-ST EK-S 200 481 710
ET-ST ET-S 200 449 672
EH-ST EH-S 200 347 373
ER-ST ER-S 100 195 347
Table 2: Test Beds for IDEAL &amp; NEAR-IDEAL
Test
Bed
MRR@1 MRR@5
MINT CoRanking MINT CoRanking
EK-ST 0.94 0.26 0.95 0.29
ET-ST 0.91 0.26 0.94 0.29
EH-ST 0.93 - 0.95 -
ER-ST 0.80 0.38 0.85 0.43
</table>
<tableCaption confidence="0.994713">
Table 4: MINT &amp; Co-Ranking in IDEAL
</tableCaption>
<page confidence="0.998914">
804
</page>
<sectionHeader confidence="0.92411" genericHeader="method">
\x0c5.2 NEAR-IDEAL Environment
</sectionHeader>
<bodyText confidence="0.9975172">
The second set of experiments investigated the
effectiveness of Stage 1 of MINT on comparable
corpora that are constituted by pairs of similar
articles, where the pairing information between
the articles is with-held. MINT reconstructed the
pairings using the cross-language document si-
milarity model and subsequently mined NETEs.
As in previous experiments, we ran our experi-
ments on test beds described in Section 4.4.
Results in the NEAR-IDEAL Environment:
There are two parts to this set of experiments. In
the first part, we investigated the effectiveness of
the cross-language document similarity model
described in Section 3.1. Since we know the
identity of the conjugate article for every article
in the test bed, and articles can be ranked accord-
ing to the cross-language document similarity
score, we simply computed the MRR for the
documents identified in each of the test beds,
considering only the top-2 results. Further, where
available, we made use of the publication date of
articles to restrict the number of target articles
that are considered in lines 4 and 5 of the MINT
algorithm in Figure 2. Table 5 shows the results
for two date windows 3 days and 1 year.
</bodyText>
<table confidence="0.968939142857143">
Test
Bed
MRR@1 MRR@2
3 days 1 year 3 days 1 year
EK-ST 0.99 0.91 0.99 0.93
ET-ST 0.96 0.83 0.97 0.87
EH-ST - 0.81 - 0.82
</table>
<tableCaption confidence="0.997498">
Table 5: MRR of Stage 1 in NEAR-IDEAL
</tableCaption>
<bodyText confidence="0.998791615384615">
Subsequently, the output of the Stage 1 was giv-
en as the input to the Stage 2 of the MINT me-
thod. In Table 6 we report the MRR @1 and @5
for the second stage, for both time windows (3
days &amp; 1 year).
It is interesting to compare the results of MINT
in NEAR-IDEAL data environment (Table 6)
with MINTs results in IDEAL environment
(Table 4). The drop in MRR@1 is small: ~2%
for EK-ST and ~3% for ET-ST. For EH-ST the
drop is relatively more (~12%) as may be ex-
pected since the time window (3 days) could not
be applied for this test bed.
</bodyText>
<subsectionHeader confidence="0.976452">
5.3 REAL Environment
</subsectionHeader>
<bodyText confidence="0.991416083333334">
The third set of experiments investigated the ef-
fectiveness of MINT on large comparable corpo-
ra. We ran the experiments on test beds created
from group L corpora.
Test-beds for the REAL Environment: The
test beds for the REAL environment (Table 7)
consisted of only English articles since we do not
know in advance whether these articles have any
similar articles in the target languages.
Results in the REAL Environment: In real
environment, we examined the top 2 articles of
returned by Stage 1 of MINT, and mined NETEs
from them. We used a date window of 3 in Stage
1. Table 8 summarizes the results for the REAL
environment.
We observe that the performance of MINT is
impressive, considering the fact that the compa-
rable corpora used in the REAL environment is
two orders of magnitude larger than those used in
IDEAL and NEAR-IDEAL environments. This
implies that MINT is able to effectively mine
NETEs whenever the Stage 1 algorithm was able
to find a good conjugate for each of the source
language articles.
</bodyText>
<subsectionHeader confidence="0.639763">
5.4 Generative Transliteration Similarity
Model
</subsectionHeader>
<bodyText confidence="0.9927725">
We employed the extended W-HMM translitera-
tion similarity model in MINT and used it in the
IDEAL data environment. Table 9 shows the
results.
</bodyText>
<table confidence="0.971901285714286">
Test
Bed
MRR@1 MRR@5
3 days 1 year 3 days 1 year
EK-ST 0.92 0.87 0.94 0.90
ET-ST 0.88 0.74 0.91 0.78
EH-ST - 0.82 - 0.87
</table>
<tableCaption confidence="0.9927">
Table 6: MRR of Stage 2 in NEAR-IDEAL
</tableCaption>
<figure confidence="0.890551733333333">
Test
Bed
Comparable
Corpora
Articles
Distinct
NEs
EK-LT EK-L 100 306
ET-LT ET-L 100 228
Table 7: Test Beds for REAL
Test Bed
MRR
@1 @5
EK-LT 0.86 0.88
ET-LT 0.82 0.85
</figure>
<tableCaption confidence="0.732292">
Table 8: MRR of Stage 2 in REAL
</tableCaption>
<table confidence="0.7929275">
Test Bed
MRR
@1 @5
EK-S 0.85 0.86
ET-S 0.81 0.82
EH-S 0.91 0.93
</table>
<tableCaption confidence="0.981514">
Table 9: MRR of Stage 2 in IDEAL using genera-
</tableCaption>
<bodyText confidence="0.581496">
tive transliteration similarity model
</bodyText>
<page confidence="0.977132">
805
</page>
<bodyText confidence="0.999509375">
\x0cWe see that the results for the generative transli-
teration similarity model are good but not as
good as those for the discriminative translitera-
tion similarity model. As we did not stem either
the English NEs or the target language words,
the generative model made more mistakes on
inflected words compared to the discriminative
model.
</bodyText>
<subsectionHeader confidence="0.978556">
5.5 Examples of Mined NETEs
</subsectionHeader>
<bodyText confidence="0.9680795">
Table 10 gives some examples of the NETEs
mined from the comparable news corpora.
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.976720952380952">
CLIR systems have been studied in several
works (Ballesteros and Croft, 1998; Kraiij et al,
2003). The limited coverage of dictionaries has
been recognized as a problem in CLIR and MT
(Demner-Fushman &amp; Oard, 2002; Mandl &amp;
Womser-hacker, 2005; Xu &amp;Weischedel, 2005).
In order to address this problem, different
kinds of approaches have been taken, from learn-
ing transformation rules from dictionaries and
applying the rules to find cross-lingual spelling
variants (Pirkola et al., 2003), to learning trans-
lation lexicon from monolingual and/or compa-
rable corpora (Fung, 1995; Al-Onaizan and
Knight, 2002; Koehn and Knight, 2002; Rapp,
1996). While these works have focused on find-
ing translation equivalents of all class of words,
we focus specifically on transliteration equiva-
lents of NEs. (Munteanu and Marcu, 2006;
Quirk et al., 2007) addresses mining of parallel
sentences and fragments from nearly parallel
sentences. In contrast, our approach mines
NETEs from article pairs that may not even have
any parallel or nearly parallel sentences.
NETE discovery from comparable corpora
using time series and transliteration model was
proposed in (Klementiev and Roth, 2006), and
extended for NETE mining for several languages
in (Saravanan and Kumaran, 2007). However,
such methods miss vast majority of the NETEs
due to their dependency on frequency signatures.
In addition, (Klementiev and Roth, 2006) may
not scale for large corpora, as they examine
every word in the target side as a potential trans-
literation equivalent. NETE mining from compa-
rable corpora using phonetic mappings was pro-
posed in (Tao et al., 2006), but the need for lan-
guage specific knowledge restricts its applicabili-
ty across languages. We proposed the idea of
mining NETEs from multilingual articles with
similar content in (Udupa, et al., 2008). In this
work, we extend the approach and provide a de-
tailed description of the empirical studies.
</bodyText>
<sectionHeader confidence="0.99723" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999596851851852">
In this paper, we showed that MINT, a simple
and intuitive technique employing cross-
language document similarity and transliteration
similarity models, is capable of mining NETEs
effectively from large comparable news corpora.
Our three stage empirical investigation showed
that MINT performed close to optimal on com-
parable corpora consisting of pairs of similar ar-
ticles when the pairings are known in advance.
MINT induced fairly good pairings and performs
exceedingly well even when the pairings are not
known in advance. Further, MINT outperformed
a state-of-the-art baseline and scaled to large
comparable corpora. Finally, we demonstrated
the language neutrality of MINT, by mining
NETEs from 4 language pairs (between English
and one of Russian, Hindi, Kannada or Tamil)
from 3 vastly different linguistic families.
As a future work, we plan to use the ex-
tended W-HMM model to get features for the
discriminative transliteration similarity model.
We also want to use a combination of the cross-
language document similarity score and the
transliteration similarity score for scoring the
NETEs. Finally, we would like to use the mined
NETEs to improve the performance of the first
stage of MINT.
</bodyText>
<sectionHeader confidence="0.952844" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.706314">
We thank Abhijit Bhole for his help and Chris
Quirk for valuable comments.
</bodyText>
<figure confidence="0.998971210526316">
Language
Pair
Source NE Transliteration
English-
Kannada
Woolmer
Kafeel
Baghdad
English-Tamil Lloyd
Mumbai
Manchester
English-Hindi Vanhanen
Trinidad
Ibuprofen
English-
Russian
Kreuzberg
Gaddafi
Karadzic
</figure>
<tableCaption confidence="0.897002">
Table 10: Examples of Mined NETEs
</tableCaption>
<page confidence="0.978821">
806
</page>
<reference confidence="0.99830487">
\x0cReferences
AbdulJaleel, N. and Larkey, L.S. 2003. Statistical translite-
ration for English-Arabic cross language information re-
trieval. Proceedings of CIKM 2003.
Al-Onaizan, Y. and Knight, K. 2002. Translating named
entities using monolingual and bilingual resources. Pro-
ceedings of the 40th Annual Meeting of ACL.
Ballesteros, L. and Croft, B. 1998. Dictionary Methods for
Cross-Lingual Information Retrieval. Proceedings of
DEXA96.
Chen, H., et al. 1998. Proper Name Translation in Cross-
Language Information Retrieval. Proceedings of the 36th
Annual Meeting of the ACL.
Demner-Fushman, D., and Oard, D. W. 2002. The effect of
bilingual term list size on dictionary-based cross-
language information retrieval. Proceedings of the 36th
Hawaii International Conference on System Sciences.
Finkel, J. Trond Grenager, and Christopher Manning. 2005.
Incorporating Non-local Information into Information
Extraction Systems by Gibbs Sampling. Proceedings of
the 43nd Annual Meeting of the ACL.
Fung, P. 1995. Compiling bilingual lexicon entries from a
non-parallel English-Chinese corpus. Proceedings of the
3rd Workshop on Very Large Corpora.
Fung, P. 1995. A pattern matching method for finding noun
and proper noun translations from noisy parallel corpora.
Proceedings of ACL 1995.
He. X. 2007: Using word dependent transition models in
HMM based word alignment for statistical machine
translation. In Proceedings of 2nd
ACL Workshop on Sta-
tistical Machine Translation .
Hermjakob, U., Knight, K., and Daume, H. 2008. Name
translation in statistical machine translation: knowing
when to transliterate. Proceedings ACL 2008.
Klementiev, A. and Roth, D. 2006. Weakly supervised
named entity transliteration and discovery from multilin-
gual comparable corpora. Proceedings of the 44th
Annual
Meeting of the ACL.
Knight, K. and Graehl, J. 1998. Machine Transliteration.
Computational Linguistics.
Koehn, P. and Knight, K. 2002. Learning a translation lex-
icon from monolingual corpora. Proceedings of Unsu-
pervised Lexical Acquisition.
Kraiij, W., Nie, J-Y. and Simard, M. 2003. Emebdding
Web-based Statistical Translation Models in Cross-
Language Information Retrieval. Computational Linguis-
tics., 29(3):381-419.
Mandl, T., and Womser-Hacker, C. 2004. How do named
entities contribute to retrieval effectiveness? Proceedings
of the 2004 Cross Language Evaluation Forum Cam-
paign 2004.
Mandl, T., and Womser-Hacker, C. 2005. The Effect of
named entities on effectiveness in cross-language infor-
mation retrieval evaluation. ACM Symposium on Applied
Computing.
Munteanu, D. and Marcu D. 2006. Extracting parallel sub-
sentential fragments from non-parallel corpora. Proceed-
ings of the ACL 2006.
Och, F. and Ney, H. 2003. A systematic comparison of var-
ious statistical alignment models. Computational Lin-
guistics.
Pirkola, A., Toivonen, J., Keskustalo, H., Visala, K. and
Jarvelin, K. 2003. Fuzzy translation of cross-lingual
spelling variants. Proceedings of SIGIR 2003.
Ponte, J. M. and Croft, B. 1998. A Language Modeling
Approach to Information Retrieval. Proceedings of ACM
SIGIR 1998.
Quirk, C., Udupa, R. and Menezes, A. 2007. Generative
models of noisy translations with applications to parallel
fragments extraction. Proceedings of the 11th
MT Sum-
mit.
Rapp, R. 1996. Automatic identification of word transla-
tions from unrelated English and German corpora. Pro-
ceedings of ACL99
Saravanan, K. and Kumaran, A. 2007. Some experiments in
mining named entity transliteration pairs from compara-
ble corpora. Proceedings of the 2nd
International Work-
shop on Cross Lingual Information Access.
Tao, T., Yoon, S., Fister, A., Sproat, R. and Zhai, C. 2006.
Unsupervised named entity transliteration using temporal
and phonetic correlation. Proceedings of EMNLP 2006.
Udupa, R., Saravanan, K., Kumaran, A. and Jagarlamudi, J.
2008. Mining Named Entity Transliteration Equivalents
from Comparable Corpora. Proceedings of the CIKM
2008.
Udupa, R., Saravanan, K., Bakalov, A. and Bhole, A. 2009.
They are out there if you know where to look: Mining
transliterations of OOV terms in cross-language informa-
tion retrieval. Proceedings of the ECIR 2009.
Virga, P. and Khudanpur, S. 2003. Transliteration of proper
names in cross-lingual information retrieval. Proceedings
of the ACL Workshop on Multilingual and Mixed Lan-
guage Named Entity Recognition.
Xu, J. and Weischedel, R. 2005. Empirical studies on the
impact of lexical resources on CLIR performance. In-
formation Processing and Management.
</reference>
<page confidence="0.963283">
807
</page>
<figure confidence="0.260828">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864420">
<note confidence="0.967394">b&amp;apos;Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799807, Athens, Greece, 30 March 3 April 2009. c 2009 Association for Computational Linguistics</note>
<title confidence="0.987953">MINT: A Method for Effective and Scalable Mining of Named Entity Transliterations from Large Comparable Corpora</title>
<author confidence="0.983052">Raghavendra Udupa K Saravanan A Kumaran Jagadeesh Jagarlamudi</author>
<affiliation confidence="0.999364">Microsoft Research India</affiliation>
<address confidence="0.984739">Bangalore 560080 INDIA</address>
<email confidence="0.999827">[raghavu,v-sarak,kumarana,jags}@microsoft.com</email>
<abstract confidence="0.998896588235294">In this paper, we address the problem of mining transliterations of Named Entities (NEs) from large comparable corpora. We leverage the empirical fact that multilingual news articles with similar news content are rich in Named Entity Transliteration Equivalents (NETEs). Our mining algorithm, MINT, uses a cross-language document similarity model to align multilingual news articles and then mines NETEs from the aligned articles using a transliteration similarity model. We show that our approach is highly effective on 6 different comparable corpora between English and 4 languages from 3 different language families. Furthermore, it performs substantially better than a state-of-the-art competitor.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences AbdulJaleel</author>
<author>N</author>
<author>L S Larkey</author>
</authors>
<title>Statistical transliteration for English-Arabic cross language information retrieval.</title>
<date>2003</date>
<booktitle>Proceedings of CIKM</booktitle>
<marker>AbdulJaleel, N, Larkey, 2003</marker>
<rawString>\x0cReferences AbdulJaleel, N. and Larkey, L.S. 2003. Statistical transliteration for English-Arabic cross language information retrieval. Proceedings of CIKM 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Al-Onaizan</author>
<author>K Knight</author>
</authors>
<title>Translating named entities using monolingual and bilingual resources.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of ACL.</booktitle>
<contexts>
<context position="1727" citStr="Al-Onaizan and Knight, 2002" startWordPosition="239" endWordPosition="242">bstantially better than a state-of-the-art competitor. 1 Introduction Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks. In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005). Traditional methods for transliterations have not proven to be very effective in CLIR. Machine Transliteration systems (AbdulJaleel and Larkey, 2003; Al-Onaizan and Knight, 2002; Virga and Khudanpur, 2003) usually produce incorrect transliterations and translation lexcions such as hand-crafted or statistical dictionaries are too static to have good coverage of NEs1 occurring in the current news events. Hence, there is a critical need for creating and continually updat* Currently with University of Utah. 1 New NEs are introduced to the vocabulary of a language every day. On an average, 260 and 452 new NEs appeared daily in the XIE and AFE segments of the LDC English Gigaword corpora respectively. ing multilingual Named Entity transliteration lexicons. The ubiquitous a</context>
<context position="27768" citStr="Al-Onaizan and Knight, 2002" startWordPosition="4743" endWordPosition="4746"> corpora. 6 Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Al-Onaizan, Y. and Knight, K. 2002. Translating named entities using monolingual and bilingual resources. Proceedings of the 40th Annual Meeting of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ballesteros</author>
<author>B Croft</author>
</authors>
<title>Dictionary Methods for Cross-Lingual Information Retrieval.</title>
<date>1998</date>
<booktitle>Proceedings of DEXA96.</booktitle>
<contexts>
<context position="17725" citStr="Ballesteros and Croft, 1998" startWordPosition="3022" endWordPosition="3025"> sizes: 11K sentence pairs in English-Kannada, 54K in English-Hindi, and 14K in English-Tamil) using the GIZA++ statistical alignment tool (Och et al., 2003), with 5 iterations each of IBM Model 1 and HMM. We did not have access to an English-Russian parallel corpus and hence could not generate a dictionary for this language pair. Hence, the NEAR-IDEAL experiments were not run for the English-Russian language pair. Although the coverage of the dictionaries was low, this turned out to be not a serious issue for our cross-language document similarity model as it might have for topic based CLIR (Ballesteros and Croft, 1998). Unlike CLIR, where the query is typically smaller in length compared to the documents, in our case we are dealing with news articles of comparable size in both source and target languages. When many translations were available for a source word, we considered only the top-4 translations. Further, we smoothed the document probability distributions with collection frequency as described in (Ponte and Croft, 1998). 4.3 Transliteration Similarity Model The transliteration similarity models for each of the 4 language pairs were produced by learning over a training corpus consisting of about 16,00</context>
<context position="27242" citStr="Ballesteros and Croft, 1998" startWordPosition="4663" endWordPosition="4666">able 9: MRR of Stage 2 in IDEAL using generative transliteration similarity model 805 \x0cWe see that the results for the generative transliteration similarity model are good but not as good as those for the discriminative transliteration similarity model. As we did not stem either the English NEs or the target language words, the generative model made more mistakes on inflected words compared to the discriminative model. 5.5 Examples of Mined NETEs Table 10 gives some examples of the NETEs mined from the comparable news corpora. 6 Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on f</context>
</contexts>
<marker>Ballesteros, Croft, 1998</marker>
<rawString>Ballesteros, L. and Croft, B. 1998. Dictionary Methods for Cross-Lingual Information Retrieval. Proceedings of DEXA96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
</authors>
<title>Proper Name Translation in CrossLanguage Information Retrieval.</title>
<date>1998</date>
<booktitle>Proceedings of the 36th Annual Meeting of the ACL.</booktitle>
<marker>Chen, 1998</marker>
<rawString>Chen, H., et al. 1998. Proper Name Translation in CrossLanguage Information Retrieval. Proceedings of the 36th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Demner-Fushman</author>
<author>D W Oard</author>
</authors>
<title>The effect of bilingual term list size on dictionary-based crosslanguage information retrieval.</title>
<date>2002</date>
<booktitle>Proceedings of the 36th Hawaii International Conference on System Sciences.</booktitle>
<contexts>
<context position="27378" citStr="Demner-Fushman &amp; Oard, 2002" startWordPosition="4686" endWordPosition="4689">nsliteration similarity model are good but not as good as those for the discriminative transliteration similarity model. As we did not stem either the English NEs or the target language words, the generative model made more mistakes on inflected words compared to the discriminative model. 5.5 Examples of Mined NETEs Table 10 gives some examples of the NETEs mined from the comparable news corpora. 6 Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, </context>
</contexts>
<marker>Demner-Fushman, Oard, 2002</marker>
<rawString>Demner-Fushman, D., and Oard, D. W. 2002. The effect of bilingual term list size on dictionary-based crosslanguage information retrieval. Proceedings of the 36th Hawaii International Conference on System Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Trond Grenager Finkel</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>Proceedings of the 43nd Annual Meeting of the ACL.</booktitle>
<marker>Finkel, Manning, 2005</marker>
<rawString>Finkel, J. Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus.</title>
<date>1995</date>
<booktitle>Proceedings of the 3rd Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="27739" citStr="Fung, 1995" startWordPosition="4741" endWordPosition="4742">parable news corpora. 6 Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006),</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Fung, P. 1995. Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus. Proceedings of the 3rd Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>A pattern matching method for finding noun and proper noun translations from noisy parallel corpora.</title>
<date>1995</date>
<contexts>
<context position="27739" citStr="Fung, 1995" startWordPosition="4741" endWordPosition="4742">parable news corpora. 6 Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006),</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Fung, P. 1995. A pattern matching method for finding noun and proper noun translations from noisy parallel corpora.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of ACL</booktitle>
<marker>1995</marker>
<rawString>Proceedings of ACL 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X</author>
</authors>
<title>Using word dependent transition models in HMM based word alignment for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of 2nd ACL Workshop on Statistical Machine Translation .</booktitle>
<marker>X, 2007</marker>
<rawString>He. X. 2007: Using word dependent transition models in HMM based word alignment for statistical machine translation. In Proceedings of 2nd ACL Workshop on Statistical Machine Translation .</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hermjakob</author>
<author>K Knight</author>
<author>H Daume</author>
</authors>
<title>Name translation in statistical machine translation: knowing when to transliterate.</title>
<date>2008</date>
<booktitle>Proceedings ACL</booktitle>
<marker>Hermjakob, Knight, Daume, 2008</marker>
<rawString>Hermjakob, U., Knight, K., and Daume, H. 2008. Name translation in statistical machine translation: knowing when to transliterate. Proceedings ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Klementiev</author>
<author>D Roth</author>
</authors>
<title>Weakly supervised named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>Proceedings of the 44th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2676" citStr="Klementiev and Roth, 2006" startWordPosition="385" endWordPosition="388">f Utah. 1 New NEs are introduced to the vocabulary of a language every day. On an average, 260 and 452 new NEs appeared daily in the XIE and AFE segments of the LDC English Gigaword corpora respectively. ing multilingual Named Entity transliteration lexicons. The ubiquitous availability of comparable news corpora in multiple languages suggests a promising alternative to Machine Transliteration, namely, the mining of Named Entity Transliteration Equivalents (NETEs) from such corpora. News stories are typically rich in NEs and therefore, comparable news corpora can be expected to contain NETEs (Klementiev and Roth, 2006; Tao et al., 2006). The large quantity and the perpetual availability of news corpora in many of the worlds languages, make mining of NETEs a viable alternative to traditional approaches. It is this opportunity that we address in our work. In this paper, we detail an effective and scalable mining method, called MINT (MIning Named-entity Transliteration equivalents), for mining of NETEs from large comparable corpora. MINT addresses several challenges in mining NETEs from large comparable corpora: exhaustiveness (in mining sparse NETEs), computational efficiency (in scaling on corpora size), la</context>
<context position="4162" citStr="Klementiev and Roth, 2006" startWordPosition="614" endWordPosition="617">uages with reasonably similar content are rich sources of NETEs (Udupa, et al., 2008). We demonstrate that the above insight can be translated into an effective approach for mining NETEs from large comparable corpora even when similar articles are not known a priori. We demonstrate MINTs effectiveness on 4 language pairs involving 5 languages (English, Hindi, Kannada, Russian, and Tamil) from 3 different language families, and its scalability on corpora of vastly different sizes (2,000 to 200,000 articles). We show that MINTs performance is significantly better than a state of the art method (Klementiev and Roth, 2006). 799 \x0cWe discuss the motivation behind our approach in Section 2 and present the details in Section 3. In Section 4, we describe the evaluation process and in Section 5, we present the results and analysis. We discuss related work in Section 6. 2 Motivation MINT is based on the hypothesis that news articles in different languages with similar content contain highly overlapping set of NEs. News articles are typically rich in NEs as news is about events involving people, locations, organizations, etc2 . It is reasonable to expect that multilingual news articles reporting the same news event </context>
<context position="22139" citStr="Klementiev and Roth, 2006" startWordPosition="3755" endWordPosition="3758">nct NETEs for the IDEAL environment are reported in Table 3. Test Bed Recall (%) Distinct NEs Distinct NETEs EK-ST 97.30 95.07 ET-ST 99.11 98.06 EH-ST 98.55 98.66 ER-ST 93.33 85.88 Table 3: Recall of MINT in IDEAL Note that in the first 3 language pairs MINT was able to mine a transliteration equivalent for almost all the distinct NEs. The performance in English-Russian pair was relatively worse, perhaps due to the noisy training data. In order to compare the effectiveness of MINT with a state-of-the-art NETE mining approach, we implemented the time series based Co-Ranking algorithm based on (Klementiev and Roth, 2006). Table 4 shows the MRR results in the IDEAL environment both for MINT and the CoRanking baseline: MINT outperformed CoRanking on all the language pairs, despite not using time series similarity in the mining process. The high MRRs (@1 and @5) indicate that in almost all the cases, the top-ranked candidate is a correct NETE. Note that Co-Ranking could not be run on the EH-ST test bed as the articles did not have a date stamp. Co-Ranking is crucially dependent on time series and hence requires date stamps for the articles. Test Bed Comparable Corpora Article Pairs Distinct NEs Distinct NETEs EK</context>
<context position="28338" citStr="Klementiev and Roth, 2006" startWordPosition="4830" endWordPosition="4833">parable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007). However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures. In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent. NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages. We proposed the idea of mining NETEs from multilingual artic</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Klementiev, A. and Roth, D. 2006. Weakly supervised named entity transliteration and discovery from multilingual comparable corpora. Proceedings of the 44th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<title>Machine Transliteration. Computational Linguistics.</title>
<date>1998</date>
<marker>Knight, Graehl, 1998</marker>
<rawString>Knight, K. and Graehl, J. 1998. Machine Transliteration. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>Proceedings of Unsupervised Lexical Acquisition.</booktitle>
<contexts>
<context position="27792" citStr="Koehn and Knight, 2002" startWordPosition="4747" endWordPosition="4750"> systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages i</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Koehn, P. and Knight, K. 2002. Learning a translation lexicon from monolingual corpora. Proceedings of Unsupervised Lexical Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kraiij</author>
<author>J-Y Nie</author>
<author>M Simard</author>
</authors>
<title>Emebdding Web-based Statistical Translation Models in CrossLanguage Information Retrieval.</title>
<date>2003</date>
<journal>Computational Linguistics.,</journal>
<pages>29--3</pages>
<contexts>
<context position="27263" citStr="Kraiij et al, 2003" startWordPosition="4667" endWordPosition="4670">AL using generative transliteration similarity model 805 \x0cWe see that the results for the generative transliteration similarity model are good but not as good as those for the discriminative transliteration similarity model. As we did not stem either the English NEs or the target language words, the generative model made more mistakes on inflected words compared to the discriminative model. 5.5 Examples of Mined NETEs Table 10 gives some examples of the NETEs mined from the comparable news corpora. 6 Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation eq</context>
</contexts>
<marker>Kraiij, Nie, Simard, 2003</marker>
<rawString>Kraiij, W., Nie, J-Y. and Simard, M. 2003. Emebdding Web-based Statistical Translation Models in CrossLanguage Information Retrieval. Computational Linguistics., 29(3):381-419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mandl</author>
<author>C Womser-Hacker</author>
</authors>
<title>How do named entities contribute to retrieval effectiveness?</title>
<date>2004</date>
<booktitle>Proceedings of the</booktitle>
<marker>Mandl, Womser-Hacker, 2004</marker>
<rawString>Mandl, T., and Womser-Hacker, C. 2004. How do named entities contribute to retrieval effectiveness? Proceedings of the 2004 Cross Language Evaluation Forum Campaign 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mandl</author>
<author>C Womser-Hacker</author>
</authors>
<title>The Effect of named entities on effectiveness in cross-language information retrieval evaluation.</title>
<date>2005</date>
<journal>ACM Symposium on Applied Computing.</journal>
<contexts>
<context position="1522" citStr="Mandl and Womser-Hacker, 2005" startWordPosition="211" endWordPosition="214">ansliteration similarity model. We show that our approach is highly effective on 6 different comparable corpora between English and 4 languages from 3 different language families. Furthermore, it performs substantially better than a state-of-the-art competitor. 1 Introduction Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks. In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005). Traditional methods for transliterations have not proven to be very effective in CLIR. Machine Transliteration systems (AbdulJaleel and Larkey, 2003; Al-Onaizan and Knight, 2002; Virga and Khudanpur, 2003) usually produce incorrect transliterations and translation lexcions such as hand-crafted or statistical dictionaries are too static to have good coverage of NEs1 occurring in the current news events. Hence, there is a critical need for creating and continually updat* Currently with University of Utah. 1 New NEs are introduced to the vocabulary of a language every </context>
</contexts>
<marker>Mandl, Womser-Hacker, 2005</marker>
<rawString>Mandl, T., and Womser-Hacker, C. 2005. The Effect of named entities on effectiveness in cross-language information retrieval evaluation. ACM Symposium on Applied Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Munteanu</author>
<author>D Marcu</author>
</authors>
<title>Extracting parallel subsentential fragments from non-parallel corpora.</title>
<date>2006</date>
<booktitle>Proceedings of the ACL</booktitle>
<contexts>
<context position="27982" citStr="Munteanu and Marcu, 2006" startWordPosition="4777" endWordPosition="4780">Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007). However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures. In addition, (Klementiev and Roth, 2006) may not scale</context>
</contexts>
<marker>Munteanu, Marcu, 2006</marker>
<rawString>Munteanu, D. and Marcu D. 2006. Extracting parallel subsentential fragments from non-parallel corpora. Proceedings of the ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models. Computational Linguistics.</title>
<date>2003</date>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. and Ney, H. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pirkola</author>
<author>J Toivonen</author>
<author>H Keskustalo</author>
<author>K Visala</author>
<author>K Jarvelin</author>
</authors>
<title>Fuzzy translation of cross-lingual spelling variants.</title>
<date>2003</date>
<booktitle>Proceedings of SIGIR</booktitle>
<contexts>
<context position="27651" citStr="Pirkola et al., 2003" startWordPosition="4726" endWordPosition="4729">ive model. 5.5 Examples of Mined NETEs Table 10 gives some examples of the NETEs mined from the comparable news corpora. 6 Related Work CLIR systems have been studied in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora </context>
</contexts>
<marker>Pirkola, Toivonen, Keskustalo, Visala, Jarvelin, 2003</marker>
<rawString>Pirkola, A., Toivonen, J., Keskustalo, H., Visala, K. and Jarvelin, K. 2003. Fuzzy translation of cross-lingual spelling variants. Proceedings of SIGIR 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Ponte</author>
<author>B Croft</author>
</authors>
<title>A Language Modeling Approach to Information Retrieval.</title>
<date>1998</date>
<booktitle>Proceedings of ACM SIGIR</booktitle>
<contexts>
<context position="18141" citStr="Ponte and Croft, 1998" startWordPosition="3088" endWordPosition="3091">hough the coverage of the dictionaries was low, this turned out to be not a serious issue for our cross-language document similarity model as it might have for topic based CLIR (Ballesteros and Croft, 1998). Unlike CLIR, where the query is typically smaller in length compared to the documents, in our case we are dealing with news articles of comparable size in both source and target languages. When many translations were available for a source word, we considered only the top-4 translations. Further, we smoothed the document probability distributions with collection frequency as described in (Ponte and Croft, 1998). 4.3 Transliteration Similarity Model The transliteration similarity models for each of the 4 language pairs were produced by learning over a training corpus consisting of about 16,000 single word NETEs, in each pair of languages. The training corpus in English-Hindi, EnglishKannada and English-Tamil were hand-crafted by professionals, the English-Russian name pairs were culled from Wikipedia interwiki links and were cleaned heuristically. Equal number of negative samples was used for training the models. To produce the negative samples, we paired each source language NE with a random nonmatc</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Ponte, J. M. and Croft, B. 1998. A Language Modeling Approach to Information Retrieval. Proceedings of ACM SIGIR 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>R Udupa</author>
<author>A Menezes</author>
</authors>
<title>Generative models of noisy translations with applications to parallel fragments extraction.</title>
<date>2007</date>
<booktitle>Proceedings of the 11th MT Summit.</booktitle>
<contexts>
<context position="28003" citStr="Quirk et al., 2007" startWordPosition="4781" endWordPosition="4784">l &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007). However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures. In addition, (Klementiev and Roth, 2006) may not scale for large corpora, a</context>
</contexts>
<marker>Quirk, Udupa, Menezes, 2007</marker>
<rawString>Quirk, C., Udupa, R. and Menezes, A. 2007. Generative models of noisy translations with applications to parallel fragments extraction. Proceedings of the 11th MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1996</date>
<booktitle>Proceedings of ACL99 Saravanan,</booktitle>
<contexts>
<context position="27805" citStr="Rapp, 1996" startWordPosition="4751" endWordPosition="4752">ed in several works (Ballesteros and Croft, 1998; Kraiij et al, 2003). The limited coverage of dictionaries has been recognized as a problem in CLIR and MT (Demner-Fushman &amp; Oard, 2002; Mandl &amp; Womser-hacker, 2005; Xu &amp;Weischedel, 2005). In order to address this problem, different kinds of approaches have been taken, from learning transformation rules from dictionaries and applying the rules to find cross-lingual spelling variants (Pirkola et al., 2003), to learning translation lexicon from monolingual and/or comparable corpora (Fung, 1995; Al-Onaizan and Knight, 2002; Koehn and Knight, 2002; Rapp, 1996). While these works have focused on finding translation equivalents of all class of words, we focus specifically on transliteration equivalents of NEs. (Munteanu and Marcu, 2006; Quirk et al., 2007) addresses mining of parallel sentences and fragments from nearly parallel sentences. In contrast, our approach mines NETEs from article pairs that may not even have any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan </context>
</contexts>
<marker>Rapp, 1996</marker>
<rawString>Rapp, R. 1996. Automatic identification of word translations from unrelated English and German corpora. Proceedings of ACL99 Saravanan, K. and Kumaran, A. 2007. Some experiments in mining named entity transliteration pairs from comparable corpora. Proceedings of the 2nd International Workshop on Cross Lingual Information Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tao</author>
<author>S Yoon</author>
<author>A Fister</author>
<author>R Sproat</author>
<author>C Zhai</author>
</authors>
<date>2006</date>
<contexts>
<context position="2695" citStr="Tao et al., 2006" startWordPosition="389" endWordPosition="392">duced to the vocabulary of a language every day. On an average, 260 and 452 new NEs appeared daily in the XIE and AFE segments of the LDC English Gigaword corpora respectively. ing multilingual Named Entity transliteration lexicons. The ubiquitous availability of comparable news corpora in multiple languages suggests a promising alternative to Machine Transliteration, namely, the mining of Named Entity Transliteration Equivalents (NETEs) from such corpora. News stories are typically rich in NEs and therefore, comparable news corpora can be expected to contain NETEs (Klementiev and Roth, 2006; Tao et al., 2006). The large quantity and the perpetual availability of news corpora in many of the worlds languages, make mining of NETEs a viable alternative to traditional approaches. It is this opportunity that we address in our work. In this paper, we detail an effective and scalable mining method, called MINT (MIning Named-entity Transliteration equivalents), for mining of NETEs from large comparable corpora. MINT addresses several challenges in mining NETEs from large comparable corpora: exhaustiveness (in mining sparse NETEs), computational efficiency (in scaling on corpora size), language independence</context>
<context position="28785" citStr="Tao et al., 2006" startWordPosition="4903" endWordPosition="4906"> any parallel or nearly parallel sentences. NETE discovery from comparable corpora using time series and transliteration model was proposed in (Klementiev and Roth, 2006), and extended for NETE mining for several languages in (Saravanan and Kumaran, 2007). However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures. In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent. NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages. We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008). In this work, we extend the approach and provide a detailed description of the empirical studies. 7 Conclusion In this paper, we showed that MINT, a simple and intuitive technique employing crosslanguage document similarity and transliteration similarity models, is capable of mining NETEs effectively from large comparable news corpora. Our three stage empirical investigation showed that MINT p</context>
</contexts>
<marker>Tao, Yoon, Fister, Sproat, Zhai, 2006</marker>
<rawString>Tao, T., Yoon, S., Fister, A., Sproat, R. and Zhai, C. 2006.</rawString>
</citation>
<citation valid="true">
<title>Unsupervised named entity transliteration using temporal and phonetic correlation.</title>
<date>2006</date>
<booktitle>Proceedings of EMNLP</booktitle>
<marker>2006</marker>
<rawString>Unsupervised named entity transliteration using temporal and phonetic correlation. Proceedings of EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>K Saravanan</author>
<author>A Kumaran</author>
<author>J Jagarlamudi</author>
</authors>
<title>Mining Named Entity Transliteration Equivalents from Comparable Corpora.</title>
<date>2008</date>
<booktitle>Proceedings of the CIKM</booktitle>
<contexts>
<context position="3621" citStr="Udupa, et al., 2008" startWordPosition="528" endWordPosition="531">g Named-entity Transliteration equivalents), for mining of NETEs from large comparable corpora. MINT addresses several challenges in mining NETEs from large comparable corpora: exhaustiveness (in mining sparse NETEs), computational efficiency (in scaling on corpora size), language independence (in being applicable to many language pairs) and linguistic frugality (in requiring minimal external linguistic resources). Our contributions are as follows: We give empirical evidence for the hypothesis that news articles in different languages with reasonably similar content are rich sources of NETEs (Udupa, et al., 2008). We demonstrate that the above insight can be translated into an effective approach for mining NETEs from large comparable corpora even when similar articles are not known a priori. We demonstrate MINTs effectiveness on 4 language pairs involving 5 languages (English, Hindi, Kannada, Russian, and Tamil) from 3 different language families, and its scalability on corpora of vastly different sizes (2,000 to 200,000 articles). We show that MINTs performance is significantly better than a state of the art method (Klementiev and Roth, 2006). 799 \x0cWe discuss the motivation behind our approach in </context>
<context position="28987" citStr="Udupa, et al., 2008" startWordPosition="4935" endWordPosition="4938">g for several languages in (Saravanan and Kumaran, 2007). However, such methods miss vast majority of the NETEs due to their dependency on frequency signatures. In addition, (Klementiev and Roth, 2006) may not scale for large corpora, as they examine every word in the target side as a potential transliteration equivalent. NETE mining from comparable corpora using phonetic mappings was proposed in (Tao et al., 2006), but the need for language specific knowledge restricts its applicability across languages. We proposed the idea of mining NETEs from multilingual articles with similar content in (Udupa, et al., 2008). In this work, we extend the approach and provide a detailed description of the empirical studies. 7 Conclusion In this paper, we showed that MINT, a simple and intuitive technique employing crosslanguage document similarity and transliteration similarity models, is capable of mining NETEs effectively from large comparable news corpora. Our three stage empirical investigation showed that MINT performed close to optimal on comparable corpora consisting of pairs of similar articles when the pairings are known in advance. MINT induced fairly good pairings and performs exceedingly well even when </context>
</contexts>
<marker>Udupa, Saravanan, Kumaran, Jagarlamudi, 2008</marker>
<rawString>Udupa, R., Saravanan, K., Kumaran, A. and Jagarlamudi, J. 2008. Mining Named Entity Transliteration Equivalents from Comparable Corpora. Proceedings of the CIKM 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Udupa</author>
<author>K Saravanan</author>
<author>A Bakalov</author>
<author>A Bhole</author>
</authors>
<date>2009</date>
<contexts>
<context position="11899" citStr="Udupa et al., 2009" startWordPosition="2010" endWordPosition="2013">he source and target language strings. Pairs of source string n-grams and target string n-grams such that difference in the start positions of the source and target ngrams is at most 2. Here n 2 , 1 . Difference in the lengths of the two strings. Generative Transliteration Similarity Model: We also experimented with an extension of Hes W-HMM model (He, 2007). The transition probability depends on both the jump width and the previous source character as in the W-HMM model. The emission probability depends on the current source character and the previous target character unlike the W-HMM model (Udupa et al., 2009). Instead of using any single alignment of characters in the pair (wS, wT), we marginalize over all possible alignments: 1 1 1 1 1 , | , | |1 j a j a j j A m j n m t s t p s a a p s t P j j Here, j t (and resp. i s ) denotes the jth (and resp. ith ) character in wT (and resp. wS) and m a A 1 is the hidden alignment between wT and wS where j t is aligned to j a s , ,m , j 1 . We estimate the parameters of the model using the EM algorithm. The transliteration similarity score of a pair (wS, wT) is log P(wT |wS) appropriately transformed. 4 Experimental Setup Our empirical investigation consists </context>
</contexts>
<marker>Udupa, Saravanan, Bakalov, Bhole, 2009</marker>
<rawString>Udupa, R., Saravanan, K., Bakalov, A. and Bhole, A. 2009.</rawString>
</citation>
<citation valid="true">
<title>They are out there if you know where to look: Mining transliterations of OOV terms in cross-language information retrieval.</title>
<date>2009</date>
<booktitle>Proceedings of the ECIR</booktitle>
<marker>2009</marker>
<rawString>They are out there if you know where to look: Mining transliterations of OOV terms in cross-language information retrieval. Proceedings of the ECIR 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Virga</author>
<author>S Khudanpur</author>
</authors>
<title>Transliteration of proper names in cross-lingual information retrieval.</title>
<date>2003</date>
<booktitle>Proceedings of the ACL Workshop on Multilingual and Mixed Language Named Entity Recognition.</booktitle>
<contexts>
<context position="1755" citStr="Virga and Khudanpur, 2003" startWordPosition="243" endWordPosition="246">te-of-the-art competitor. 1 Introduction Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks. In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005). Traditional methods for transliterations have not proven to be very effective in CLIR. Machine Transliteration systems (AbdulJaleel and Larkey, 2003; Al-Onaizan and Knight, 2002; Virga and Khudanpur, 2003) usually produce incorrect transliterations and translation lexcions such as hand-crafted or statistical dictionaries are too static to have good coverage of NEs1 occurring in the current news events. Hence, there is a critical need for creating and continually updat* Currently with University of Utah. 1 New NEs are introduced to the vocabulary of a language every day. On an average, 260 and 452 new NEs appeared daily in the XIE and AFE segments of the LDC English Gigaword corpora respectively. ing multilingual Named Entity transliteration lexicons. The ubiquitous availability of comparable ne</context>
</contexts>
<marker>Virga, Khudanpur, 2003</marker>
<rawString>Virga, P. and Khudanpur, S. 2003. Transliteration of proper names in cross-lingual information retrieval. Proceedings of the ACL Workshop on Multilingual and Mixed Language Named Entity Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>R Weischedel</author>
</authors>
<title>Empirical studies on the impact of lexical resources on CLIR performance. Information Processing and Management.</title>
<date>2005</date>
<contexts>
<context position="1548" citStr="Xu and Weischedel, 2005" startWordPosition="215" endWordPosition="218"> We show that our approach is highly effective on 6 different comparable corpora between English and 4 languages from 3 different language families. Furthermore, it performs substantially better than a state-of-the-art competitor. 1 Introduction Named Entities (NEs) play a critical role in many Natural Language Processing and Information Retrieval (IR) tasks. In Cross-Language Information Retrieval (CLIR) systems, they play an even more important role as the accuracy of their transliterations is shown to correlate highly with the performance of the CLIR systems (Mandl and Womser-Hacker, 2005, Xu and Weischedel, 2005). Traditional methods for transliterations have not proven to be very effective in CLIR. Machine Transliteration systems (AbdulJaleel and Larkey, 2003; Al-Onaizan and Knight, 2002; Virga and Khudanpur, 2003) usually produce incorrect transliterations and translation lexcions such as hand-crafted or statistical dictionaries are too static to have good coverage of NEs1 occurring in the current news events. Hence, there is a critical need for creating and continually updat* Currently with University of Utah. 1 New NEs are introduced to the vocabulary of a language every day. On an average, 260 an</context>
</contexts>
<marker>Xu, Weischedel, 2005</marker>
<rawString>Xu, J. and Weischedel, R. 2005. Empirical studies on the impact of lexical resources on CLIR performance. Information Processing and Management.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>