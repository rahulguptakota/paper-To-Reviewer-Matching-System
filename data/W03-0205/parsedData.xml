<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.258839666666667">
b&apos;A Comparison of Tutor and Student Behavior in Speech Versus Text Based
Tutoring
Carolyn P. Rose, Diane Litman, Dumisizwe Bhembe, Kate Forbes, Scott Silliman,
</title>
<author confidence="0.741748">
Ramesh Srivastava, Kurt VanLehn
</author>
<affiliation confidence="0.576508">
Learning Research and Development Center, University of Pittsburgh,
</affiliation>
<address confidence="0.885716">
3939 OHara St., Pittsburgh, PA 15260
</address>
<email confidence="0.9921785">
rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edu
litman,vanlehn@cs.pitt.edu
</email>
<sectionHeader confidence="0.990597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99971464">
This paper describes preliminary work in ex-
ploring the relative effectiveness of speech ver-
sus text based tutoring. Most current tuto-
rial dialogue systems are text based (Evens et
al., 2001; Rose and Aleven, 2002; Zinn et
al., 2002; Aleven et al., 2001; VanLehn et al.,
2002). However, prior studies have shown con-
siderable benefits of tutoring through spoken
interactions (Lemke, 1990; Chi et al., 1994;
Hausmann and Chi, 2002). Thus, we are cur-
rently developing a speech based dialogue sys-
tem that uses a text based system for tutoring
conceptual physics (VanLehn et al., 2002) as
its back-end. In order to explore the relative
effectiveness between these two input modal-
ities in our task domain, we have started by
collecting parallel human-human tutoring cor-
pora both for text based and speech based tu-
toring. In both cases, students interact with the
tutor through a web interface. We present here
a comparison between the two on a number
of features of dialogue that have been demon-
strated to correlate reliably with learning gains
with students interacting with the tutor using
the text based interface (Rose et al., submitted).
</bodyText>
<sectionHeader confidence="0.998338" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998837454545455">
This paper describes preliminary work in exploring the
relative effectiveness of speech versus text based tuto-
rial dialogue systems. Tutorial dialogue is a natural way
to provide students with a learning environment that ex-
hibits characteristics that have been shown to correlate
with student learning gains, such as student activity. For
example, it has been demonstrated that generating words
rather than simply reading them promotes subsequent re-
call of those words (Slamecka and Graf, 1978). (Chi
et al., 1994) notes that there is a general momentum in
the science education literature toward the importance
of talking, reflecting and explaining as ways to learn
(Lemke, 1990). Moreover, encouraging student self-
explanation, which includes both generating inferences
from material they have read and relating new material to
old material, has been shown to correlate with learning
(Chi et al., 1981; Chi et al., 1994; Renkl, 1997; Press-
ley et al., 1992). In a further study, prompting students
with zero content prompts to encourage them to self-
explain was also associated with student learning (Chi et
al., 2001). A second important advantage to dialogue is
that it affords the tutor the opportunity to tailor instruc-
tion to the needs of the student. While human tutors may
not always choose to tailor their instruction to the indi-
vidual characteristics of the knowledge state of their stu-
dents, tutors who ignore signs of student confusion may
run the risk of preventing learning (Chi, 1996). (Rose et
al., submitted) explore the benefits of tutor adaptation by
comparing learning gains for naive learners and review
learners in a human tutoring condition and a non-adaptive
reading condition.
In recent years tutorial dialogue systems have be-
come more and more prevalent, most of which are text
based (Evens et al., 2001; Rose and Aleven, 2002; Zinn
et al., 2002; Aleven et al., 2001; VanLehn et al., 2002).
Many of these systems have yielded successful evalu-
ations with students (Rose et al., 2001; Heffernan and
Koedinger, 2002; Ashley et al., 2002; Graesser et al.,
2001a). However, while the majority of current tutorial
dialogue systems are text based, there is reason to believe
that speech based tutorial dialogue systems could be more
effective.
Prior studies have shown considerable benefits of
human-human tutoring through spoken interactions
(Lemke, 1990; Chi et al., 1994). (Hausmann and Chi,
2002) has shown that spontaneous self-explanation oc-
curs much more frequently in spoken tutoring then in
\x0ctext based tutoring, suggesting that typing requires ad-
ditional cognitive capacity and thus reduces the cogni-
tive resources available for spontaneous self-explanation.
Other research projects (Mostow and Aist, 2001; Fry
et al., 2001) have shown that basic spoken natural lan-
guage capabilities can be implemented quite effectively
in computer tutoring systems. Moreover, speech con-
tains prosodic and acoustic information which has been
shown to improve the accuracy of predicting emotional
states (Ang et al., 2002; Batliner et al., 2000) and user
responses to system errors (Litman et al., 2001) that are
useful for triggering system adaptation. We are thus cur-
rently developing a speech based dialogue system that
uses a text based system (VanLehn et al., 2002) as its
back-end. These systems and their goals will be dis-
cussed in Section 2.
We expect that the different modalities used by these
systems (e.g. text based vs speech based) will display
interesting differences with respect to the characteristics
of dialogue interaction that may determine their relative
merits with respect to increasing student performance.
Although human-computer data from the speech based
system is not yet available for comparison, we have col-
lected parallel human-human corpora both for text based
and speech based tutoring, as discussed in Sections 3-4,
and these corpora already display similarities and differ-
ences with respect to features of their dialogue interac-
tions, as discussed in Section 5, that are wholly modality
based and that will likely be displayed to an even greater
extent in the comparable human-computer data.
</bodyText>
<sectionHeader confidence="0.952062" genericHeader="introduction">
2 Why2-Atlas and ITSPOKE Dialogue
</sectionHeader>
<subsectionHeader confidence="0.549926">
Systems
</subsectionHeader>
<bodyText confidence="0.998511963855422">
Why2-Atlas is a text based intelligent tutoring dialogue
system (Rose et al., 2002a; VanLehn et al., 2002). The
goal of Why2-Atlas is to provide a platform for testing
whether deep approaches to natural language process-
ing elicit more learning than shallower approaches, for
the task domain of qualitative physics explanation gen-
eration. Using Why2-Atlas, the activity in which stu-
dents engage is answering deep reasoning questions in-
volving topics in conceptual physics. One such question
that we used is, A lightweight car and a massive truck
have a head-on collision. On which vehicle is the im-
pact force greater? Which vehicle undergoes the greater
change in its motion? Explain. This is an appropriate
task domain for pursuing questions about the benefits of
tutorial dialogue for learning because questions like this
one are known to elicit robust, persistent misconceptions
from students, such as heavier objects exert more force.
(Hake, 1998; Halloun and Hestenes, 1985). We designed
a set of 10 essay questions to use as training problems.
Two physics professors and a computer science profes-
sor worked together to select a set of expectations (i.e.,
correct propositions that the tutors expected students to
include in their essays) and potential misconceptions as-
sociated with each question. Additionally, they agreed
on an ideal essay answer for each problem. In Why2-
Atlas, a student first types an essay answering a qualita-
tive physics problem. A computer tutor then engages the
student in a natural language dialogue to provide feed-
back, correct misconceptions, and to elicit more complete
explanations. The first version of Why2-Atlas was de-
ployed and evaluated with undergraduate students in the
spring of 2002; the system is continuing to be actively
developed (Graesser et al., 2002).
We are currently developing a speech-enabled version
of Why2-ATLAS, called ITSPOKE (Intelligent Tutor-
ing SPOKEn dialogue system), that uses the Why2-Atlas
system as its back-end. To date we have interfaced
the Sphinx2 speech recognizer (Huang et al., 1993) with
stochastic language models trained from example user ut-
terances, and the Festival speech synthesizer (Black and
Taylor, 1997) for text-to-speech, to the Why2-Atlas back-
end. The rest of the needed natural language process-
ing components, e.g. the sentence-level syntactic and
semantic analysis modules (Rose, 2000), discourse and
domain level processors (Makatchev et al., 2002), and a
finite-state dialogue manager (Rose et al., 2001), are pro-
vided by a toolkit that is part of the Why2-Atlas back-
end. The student speech is digitized from microphone
input, while the tutors synthesized speech is played to
the student using a speaker and/or headphone. We are
now in the process of adapting the knowledge sources
needed by the spoken language components to our ap-
plication domain. For example, we have developed a set
of dialogue dependent language models using the exper-
imental human-computer typed corpus (4551 student ut-
terances) obtained during the Why2-Atlas 2002 evalua-
tion. Our language models will soon be enhanced using
student utterances from our parallel human-human spo-
ken language corpus.
One goal of the ITSPOKE system is simply replacing
text based dialogue interaction with spoken dialogue in-
teraction and leaving the rest of the Why2-Atlas back-end
unchanged, in order to test the hypothesis that student
self-explanation (which leads to greater learning (Haus-
mann and Chi, 2002)) might be easier to achieve in spo-
ken dialogues. This hypothesis is discussed further in
Section 5. Although not the focus of this paper, an-
other goal of the ITSPOKE system is to take full ad-
vantage of the speech modality. For example, speech
contains rich acoustic and prosodic information about
the speakers current emotional state that isnt present
in typed dialogue. Connections between learning and
emotion have been well documented (Coles, 1999), so it
seems likely that the success of computer-based tutoring
\x0csystems could be greatly increased if they were capable
of predicting and adapting to student emotional states,
e.g. reinforcing positive states, while rectifying nega-
tive states (Evens, 2002). Preliminary machine learn-
ing experiments involving emotion annotation and au-
tomatic feature extraction from our corpus suggest that
ITSPOKE can indeed be enhanced to automatically pre-
dict and adapt to student emotional states (Litman et al.,
2003).
</bodyText>
<sectionHeader confidence="0.956154" genericHeader="method">
3 Typed Human-Human Tutoring Corpus
</sectionHeader>
<bodyText confidence="0.997400225">
The Why2-Atlas Human-Human Typed Tutoring Cor-
pus is a collection of typed tutoring dialogues between
(human) tutor and student collected via typed interface,
which the tutor plays the same role that Why2-Atlas
is designed to perform. The experimental procedure is
as follows: 1) students are given a pretest measuring
their knowledge of physics, 2) students are asked to read
through a small document of background material, 3) stu-
dents work through a set of up to 10 Why2-Atlas training
problems with the human tutor, and 4) students are given
a post-test that is similar to the pretest. The entire ex-
periment takes no more than 15 hours per student, and is
usually performed in 1-3 sessions of no more than 4 hours
each. Data collection began in the Fall 2002 semester and
is continuing in the Spring 2003 semester. The subjects
are all University of Pittsburgh students who have never
taken any college physics courses. One tutor currently
participates in the study.
As in the Why2-Atlas system, when the tutoring ses-
sion then begins, the student first types an essay answer-
ing a qualitative physics problem. Once the student sub-
mits his/her essay, the tutor then engages the student in a
typed natural language dialogue to provide feedback and
correct misconceptions, and to elicit more complete ex-
planations. This instruction is in the form of a dialogue
between the student and the tutor through a text based
chat interface with student and tutor in separate rooms.
At key points in the dialogue, the tutor asks the student
to revise the essay. This cycle of instruction and revision
continues until the tutor is satisfied with the students es-
say. A sample tutoring dialogue from the Why2-Atlas
typed human-human tutoring corpus is displayed in Fig-
ure 1.
The tutor was instructed to cover the expectations for
each problem, to watch for the specific set of expectations
and misconceptions associated with the problem, and to
end the discussion of each problem by showing the ideal
essay to the student. He was encouraged to avoid lectur-
ing the student and to attempt to draw out the students
own reasoning. He knew that transcripts of his tutoring
would be analyzed. Nevertheless, he was not required to
follow any prescribed tutoring strategies. So his tutoring
style was much more naturalistic than in previous stud-
ies such as the BEE study (Rose et al., 2001) in which
two specific tutoring styles, namely Socratic and Didac-
tic, were contrasted. The results of that study revealed a
trend for students in the Socratic condition to learn more
than those in the Didactic condition. A further analysis
of the corpus collected during the BEE study (Core et
al., 2002) verified that the Socratic dialogues from the
BEE study were more interactive than the Didactic ones.
The biggest reliable difference between the two sets of
tutoring dialogues was the percentage of words spoken
by the student, i.e, number of student words divided by
total number of words. The Didactic dialogues contained
on average 26% student words, whereas the Socratic di-
alogues contained 33% student words. On average with
respect to percentage of student words, the dialogues in
our text based human tutoring corpus were more like the
Didactic dialogues from the BEE study, with average per-
centage of student text being 27%. Nevertheless, because
the tutor was not constrained to follow a prescribed tutor-
ing style, the level of interactivity varied widely through-
out the transcripts, at times being highly Socratic, and at
other times being highly Didactic.
Pre and post tests were used to measure learning gains
to be used for evaluating the effectiveness of various fea-
tures of tutorial dialogue found in our corpora. Thus, we
developed two tests: versions A and B, which were iso-
morphic to one another. That is, the problems on test A
and B differed only in the identities of the objects (e.g.,
cars vs. trucks) and other surface features that should not
affect the reasoning required to solve them. Each ver-
sion of the test (A and B) consisted of 40 multiple choice
questions. Each multiple choice question was written to
address a single expectation covered in the training prob-
lems. Some students were not able to complete all 10
problems before they reached the end of their participa-
tion time. Thus, they took the post-test after only working
through a subset of the training problems.
</bodyText>
<sectionHeader confidence="0.5151365" genericHeader="method">
4 Spoken Human-Human Tutoring
Corpus
</sectionHeader>
<bodyText confidence="0.975427">
The ITSPOKE Human-Human Spoken Tutoring Corpus
is a parallel collection of spoken tutoring dialogues col-
lected via a web interface supplemented with a high qual-
ity audio link, where a human tutor performs the same
task that our ITSPOKE system is being designed to per-
form. The experimental procedure used to collect the cor-
pus is exactly the same as the procedure used to gather
the Why2-Atlas Human-Human Corpus: the same tutor
is used, the same subject pool1
is used, the same pre-test
and post-test are used, and the same set of physics prob-
</bodyText>
<page confidence="0.768045">
1
</page>
<bodyText confidence="0.977350333333333">
We assigned a greater percentage of students to the text
based condition as part of a separate experiment. Thus, the text
based corpus is larger than the speech based corpus.
\x0cPROBLEM: Suppose that you released 3 identical balls of clay in a vacuum at exactly the same instant. They would all hit the
ground at the same instant. Now you stick two of the balls together, forming one ball that is twice as heavy as the remaining,
untouched clay ball. Both balls are released in a vacuum at exactly the same instant. Which ball hits the ground first?
ESSAY: Both balls will hit the ground at the same time. The balls are in free fall (only gravitational forces). The ratio of the
masses and weight are equal.
...excerpt from 2 minutes into a typed dialogue ...
</bodyText>
<footnote confidence="0.616975611111111">
Tutor1: You have correctly said the balls are in free fall. What do you conclude from this fact?
Student1: There is no air resistance and the balls accelerations are constant in regards to one another
Tutor2: Right, so the conclusion is that they both have the same acceleration, the free fall acceleration. Now you have to show
that time taken by them to reach the ground will be the same. How do you do that?
Student2: F (net forces) / mass = f/m because F = m*a therefore ratio should equal one another
Tutor3: But that does not tell you anything about the time taken. What physical quantities will determine the time taken. Will the
initial velocity of the balls matter?
Student3: yes, assuming starting at rest? sorry, kind of at a loss at the moment
Tutor4: The initial velocity will matter, and here they both have the same initial velocity, zero. What else, will the height through
wich they fall matter?
Student4: not so much as long as they are both dropped from the same height
Tutor5: Height of fall will determine the time of fall, and here the height of fall is the same for both the balls. Now on the basis of
these facts can you conclude about the time taken by them to reach the ground?
Student5: since initial velocities are equal and the height of falls are equal, then the time taken is equal
Tutor6: How about acceleration, if they had different accelerations, even if they fall through the same height and have same initial
velocity , will they reach the ground at the same time?
Student6: no...
Tutor7: Right...
</footnote>
<figureCaption confidence="0.994513">
Figure 1: Excerpt from Human-Human Typed Dialogue Corpus
</figureCaption>
<bodyText confidence="0.963647828571429">
lems are used. Only the modality differs. In particular,
once the tutoring session begins and the student submits
his/her typed essay, the tutor and student then discuss the
students essay using spoken English. In contrast to the
text condition, where strict turn-taking is enforced, in the
spoken condition, interruptions and overlapping speech
are common. An example excerpt from the corpus is
shown in Figure 2. Note that turns ending in - indi-
cate speech overlapping with the following turn. Even-
tually, the student will edit his/her typed explanation. As
in the text condition, the tutor then either expresses satis-
faction and ends the tutoring for the current problem, or
continues with another round of spoken dialogue interac-
tion and typed essay revision. As in the text condition,
students are presented with the ideal essay answer for a
problem upon completing that problem.
5 Differences between Typed and Spoken
Human-Tutoring
(Rose et al., submitted) presents an analysis to uncover
which aspects of the tutorial dialogue were responsible
for its effectiveness in the text based condition. Longer
student answers to tutor questions reveal more of a stu-
dents reasoning. Very short answers, i.e., 10 words or
less, are normally composed of a single clause at most.
Longer, multi-clausal answers have the potential to com-
municate many more inter-connections between ideas.
Thus, if a tutor is attending to and responding directly
to the students revealed knowledge state, it would be
expected that the effectiveness of the tutors instruction
would increase as average student turn length increases.
To test this prediction, we computed a linear regression
of the sequence of student turn lengths over time for each
student in the text based condition in order to obtain an in-
tercept and a slope, since student turn lengths have been
observed to decline on average over the course of their
</bodyText>
<footnote confidence="0.76918776">
\x0cPROBLEM: If a car is able to accelerate at 2 m/s2, what acceleration can it attain if it is towing another car of equal mass?
ESSAY: If the car is towing another car of equal mass, the maximum acceleration would be the same because the car would be
towed behind and the friction caused would only be by the front of the first car.
...excerpt from 6.5 minutes into spoken dialogue ...
Tutor1: So twice the mass multiplied by the acceleration should be equal to the force which you have already determined as the
mass of the first car times the acceleration. So essentially you are dividing it by two and that gives you the uh acceleration by
twice the because mass has become twice. Now this law that force is equal to mass times acceleration, whats this law called?
This is uh since this it is a very important basic uh fact uh it is it is a law of physics. Um you have you have read it in the
background material. Can you recall it?
Student1: Um no it was one of Newtons laws-
Tutor2: Right, right-
Student2: but I dont remember which one. (laugh)
Tutor3: That-
Student3: he I-
Tutor4: is Newtons second law of motion.
Student4: Ok, because I remember one, two, and three, but I didnt know if there was a different name
Tutor5: Yeah thats right you know Newton was a genius and-
Student5: (laugh)-
Tutor6: uh he looked at a large number of experiments and experimental data that was available and from that he could come to
this general law and it is known as Newtons second law of motion. Um many many other scientists before him had seen all
this data which was collected by scientists but had not concluded this. Now it looks very simple but to come to a conclusion
from a mass of data was something which required the genius of Newton.
Student6: mm hm
Tutor7: So now you will give Newton full credit isnt it? (laugh)
Student7: (laugh)
</footnote>
<figureCaption confidence="0.987685">
Figure 2: Excerpt from Human-Human Spoken Dialogue Corpus.
</figureCaption>
<bodyText confidence="0.964296266666667">
interaction with the turn. We then computed a multiple
regression with pre-test score, intercept, and gradient as
independent variables and post test score as the depen-
dent variable. We found a reliable correlation between
intercept and learning, with pre-test scores and gradients
regressed out (R=.836; p .05). This result is consistent
with (Core et al., 2002) where percentage of student talk
is strongly correlated with learning. Consistent with this,
we found a strong and reliable correlation between ra-
tio of student words to tutor words and learning2
. We
computed a correlation between ratio of student words to
tutor words and post-test score after pre-test scores were
regressed out (R=.866, p .05).
One of our current research objectives is to compare
</bodyText>
<page confidence="0.964894">
2
</page>
<bodyText confidence="0.999064018518519">
Note that ratio of student words to tutor words is number
of student words divided by number of tutor words, whereas
percentage of student words is number of student words divided
by total number of words
the relative effectiveness of speech based and text based
tutoring. Thus, when we have enough speech data, we
would like to compare learning gains between the speech
and text based conditions to test whether or not speech
based tutoring is more effective than text based tutoring.
We also plan to test whether the same features that cor-
relate with learning in the text based condition also cor-
relate with learning in the speech based condition. Since
both average student turn length and overall ratio of stu-
dent words to tutor words correlated strongly with learn-
ing gains in the text based condition, in this paper we
compare these two measures between the text based tutor-
ing condition and the speech based tutoring condition, but
not yet in connection with learning gains in the speech-
based corpus.
Since strict turn taking was not enforced in the speech
condition, turn boundaries were manually annotated
(based on consensus labellings from two coders) when ei-
\x0cther (1) the speaker stopped speaking and the other party
in the dialogue began to speak, (2) when the speaker
asked a question and stopped speaking to wait for an an-
swer, or (3) when the other party in the dialogue inter-
rupted the speaker and the speaker paused to allow the
other party to speak.
Currently, 13 students have started the typed human-
human tutoring experiment, 7 of whom have finished.
We have so far collected 78 typed dialogues from the
text based condition, 69 of which were used in our anal-
ysis. 9 students have started the spoken human-human
tutoring experiment, 6 of whom have finished. Thus, we
have collected 63 speech based dialogues (1290 minutes
of speech from 4 female and 4 male subjects), and have
transcribed 25 of them. We hope to have an analysis cov-
ering all of our data in both conditions by the time of the
workshop.
As shown in Table 1, analysis of the data that has
been collected and transcribed to date is already show-
ing interesting differences between the ITSPOKE (spo-
ken) and WHY2-ATLAS (text) corpora of human-human
dialogues. The #trns columns show mean and standard
deviation for the total number of turns taken by the stu-
dents or tutor in each problem dialogue, while the next
pair of columns show the mean and standard deviation
for the total number of words spoken or typed by the stu-
dents or tutor (#wds) in each problem dialogue. The last
pair of columns show mean and standard deviation for the
average number of student or tutor words per turn in each
problem dialogue.
Due to the fact that data is still being collected for both
corpora (and the fact that the speech corpus also requires
manual transcription), the sizes of the two data sets rep-
resented in the table differ somewhat. However, even at
this early stage in the development of both corpora, these
figures already show that the style of the interactions are
very different in each modality. In particular, in spoken
tutoring, both student and tutor take more turns on av-
erage than in text based tutoring, but these spoken turns
are on average shorter. Moreover, in spoken tutoring both
student and tutor on average use more words to commu-
nicate than in text based tutoring. Another interesting dif-
ference is that although in the speech condition both stu-
dent and tutor take more turns, students finish the speech
condition in less time. In particular, on average, students
in the text based tutoring condition require 370.58 min-
utes to finish the training problems, with a standard devi-
ation of 134.29 minutes, students in the speech condition
require only 159.9 minutes on average, with a standard
deviation of 58.6 minutes. We measured the statistical re-
liability of the difference between the two measures that
correlated reliably with learning in the text-based condi-
tion. A 2-tailed unpaired t-test indicates that this differ-
ence is significant (t(30)=8.99, p .01). There are also
similarities across the two conditions. In particular, a 2-
tailed unpaired t-test shows that the relative proportion
of student and tutor word or turns do not differ signifi-
cantly on average across the two modalities (t(13)=1.225,
p=.242). As an illustration, Table 2 shows mean and stan-
dard deviation for the ratios of the total number of student
and tutor words (#Swds/#Twds) and turns (#Strns/#Ttrns)
in each problem dialogue3
.
Average student turn length is significantly lower in
the speech based condition (t(13) = 4.5, p .001). This
might predict that speech based tutoring may be less ef-
fective than text based tutoring. However, since ratio of
student words to tutor words does not differ significantly,
this would predict that learning will also not differ sig-
nificantly between conditions. Total number of words
uttered in the speech condition is larger than in the text
based condition as are total number of turns. This dif-
ference between the two conditions will likely be even
more pronounced in the human-computer comparison,
due to noisy student input that results from use of au-
tomatic speech recognition. For example, clarifications
and corrections made necessary by this will likely lead
to an increase in dialogue length. More careful analy-
sis is required to determine whether this means that more
self-explanation took place overall in the speech based
condition. If so, this would predict that the speech based
condition would be more effective for learning than the
text based condition. Thus, much interesting exploration
is left to be done after we have collected enough speech
data to compute a reliable comparison between the two
conditions.
</bodyText>
<sectionHeader confidence="0.988855" genericHeader="method">
6 Current Directions
</sectionHeader>
<bodyText confidence="0.999782722222222">
Currently we are continuing to collect data both in the
speech and text based human tutoring conditions. Since
human tutors differ with each other with respect to both
their tutoring styles and their conversational styles, we
plan to collect data using several different human tutors
in order to test the robustness of our comparisons between
speech and text based human tutoring. Another possible
direction for further inquiry would be to contrast natural-
istic speech (where strict turn taking is not enforced, as
in this data collection effort), with a speech condition in
which strict turn taking is enforced, in order to separate
the effects of speech on learning from the effects of alter-
native turn taking policies.
As discussed in Section 2, we are currently develop-
ing both text based and speech based human-computer
tutorial systems. Our ultimate goal is to test the relative
effectiveness of speech versus text based computer tutors.
We expect differences both between text and speech con-
</bodyText>
<page confidence="0.982813">
3
</page>
<bodyText confidence="0.7614765">
Note that strict-turn taking is enforced in the text condition,
but not in the speech condition.
</bodyText>
<tableCaption confidence="0.848225">
\x0cTable 1: Student and Tutor Characteristics in Human-Human Speech and Text Conditions
</tableCaption>
<table confidence="0.949512636363636">
#trns #wds Avg#wds/trn
Condition Participant Mean STD Mean STD Mean STD
Speech Student 47.49 25.95 264.18 125.47 5.72 1.35
Text Student 9.71 6.79 146.72 57.96 13.39 5.55
Speech Tutor 46.94 20.90 1199.14 605.87 26.78 14.20
Text Tutor 11.03 7.04 391.85 136.89 39.04 6.23
Table 2: Student-Tutor Word and Turn Ratios in Speech and Text Conditions
Speech Condition Text Condition
#Swds/#Twds #Strns/#Ttrns #Swds/#Twds #Strns/#Ttrns
Mean STD Mean STD Mean STD Mean STD
0.29 0.15 0.99 0.15 0.37 0.08 0.81 0.15
</table>
<bodyText confidence="0.999223176470588">
ditions in the human-computer data and between human-
human and human-computer data. One of our first tasks
will thus be to use the baseline version of ITSPOKE
described in Section 2 to generate a corpus of human-
computer spoken dialogues, using a process comparable
to the human-human corpus collection described here.
This will allow us to 1) compare the ITSPOKE human-
human and human-computer corpora 2) compare the IT-
SPOKE human-computer spoken corpus with a compa-
rable Why2-Atlas text corpus, e.g. by expanding on the
just described pilot study of the two human-human cor-
pora, and 3) use the ITSPOKE human-computer corpus
to guide the development of a new version of ITSPOKE
that will attempt to increase its performance, by taking
advantage of information that is only available in speech,
and modifying its behavior in other ways to respect the
interaction differences in item 2.
</bodyText>
<sectionHeader confidence="0.998331" genericHeader="conclusions">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.90786375">
This research was supported by the Office of Naval Re-
search, Cognitive Science Division under grant number
N00014-0-1-0600 and by NSF grant number 9720359 to
CIRCLE.
</bodyText>
<sectionHeader confidence="0.943447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986973657342657">
V. Aleven, O. Popescu, and K. Koedinger. 2001.
Towards tutorial dialog to support self-explanation:
Adding natural language understanding to a cognitive
tutor. In J. D. Moore, C. L. Redfield, and W. L. John-
son, editors, Proceedings of Artificial Intelligence in
Education, pages 246255.
J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stol-
cke. 2002. Prosody-based automatic detection of an-
noyance and frustration in human-computer dialog. In
Proc. ICSLP.
K. D. Ashley, R. Desai, and J. M. Levine. 2002. Teach-
ing case-based argumentation concepts using didactic
arguments vs. didactic explanations. In Proceedings
of the Intelligent Tutoring Systems Conference, pages
585595.
A. Batliner, R. Huber, H. R. Niemann, E. Noth, J. Spilker,
and K. Fischer. 2000. The recognition of emotion. In
Proc. of the ISCA Workshop on Speech and Emotion.
A. Black and P. Taylor. 1997. Festival speech synthesis
system: system documentation (1.1.1). Human Com-
munication Research Centre Technical Report 83, Uni-
versity of Edinburgh.
M. Chi, N. de Leeuw, M. Chiu, and C. LaVancher.
1981. Eliciting self-explanations improves under-
standing. Cognitive Science, 18(3).
Michelene Chi, Nicholas De Leeuw, Mei-Hung Chiu, and
Christian Lavancher. 1994. Eliciting self-explanations
improves understanding. Cognitive Science, 18:439
477.
M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, and
R. G. Hausmann. 2001. Learning from human tutor-
ing. Cognitive Science, (25):471533.
M. T. H. Chi. 1996. Learning processes in tutoring. Ap-
plied Cognitive Psychology, 10:S33S49.
G. Coles. 1999. Literacy, emotions, and the brain. Read-
ing Online, March 1999.
M. Core, J. D. Moore, and C. Zinn. 2002. Initiative in
tutorial dialogue. In Proceedings of the ITS Workshop
on Empirical Methods for Tutorial Dialogue Systems,
pages 4655.
M. Evens, S. Brandle, R. Chang, R. Freedman, M. Glass,
Y. H. Lee, L. S. Shim, C. W. Woo, Y. Zhang, Y. Zhou,
\x0cJ. A. Michaeland, and A. A. Rovick. 2001. Circsim-
tutor: An intelligent tutoring system using natural lan-
guage dialogue. In Proceedings of the Twelfth Midwest
AI and Cognitive Science Conference, MAICS 2001,
pages 1623, Oxford, OH.
M. Evens. 2002. New questions for Circsim-Tutor. Pre-
sentation at the 2002 Symposium on Natural Language
Tutoring, University of Pittsburgh.
J. Fry, M. Ginzton, S. Peters, B. Clark, and H. Pon-Barry.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proc. 2nd SigDial Work-
shop on Discourse and Dialogue.
A. Graesser, N. Person, and D. Harter et al. 2001a.
Teaching tactics and dialog in Autotutor. International
Journal of Artificial Intelligence in Education.
A. Graesser, K. Vanlehn, TRG, and NLT Group. 2002.
Why2 report: Evaluation of why/atlas, why/autotutor,
and accomplished human tutors on learning gains for
qualitative physics problems and explanations. Tech-
nical report, LRDC Tech Report, University of Pitts-
burgh.
R. R. Hake. 1998. Interactive-engagement versus tra-
ditional methods: A six-thousand student survey of
mechanics test data for introductory physics students.
American Journal of Physics, 66(64).
I. A. Halloun and D. Hestenes. 1985. The initial knowl-
edge state of college physics students. American Jour-
nal of Physics, 53(11):10431055.
Robert Hausmann and Michelene Chi. 2002. Can a com-
puter interface support self-explaining? The Interna-
tional Journal of Cognitive Technology, 7(1).
N. T. Heffernan and K. R. Koedinger. 2002. An intelli-
gent tutoring system incorporating a model of an expe-
rienced tutor. In Proceedings of the Intelligent Tutor-
ing Systems Conference, pages 596608.
X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F.
Lee, and R. Rosenfeld. 1993. The SphinxII speech
recognition system: An overview. Computer, Speech
and Language.
J. L. Lemke. 1990. Talking Science: Language, Learn-
ing and Values. Ablex, Norwood, NJ.
D. Litman, J. Hirschberg, and M. Swerts. 2001. Predict-
ing user reactions to system error. In Proc.of ACL.
Diane Litman, Kate Forbes, and Scott Silliman. 2003.
Towards emotion prediction in spoken tutoring dia-
logues. Submitted.
M. Makatchev, P. Jordan, and K. VanLehn. 2002. Dis-
course processing for explanatory essays in tutorial ap-
plications. In Proceedings of the 3rd SIGdial Work-
shop on Discourse and Dialogue.
J. Mostow and G. Aist. 2001. Evaluating tutors that lis-
ten: An overview of Project LISTEN. In K. Forbus and
P. Feltovich, editors, Smart Machines in Education.
M. Pressley, E. Wood, V. E. Woloshyn, V. Martin,
A. King, and D. Menke. 1992. Encouraging mind-
ful use of prior knowledge: Attempting to construct
explanatory answers facilitates learning. Educational
Psychologist, 27:91109.
A. Renkl. 1997. Learning from worked-out examples:
A study on individual differences. Cognitive Science,
21(1):129.
C. P. Rose and V. Aleven. 2002. Proc. of the ITS 2002
workshop on empirical methods for tutorial dialogue
systems. Technical report, San Sebastian, Spain, June.
C. P. Rose, J. D. Moore, K. VanLehn, and D. Allbritton.
2001. A comparative evaluation of socratic versus di-
dactic tutoring. In Proceedings of the 23rd Annual
Conference of the Cognitive Science Society, pages
869874.
C. P. Rose, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. Interactive conceptual
tutoring in atlas-andes. In Proceedings of Artificial In-
telligence in Education, pages 256266.
C. P. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivas-
tava, and K. Vanlehn. 2002a. A hybrid language un-
derstanding approach for robust selection of tutoring
goals. In Proceedings of the Intelligent Tutoring Sys-
tems Conference, pages 552561.
C. P. Rose, K. VanLehn, and The Natural Language Tu-
toring Group. Submitted. Is human tutoring always
more effective than reading. In Annual Meeting of the
Cognitive Science Society.
C. P. Rose. 2000. A framework for robust sentence
level interpretation. In Proceedings of the First Meet-
ing of the North American Chapter of the Association
for Computational Linguistics, pages 11291135.
N. J. Slamecka and P. Graf. 1978. The generation ef-
fect: Delineation of a phenomenon. Journal of Exper-
imental Psychology: Human Learning and Memory,
(4):592604.
K. VanLehn, P. Jordan, C. Rose, D. Bhembe, M. Bottner,
A. Gaydos, M. Makatchev, U. Pappuswamy, M. Rin-
genberg, A. Roque, S. Siler, R. Srivastava, and R. Wil-
son. 2002. The architecture of Why2-Atlas: A coach
for qualitative physics essay writing. In Proc. of ITS.
Claus Zinn, Johanna D. Moore, and Mark G. Core. 2002.
A 3-tier planning architecture for managing tutorial di-
alogue. In Proceedings Intelligent Tutoring Systems,
Sixth International Conference (ITS 2002), Biarritz,
France, June.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.785168">
<title confidence="0.997188">b&apos;A Comparison of Tutor and Student Behavior in Speech Versus Text Based Tutoring</title>
<author confidence="0.998388">Carolyn P Rose</author>
<author confidence="0.998388">Diane Litman</author>
<author confidence="0.998388">Dumisizwe Bhembe</author>
<author confidence="0.998388">Kate Forbes</author>
<author confidence="0.998388">Scott Silliman</author>
<author confidence="0.998388">Ramesh Srivastava</author>
<author confidence="0.998388">Kurt VanLehn</author>
<affiliation confidence="0.999775">Learning Research and Development Center, University of Pittsburgh,</affiliation>
<address confidence="0.99983">3939 OHara St., Pittsburgh, PA 15260</address>
<email confidence="0.910964">rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edulitman,vanlehn@cs.pitt.edu</email>
<abstract confidence="0.998211076923077">This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring. Most current tutorial dialogue systems are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). However, prior studies have shown considerable benefits of tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994; Hausmann and Chi, 2002). Thus, we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics (VanLehn et al., 2002) as its back-end. In order to explore the relative effectiveness between these two input modalities in our task domain, we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring. In both cases, students interact with the tutor through a web interface. We present here a comparison between the two on a number of features of dialogue that have been demonstrated to correlate reliably with learning gains with students interacting with the tutor using the text based interface (Rose et al., submitted).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>O Popescu</author>
<author>K Koedinger</author>
</authors>
<title>Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor.</title>
<date>2001</date>
<booktitle>Proceedings of Artificial Intelligence in Education,</booktitle>
<pages>246255</pages>
<editor>In J. D. Moore, C. L. Redfield, and W. L. Johnson, editors,</editor>
<contexts>
<context position="631" citStr="Aleven et al., 2001" startWordPosition="84" endWordPosition="87">on of Tutor and Student Behavior in Speech Versus Text Based Tutoring Carolyn P. Rose, Diane Litman, Dumisizwe Bhembe, Kate Forbes, Scott Silliman, Ramesh Srivastava, Kurt VanLehn Learning Research and Development Center, University of Pittsburgh, 3939 OHara St., Pittsburgh, PA 15260 rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edu litman,vanlehn@cs.pitt.edu Abstract This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring. Most current tutorial dialogue systems are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). However, prior studies have shown considerable benefits of tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994; Hausmann and Chi, 2002). Thus, we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics (VanLehn et al., 2002) as its back-end. In order to explore the relative effectiveness between these two input modalities in our task domain, we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring. In both cases, students interact with </context>
<context position="3420" citStr="Aleven et al., 2001" startWordPosition="535" endWordPosition="538">ors may not always choose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently </context>
</contexts>
<marker>Aleven, Popescu, Koedinger, 2001</marker>
<rawString>V. Aleven, O. Popescu, and K. Koedinger. 2001. Towards tutorial dialog to support self-explanation: Adding natural language understanding to a cognitive tutor. In J. D. Moore, C. L. Redfield, and W. L. Johnson, editors, Proceedings of Artificial Intelligence in Education, pages 246255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer dialog.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP.</booktitle>
<contexts>
<context position="4564" citStr="Ang et al., 2002" startWordPosition="705" endWordPosition="708">has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its back-end. These systems and their goals will be discussed in Section 2. We expect that the different modalities used by these systems (e.g. text based vs speech based) will display interesting differences with respect to the characteristics of dialogue interaction that may determine their relative merits with respect to increasing student pe</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer dialog. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K D Ashley</author>
<author>R Desai</author>
<author>J M Levine</author>
</authors>
<title>Teaching case-based argumentation concepts using didactic arguments vs. didactic explanations.</title>
<date>2002</date>
<booktitle>In Proceedings of the Intelligent Tutoring Systems Conference,</booktitle>
<pages>585595</pages>
<contexts>
<context position="3587" citStr="Ashley et al., 2002" startWordPosition="562" endWordPosition="565">nfusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available fo</context>
</contexts>
<marker>Ashley, Desai, Levine, 2002</marker>
<rawString>K. D. Ashley, R. Desai, and J. M. Levine. 2002. Teaching case-based argumentation concepts using didactic arguments vs. didactic explanations. In Proceedings of the Intelligent Tutoring Systems Conference, pages 585595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>R Huber</author>
<author>H R Niemann</author>
<author>E Noth</author>
<author>J Spilker</author>
<author>K Fischer</author>
</authors>
<title>The recognition of emotion.</title>
<date>2000</date>
<booktitle>In Proc. of the ISCA Workshop on Speech and Emotion.</booktitle>
<contexts>
<context position="4588" citStr="Batliner et al., 2000" startWordPosition="709" endWordPosition="712">ntaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its back-end. These systems and their goals will be discussed in Section 2. We expect that the different modalities used by these systems (e.g. text based vs speech based) will display interesting differences with respect to the characteristics of dialogue interaction that may determine their relative merits with respect to increasing student performance. Although huma</context>
</contexts>
<marker>Batliner, Huber, Niemann, Noth, Spilker, Fischer, 2000</marker>
<rawString>A. Batliner, R. Huber, H. R. Niemann, E. Noth, J. Spilker, and K. Fischer. 2000. The recognition of emotion. In Proc. of the ISCA Workshop on Speech and Emotion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Black</author>
<author>P Taylor</author>
</authors>
<title>Festival speech synthesis system: system documentation (1.1.1). Human Communication Research Centre</title>
<date>1997</date>
<tech>Technical Report 83,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="7905" citStr="Black and Taylor, 1997" startWordPosition="1227" endWordPosition="1230">eptions, and to elicit more complete explanations. The first version of Why2-Atlas was deployed and evaluated with undergraduate students in the spring of 2002; the system is continuing to be actively developed (Graesser et al., 2002). We are currently developing a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its back-end. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Rose, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Rose et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutors synthesized speech is played to the student using a speaker and/or headphone. We are now in the process of adapting the knowledge sources needed by the </context>
</contexts>
<marker>Black, Taylor, 1997</marker>
<rawString>A. Black and P. Taylor. 1997. Festival speech synthesis system: system documentation (1.1.1). Human Communication Research Centre Technical Report 83, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chi</author>
<author>N de Leeuw</author>
<author>M Chiu</author>
<author>C LaVancher</author>
</authors>
<title>Eliciting self-explanations improves understanding.</title>
<date>1981</date>
<journal>Cognitive Science,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>Chi, de Leeuw, Chiu, LaVancher, 1981</marker>
<rawString>M. Chi, N. de Leeuw, M. Chiu, and C. LaVancher. 1981. Eliciting self-explanations improves understanding. Cognitive Science, 18(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelene Chi</author>
<author>Nicholas De Leeuw</author>
<author>Mei-Hung Chiu</author>
<author>Christian Lavancher</author>
</authors>
<title>Eliciting self-explanations improves understanding.</title>
<date>1994</date>
<journal>Cognitive Science,</journal>
<volume>18</volume>
<pages>477</pages>
<marker>Chi, De Leeuw, Chiu, Lavancher, 1994</marker>
<rawString>Michelene Chi, Nicholas De Leeuw, Mei-Hung Chiu, and Christian Lavancher. 1994. Eliciting self-explanations improves understanding. Cognitive Science, 18:439 477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T H Chi</author>
<author>S A Siler</author>
<author>H Jeong</author>
<author>T Yamauchi</author>
<author>R G Hausmann</author>
</authors>
<title>Learning from human tutoring. Cognitive Science,</title>
<date>2001</date>
<contexts>
<context position="2647" citStr="Chi et al., 2001" startWordPosition="406" endWordPosition="409"> et al., 1994) notes that there is a general momentum in the science education literature toward the importance of talking, reflecting and explaining as ways to learn (Lemke, 1990). Moreover, encouraging student selfexplanation, which includes both generating inferences from material they have read and relating new material to old material, has been shown to correlate with learning (Chi et al., 1981; Chi et al., 1994; Renkl, 1997; Pressley et al., 1992). In a further study, prompting students with zero content prompts to encourage them to selfexplain was also associated with student learning (Chi et al., 2001). A second important advantage to dialogue is that it affords the tutor the opportunity to tailor instruction to the needs of the student. While human tutors may not always choose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years t</context>
</contexts>
<marker>Chi, Siler, Jeong, Yamauchi, Hausmann, 2001</marker>
<rawString>M. T. H. Chi, S. A. Siler, H. Jeong, T. Yamauchi, and R. G. Hausmann. 2001. Learning from human tutoring. Cognitive Science, (25):471533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T H Chi</author>
</authors>
<title>Learning processes in tutoring. Applied Cognitive Psychology,</title>
<date>1996</date>
<contexts>
<context position="3027" citStr="Chi, 1996" startWordPosition="472" endWordPosition="473">hi et al., 1981; Chi et al., 1994; Renkl, 1997; Pressley et al., 1992). In a further study, prompting students with zero content prompts to encourage them to selfexplain was also associated with student learning (Chi et al., 2001). A second important advantage to dialogue is that it affords the tutor the opportunity to tailor instruction to the needs of the student. While human tutors may not always choose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, whil</context>
</contexts>
<marker>Chi, 1996</marker>
<rawString>M. T. H. Chi. 1996. Learning processes in tutoring. Applied Cognitive Psychology, 10:S33S49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Coles</author>
</authors>
<title>Literacy, emotions, and the brain. Reading Online,</title>
<date>1999</date>
<contexts>
<context position="9626" citStr="Coles, 1999" startWordPosition="1500" endWordPosition="1501">st of the Why2-Atlas back-end unchanged, in order to test the hypothesis that student self-explanation (which leads to greater learning (Hausmann and Chi, 2002)) might be easier to achieve in spoken dialogues. This hypothesis is discussed further in Section 5. Although not the focus of this paper, another goal of the ITSPOKE system is to take full advantage of the speech modality. For example, speech contains rich acoustic and prosodic information about the speakers current emotional state that isnt present in typed dialogue. Connections between learning and emotion have been well documented (Coles, 1999), so it seems likely that the success of computer-based tutoring \x0csystems could be greatly increased if they were capable of predicting and adapting to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Preliminary machine learning experiments involving emotion annotation and automatic feature extraction from our corpus suggest that ITSPOKE can indeed be enhanced to automatically predict and adapt to student emotional states (Litman et al., 2003). 3 Typed Human-Human Tutoring Corpus The Why2-Atlas Human-Human Typed Tutoring Corpus is </context>
</contexts>
<marker>Coles, 1999</marker>
<rawString>G. Coles. 1999. Literacy, emotions, and the brain. Reading Online, March 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Core</author>
<author>J D Moore</author>
<author>C Zinn</author>
</authors>
<title>Initiative in tutorial dialogue.</title>
<date>2002</date>
<booktitle>In Proceedings of the ITS Workshop on Empirical Methods for Tutorial Dialogue Systems,</booktitle>
<pages>4655</pages>
<contexts>
<context position="12846" citStr="Core et al., 2002" startWordPosition="2027" endWordPosition="2030">ent and to attempt to draw out the students own reasoning. He knew that transcripts of his tutoring would be analyzed. Nevertheless, he was not required to follow any prescribed tutoring strategies. So his tutoring style was much more naturalistic than in previous studies such as the BEE study (Rose et al., 2001) in which two specific tutoring styles, namely Socratic and Didactic, were contrasted. The results of that study revealed a trend for students in the Socratic condition to learn more than those in the Didactic condition. A further analysis of the corpus collected during the BEE study (Core et al., 2002) verified that the Socratic dialogues from the BEE study were more interactive than the Didactic ones. The biggest reliable difference between the two sets of tutoring dialogues was the percentage of words spoken by the student, i.e, number of student words divided by total number of words. The Didactic dialogues contained on average 26% student words, whereas the Socratic dialogues contained 33% student words. On average with respect to percentage of student words, the dialogues in our text based human tutoring corpus were more like the Didactic dialogues from the BEE study, with average perc</context>
<context position="21641" citStr="Core et al., 2002" startWordPosition="3539" endWordPosition="3542">t to come to a conclusion from a mass of data was something which required the genius of Newton. Student6: mm hm Tutor7: So now you will give Newton full credit isnt it? (laugh) Student7: (laugh) Figure 2: Excerpt from Human-Human Spoken Dialogue Corpus. interaction with the turn. We then computed a multiple regression with pre-test score, intercept, and gradient as independent variables and post test score as the dependent variable. We found a reliable correlation between intercept and learning, with pre-test scores and gradients regressed out (R=.836; p .05). This result is consistent with (Core et al., 2002) where percentage of student talk is strongly correlated with learning. Consistent with this, we found a strong and reliable correlation between ratio of student words to tutor words and learning2 . We computed a correlation between ratio of student words to tutor words and post-test score after pre-test scores were regressed out (R=.866, p .05). One of our current research objectives is to compare 2 Note that ratio of student words to tutor words is number of student words divided by number of tutor words, whereas percentage of student words is number of student words divided by total number </context>
</contexts>
<marker>Core, Moore, Zinn, 2002</marker>
<rawString>M. Core, J. D. Moore, and C. Zinn. 2002. Initiative in tutorial dialogue. In Proceedings of the ITS Workshop on Empirical Methods for Tutorial Dialogue Systems, pages 4655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Michaeland</author>
<author>A A Rovick</author>
</authors>
<title>Circsimtutor: An intelligent tutoring system using natural language dialogue.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth Midwest AI and Cognitive Science Conference, MAICS</booktitle>
<pages>1623</pages>
<location>Oxford, OH.</location>
<marker>Michaeland, Rovick, 2001</marker>
<rawString>M. Evens, S. Brandle, R. Chang, R. Freedman, M. Glass, Y. H. Lee, L. S. Shim, C. W. Woo, Y. Zhang, Y. Zhou, \x0cJ. A. Michaeland, and A. A. Rovick. 2001. Circsimtutor: An intelligent tutoring system using natural language dialogue. In Proceedings of the Twelfth Midwest AI and Cognitive Science Conference, MAICS 2001, pages 1623, Oxford, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Evens</author>
</authors>
<title>New questions for Circsim-Tutor. Presentation at the 2002 Symposium on Natural Language Tutoring,</title>
<date>2002</date>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="9887" citStr="Evens, 2002" startWordPosition="1538" endWordPosition="1539">Although not the focus of this paper, another goal of the ITSPOKE system is to take full advantage of the speech modality. For example, speech contains rich acoustic and prosodic information about the speakers current emotional state that isnt present in typed dialogue. Connections between learning and emotion have been well documented (Coles, 1999), so it seems likely that the success of computer-based tutoring \x0csystems could be greatly increased if they were capable of predicting and adapting to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Preliminary machine learning experiments involving emotion annotation and automatic feature extraction from our corpus suggest that ITSPOKE can indeed be enhanced to automatically predict and adapt to student emotional states (Litman et al., 2003). 3 Typed Human-Human Tutoring Corpus The Why2-Atlas Human-Human Typed Tutoring Corpus is a collection of typed tutoring dialogues between (human) tutor and student collected via typed interface, which the tutor plays the same role that Why2-Atlas is designed to perform. The experimental procedure is as follows: 1) students are given a pretest measu</context>
</contexts>
<marker>Evens, 2002</marker>
<rawString>M. Evens. 2002. New questions for Circsim-Tutor. Presentation at the 2002 Symposium on Natural Language Tutoring, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fry</author>
<author>M Ginzton</author>
<author>S Peters</author>
<author>B Clark</author>
<author>H Pon-Barry</author>
</authors>
<title>Automated tutoring dialogues for training in shipboard damage control.</title>
<date>2001</date>
<booktitle>In Proc. 2nd SigDial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="4284" citStr="Fry et al., 2001" startWordPosition="663" endWordPosition="666">gue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its back-end. These systems and their goals will be discussed in Se</context>
</contexts>
<marker>Fry, Ginzton, Peters, Clark, Pon-Barry, 2001</marker>
<rawString>J. Fry, M. Ginzton, S. Peters, B. Clark, and H. Pon-Barry. 2001. Automated tutoring dialogues for training in shipboard damage control. In Proc. 2nd SigDial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Graesser</author>
<author>N Person</author>
<author>D Harter</author>
</authors>
<title>Teaching tactics and dialog in Autotutor.</title>
<date>2001</date>
<journal>International Journal of Artificial Intelligence in Education.</journal>
<contexts>
<context position="3610" citStr="Graesser et al., 2001" startWordPosition="566" endWordPosition="569">isk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-expl</context>
</contexts>
<marker>Graesser, Person, Harter, 2001</marker>
<rawString>A. Graesser, N. Person, and D. Harter et al. 2001a. Teaching tactics and dialog in Autotutor. International Journal of Artificial Intelligence in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Graesser</author>
<author>K Vanlehn</author>
<author>TRG</author>
<author>NLT Group</author>
</authors>
<title>Why2 report: Evaluation of why/atlas, why/autotutor, and accomplished human tutors on learning gains for qualitative physics problems and explanations.</title>
<date>2002</date>
<tech>Technical report, LRDC Tech Report,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="7516" citStr="Graesser et al., 2002" startWordPosition="1170" endWordPosition="1173">that the tutors expected students to include in their essays) and potential misconceptions associated with each question. Additionally, they agreed on an ideal essay answer for each problem. In Why2- Atlas, a student first types an essay answering a qualitative physics problem. A computer tutor then engages the student in a natural language dialogue to provide feedback, correct misconceptions, and to elicit more complete explanations. The first version of Why2-Atlas was deployed and evaluated with undergraduate students in the spring of 2002; the system is continuing to be actively developed (Graesser et al., 2002). We are currently developing a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its back-end. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Rose, 2000), discourse and domain l</context>
</contexts>
<marker>Graesser, Vanlehn, TRG, Group, 2002</marker>
<rawString>A. Graesser, K. Vanlehn, TRG, and NLT Group. 2002. Why2 report: Evaluation of why/atlas, why/autotutor, and accomplished human tutors on learning gains for qualitative physics problems and explanations. Technical report, LRDC Tech Report, University of Pittsburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R R Hake</author>
</authors>
<title>Interactive-engagement versus traditional methods: A six-thousand student survey of mechanics test data for introductory physics students.</title>
<date>1998</date>
<journal>American Journal of Physics,</journal>
<volume>66</volume>
<issue>64</issue>
<contexts>
<context position="6662" citStr="Hake, 1998" startWordPosition="1037" endWordPosition="1038">sing Why2-Atlas, the activity in which students engage is answering deep reasoning questions involving topics in conceptual physics. One such question that we used is, A lightweight car and a massive truck have a head-on collision. On which vehicle is the impact force greater? Which vehicle undergoes the greater change in its motion? Explain. This is an appropriate task domain for pursuing questions about the benefits of tutorial dialogue for learning because questions like this one are known to elicit robust, persistent misconceptions from students, such as heavier objects exert more force. (Hake, 1998; Halloun and Hestenes, 1985). We designed a set of 10 essay questions to use as training problems. Two physics professors and a computer science professor worked together to select a set of expectations (i.e., correct propositions that the tutors expected students to include in their essays) and potential misconceptions associated with each question. Additionally, they agreed on an ideal essay answer for each problem. In Why2- Atlas, a student first types an essay answering a qualitative physics problem. A computer tutor then engages the student in a natural language dialogue to provide feedb</context>
</contexts>
<marker>Hake, 1998</marker>
<rawString>R. R. Hake. 1998. Interactive-engagement versus traditional methods: A six-thousand student survey of mechanics test data for introductory physics students. American Journal of Physics, 66(64).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Halloun</author>
<author>D Hestenes</author>
</authors>
<title>The initial knowledge state of college physics students.</title>
<date>1985</date>
<journal>American Journal of Physics,</journal>
<volume>53</volume>
<issue>11</issue>
<contexts>
<context position="6691" citStr="Halloun and Hestenes, 1985" startWordPosition="1039" endWordPosition="1042">las, the activity in which students engage is answering deep reasoning questions involving topics in conceptual physics. One such question that we used is, A lightweight car and a massive truck have a head-on collision. On which vehicle is the impact force greater? Which vehicle undergoes the greater change in its motion? Explain. This is an appropriate task domain for pursuing questions about the benefits of tutorial dialogue for learning because questions like this one are known to elicit robust, persistent misconceptions from students, such as heavier objects exert more force. (Hake, 1998; Halloun and Hestenes, 1985). We designed a set of 10 essay questions to use as training problems. Two physics professors and a computer science professor worked together to select a set of expectations (i.e., correct propositions that the tutors expected students to include in their essays) and potential misconceptions associated with each question. Additionally, they agreed on an ideal essay answer for each problem. In Why2- Atlas, a student first types an essay answering a qualitative physics problem. A computer tutor then engages the student in a natural language dialogue to provide feedback, correct misconceptions, </context>
</contexts>
<marker>Halloun, Hestenes, 1985</marker>
<rawString>I. A. Halloun and D. Hestenes. 1985. The initial knowledge state of college physics students. American Journal of Physics, 53(11):10431055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Hausmann</author>
<author>Michelene Chi</author>
</authors>
<title>Can a computer interface support self-explaining?</title>
<date>2002</date>
<journal>The International Journal of Cognitive Technology,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="807" citStr="Hausmann and Chi, 2002" startWordPosition="111" endWordPosition="114">VanLehn Learning Research and Development Center, University of Pittsburgh, 3939 OHara St., Pittsburgh, PA 15260 rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edu litman,vanlehn@cs.pitt.edu Abstract This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring. Most current tutorial dialogue systems are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). However, prior studies have shown considerable benefits of tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994; Hausmann and Chi, 2002). Thus, we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics (VanLehn et al., 2002) as its back-end. In order to explore the relative effectiveness between these two input modalities in our task domain, we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring. In both cases, students interact with the tutor through a web interface. We present here a comparison between the two on a number of features of dialogue that have been demonstrated to correlate reliably with learn</context>
<context position="3947" citStr="Hausmann and Chi, 2002" startWordPosition="615" endWordPosition="618">re text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states </context>
<context position="9174" citStr="Hausmann and Chi, 2002" startWordPosition="1424" endWordPosition="1428">domain. For example, we have developed a set of dialogue dependent language models using the experimental human-computer typed corpus (4551 student utterances) obtained during the Why2-Atlas 2002 evaluation. Our language models will soon be enhanced using student utterances from our parallel human-human spoken language corpus. One goal of the ITSPOKE system is simply replacing text based dialogue interaction with spoken dialogue interaction and leaving the rest of the Why2-Atlas back-end unchanged, in order to test the hypothesis that student self-explanation (which leads to greater learning (Hausmann and Chi, 2002)) might be easier to achieve in spoken dialogues. This hypothesis is discussed further in Section 5. Although not the focus of this paper, another goal of the ITSPOKE system is to take full advantage of the speech modality. For example, speech contains rich acoustic and prosodic information about the speakers current emotional state that isnt present in typed dialogue. Connections between learning and emotion have been well documented (Coles, 1999), so it seems likely that the success of computer-based tutoring \x0csystems could be greatly increased if they were capable of predicting and adapt</context>
</contexts>
<marker>Hausmann, Chi, 2002</marker>
<rawString>Robert Hausmann and Michelene Chi. 2002. Can a computer interface support self-explaining? The International Journal of Cognitive Technology, 7(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N T Heffernan</author>
<author>K R Koedinger</author>
</authors>
<title>An intelligent tutoring system incorporating a model of an experienced tutor.</title>
<date>2002</date>
<booktitle>In Proceedings of the Intelligent Tutoring Systems Conference,</booktitle>
<pages>596608</pages>
<contexts>
<context position="3566" citStr="Heffernan and Koedinger, 2002" startWordPosition="558" endWordPosition="561"> who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive r</context>
</contexts>
<marker>Heffernan, Koedinger, 2002</marker>
<rawString>N. T. Heffernan and K. R. Koedinger. 2002. An intelligent tutoring system incorporating a model of an experienced tutor. In Proceedings of the Intelligent Tutoring Systems Conference, pages 596608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X D Huang</author>
<author>F Alleva</author>
<author>H W Hon</author>
<author>M Y Hwang</author>
<author>K F Lee</author>
<author>R Rosenfeld</author>
</authors>
<title>The SphinxII speech recognition system: An overview.</title>
<date>1993</date>
<journal>Computer, Speech and Language.</journal>
<contexts>
<context position="7774" citStr="Huang et al., 1993" startWordPosition="1208" endWordPosition="1211"> physics problem. A computer tutor then engages the student in a natural language dialogue to provide feedback, correct misconceptions, and to elicit more complete explanations. The first version of Why2-Atlas was deployed and evaluated with undergraduate students in the spring of 2002; the system is continuing to be actively developed (Graesser et al., 2002). We are currently developing a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its back-end. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Rose, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Rose et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutors synthesized speech is</context>
</contexts>
<marker>Huang, Alleva, Hon, Hwang, Lee, Rosenfeld, 1993</marker>
<rawString>X. D. Huang, F. Alleva, H. W. Hon, M. Y. Hwang, K. F. Lee, and R. Rosenfeld. 1993. The SphinxII speech recognition system: An overview. Computer, Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Lemke</author>
</authors>
<title>Talking Science: Language, Learning and Values. Ablex,</title>
<date>1990</date>
<location>Norwood, NJ.</location>
<contexts>
<context position="764" citStr="Lemke, 1990" startWordPosition="105" endWordPosition="106">liman, Ramesh Srivastava, Kurt VanLehn Learning Research and Development Center, University of Pittsburgh, 3939 OHara St., Pittsburgh, PA 15260 rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edu litman,vanlehn@cs.pitt.edu Abstract This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring. Most current tutorial dialogue systems are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). However, prior studies have shown considerable benefits of tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994; Hausmann and Chi, 2002). Thus, we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics (VanLehn et al., 2002) as its back-end. In order to explore the relative effectiveness between these two input modalities in our task domain, we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring. In both cases, students interact with the tutor through a web interface. We present here a comparison between the two on a number of features of dialogue that have been de</context>
<context position="2210" citStr="Lemke, 1990" startWordPosition="338" endWordPosition="339">ctiveness of speech versus text based tutorial dialogue systems. Tutorial dialogue is a natural way to provide students with a learning environment that exhibits characteristics that have been shown to correlate with student learning gains, such as student activity. For example, it has been demonstrated that generating words rather than simply reading them promotes subsequent recall of those words (Slamecka and Graf, 1978). (Chi et al., 1994) notes that there is a general momentum in the science education literature toward the importance of talking, reflecting and explaining as ways to learn (Lemke, 1990). Moreover, encouraging student selfexplanation, which includes both generating inferences from material they have read and relating new material to old material, has been shown to correlate with learning (Chi et al., 1981; Chi et al., 1994; Renkl, 1997; Pressley et al., 1992). In a further study, prompting students with zero content prompts to encourage them to selfexplain was also associated with student learning (Chi et al., 2001). A second important advantage to dialogue is that it affords the tutor the opportunity to tailor instruction to the needs of the student. While human tutors may n</context>
<context position="3902" citStr="Lemke, 1990" startWordPosition="609" endWordPosition="610">d more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve</context>
</contexts>
<marker>Lemke, 1990</marker>
<rawString>J. L. Lemke. 1990. Talking Science: Language, Learning and Values. Ablex, Norwood, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>J Hirschberg</author>
<author>M Swerts</author>
</authors>
<title>Predicting user reactions to system error.</title>
<date>2001</date>
<booktitle>In Proc.of ACL.</booktitle>
<contexts>
<context position="4646" citStr="Litman et al., 2001" startWordPosition="719" endWordPosition="722">ken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its back-end. These systems and their goals will be discussed in Section 2. We expect that the different modalities used by these systems (e.g. text based vs speech based) will display interesting differences with respect to the characteristics of dialogue interaction that may determine their relative merits with respect to increasing student performance. Although human-computer data from the speech based system is not yet av</context>
</contexts>
<marker>Litman, Hirschberg, Swerts, 2001</marker>
<rawString>D. Litman, J. Hirschberg, and M. Swerts. 2001. Predicting user reactions to system error. In Proc.of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Litman</author>
<author>Kate Forbes</author>
<author>Scott Silliman</author>
</authors>
<title>Towards emotion prediction in spoken tutoring dialogues.</title>
<date>2003</date>
<note>Submitted.</note>
<contexts>
<context position="10136" citStr="Litman et al., 2003" startWordPosition="1573" endWordPosition="1576">nt present in typed dialogue. Connections between learning and emotion have been well documented (Coles, 1999), so it seems likely that the success of computer-based tutoring \x0csystems could be greatly increased if they were capable of predicting and adapting to student emotional states, e.g. reinforcing positive states, while rectifying negative states (Evens, 2002). Preliminary machine learning experiments involving emotion annotation and automatic feature extraction from our corpus suggest that ITSPOKE can indeed be enhanced to automatically predict and adapt to student emotional states (Litman et al., 2003). 3 Typed Human-Human Tutoring Corpus The Why2-Atlas Human-Human Typed Tutoring Corpus is a collection of typed tutoring dialogues between (human) tutor and student collected via typed interface, which the tutor plays the same role that Why2-Atlas is designed to perform. The experimental procedure is as follows: 1) students are given a pretest measuring their knowledge of physics, 2) students are asked to read through a small document of background material, 3) students work through a set of up to 10 Why2-Atlas training problems with the human tutor, and 4) students are given a post-test that </context>
</contexts>
<marker>Litman, Forbes, Silliman, 2003</marker>
<rawString>Diane Litman, Kate Forbes, and Scott Silliman. 2003. Towards emotion prediction in spoken tutoring dialogues. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Makatchev</author>
<author>P Jordan</author>
<author>K VanLehn</author>
</authors>
<title>Discourse processing for explanatory essays in tutorial applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="8156" citStr="Makatchev et al., 2002" startWordPosition="1263" endWordPosition="1266">y developing a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its back-end. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Rose, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Rose et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutors synthesized speech is played to the student using a speaker and/or headphone. We are now in the process of adapting the knowledge sources needed by the spoken language components to our application domain. For example, we have developed a set of dialogue dependent language models using the experimental human-computer typed corpus (4551 student utterances) obtained during the Why2-Atlas 2002 evaluatio</context>
</contexts>
<marker>Makatchev, Jordan, VanLehn, 2002</marker>
<rawString>M. Makatchev, P. Jordan, and K. VanLehn. 2002. Discourse processing for explanatory essays in tutorial applications. In Proceedings of the 3rd SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mostow</author>
<author>G Aist</author>
</authors>
<title>Evaluating tutors that listen: An overview of Project LISTEN.</title>
<date>2001</date>
<booktitle>Smart Machines in Education.</booktitle>
<editor>In K. Forbus and P. Feltovich, editors,</editor>
<contexts>
<context position="4265" citStr="Mostow and Aist, 2001" startWordPosition="659" endWordPosition="662"> current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity and thus reduces the cognitive resources available for spontaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its back-end. These systems and their goals will</context>
</contexts>
<marker>Mostow, Aist, 2001</marker>
<rawString>J. Mostow and G. Aist. 2001. Evaluating tutors that listen: An overview of Project LISTEN. In K. Forbus and P. Feltovich, editors, Smart Machines in Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pressley</author>
<author>E Wood</author>
<author>V E Woloshyn</author>
<author>V Martin</author>
<author>A King</author>
<author>D Menke</author>
</authors>
<title>Encouraging mindful use of prior knowledge: Attempting to construct explanatory answers facilitates learning.</title>
<date>1992</date>
<booktitle>Educational Psychologist,</booktitle>
<pages>27--91109</pages>
<contexts>
<context position="2487" citStr="Pressley et al., 1992" startWordPosition="379" endWordPosition="383">For example, it has been demonstrated that generating words rather than simply reading them promotes subsequent recall of those words (Slamecka and Graf, 1978). (Chi et al., 1994) notes that there is a general momentum in the science education literature toward the importance of talking, reflecting and explaining as ways to learn (Lemke, 1990). Moreover, encouraging student selfexplanation, which includes both generating inferences from material they have read and relating new material to old material, has been shown to correlate with learning (Chi et al., 1981; Chi et al., 1994; Renkl, 1997; Pressley et al., 1992). In a further study, prompting students with zero content prompts to encourage them to selfexplain was also associated with student learning (Chi et al., 2001). A second important advantage to dialogue is that it affords the tutor the opportunity to tailor instruction to the needs of the student. While human tutors may not always choose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor ada</context>
</contexts>
<marker>Pressley, Wood, Woloshyn, Martin, King, Menke, 1992</marker>
<rawString>M. Pressley, E. Wood, V. E. Woloshyn, V. Martin, A. King, and D. Menke. 1992. Encouraging mindful use of prior knowledge: Attempting to construct explanatory answers facilitates learning. Educational Psychologist, 27:91109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Renkl</author>
</authors>
<title>Learning from worked-out examples: A study on individual differences.</title>
<date>1997</date>
<journal>Cognitive Science,</journal>
<volume>21</volume>
<issue>1</issue>
<contexts>
<context position="2463" citStr="Renkl, 1997" startWordPosition="377" endWordPosition="378">nt activity. For example, it has been demonstrated that generating words rather than simply reading them promotes subsequent recall of those words (Slamecka and Graf, 1978). (Chi et al., 1994) notes that there is a general momentum in the science education literature toward the importance of talking, reflecting and explaining as ways to learn (Lemke, 1990). Moreover, encouraging student selfexplanation, which includes both generating inferences from material they have read and relating new material to old material, has been shown to correlate with learning (Chi et al., 1981; Chi et al., 1994; Renkl, 1997; Pressley et al., 1992). In a further study, prompting students with zero content prompts to encourage them to selfexplain was also associated with student learning (Chi et al., 2001). A second important advantage to dialogue is that it affords the tutor the opportunity to tailor instruction to the needs of the student. While human tutors may not always choose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore t</context>
</contexts>
<marker>Renkl, 1997</marker>
<rawString>A. Renkl. 1997. Learning from worked-out examples: A study on individual differences. Cognitive Science, 21(1):129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
<author>V Aleven</author>
</authors>
<title>workshop on empirical methods for tutorial dialogue systems.</title>
<date>2002</date>
<booktitle>Proc. of the ITS</booktitle>
<tech>Technical report,</tech>
<location>San Sebastian, Spain,</location>
<contexts>
<context position="3380" citStr="Rose and Aleven, 2002" startWordPosition="527" endWordPosition="530"> the needs of the student. While human tutors may not always choose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-</context>
</contexts>
<marker>Rose, Aleven, 2002</marker>
<rawString>C. P. Rose and V. Aleven. 2002. Proc. of the ITS 2002 workshop on empirical methods for tutorial dialogue systems. Technical report, San Sebastian, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
<author>J D Moore</author>
<author>K VanLehn</author>
<author>D Allbritton</author>
</authors>
<title>A comparative evaluation of socratic versus didactic tutoring.</title>
<date>2001</date>
<booktitle>In Proceedings of the 23rd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>869874</pages>
<contexts>
<context position="3535" citStr="Rose et al., 2001" startWordPosition="554" endWordPosition="557">ir students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity a</context>
<context position="8213" citStr="Rose et al., 2001" startWordPosition="1272" endWordPosition="1275">SPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its back-end. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Rose, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Rose et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutors synthesized speech is played to the student using a speaker and/or headphone. We are now in the process of adapting the knowledge sources needed by the spoken language components to our application domain. For example, we have developed a set of dialogue dependent language models using the experimental human-computer typed corpus (4551 student utterances) obtained during the Why2-Atlas 2002 evaluation. Our language models will soon be enhanced using studen</context>
<context position="12542" citStr="Rose et al., 2001" startWordPosition="1976" endWordPosition="1979"> Figure 1. The tutor was instructed to cover the expectations for each problem, to watch for the specific set of expectations and misconceptions associated with the problem, and to end the discussion of each problem by showing the ideal essay to the student. He was encouraged to avoid lecturing the student and to attempt to draw out the students own reasoning. He knew that transcripts of his tutoring would be analyzed. Nevertheless, he was not required to follow any prescribed tutoring strategies. So his tutoring style was much more naturalistic than in previous studies such as the BEE study (Rose et al., 2001) in which two specific tutoring styles, namely Socratic and Didactic, were contrasted. The results of that study revealed a trend for students in the Socratic condition to learn more than those in the Didactic condition. A further analysis of the corpus collected during the BEE study (Core et al., 2002) verified that the Socratic dialogues from the BEE study were more interactive than the Didactic ones. The biggest reliable difference between the two sets of tutoring dialogues was the percentage of words spoken by the student, i.e, number of student words divided by total number of words. The </context>
</contexts>
<marker>Rose, Moore, VanLehn, Allbritton, 2001</marker>
<rawString>C. P. Rose, J. D. Moore, K. VanLehn, and D. Allbritton. 2001. A comparative evaluation of socratic versus didactic tutoring. In Proceedings of the 23rd Annual Conference of the Cognitive Science Society, pages 869874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
<author>P Jordan</author>
<author>M Ringenberg</author>
<author>S Siler</author>
<author>K VanLehn</author>
<author>A Weinstein</author>
</authors>
<title>Interactive conceptual tutoring in atlas-andes.</title>
<date>2001</date>
<booktitle>In Proceedings of Artificial Intelligence in Education,</booktitle>
<pages>256266</pages>
<contexts>
<context position="3535" citStr="Rose et al., 2001" startWordPosition="554" endWordPosition="557">ir students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then in \x0ctext based tutoring, suggesting that typing requires additional cognitive capacity a</context>
<context position="8213" citStr="Rose et al., 2001" startWordPosition="1272" endWordPosition="1275">SPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its back-end. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Rose, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Rose et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutors synthesized speech is played to the student using a speaker and/or headphone. We are now in the process of adapting the knowledge sources needed by the spoken language components to our application domain. For example, we have developed a set of dialogue dependent language models using the experimental human-computer typed corpus (4551 student utterances) obtained during the Why2-Atlas 2002 evaluation. Our language models will soon be enhanced using studen</context>
<context position="12542" citStr="Rose et al., 2001" startWordPosition="1976" endWordPosition="1979"> Figure 1. The tutor was instructed to cover the expectations for each problem, to watch for the specific set of expectations and misconceptions associated with the problem, and to end the discussion of each problem by showing the ideal essay to the student. He was encouraged to avoid lecturing the student and to attempt to draw out the students own reasoning. He knew that transcripts of his tutoring would be analyzed. Nevertheless, he was not required to follow any prescribed tutoring strategies. So his tutoring style was much more naturalistic than in previous studies such as the BEE study (Rose et al., 2001) in which two specific tutoring styles, namely Socratic and Didactic, were contrasted. The results of that study revealed a trend for students in the Socratic condition to learn more than those in the Didactic condition. A further analysis of the corpus collected during the BEE study (Core et al., 2002) verified that the Socratic dialogues from the BEE study were more interactive than the Didactic ones. The biggest reliable difference between the two sets of tutoring dialogues was the percentage of words spoken by the student, i.e, number of student words divided by total number of words. The </context>
</contexts>
<marker>Rose, Jordan, Ringenberg, Siler, VanLehn, Weinstein, 2001</marker>
<rawString>C. P. Rose, P. Jordan, M. Ringenberg, S. Siler, K. VanLehn, and A. Weinstein. 2001. Interactive conceptual tutoring in atlas-andes. In Proceedings of Artificial Intelligence in Education, pages 256266.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C P Rose</author>
<author>D Bhembe</author>
<author>A Roque</author>
<author>S Siler</author>
<author>R Srivastava</author>
<author>K Vanlehn</author>
</authors>
<title>A hybrid language understanding approach for robust selection of tutoring goals.</title>
<date>2002</date>
<booktitle>In Proceedings of the Intelligent Tutoring Systems Conference,</booktitle>
<pages>552561</pages>
<contexts>
<context position="5795" citStr="Rose et al., 2002" startWordPosition="899" endWordPosition="902">hough human-computer data from the speech based system is not yet available for comparison, we have collected parallel human-human corpora both for text based and speech based tutoring, as discussed in Sections 3-4, and these corpora already display similarities and differences with respect to features of their dialogue interactions, as discussed in Section 5, that are wholly modality based and that will likely be displayed to an even greater extent in the comparable human-computer data. 2 Why2-Atlas and ITSPOKE Dialogue Systems Why2-Atlas is a text based intelligent tutoring dialogue system (Rose et al., 2002a; VanLehn et al., 2002). The goal of Why2-Atlas is to provide a platform for testing whether deep approaches to natural language processing elicit more learning than shallower approaches, for the task domain of qualitative physics explanation generation. Using Why2-Atlas, the activity in which students engage is answering deep reasoning questions involving topics in conceptual physics. One such question that we used is, A lightweight car and a massive truck have a head-on collision. On which vehicle is the impact force greater? Which vehicle undergoes the greater change in its motion? Explain</context>
</contexts>
<marker>Rose, Bhembe, Roque, Siler, Srivastava, Vanlehn, 2002</marker>
<rawString>C. P. Rose, D. Bhembe, A. Roque, S. Siler, R. Srivastava, and K. Vanlehn. 2002a. A hybrid language understanding approach for robust selection of tutoring goals. In Proceedings of the Intelligent Tutoring Systems Conference, pages 552561. C. P. Rose, K. VanLehn, and The Natural Language Tutoring Group. Submitted. Is human tutoring always more effective than reading. In Annual Meeting of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Rose</author>
</authors>
<title>A framework for robust sentence level interpretation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>11291135</pages>
<contexts>
<context position="8092" citStr="Rose, 2000" startWordPosition="1256" endWordPosition="1257">y developed (Graesser et al., 2002). We are currently developing a speech-enabled version of Why2-ATLAS, called ITSPOKE (Intelligent Tutoring SPOKEn dialogue system), that uses the Why2-Atlas system as its back-end. To date we have interfaced the Sphinx2 speech recognizer (Huang et al., 1993) with stochastic language models trained from example user utterances, and the Festival speech synthesizer (Black and Taylor, 1997) for text-to-speech, to the Why2-Atlas backend. The rest of the needed natural language processing components, e.g. the sentence-level syntactic and semantic analysis modules (Rose, 2000), discourse and domain level processors (Makatchev et al., 2002), and a finite-state dialogue manager (Rose et al., 2001), are provided by a toolkit that is part of the Why2-Atlas backend. The student speech is digitized from microphone input, while the tutors synthesized speech is played to the student using a speaker and/or headphone. We are now in the process of adapting the knowledge sources needed by the spoken language components to our application domain. For example, we have developed a set of dialogue dependent language models using the experimental human-computer typed corpus (4551 s</context>
</contexts>
<marker>Rose, 2000</marker>
<rawString>C. P. Rose. 2000. A framework for robust sentence level interpretation. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, pages 11291135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Slamecka</author>
<author>P Graf</author>
</authors>
<title>The generation effect: Delineation of a phenomenon.</title>
<date>1978</date>
<journal>Journal of Experimental Psychology: Human Learning and Memory,</journal>
<pages>4--592604</pages>
<contexts>
<context position="2024" citStr="Slamecka and Graf, 1978" startWordPosition="306" endWordPosition="309">h learning gains with students interacting with the tutor using the text based interface (Rose et al., submitted). 1 Introduction This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutorial dialogue systems. Tutorial dialogue is a natural way to provide students with a learning environment that exhibits characteristics that have been shown to correlate with student learning gains, such as student activity. For example, it has been demonstrated that generating words rather than simply reading them promotes subsequent recall of those words (Slamecka and Graf, 1978). (Chi et al., 1994) notes that there is a general momentum in the science education literature toward the importance of talking, reflecting and explaining as ways to learn (Lemke, 1990). Moreover, encouraging student selfexplanation, which includes both generating inferences from material they have read and relating new material to old material, has been shown to correlate with learning (Chi et al., 1981; Chi et al., 1994; Renkl, 1997; Pressley et al., 1992). In a further study, prompting students with zero content prompts to encourage them to selfexplain was also associated with student lear</context>
</contexts>
<marker>Slamecka, Graf, 1978</marker>
<rawString>N. J. Slamecka and P. Graf. 1978. The generation effect: Delineation of a phenomenon. Journal of Experimental Psychology: Human Learning and Memory, (4):592604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>P Jordan</author>
<author>C Rose</author>
<author>D Bhembe</author>
<author>M Bottner</author>
<author>A Gaydos</author>
<author>M Makatchev</author>
<author>U Pappuswamy</author>
<author>M Ringenberg</author>
<author>A Roque</author>
<author>S Siler</author>
<author>R Srivastava</author>
<author>R Wilson</author>
</authors>
<title>The architecture of Why2-Atlas: A coach for qualitative physics essay writing.</title>
<date>2002</date>
<booktitle>In Proc. of ITS.</booktitle>
<contexts>
<context position="654" citStr="VanLehn et al., 2002" startWordPosition="88" endWordPosition="91">nt Behavior in Speech Versus Text Based Tutoring Carolyn P. Rose, Diane Litman, Dumisizwe Bhembe, Kate Forbes, Scott Silliman, Ramesh Srivastava, Kurt VanLehn Learning Research and Development Center, University of Pittsburgh, 3939 OHara St., Pittsburgh, PA 15260 rosecp,bhembe,forbesk,scotts,rcsriva@pitt.edu litman,vanlehn@cs.pitt.edu Abstract This paper describes preliminary work in exploring the relative effectiveness of speech versus text based tutoring. Most current tutorial dialogue systems are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). However, prior studies have shown considerable benefits of tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994; Hausmann and Chi, 2002). Thus, we are currently developing a speech based dialogue system that uses a text based system for tutoring conceptual physics (VanLehn et al., 2002) as its back-end. In order to explore the relative effectiveness between these two input modalities in our task domain, we have started by collecting parallel human-human tutoring corpora both for text based and speech based tutoring. In both cases, students interact with the tutor through a web</context>
<context position="3443" citStr="VanLehn et al., 2002" startWordPosition="539" endWordPosition="542">oose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs much more frequently in spoken tutoring then</context>
<context position="4813" citStr="VanLehn et al., 2002" startWordPosition="747" endWordPosition="750">ntaneous self-explanation. Other research projects (Mostow and Aist, 2001; Fry et al., 2001) have shown that basic spoken natural language capabilities can be implemented quite effectively in computer tutoring systems. Moreover, speech contains prosodic and acoustic information which has been shown to improve the accuracy of predicting emotional states (Ang et al., 2002; Batliner et al., 2000) and user responses to system errors (Litman et al., 2001) that are useful for triggering system adaptation. We are thus currently developing a speech based dialogue system that uses a text based system (VanLehn et al., 2002) as its back-end. These systems and their goals will be discussed in Section 2. We expect that the different modalities used by these systems (e.g. text based vs speech based) will display interesting differences with respect to the characteristics of dialogue interaction that may determine their relative merits with respect to increasing student performance. Although human-computer data from the speech based system is not yet available for comparison, we have collected parallel human-human corpora both for text based and speech based tutoring, as discussed in Sections 3-4, and these corpora a</context>
</contexts>
<marker>VanLehn, Jordan, Rose, Bhembe, Bottner, Gaydos, Makatchev, Pappuswamy, Ringenberg, Roque, Siler, Srivastava, Wilson, 2002</marker>
<rawString>K. VanLehn, P. Jordan, C. Rose, D. Bhembe, M. Bottner, A. Gaydos, M. Makatchev, U. Pappuswamy, M. Ringenberg, A. Roque, S. Siler, R. Srivastava, and R. Wilson. 2002. The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In Proc. of ITS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claus Zinn</author>
<author>Johanna D Moore</author>
<author>Mark G Core</author>
</authors>
<date>2002</date>
<contexts>
<context position="3399" citStr="Zinn et al., 2002" startWordPosition="531" endWordPosition="534">nt. While human tutors may not always choose to tailor their instruction to the individual characteristics of the knowledge state of their students, tutors who ignore signs of student confusion may run the risk of preventing learning (Chi, 1996). (Rose et al., submitted) explore the benefits of tutor adaptation by comparing learning gains for naive learners and review learners in a human tutoring condition and a non-adaptive reading condition. In recent years tutorial dialogue systems have become more and more prevalent, most of which are text based (Evens et al., 2001; Rose and Aleven, 2002; Zinn et al., 2002; Aleven et al., 2001; VanLehn et al., 2002). Many of these systems have yielded successful evaluations with students (Rose et al., 2001; Heffernan and Koedinger, 2002; Ashley et al., 2002; Graesser et al., 2001a). However, while the majority of current tutorial dialogue systems are text based, there is reason to believe that speech based tutorial dialogue systems could be more effective. Prior studies have shown considerable benefits of human-human tutoring through spoken interactions (Lemke, 1990; Chi et al., 1994). (Hausmann and Chi, 2002) has shown that spontaneous self-explanation occurs </context>
</contexts>
<marker>Zinn, Moore, Core, 2002</marker>
<rawString>Claus Zinn, Johanna D. Moore, and Mark G. Core. 2002.</rawString>
</citation>
<citation valid="true">
<title>A 3-tier planning architecture for managing tutorial dialogue.</title>
<date></date>
<booktitle>In Proceedings Intelligent Tutoring Systems, Sixth International Conference (ITS 2002),</booktitle>
<location>Biarritz, France,</location>
<marker></marker>
<rawString>A 3-tier planning architecture for managing tutorial dialogue. In Proceedings Intelligent Tutoring Systems, Sixth International Conference (ITS 2002), Biarritz, France, June. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>