<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000021">
<title confidence="0.865394">
b&apos;The Necessity of Parsing for Predicate Argument Recognition
</title>
<author confidence="0.986289">
Daniel Gildea and Martha Palmer
</author>
<affiliation confidence="0.998962">
University of Pennsylvania
</affiliation>
<email confidence="0.99695">
dgildea,mpalmer@cis.upenn.edu
</email>
<sectionHeader confidence="0.990523" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994966214285714">
Broad-coverage corpora annotated with
semantic role, or argument structure, in-
formation are becoming available for the
\x0crst time. Statistical systems have been
trained to automatically label seman-
tic roles from the output of statistical
parsers on unannotated text. In this pa-
per, we quantify the e\x0bect of parser accu-
racy on these systems\&apos; performance, and
examine the question of whether a
at-
ter \\chunked&quot; representation of the in-
put can be as e\x0bective for the purposes
of semantic role identi\x0ccation.
</bodyText>
<sectionHeader confidence="0.998222" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986239153846154">
Over the past decade, most work in the \x0celd of
information extraction has shifted from complex
rule-based, systems designed to handle a wide
variety of semantic phenomena including quan-
ti\x0ccation, anaphora, aspect and modality (e.g.
Alshawi (1992)), to simpler \x0cnite-state or sta-
tistical systems such as Hobbs et al. (1997) and
Miller et al. (1998). Much of the evaluation of
these systems has been conducted on extracting
relations for speci\x0cc semantic domains such as
corporate acquisitions or terrorist events in the
framework of the DARPA Message Understand-
ing Conferences.
Recently, attention has turned to creating cor-
pora annotated for argument structure for a
broader range of predicates. The Propbank
project at the University of Pennsylvania (Kings-
bury and Palmer, 2002) and the FrameNet project
at the International Computer Science Institute
(Baker et al., 1998) share the goal of document-
ing the syntactic realization of arguments of the
predicates of the general English lexicon by an-
notating a corpus with semantic roles. Even for
a single predicate, semantic arguments often have
multiple syntactic realizations, as shown by the
following paraphrases:
</bodyText>
<listItem confidence="0.859616">
(1) John will meet with Mary.
John will meet Mary.
John and Mary will meet.
(2) The door opened.
</listItem>
<bodyText confidence="0.986230814814815">
Mary opened the door.
Correctly identifying the semantic roles of the
sentence constituents is a crucial part of interpret-
ing text, and in addition to forming an important
part of the information extraction problem, can
serve as an intermediate step in machine trans-
lation or automatic summarization. In this pa-
per, we examine how the information provided by
modern statistical parsers such as Collins (1997)
and Charniak (1997) contributes to solving this
problem. We measure the e\x0bect of parser accu-
racy on semantic role prediction from parse trees,
and determine whether a complete tree is indeed
necessary for accurate role prediction.
Gildea and Jurafsky (2002) describe a statisti-
cal system trained on the data from the FrameNet
project to automatically assign semantic roles.
The system \x0crst passed sentences through an au-
tomatic parser, extracted syntactic features from
the parses, and estimated probabilities for seman-
tic roles from the syntactic and lexical features.
Both training and test sentences were automat-
ically parsed, as no hand-annotated parse trees
were available for the corpus. While the errors
introduced by the parser no doubt negatively af-
fected the results obtained, there was no direct
way of quantifying this e\x0bect. Of the systems
evaluated for the Message Understanding Confer-
ence task, Miller et al. (1998) made use of an inte-
grated syntactic and semantic model producing a
full parse tree, and achieved results comparable to
other systems that did not make use of a complete
parse. As in the FrameNet case, the parser was
not trained on the corpus for which semantic an-
notations were available, and the e\x0bect of better,
or even perfect, parses could not be measured.
One of the di\x0berences between the two semantic
annotation projects is that the sentences chosen
Computational Linguistics (ACL), Philadelphia, July 2002, pp. 239-246.
Proceedings of the 40th Annual Meeting of the Association for
\x0cfor annotation for Propbank are from the same
Wall Street Journal corpus chosen for annotation
for the original Penn Treebank project, and thus
hand-checked syntactic parse trees are available
for the entire dataset. In this paper, we com-
pare the performance of a system based on gold-
standard parses with one using automatically gen-
erated parser output. We also examine whether it
is possible that the additional information con-
tained in a full parse tree is negated by the errors
present in automatic parser output, by testing a
role-labeling system based on a
at or \\chunked&quot;
representation of the input.
</bodyText>
<sectionHeader confidence="0.976024" genericHeader="method">
2 The Data
</sectionHeader>
<bodyText confidence="0.999286550561798">
The results in this paper are primarily derived
from the Propbank corpus, and will be compared
to earlier results from the FrameNet corpus. Be-
fore proceeding to the experiments, this section
will brie
y describe the similarities and di\x0berences
between the two sets of data.
While the goals of the two projects are similar in
many respects, their methodologies are quite dif-
ferent. FrameNet is focused on semantic frames,
which are de\x0cned as schematic representation of
situations involving various participants, props,
and other conceptual roles (Fillmore, 1976). The
project methodology has proceeded on a frame-
by-frame basis, that is by \x0crst choosing a semantic
frame, de\x0cning the frame and its participants or
frame elements, and listing the various lexical
predicates which invoke the frame, and then \x0cnd-
ing example sentences of each predicate in the cor-
pus (the British National Corpus was used) and
annotating each frame element. The example sen-
tences were chosen primarily for coverage of all
the syntactic realizations of the frame elements,
and simple examples of these realizations were
preferred over those involving complex syntactic
structure not immediate relevant to the lexical
predicate itself. From the perspective of an auto-
matic classi\x0ccation system, the overrepresentation
of rare syntactic realizations may cause the system
to perform more poorly than it might on more sta-
tistically representative data. On the other hand,
the exclusion of complex examples may make the
task arti\x0ccially easy. Only sentences where the
lexical predicate was used \\in frame&quot; were anno-
tated. A word with multiple distinct senses would
generally be analyzed as belonging to di\x0berent
frames in each sense, but may only be found in the
FrameNet corpus in the sense for which a frame
has been de\x0cned. It is interesting to note that the
semantic frames are a helpful way of generalizing
between predicates; words in the same frame have
been found frequently to share the same syntactic
argument structure. A more complete description
of the FrameNet project can be found in (Baker
et al., 1998; Johnson et al., 2001), and the rami-
\x0ccations for automatic classi\x0ccation are discussed
more thoroughly in (Gildea and Jurafsky, 2002).
The philosophy of the Propbank project can be
likened to FrameNet without frames. While the
semantic roles of FrameNet are de\x0cned at the level
of the frame, in Propbank, roles are de\x0cned on a
per-predicate basis. The core arguments of each
predicate are simply numbered, while remaining
arguments are given labels such as \\temporal&quot; or
\\locative&quot;. While the two types of label names are
reminiscent of the traditional argument/adjunct
distinction, this is primarily as a convenience in
de\x0cning roles, and no claims are intended as to
optionality or other traditional argument/adjunct
tests. To date, Propbank has addressed only
verbs, where FrameNet includes nouns and ad-
jectives. Propbank\&apos;s annotation process has pro-
ceeded from the most to least common verbs, and
all examples of each verb from the corpus are an-
notated. Thus, the data for each predicate are
statistically representative of the corpus, as are
the frequencies of the predicates themselves. An-
notation takes place with reference to the Penn
Treebank trees  |not only are annotators shown
the trees when analyzing a sentence, they are con-
strained to assign the semantic labels to portions
of the sentence corresponding to nodes in the tree.
Propbank annotators tag all examples of a given
verb, regardless of word sense. The tagging guide-
lines for a verb may contain many \\rolesets&quot;, cor-
responding to word sense at a relatively coarse-
grained level. The need for multiple rolesets is
determined by the roles themselves, that is, uses
of the verb with di\x0berent arguments are given sep-
arate rolesets. However, the preliminary version
of the data used in the experiments below are
not tagged for word sense, or for the roleset used.
Sense tagging is planned for a second pass through
the data. In many cases the roleset can be deter-
mined from the argument annotations themselves.
However, we did not make any attempt to distin-
guish sense in our experiments, and simply at-
tempted to predict argument labels based on the
identity of the lexical predicate.
</bodyText>
<sectionHeader confidence="0.987657" genericHeader="method">
3 The Experiments
</sectionHeader>
<bodyText confidence="0.990404882352941">
In previous work using the FrameNet corpus,
Gildea and Jurafsky (2002) developed a system to
predict semantic roles from sentences and their
parse trees as determined by the statistical parser
of Collins (1997). We will brie
y review their
\x0cprobability model before adapting the system to
handle unparsed data.
Probabilities of a parse constituent belonging
to a given semantic role were calculated from the
following features:
Phrase Type: This feature indicates the syntac-
tic type of the phrase expressing the semantic
roles: examples include noun phrase (NP),
verb phrase (VP), and clause (S). Phrase
types were derived automatically from parse
trees generated by the parser, as shown in
</bodyText>
<figureCaption confidence="0.758184">
Figure 1. The parse constituent spanning
</figureCaption>
<bodyText confidence="0.985584682926829">
each set of words annotated as an argument
was found, and the constituent\&apos;s nonterminal
label was taken as the phrase type. As an
example of how this feature is useful, in com-
munication frames, the Speaker is likely to
appear as a noun phrase, Topic as a prepo-
sitional phrase or noun phrase, and Medium
as a prepositional phrase, as in: \\We talked
about the proposal over the phone.&quot; When
no parse constituent was found with bound-
aries matching those of an argument during
testing, the largest constituent beginning at
the argument\&apos;s left boundary and lying en-
tirely within the element was used to calcu-
late the features.
Parse Tree Path: This feature is designed to
capture the syntactic relation of a constituent
to the predicate. It is de\x0cned as the path
from the predicate through the parse tree
to the constituent in question, represented
as a string of parse tree nonterminals linked
by symbols indicating upward or downward
movement through the tree, as shown in Fig-
ure 2. Although the path is composed as a
string of symbols, our systems will treat the
string as an atomic value. The path includes,
as the \x0crst element of the string, the part of
speech of the predicate, and, as the last ele-
ment, the phrase type or syntactic category
of the sentence constituent marked as an ar-
gument.
Position: This feature simply indicates whether
the constituent to be labeled occurs before
or after the predicate de\x0cning the semantic
frame. This feature is highly correlated with
grammatical function, since subjects will gen-
erally appear before a verb, and objects after.
This feature may overcome the shortcom-
ings of reading grammatical function from the
parse tree, as well as errors in the parser out-
put.
</bodyText>
<figure confidence="0.99700225">
S
NP
VP
NP
He ate some pancakes
PRP
DT NN
VB
</figure>
<figureCaption confidence="0.981287">
Figure 2: In this example, the path from the pred-
</figureCaption>
<bodyText confidence="0.9895321">
icate ate to the argument He can be represented
as VB&quot;VP&quot;S#NP, with &quot; indicating upward move-
ment in the parse tree and # downward movement.
Voice: The distinction between active and pas-
sive verbs plays an important role in the con-
nection between semantic role and grammat-
ical function, since direct objects of active
verbs correspond to subjects of passive verbs.
From the parser output, verbs were classi\x0ced
as active or passive by building a set of 10
passive-identifying patterns. Each of the pat-
terns requires both a passive auxiliary (some
form of \\to be&quot; or \\to get&quot;) and a past par-
ticiple.
Head Word: Lexical dependencies provide im-
portant information in labeling semantic
roles, as one might expect from their use
in statistical models for parsing. Since the
parser used assigns each constituent a head
word as an integral part of the parsing model,
the head words of the constituents can be
read from the parser output. For example, in
a communication frame, noun phrases headed
by \\Bill&quot;, \\brother&quot;, or \\he&quot; are more likely
to be the Speaker, while those headed by
\\proposal&quot;, \\story&quot;, or \\question&quot; are more
likely to be the Topic.
To predict argument roles in new data, we
wish to estimate the probability of each role
given these \x0cve features and the predicate p:
P (rjpt; path; position; voice; hw; p). Due to the
sparsity of the data, it is not possible to estimate
this probability from the counts in the training.
Instead, we estimate probabilities from various
subsets of the features, and interpolate a linear
combination of the resulting distributions. The
interpolation is performed over the most speci\x0cc
distributions for which data are available, which
can be thought of as choosing the topmost distri-
butions available from a backo\x0b lattice, shown in
</bodyText>
<figureCaption confidence="0.946595">
Figure 3.
</figureCaption>
<figure confidence="0.999304290322581">
\x0cHe
PRP
NP
heard
VBD
the sound of liquid slurping in a metal container
NP
as
IN
Farrell
NNP
NP
approached
VBD
him
PRP
NP
from
IN
behind
NN
NP
PP
VP
S
SBAR
VP
S
predicate Source
Goal
Theme
</figure>
<figureCaption confidence="0.754085">
Figure 1: A sample sentence with parser output (above) and argument structure annotation (below).
Parse constituents corresponding to frame elements are highlighted.
</figureCaption>
<equation confidence="0.999772375">
P(r  |h)
P(r  |pt, voice)
P(r  |h, pt, p)
P(r  |pt, voice, p)
P(r  |pt, p)
P(r  |p)
P(r  |pt, path, p)
P(r  |h, p)
</equation>
<figureCaption confidence="0.9088705">
Figure 3: Backo\x0b lattice with more speci\x0cc distri-
butions towards the top.
</figureCaption>
<bodyText confidence="0.99021370212766">
We applied the same system, using the same
features to a preliminary release of the Propbank
data. The dataset used contained annotations for
26,138 predicate-argument structures containing
65,364 individual arguments and containing exam-
ples from 1,527 lexical predicates (types). In order
to provide results comparable with the statistical
parsing literature, annotations from Section 23 of
the Treebank were used as the test set; all other
sections were included in the training set.
The system was tested under two conditions,
one in which it is given the constituents which
are arguments to the predicate and merely has to
predict the correct role, and one in which it has to
both \x0cnd the arguments in the sentence and label
them correctly. Results are shown in Tables 1 and
2.
Although results for Propbank are lower than
for FrameNet, this appears to be primarily due to
the smaller number of training examples for each
predicate, rather than the di\x0berence in annotation
style between the two corpora. The FrameNet
data contained at least ten examples from each
predicate, while 17% of the Propbank data had
fewer than ten training examples. Removing these
examples from the test set gives 84.1% accuracy
with gold-standard parses and 80.5% accuracy
with automatic parses.
As our path feature is a somewhat unusual way
of looking at parse trees, its behavior in the sys-
tem warrants a closer look. The path feature is
most useful as a way of \x0cnding arguments in the
unknown boundary condition. Removing the path
feature from the known-boundary system results
in only a small degradation in performance, from
82.3% to 81.7%. One reason for the relatively
small impact may be sparseness of the feature |
7% of paths in the test set are unseen in training
data. The most common values of the feature are
shown in Table 3, where the \x0crst two rows cor-
respond to standard subject and object positions.
One reason for sparsity is seen in the third row:
in the Treebank, the adjunction of an adverbial
phrase or modal verb can cause an additional VP
node to appear in our path feature. We tried two
variations of the path feature to address this prob-
lem. The \x0crst collapses sequences of nodes with
</bodyText>
<table confidence="0.9158294">
\x0cAccuracy
FrameNet Propbank Propbank
&amp;gt; 10 ex.
Gold-standard parses 82.8 84.1
Automatic parses 82.0 79.2 80.5
</table>
<tableCaption confidence="0.862085">
Table 1: Accuracy of semantic role prediction for known boundaries  |the system is given the con-
stituents to classify.
</tableCaption>
<table confidence="0.994344">
FrameNet Propbank Propbank &amp;gt; 10
Precision Recall Precision Recall Precision Recall
Gold-standard parses 71.1 64.4 73.5 71.7
Automatic parses 64.6 61.2 57.7 50.0 59.0 55.4
</table>
<tableCaption confidence="0.847582">
Table 2: Accuracy of semantic role prediction for unknown boundaries  |the system must identify the
correct constituents as arguments and give them the correct roles.
</tableCaption>
<figure confidence="0.8436053">
Path Frequency
VB&quot;VP#NP 17:6%
VB&quot;VP&quot;S#NP 16:4
VB&quot;VP&quot;VP&quot;S#NP 7:8
VB&quot;VP#PP 7:6
VB&quot;VP#PP#NP 7:3
VB&quot;VP#SBAR#S 4:3
VB&quot;VP#S 4:3
VB&quot;VP#ADVP 2:4
1031 others 76:0
</figure>
<tableCaption confidence="0.87128">
Table 3: Common values for parse tree path in
Propbank data, using gold-standard parses.
</tableCaption>
<bodyText confidence="0.987473307692308">
the same label, for example combining rows 2 and
3 of Table 3. The second variation uses only two
values for the feature: NP under S (subject posi-
tion), and NP under VP (object position). Nei-
ther variation improved performance in the known
boundary condition. As a gauge of how closely the
Propbank argument labels correspond to the path
feature overall, we note that by always assigning
the most common role for each path, for example
always assigning ARG0 to the subject position,
and using no other features, we obtain the correct
role 69.4% of the time, vs. 82.3% for the complete
system.
</bodyText>
<sectionHeader confidence="0.962921" genericHeader="method">
4 Is Parsing Necessary?
</sectionHeader>
<bodyText confidence="0.992830094117647">
Many recent information extraction systems for
limited domains have relied on \x0cnite-state systems
that do not build a full parse tree for the sentence
being analyzed. Among such systems, (Hobbs et
al., 1997) built \x0cnite-state recognizers for vari-
ous entities, which were then cascaded to form
recognizers for higher-level relations, while (Ray
and Craven, 2001) used low-level \\chunks&quot; from
a general-purpose syntactic analyzer as observa-
tions in a trained Hidden Markov Model. Such
an approach has a large advantage in speed, as
the extensive search of modern statistical parsers
is avoided. It is also possible that this approach
may be more robust to error than parsers. Al-
though we expect the attachment decisions made
by a parser to be relevant to determining whether
a constituent of a sentence is an argument of a
particular predicate, and what its relation to the
predicate is, those decisions may be so frequently
incorrect that a much simpler system can do just
as well. In this section we test this hypothesis
by comparing a system which is given only a
at,
\\chunked&quot; representation of the input sentence to
the parse-tree-based systems described above. In
this representation, base-level constituent bound-
aries and labels are present, but there are no de-
pendencies between constituents, as shown by the
following sample sentence:
(3) [NP Big investment banks] [VP refused to
step] [ADVP up] [PP to] [NP the plate]
[VP to support] [NP the beleaguered
oor
traders] [PP by] [VP buying] [NP big blocks]
[PP of] [NP stock] , [NP traders] [VP say] .
Our chunks were derived from the Tree-
bank trees using the conversion described by
Tjong Kim Sang and Buchholz (2000). Thus,
the experiments were carried out using \\gold-
standard&quot; rather than automatically derived
chunk boundaries, which we believe will provide
an upper bound on the performance of a chunk-
based system.
The information provided by the parse tree can
be decomposed into three pieces: the constituent
boundaries, the grammatical relationship between
predicate and argument, expressed by our path
feature, and the head word of each candidate con-
stituent. We will examine the contribution of each
\x0cof these information sources, beginning with the
problem of assigning the correct role in the case
where the boundaries of the arguments in the sen-
tence are known, and then turning to the problem
of \x0cnding arguments in the sentence.
When the argument boundaries are known, the
grammatical relationship of the the constituent
to the predicate turns out to be of little value.
Removing the path feature from the system de-
scribed above results in only a small degradation
in performance, from 82.3% to 81.7%. While the
path feature serves to distinguish subjects from
objects, the combination of the constituent po-
sition before or after the predicate and the ac-
tive/passive voice feature serves the same purpose.
However, this result still makes use of the parser
output for \x0cnding the constituent\&apos;s head word.
We implemented a simple algorithm to guess the
argument\&apos;s head word from the chunked output: if
the argument begins at a chunk boundary, taking
the last word of the chunk, and in all other cases,
taking the \x0crst word of the argument. This heuris-
tic matches the head word read from the parse tree
77% of the the time, as it correctly identi\x0ces the
\x0cnal word of simple noun phrases as the head, the
preposition as the head of prepositional phrases,
and the complementizer as the head of sentential
complements. Using this process for determining
head words, the system drops to 77.0% accuracy,
indicating that identifying the relevant head word
from semantic role prediction is in itself an impor-
tant function of the parser. This chunker-based re-
sult is only slightly lower than the 79.2% obtained
using automatic parses in the known boundary
condition. These results for the known boundary
condition are summarized in Table 4.
</bodyText>
<table confidence="0.2143162">
Path Head Accuracy
gold parse gold parse 82.3
auto parse auto parse 79.2
not used gold parse 81.7
not used chunks 77.0
</table>
<tableCaption confidence="0.78057">
Table 4: Summary of results for known boundary
condition
</tableCaption>
<bodyText confidence="0.999307659090909">
We might expect the information provided by
the parser to be more important in identifying the
arguments in the sentence than in assigning them
the correct role. While it is easy to guess whether
a noun phrase is a subject or object given only
its position relative to the predicate, identifying
complex noun phrases and determining whether
they are arguments of a verb may be more dicult
without the attachment information provided by
the parser.
To test this, we implemented a system in which
the argument labels were assigned to chunks, with
the path feature used by the parse-tree-based sys-
tem replaced by a number expressing the distance
in chunks to the left or right of the predicate.
Of the 3990 arguments in our test set, only
39.8% correspond to a single chunk in the
at-
tened sentence representation, giving an upper
bound to the performance of this system. In par-
ticular, sentential complements (which comprise
11% of the data) and prepositional phrases (which
comprise 10%) always correspond to more than
one chunk, and therefore cannot be correctly la-
beled by our system which assigns roles to single
chunks. In fact, this system achieves 27.6% preci-
sion and 22.0% recall.
In order to see how much of the performance
degradation is caused by the diculty of \x0cnding
exact argument boundaries in the chunked rep-
resentation, we can relax the scoring criteria to
count as correct all cases where the system cor-
rectly identi\x0ces the \x0crst chunk belonging to an
argument. For example, if the system assigns the
correct label to the preposition beginning a prepo-
sitional phrase, the argument will be counted as
correct, even though the system does not \x0cnd
the argument\&apos;s righthand boundary. With this
scoring regime, the chunk-based system performs
at 49.5% precision and 35.1% recall, still signi\x0c-
cantly lower than the 57.7% precision/50.0% recall
for exact matches using automatically generated
parses. Results for the unknown boundary condi-
tion are summarized in Table 5.
</bodyText>
<table confidence="0.992005">
Precision Recall
gold parse 71.1 64.4
auto parse 57.7 50.0
chunk 27.6 22.0
</table>
<tableCaption confidence="0.7614165">
chunk, relaxed scoring 49.5 35.1
Table 5: Summary of results for unknown bound-
</tableCaption>
<bodyText confidence="0.989598837837838">
ary condition
As an example for comparing the behavior of
the tree-based and chunk-based systems, consider
the following sentence, with human annotations
showing the arguments of the predicate support:
(4) [ARG0 Big investment banks] refused to step
up to the plate to support [ARG1 the
beleaguered
oor traders] [MNR by buying
big blocks of stock] , traders say .
Our tree-based system assigned the following anal-
ysis:
(5) Big investment banks refused to step up to
the plate to support [ARG1 the beleaguered
\x0c
oor traders] [MNR by buying big blocks of
stock] , traders say .
In this case, the system failed to \x0cnd the predi-
cate\&apos;s ARG0 relation, because it is syntactically
distant from the verb support. The original Tree-
bank syntactic tree contains a trace which would
allow one to recover this relation, co-indexing the
empty subject position of support with the noun
phrase \\Big investment banks&quot;. However, our
automatic parser output does not include such
traces, nor does our system make use of them.
The chunk-based system assigns the following ar-
gument labels:
(6) Big investment banks refused to step up to
[ARG0 the plate] to support [ARG1 the
beleaguered
oor traders] by buying big
blocks of stock , traders say .
Here, as before, the true ARG0 relation is not
found, and it would be dicult to imagine iden-
tifying it without building a complete syntactic
parse of the sentence. But now, unlike in the
tree-based output, the ARG0 label is mistakenly
attached to a noun phrase immediately before the
predicate. The ARG1 relation in direct object po-
sition is fairly easily identi\x0cable in the chunked
representation as a noun phrase directly follow-
ing the verb. The prepositional phrase expressing
the Manner relation, however, is not identi\x0ced by
the chunk-based system. The tree-based system\&apos;s
path feature for this constituent is VB&quot;VP#PP,
which identi\x0ces the prepositional phrase as at-
taching to the verb, and increases its probability
of being assigned an argument label. The chunk-
based system sees this as a prepositional phrase
appearing as the second chunk after the predi-
cate. Although this may be a typical position for
the Manner relation, the fact that the preposition
attaches to the predicate rather than to its direct
object is not represented.
In interpreting these results, it is important to
keep in mind the di\x0berences between this task
and other information extraction datasets. In
comparison to the domain-speci\x0cc relations eval-
uated by the Message Understanding Conference
(MUC) tasks, we have a wider variety of relations
but fewer training instances for each. The rela-
tions may themselves be less susceptible to \x0cnite
state methods. For example, a named-entity sys-
tem which indenti\x0ces corporation names can go
a long way towards \x0cnding the \\employment&quot; re-
lation of MUC, and similarly systems for tagging
genes and proteins help a great deal for relations
in the biomedical domain. Both Propbank and
FrameNet tend to include longer arguments with
internal syntactic structure, making parsing deci-
sions more important in \x0cnding argument bound-
aries. They also involve abstract relations, with a
wide variety of possible \x0cllers for each role.
</bodyText>
<sectionHeader confidence="0.998343" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99757364516129">
Our chunk-based system takes the last word of
the chunk as its head word for the purposes of
predicting roles, but does not make use of the
identities of the chunk\&apos;s other words or the inter-
vening words between a chunk and the predicate,
unlike Hidden Markov Model-like systems such as
Bikel et al. (1997), McCallum et al. (2000) and
La\x0berty et al. (2001). While a more elaborate
\x0cnite-state system might do better, it is possible
that additional features would not be helpful given
the small amount of data for each predicate. By
using a gold-standard chunking representation, we
have obtained higher performance over what could
be expected from an entirely automatic system
based on a
at representation of the data.
We feel that our results show that statistical
parsers, although computationally expensive, do
a good job of providing relevant information for
semantic interpretation. Not only the constituent
structure but also head word information, pro-
duced as a side product, are important features.
Parsers, however, still have a long way to go.
Our results using hand-annotated parse trees show
that improvements in parsing should translate di-
rectly into better semantic interpretations.
Acknowledgments This work was undertaken
with funding from the Institute for Research in
Cognitive Science at the University of Pennsylva-
nia and from the Propbank project, DoD Grant
MDA904-00C-2136.
</bodyText>
<sectionHeader confidence="0.98573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998523552631579">
Hiyan Alshawi, editor. 1992. The Core Language
Engine. MIT Press, Cambridge, MA.
Collin F. Baker, Charles J. Fillmore, and John B.
Lowe. 1998. The Berkeley FrameNet project.
In Proceedings of COLING/ACL, pages 86{90,
Montreal, Canada.
D. M. Bikel, S. Miller, R. Schwartz, and
R. Weischedel. 1997. Nymble: a high-
performance learning name-\x0cnder. In Proceed-
ings of the Fifth Conference on Applied Natural
Language Processing.
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
AAAI-97, pages 598{603, Menlo Park, August.
AAAI Press.
\x0cMichael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annual Meeting of the ACL,
pages 16{23, Madrid, Spain.
Charles J. Fillmore. 1976. Frame semantics
and the nature of language. In Annals of the
New York Academy of Sciences: Conference on
the Origin and Development of Language and
Speech, volume 280, pages 20{32.
Daniel Gildea and Daniel Jurafsky. 2002. Auto-
matic labeling of semantic roles. Computational
Linguistics, in press.
Jerry R. Hobbs, Douglas Appelt, John Bear,
David Israel, Megumi Kameyama, Mark E.
Stickel, and Mabry Tyson. 1997. FASTUS:
A cascaded \x0cnite-state transducer for extract-
ing information from natural-language text. In
Emmanuel Roche and Yves Schabes, editors,
Finite-State Language Processing, pages 383{
406. MIT Press, Cambridge, MA.
Christopher R. Johnson, Charles J. Fillmore,
Esther J. Wood, Josef Ruppenhofer, Mar-
garet Urban, Miriam R. L. Petruk, and
Collin F. Baker. 2001. The FrameNet
project: Tools for lexicon building. Version 0.7,
http://www.icsi.berkeley.edu/~framenet/book.html.
Paul Kingsbury and Martha Palmer. 2002. From
Treebank to PropBank. In Proceedings of the
3rd International Conference on Language Re-
sources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
John La\x0berty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random \x0celds:
Probabilistic models for segmenting and label-
ing sequence data. In Machine Learning: Pro-
ceedings of the Eighteenth International Confer-
ence (ICML 2001), Stanford, California.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy Markov mod-
els for information extraction and segmenta-
tion. In Machine Learning: Proceedings of the
Seventeenth International Conference (ICML
2000), pages 591{598, Stanford, California.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone,
Ralph Weischedel, and the Annotation Group.
1998. Algorithms that learn to extract informa-
tion { BBN: Description of the SIFT system as
used for MUC-7. In Proceedings of the Seventh
Message Understanding Conference (MUC-7),
April.
Soumya Ray and Mark Craven. 2001. Represent-
ing sentence structure in hidden markov model
for information extraction. In Seventeenth In-
ternational Joint Conference on Arti\x0ccial Intel-
ligence (IJCAI-01), Seattle, Washington.
Erik F. Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the conll-2000 shared
task: Chunking. In Proceedings of CoNLL-2000
and LLL-2000, Lisbon, Portugal.
\x0c&apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963988">
<title confidence="0.99988">b&apos;The Necessity of Parsing for Predicate Argument Recognition</title>
<author confidence="0.999977">Daniel Gildea</author>
<author confidence="0.999977">Martha Palmer</author>
<affiliation confidence="0.999514">University of Pennsylvania</affiliation>
<email confidence="0.999866">dgildea,mpalmer@cis.upenn.edu</email>
<abstract confidence="0.997179733333333">Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the \x0crst time. Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the e\x0bect of parser accuracy on these systems\&apos; performance, and examine the question of whether a atter \\chunked&quot; representation of the input can be as e\x0bective for the purposes of semantic role identi\x0ccation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Core Language Engine.</title>
<date>1992</date>
<editor>Hiyan Alshawi, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="944" citStr="(1992)" startWordPosition="136" endWordPosition="136">matically label semantic roles from the output of statistical parsers on unannotated text. In this paper, we quantify the e\x0bect of parser accuracy on these systems\&apos; performance, and examine the question of whether a atter \\chunked&quot; representation of the input can be as e\x0bective for the purposes of semantic role identi\x0ccation. 1 Introduction Over the past decade, most work in the \x0celd of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quanti\x0ccation, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler \x0cnite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for speci\x0cc semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Scien</context>
</contexts>
<marker>1992</marker>
<rawString>Hiyan Alshawi, editor. 1992. The Core Language Engine. MIT Press, Cambridge, MA. Collin F. Baker, Charles J. Fillmore, and John B.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL,</booktitle>
<pages>86--90</pages>
<location>Montreal, Canada.</location>
<marker>Lowe, 1998</marker>
<rawString>Lowe. 1998. The Berkeley FrameNet project. In Proceedings of COLING/ACL, pages 86{90, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a highperformance learning name-\x0cnder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="27156" citStr="Bikel et al. (1997)" startWordPosition="4413" endWordPosition="4416">relations in the biomedical domain. Both Propbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing decisions more important in \x0cnding argument boundaries. They also involve abstract relations, with a wide variety of possible \x0cllers for each role. 5 Conclusion Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk\&apos;s other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al. (2000) and La\x0berty et al. (2001). While a more elaborate \x0cnite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate. By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a at representation of the data. We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation. Not only the</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: a highperformance learning name-\x0cnder. In Proceedings of the Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In AAAI-97,</booktitle>
<pages>598--603</pages>
<publisher>AAAI Press.</publisher>
<location>Menlo Park,</location>
<contexts>
<context position="2396" citStr="Charniak (1997)" startWordPosition="364" endWordPosition="365">uments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e\x0bect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system \x0crst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were autom</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In AAAI-97, pages 598{603, Menlo Park, August. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cMichael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="2376" citStr="Collins (1997)" startWordPosition="361" endWordPosition="362">icate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e\x0bect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system \x0crst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test </context>
<context position="9055" citStr="Collins (1997)" startWordPosition="1427" endWordPosition="1428">ts below are not tagged for word sense, or for the roleset used. Sense tagging is planned for a second pass through the data. In many cases the roleset can be determined from the argument annotations themselves. However, we did not make any attempt to distinguish sense in our experiments, and simply attempted to predict argument labels based on the identity of the lexical predicate. 3 The Experiments In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). We will brie y review their \x0cprobability model before adapting the system to handle unparsed data. Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S). Phrase types were derived automatically from parse trees generated by the parser, as shown in Figure 1. The parse constituent spanning each set of words annotated as an argument was found, and the constituent\&apos;</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>\x0cMichael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL, pages 16{23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
</authors>
<title>Frame semantics and the nature of language.</title>
<date>1976</date>
<booktitle>In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech,</booktitle>
<volume>280</volume>
<pages>20--32</pages>
<contexts>
<context position="5104" citStr="Fillmore, 1976" startWordPosition="793" endWordPosition="794">unked&quot; representation of the input. 2 The Data The results in this paper are primarily derived from the Propbank corpus, and will be compared to earlier results from the FrameNet corpus. Before proceeding to the experiments, this section will brie y describe the similarities and di\x0berences between the two sets of data. While the goals of the two projects are similar in many respects, their methodologies are quite different. FrameNet is focused on semantic frames, which are de\x0cned as schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore, 1976). The project methodology has proceeded on a frameby-frame basis, that is by \x0crst choosing a semantic frame, de\x0cning the frame and its participants or frame elements, and listing the various lexical predicates which invoke the frame, and then \x0cnding example sentences of each predicate in the corpus (the British National Corpus was used) and annotating each frame element. The example sentences were chosen primarily for coverage of all the syntactic realizations of the frame elements, and simple examples of these realizations were preferred over those involving complex syntactic structu</context>
</contexts>
<marker>Fillmore, 1976</marker>
<rawString>Charles J. Fillmore. 1976. Frame semantics and the nature of language. In Annals of the New York Academy of Sciences: Conference on the Origin and Development of Language and Speech, volume 280, pages 20{32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics, in</journal>
<publisher>E.</publisher>
<location>Douglas Appelt, John Bear, David Israel, Megumi Kameyama, Mark</location>
<contexts>
<context position="2637" citStr="Gildea and Jurafsky (2002)" startWordPosition="399" endWordPosition="402"> the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. In this paper, we examine how the information provided by modern statistical parsers such as Collins (1997) and Charniak (1997) contributes to solving this problem. We measure the e\x0bect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction. Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. The system \x0crst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this e\x0bect. Of the system</context>
<context position="6794" citStr="Gildea and Jurafsky, 2002" startWordPosition="1058" endWordPosition="1061">iple distinct senses would generally be analyzed as belonging to di\x0berent frames in each sense, but may only be found in the FrameNet corpus in the sense for which a frame has been de\x0cned. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure. A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the rami\x0ccations for automatic classi\x0ccation are discussed more thoroughly in (Gildea and Jurafsky, 2002). The philosophy of the Propbank project can be likened to FrameNet without frames. While the semantic roles of FrameNet are de\x0cned at the level of the frame, in Propbank, roles are de\x0cned on a per-predicate basis. The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as \\temporal&quot; or \\locative&quot;. While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is primarily as a convenience in de\x0cning roles, and no claims are intended as to optionality or other traditional argument/adjunct tes</context>
<context position="8915" citStr="Gildea and Jurafsky (2002)" startWordPosition="1403" endWordPosition="1406">es, that is, uses of the verb with di\x0berent arguments are given separate rolesets. However, the preliminary version of the data used in the experiments below are not tagged for word sense, or for the roleset used. Sense tagging is planned for a second pass through the data. In many cases the roleset can be determined from the argument annotations themselves. However, we did not make any attempt to distinguish sense in our experiments, and simply attempted to predict argument labels based on the identity of the lexical predicate. 3 The Experiments In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). We will brie y review their \x0cprobability model before adapting the system to handle unparsed data. Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S). Phrase types were derived automatically from parse trees generated by </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, in press. Jerry R. Hobbs, Douglas Appelt, John Bear, David Israel, Megumi Kameyama, Mark E.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stickel</author>
<author>Mabry Tyson</author>
</authors>
<title>FASTUS: A cascaded \x0cnite-state transducer for extracting information from natural-language text.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing,</booktitle>
<pages>383--406</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Stickel, Tyson, 1997</marker>
<rawString>Stickel, and Mabry Tyson. 1997. FASTUS: A cascaded \x0cnite-state transducer for extracting information from natural-language text. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, pages 383{ 406. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher R Johnson</author>
<author>Charles J Fillmore</author>
<author>Esther J Wood</author>
<author>Josef Ruppenhofer</author>
<author>Margaret Urban</author>
<author>Miriam R L Petruk</author>
<author>Collin F Baker</author>
</authors>
<title>The FrameNet project: Tools for lexicon building.</title>
<date>2001</date>
<journal>Version</journal>
<volume>0</volume>
<pages>http://www.icsi.berkeley.edu/~framenet/book.html.</pages>
<contexts>
<context position="6677" citStr="Johnson et al., 2001" startWordPosition="1042" endWordPosition="1045">x0ccially easy. Only sentences where the lexical predicate was used \\in frame&quot; were annotated. A word with multiple distinct senses would generally be analyzed as belonging to di\x0berent frames in each sense, but may only be found in the FrameNet corpus in the sense for which a frame has been de\x0cned. It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure. A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the rami\x0ccations for automatic classi\x0ccation are discussed more thoroughly in (Gildea and Jurafsky, 2002). The philosophy of the Propbank project can be likened to FrameNet without frames. While the semantic roles of FrameNet are de\x0cned at the level of the frame, in Propbank, roles are de\x0cned on a per-predicate basis. The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as \\temporal&quot; or \\locative&quot;. While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is primarily as a co</context>
</contexts>
<marker>Johnson, Fillmore, Wood, Ruppenhofer, Urban, Petruk, Baker, 2001</marker>
<rawString>Christopher R. Johnson, Charles J. Fillmore, Esther J. Wood, Josef Ruppenhofer, Margaret Urban, Miriam R. L. Petruk, and Collin F. Baker. 2001. The FrameNet project: Tools for lexicon building. Version 0.7, http://www.icsi.berkeley.edu/~framenet/book.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<location>Las Palmas, Canary Islands,</location>
<contexts>
<context position="1483" citStr="Kingsbury and Palmer, 2002" startWordPosition="215" endWordPosition="219">phenomena including quanti\x0ccation, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler \x0cnite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for speci\x0cc semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments of the predicates of the general English lexicon by annotating a corpus with semantic roles. Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: (1) John will meet with Mary. John will meet Mary. John and Mary will meet. (2) The door opened. Mary opened the door. Correctly identifying the semantic roles of the sentence constituents is a crucial part of in</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John La\x0berty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random \x0celds: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Machine Learning: Proceedings of the Eighteenth International Conference (ICML 2001),</booktitle>
<location>Stanford, California.</location>
<marker>La\x0berty, McCallum, Pereira, 2001</marker>
<rawString>John La\x0berty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random \x0celds: Probabilistic models for segmenting and labeling sequence data. In Machine Learning: Proceedings of the Eighteenth International Conference (ICML 2001), Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum entropy Markov models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Machine Learning: Proceedings of the Seventeenth International Conference (ICML</booktitle>
<pages>591--598</pages>
<location>Stanford, California.</location>
<contexts>
<context position="27180" citStr="McCallum et al. (2000)" startWordPosition="4417" endWordPosition="4420">edical domain. Both Propbank and FrameNet tend to include longer arguments with internal syntactic structure, making parsing decisions more important in \x0cnding argument boundaries. They also involve abstract relations, with a wide variety of possible \x0cllers for each role. 5 Conclusion Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk\&apos;s other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al. (2000) and La\x0berty et al. (2001). While a more elaborate \x0cnite-state system might do better, it is possible that additional features would not be helpful given the small amount of data for each predicate. By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a at representation of the data. We feel that our results show that statistical parsers, although computationally expensive, do a good job of providing relevant information for semantic interpretation. Not only the constituent structure b</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. In Machine Learning: Proceedings of the Seventeenth International Conference (ICML 2000), pages 591{598, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Richard Schwartz</author>
<author>Rebecca Stone</author>
<author>Ralph Weischedel</author>
</authors>
<title>and the Annotation Group.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<contexts>
<context position="1048" citStr="Miller et al. (1998)" startWordPosition="151" endWordPosition="154"> In this paper, we quantify the e\x0bect of parser accuracy on these systems\&apos; performance, and examine the question of whether a atter \\chunked&quot; representation of the input can be as e\x0bective for the purposes of semantic role identi\x0ccation. 1 Introduction Over the past decade, most work in the \x0celd of information extraction has shifted from complex rule-based, systems designed to handle a wide variety of semantic phenomena including quanti\x0ccation, anaphora, aspect and modality (e.g. Alshawi (1992)), to simpler \x0cnite-state or statistical systems such as Hobbs et al. (1997) and Miller et al. (1998). Much of the evaluation of these systems has been conducted on extracting relations for speci\x0cc semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences. Recently, attention has turned to creating corpora annotated for argument structure for a broader range of predicates. The Propbank project at the University of Pennsylvania (Kingsbury and Palmer, 2002) and the FrameNet project at the International Computer Science Institute (Baker et al., 1998) share the goal of documenting the syntactic realization of arguments o</context>
<context position="3316" citStr="Miller et al. (1998)" startWordPosition="504" endWordPosition="507"> FrameNet project to automatically assign semantic roles. The system \x0crst passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features. Both training and test sentences were automatically parsed, as no hand-annotated parse trees were available for the corpus. While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this e\x0bect. Of the systems evaluated for the Message Understanding Conference task, Miller et al. (1998) made use of an integrated syntactic and semantic model producing a full parse tree, and achieved results comparable to other systems that did not make use of a complete parse. As in the FrameNet case, the parser was not trained on the corpus for which semantic annotations were available, and the e\x0bect of better, or even perfect, parses could not be measured. One of the di\x0berences between the two semantic annotation projects is that the sentences chosen Computational Linguistics (ACL), Philadelphia, July 2002, pp. 239-246. Proceedings of the 40th Annual Meeting of the Association for \x0</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1998</marker>
<rawString>Scott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz, Rebecca Stone, Ralph Weischedel, and the Annotation Group. 1998. Algorithms that learn to extract information { BBN: Description of the SIFT system as used for MUC-7. In Proceedings of the Seventh Message Understanding Conference (MUC-7), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumya Ray</author>
<author>Mark Craven</author>
</authors>
<title>Representing sentence structure in hidden markov model for information extraction.</title>
<date>2001</date>
<booktitle>In Seventeenth International Joint Conference on Arti\x0ccial Intelligence (IJCAI-01),</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="17743" citStr="Ray and Craven, 2001" startWordPosition="2871" endWordPosition="2874">that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system. 4 Is Parsing Necessary? Many recent information extraction systems for limited domains have relied on \x0cnite-state systems that do not build a full parse tree for the sentence being analyzed. Among such systems, (Hobbs et al., 1997) built \x0cnite-state recognizers for various entities, which were then cascaded to form recognizers for higher-level relations, while (Ray and Craven, 2001) used low-level \\chunks&quot; from a general-purpose syntactic analyzer as observations in a trained Hidden Markov Model. Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided. It is also possible that this approach may be more robust to error than parsers. Although we expect the attachment decisions made by a parser to be relevant to determining whether a constituent of a sentence is an argument of a particular predicate, and what its relation to the predicate is, those decisions may be so frequently incorrect that a much simpler system </context>
</contexts>
<marker>Ray, Craven, 2001</marker>
<rawString>Soumya Ray and Mark Craven. 2001. Representing sentence structure in hidden markov model for information extraction. In Seventeenth International Joint Conference on Arti\x0ccial Intelligence (IJCAI-01), Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the conll-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL-2000 and LLL-2000,</booktitle>
<location>Lisbon,</location>
<contexts>
<context position="19056" citStr="Sang and Buchholz (2000)" startWordPosition="3092" endWordPosition="3095">is given only a at, \\chunked&quot; representation of the input sentence to the parse-tree-based systems described above. In this representation, base-level constituent boundaries and labels are present, but there are no dependencies between constituents, as shown by the following sample sentence: (3) [NP Big investment banks] [VP refused to step] [ADVP up] [PP to] [NP the plate] [VP to support] [NP the beleaguered oor traders] [PP by] [VP buying] [NP big blocks] [PP of] [NP stock] , [NP traders] [VP say] . Our chunks were derived from the Treebank trees using the conversion described by Tjong Kim Sang and Buchholz (2000). Thus, the experiments were carried out using \\goldstandard&quot; rather than automatically derived chunk boundaries, which we believe will provide an upper bound on the performance of a chunkbased system. The information provided by the parse tree can be decomposed into three pieces: the constituent boundaries, the grammatical relationship between predicate and argument, expressed by our path feature, and the head word of each candidate constituent. We will examine the contribution of each \x0cof these information sources, beginning with the problem of assigning the correct role in the case wher</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the conll-2000 shared task: Chunking. In Proceedings of CoNLL-2000 and LLL-2000, Lisbon, Portugal. \x0c&apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>