<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9372695">
b&amp;apos;Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10391047,
Suntec, Singapore, 2-7 August 2009. c
</note>
<sectionHeader confidence="0.301028" genericHeader="abstract">
2009 ACL and AFNLP
</sectionHeader>
<title confidence="0.987474">
Semi-Supervised Active Learning for Sequence Labeling
</title>
<author confidence="0.983614">
Katrin Tomanek and Udo Hahn
</author>
<affiliation confidence="0.996538">
Jena University Language &amp; Information Engineering (JULIE) Lab
</affiliation>
<address confidence="0.699814">
Friedrich-Schiller-Universitat Jena, Germany
</address>
<email confidence="0.792557">
{katrin.tomanek|udo.hahn}@uni-jena.de
</email>
<sectionHeader confidence="0.946111" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.984247294117647">
While Active Learning (AL) has already
been shown to markedly reduce the anno-
tation efforts for many sequence labeling
tasks compared to random selection, AL
remains unconcerned about the internal
structure of the selected sequences (typ-
ically, sentences). We propose a semi-
supervised AL approach for sequence la-
beling where only highly uncertain sub-
sequences are presented to human anno-
tators, while all others in the selected se-
quences are automatically labeled. For the
task of entity recognition, our experiments
reveal that this approach reduces annota-
tion efforts in terms of manually labeled
tokens by up to 60 % compared to the stan-
dard, fully supervised AL scheme.
</bodyText>
<sectionHeader confidence="0.997863" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999295852459017">
Supervised machine learning (ML) approaches are
currently the methodological backbone for lots of
NLP activities. Despite their success they create a
costly follow-up problem, viz. the need for human
annotators to supply large amounts of golden
annotation data on which ML systems can be
trained. In most annotation campaigns, the lan-
guage material chosen for manual annotation is se-
lected randomly from some reference corpus.
Active Learning (AL) has recently shaped as a
much more efficient alternative for the creation of
precious training material. In the AL paradigm,
only examples of high training utility are selected
for manual annotation in an iterative manner. Dif-
ferent approaches to AL have been successfully
applied to a wide range of NLP tasks (Engel-
son and Dagan, 1996; Ngai and Yarowsky, 2000;
Tomanek et al., 2007; Settles and Craven, 2008).
When used for sequence labeling tasks such as
POS tagging, chunking, or named entity recogni-
tion (NER), the examples selected by AL are se-
quences of text, typically sentences. Approaches
to AL for sequence labeling are usually uncon-
cerned about the internal structure of the selected
sequences. Although a high overall training util-
ity might be attributed to a sequence as a whole,
the subsequences it is composed of tend to ex-
hibit different degrees of training utility. In the
NER scenario, e.g., large portions of the text do
not contain any target entity mention at all. To
further exploit this observation for annotation pur-
poses, we here propose an approach to AL where
human annotators are required to label only uncer-
tain subsequences within the selected sentences,
while the remaining subsequences are labeled au-
tomatically based on the model available from the
previous AL iteration round. The hardness of sub-
sequences is characterized by the classifiers con-
fidence in the predicted labels. Accordingly, our
approach is a combination of AL and self-training
to which we will refer as semi-supervised Active
Learning (SeSAL) for sequence labeling.
While self-training and other bootstrapping ap-
proaches often fail to produce good results on NLP
tasks due to an inherent tendency of deteriorated
data quality, SeSAL circumvents this problem and
still yields large savings in terms annotation de-
cisions, i.e., tokens to be manually labeled, com-
pared to a standard, fully supervised AL approach.
After a brief overview of the formal underpin-
nings of Conditional Random Fields, our base
classifier for sequence labeling tasks (Section 2),
a fully supervised approach to AL for sequence
labeling is introduced and complemented by our
semi-supervised approach in Section 3. In Section
4, we discuss SeSAL in relation to bootstrapping
and existing AL techniques. Our experiments are
laid out in Section 5 where we compare fully and
semi-supervised AL for NER on two corpora, the
newspaper selection of MUC7 and PENNBIOIE, a
biological abstracts corpus.
</bodyText>
<page confidence="0.990397">
1039
</page>
<subsectionHeader confidence="0.469672">
\x0c2 Conditional Random Fields for
Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.912276333333333">
Many NLP tasks, such as POS tagging, chunking,
or NER, are sequence labeling problems where a
sequence of class labels ~
</bodyText>
<equation confidence="0.661221">
y = (y1, . . . ,yn) Yn
</equation>
<bodyText confidence="0.935617">
are assigned to a sequence of input units
</bodyText>
<equation confidence="0.826213">
~
x = (x1, . . . ,xn) Xn. Input units xj are usually
</equation>
<bodyText confidence="0.856642">
tokens, class labels yj can be POS tags or entity
classes.
Conditional Random Fields (CRFs) (Lafferty et
al., 2001) are a probabilistic framework for label-
ing structured data and model P~
(~
y|~
x). We focus
on first-order linear-chain CRFs, a special form of
CRFs for sequential data, where
</bodyText>
<equation confidence="0.983946619047619">
P~
(~
y|~
x) =
1
Z~
(~
x)
exp
\x10 n
X
j=1
m
X
i=1
ifi(yj1,yj,~
x, j)
\x11
(1)
with normalization factor Z~
(~
</equation>
<bodyText confidence="0.9901406">
x), feature functions
fi(), and feature weights i.
Parameter Estimation. The model parameters
i are set to maximize the penalized log-likelihood
L on some training data T :
</bodyText>
<equation confidence="0.994791461538461">
L(T ) =
X
(~
x,~
y)T
log p(~
y|~
x)
m
X
i=1
2
i
</equation>
<page confidence="0.974882">
22
</page>
<bodyText confidence="0.9582705">
(2)
The partial derivations of L(T ) are
</bodyText>
<equation confidence="0.965427333333333">
L(T )
i
= E(fi) E(fi)
i
2
(3)
</equation>
<bodyText confidence="0.99449575">
where E(fi) is the empirical expectation of fea-
ture fi and can be calculated by counting the oc-
currences of fi in T . E(fi) is the model expecta-
tion of fi and can be written as
</bodyText>
<equation confidence="0.99978015">
E(fi) =
X
(~
x,~
y)T
X
~
y Yn
P~
(~
y
|~
x)
n
X
j=1
fi(y
j1, y
j, ~
x,j) (4)
</equation>
<bodyText confidence="0.9894858">
Direct computation of E(fi) is intractable due to
the sum over all possible label sequences ~
y Yn.
The Forward-Backward algorithm (Rabiner, 1989)
solves this problem efficiently. Forward () and
</bodyText>
<equation confidence="0.984100481481482">
backward () scores are defined by
j(y|~
x) =
X
yT1
j (y)
j1(y
|~
x) j(~
x, y
, y)
j(y|~
x) =
X
yTj(y)
j+1(y
|~
x) j(~
x, y, y
)
where j(~
x,a,b) = exp
\x10 Pm
i=1 ifi(a,b,~
x, j)
\x11
,
</equation>
<bodyText confidence="0.978907">
Tj(y) is the set of all successors of a state y at a
specified position j, and, accordingly, T1
</bodyText>
<equation confidence="0.713047">
j (y) is
</equation>
<bodyText confidence="0.990071">
the set of predecessors.
Normalized forward and backward scores
are inserted into Equation (4) to replace
</bodyText>
<equation confidence="0.936221">
P
~
y Yn P~
(~
y |~
x) so that L(T ) can be opti-
</equation>
<figureCaption confidence="0.30023225">
mized with gradient-based or iterative-scaling
methods.
Inference and Probabilities. The marginal
probability
</figureCaption>
<equation confidence="0.995096636363636">
P~
(yj = y
|~
x) =
j(y|~
x) j(y|~
x)
Z~
(~
x)
(5)
</equation>
<bodyText confidence="0.934314125">
specifies the models confidence in label y at po-
sition j of an input sequence ~
x. The forward
and backward scores are obtained by applying the
Forward-Backward algorithm on ~
x. The normal-
ization factor is efficiently calculated by summing
over all forward scores:
</bodyText>
<equation confidence="0.995470714285714">
Z~
(~
x) =
X
yY
n(y|~
x) (6)
</equation>
<bodyText confidence="0.521588">
The most likely label sequence
</bodyText>
<equation confidence="0.99849325">
~
y
= argmax
~
yYn
exp
\x10 n
X
j=1
m
X
i=1
ifi(yj1,yj,~
x, j)
\x11
(7)
</equation>
<bodyText confidence="0.605667">
is computed using the Viterbi algorithm (Rabiner,
1989). See Equation (1) for the conditional prob-
</bodyText>
<equation confidence="0.985431">
ability P~
(~
y |~
x) with Z~
</equation>
<bodyText confidence="0.868049">
calculated as in Equa-
tion (6). The marginal and conditional probabili-
ties are used by our AL approaches as confidence
estimators.
</bodyText>
<sectionHeader confidence="0.903347" genericHeader="method">
3 Active Learning for Sequence Labeling
</sectionHeader>
<bodyText confidence="0.985240222222222">
AL is a selective sampling technique where the
learning protocol is in control of the data to be
used for training. The intention with AL is to re-
duce the amount of labeled training material by
querying labels only for examples which are as-
sumed to have a high training utility. This section,
first, describes a common approach to AL for se-
quential data, and then presents our approach to
semi-supervised AL.
</bodyText>
<subsectionHeader confidence="0.998668">
3.1 Fully Supervised Active Learning
</subsectionHeader>
<bodyText confidence="0.996622">
Algorithm 1 describes the general AL framework.
A utility function UM(pi) is the core of each AL
approach it estimates how useful it would be for
</bodyText>
<page confidence="0.94576">
1040
</page>
<bodyText confidence="0.79241175">
\x0cAlgorithm 1 General AL framework
Given:
B: number of examples to be selected
L: set of labeled examples
P: set of unlabeled examples
UM: utility function
Algorithm:
loop until stopping criterion is met
</bodyText>
<listItem confidence="0.910470833333333">
1. learn model M from L
2. for all pi P : upi UM(pi)
3. select B examples pi P with highest utility upi
4. query human annotator for labels of all B examples
5. move newly labeled examples from P to L
return L
</listItem>
<bodyText confidence="0.991234909090909">
a specific base learner to have an unlabeled exam-
ple labeled and, subsequently included in the train-
ing set.
In the sequence labeling scenario, such an ex-
ample is a stream of linguistic items a sentence
is usually considered as proper sequence unit. We
apply CRFs as our base learner throughout this pa-
per and employ a utility function which is based
on the conditional probability of the most likely
label sequence ~
y for an observation sequence ~
</bodyText>
<equation confidence="0.884624222222222">
x
(cf. Equations (1) and (7)):
U~
(~
x) = 1 P~
(~
y
|~
x) (8)
</equation>
<bodyText confidence="0.999036">
Sequences for which the current model is least
confident on the most likely label sequence are
preferably selected.1 These selected sentences are
fully manually labeled. We refer to this AL mode
as fully supervised Active Learning (FuSAL).
</bodyText>
<subsectionHeader confidence="0.999767">
3.2 Semi-Supervised Active Learning
</subsectionHeader>
<bodyText confidence="0.957392">
In the sequence labeling scenario, an example
which, as a whole, has a high utility U~
(~
x), can
still exhibit subsequences which do not add much
to the overall utility and thus are fairly easy for the
current model to label correctly. One might there-
fore doubt whether it is reasonable to manually la-
bel the entire sequence. Within many sequences
of natural language data, there are probably large
subsequences on which the current model already
does quite well and thus could automatically gen-
erate annotations with high quality. This might, in
particular, apply to NER where larger stretches of
sentences do not contain any entity mention at all,
or merely trivial instances of an entity class easily
predictable by the current model.
</bodyText>
<page confidence="0.892848">
1
</page>
<bodyText confidence="0.997589105263158">
There are many more sophisticated utility functions for
sequence labeling. We have chosen this straightforward one
for simplicity and because it has proven to be very effective
(Settles and Craven, 2008).
For the sequence labeling scenario, we accord-
ingly modify the fully supervised AL approach
from Section 3.1. Only those tokens remain to be
manually labeled on which the current model is
highly uncertain regarding their class labels, while
all other tokens (those on which the model is suf-
ficiently certain how to label them correctly) are
automatically tagged.
To select the sequence examples the same util-
ity function as for FuSAL (cf. Equation (8)) is ap-
plied. To identify tokens xj from the selected se-
quences which still have to be manually labeled,
the models confidence in label y
j is estimated by
the marginal probability (cf. Equation (5))
</bodyText>
<equation confidence="0.985887">
C~
(y
j ) = P~
(yj = y
j |~
x) (9)
where y
</equation>
<bodyText confidence="0.978626852941176">
j specifies the label at the respective po-
sition of the most likely label sequence ~
y (cf.
Equation (7)). If C~
(y
j ) exceeds a certain con-
fidence threshold t, y
j is assumed to be the correct
label for this token and assigned to it.2 Otherwise,
manual annotation of this token is required. So,
compared to FuSAL as described in Algorithm 1
only the third step step is modified.
We call this semi-supervised Active Learning
(SeSAL) for sequence labeling. SeSAL joins the
standard, fully supervised AL schema with a boot-
strapping mode, namely self-training, to combine
the strengths of both approaches. Examples with
high training utility are selected using AL, while
self-tagging of certain safe regions within such
examples additionally reduces annotation effort.
Through this combination, SeSAL largely evades
the problem of deteriorated data quality, a limiting
factor of pure bootstrapping approaches.
This approach requires two parameters to be set:
Firstly, the confidence threshold t which directly
influences the portion of tokens to be manually
labeled. Using lower thresholds, the self-tagging
component of SeSAL has higher impact presum-
ably leading to larger amounts of tagging errors.
Secondly, a delay factor d can be specified which
channels the amount of manually labeled tokens
obtained with FuSAL before SeSAL is to start.
Only with d = 0, SeSAL will already affect the
first AL iteration. Otherwise, several iterations of
</bodyText>
<figure confidence="0.401776666666667">
FuSAL are run until a switch to SeSAL will hap-
pen.
2
Sequences of consecutive tokens xj for which C~
(y
j )
</figure>
<bodyText confidence="0.8848495">
t are presented to the human annotator instead of single, iso-
lated tokens.
</bodyText>
<page confidence="0.866103">
1041
</page>
<bodyText confidence="0.998191">
\x0cIt is well known that the performance of boot-
strapping approaches crucially depends on the size
of the seed set the amount of labeled examples
available to train the initial model. If class bound-
aries are poorly defined by choosing the seed set
too small, a bootstrapping system cannot learn
anything reasonable due to high error rates. If, on
the other hand, class boundaries are already too
well defined due to an overly large seed set, noth-
ing to be learned is left. Thus, together with low
thresholds, a delay rate of d &amp;gt; 0 might be crucial
to obtain models of high performance.
</bodyText>
<sectionHeader confidence="0.999361" genericHeader="method">
4 Related Work
</sectionHeader>
<bodyText confidence="0.998307034090909">
Common approaches to AL are variants of the
Query-By-Committee approach (Seung et al.,
1992) or based on uncertainty sampling (Lewis
and Catlett, 1994). Query-by-Committee uses a
committee of classifiers, and examples on which
the classifiers disagree most regarding their pre-
dictions are considered highly informative and
thus selected for annotation. Uncertainty sam-
pling selects examples on which a single classi-
fier is least confident. AL has been successfully
applied to many NLP tasks; Settles and Craven
(2008) compare the effectiveness of several AL
approaches for sequence labeling tasks of NLP.
Self-training (Yarowsky, 1995) is a form of
semi-supervised learning. From a seed set of la-
beled examples a weak model is learned which
subsequently gets incrementally refined. In each
step, unlabeled examples on which the current
model is very confident are labeled with their pre-
dictions, added to the training set, and a new
model is learned. Similar to self-training, co-
training (Blum and Mitchell, 1998) augments the
training set by automatically labeled examples.
It is a multi-learner algorithm where the learners
have independent views on the data and mutually
produce labeled examples for each other.
Bootstrapping approaches often fail when ap-
plied to NLP tasks where large amounts of training
material are required to achieve acceptable perfor-
mance levels. Pierce and Cardie (2001) showed
that the quality of the automatically labeled train-
ing data is crucial for co-training to perform well
because too many tagging errors prevent a high-
performing model from being learned. Also, the
size of the seed set is an important parameter.
When it is chosen too small data quality gets dete-
riorated quickly, when it is chosen too large no im-
provement over the initial model can be expected.
To address the problem of data pollution by tag-
ging errors, Pierce and Cardie (2001) propose cor-
rected co-training. In this mode, a human is put
into the co-training loop to review and, if neces-
sary, to correct the machine-labeled examples. Al-
though this effectively evades the negative side ef-
fects of deteriorated data quality, one may find the
correction of labeled data to be as time-consuming
as annotations from the scratch. Ideally, a human
should not get biased by the proposed label but
independently examine the example so that cor-
rection eventually becomes annotation.
In contrast, our SeSAL approach which also ap-
plies bootstrapping, aims at avoiding to deteriorate
data quality by explicitly pointing human annota-
tors to classification-critical regions. While those
regions require full annotation, regions of high
confidence are automatically labeled and thus do
not require any manual inspection. Self-training
and co-training, in contradistinction, select exam-
ples of high confidence only. Thus, these boot-
strapping methods will presumably not find the
most useful unlabeled examples but require a hu-
man to review data points of limited training util-
ity (Pierce and Cardie, 2001). This shortcoming is
also avoided by our SeSAL approach, as we inten-
tionally select informative examples only.
A combination of active and semi-supervised
learning has first been proposed by McCallum and
Nigam (1998) for text classification. Committee-
based AL is used for the example selection. The
committee members are first trained on the labeled
examples and then augmented by means of Expec-
tation Maximization (EM) (Dempster et al., 1977)
including the unlabeled examples. The idea is
to avoid manual labeling of examples whose la-
bels can be reliably assigned by EM. Similarly,
co-testing (Muslea et al., 2002), a multi-view AL
algorithms, selects examples for the multi-view,
semi-supervised Co-EM algorithm. In both works,
semi-supervision is based on variants of the EM
algorithm in combination with all unlabeled ex-
amples from the pool. Our approach to semi-
supervised AL is different as, firstly, we aug-
ment the training data using a self-tagging mech-
anism (McCallum and Nigam (1998) and Muslea
et al. (2002) performed semi-supervision to aug-
ment the models using EM), and secondly, we op-
erate in the sequence labeling scenario where an
example is made up of several units each requiring
</bodyText>
<page confidence="0.965556">
1042
</page>
<bodyText confidence="0.997267636363636">
\x0ca label partial labeling of sequence examples is
a central characteristic of our approach. Another
work also closely related to ours is that of Krist-
jansson et al. (2004). In an information extraction
setting, the confidence per extracted field is cal-
culated by a constrained variant of the Forward-
Backward algorithm. Unreliable fields are high-
lighted so that the automatically annotated corpus
can be corrected. In contrast, AL selection of ex-
amples together with partial manual labeling of the
selected examples are the main foci of our work.
</bodyText>
<sectionHeader confidence="0.993669" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99963625">
In this section, we turn to the empirical assessment
of semi-supervised AL (SeSAL) for sequence la-
beling on the NLP task of named entity recogni-
tion. By the nature of this task, the sequences
in this case, sentences are only sparsely popu-
lated with entity mentions and most of the tokens
belong to the OUTSIDE class3 so that SeSAL can
be expected to be very beneficial.
</bodyText>
<subsectionHeader confidence="0.987591">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.994947038461538">
In all experiments, we employ the linear-chain
CRF model described in Section 2 as the base
learner. A set of common feature functions was
employed, including orthographical (regular ex-
pression patterns), lexical and morphological (suf-
fixes/prefixes, lemmatized tokens), and contextual
(features of neighboring tokens) ones.
All experiments start from a seed set of 20 ran-
domly selected examples and, in each iteration,
50 new examples are selected using AL. The ef-
ficiency of the different selection mechanisms is
determined by learning curves which relate the an-
notation costs to the performance achieved by the
respective model in terms of F1-score. The unit of
annotation costs are manually labeled tokens. Al-
though the assumption of uniform costs per token
has already been subject of legitimate criticism
(Settles et al., 2008), we believe that the number
of annotated tokens is still a reasonable approxi-
mation in the absence of an empirically more ade-
quate task-specific annotation cost model.
We ran the experiments on two entity-annotated
corpora. From the general-language newspaper
domain, we took the training part of the MUC7
corpus (Linguistic Data Consortium, 2001) which
incorporates seven different entity types, viz. per-
</bodyText>
<page confidence="0.987363">
3
</page>
<bodyText confidence="0.881961">
The OUTSIDE class is assigned to each token that does
not denote an entity in the underlying domain of discourse.
corpus entity classes sentences tokens
</bodyText>
<table confidence="0.9886935">
MUC7 7 3,020 78,305
PENNBIOIE 3 10,570 267,320
</table>
<tableCaption confidence="0.998609">
Table 1: Quantitative characteristics of the chosen corpora
</tableCaption>
<bodyText confidence="0.992807">
sons, organizations, locations, times, dates, mone-
tary expressions, and percentages. From the sub-
language biology domain, we used the oncology
part of the PENNBIOIE corpus (Kulick et al.,
2004) and removed all but three gene entity sub-
types (generic, protein, and rna). Table 1 summa-
rizes the quantitative characteristics of both cor-
pora.4 The results reported below are averages of
20 independent runs. For each run, we randomly
split each corpus into a pool of unlabeled examples
to select from (90 % of the corpus), and a comple-
mentary evaluation set (10 % of the corpus).
</bodyText>
<subsectionHeader confidence="0.999344">
5.2 Empirical Evaluation
</subsectionHeader>
<bodyText confidence="0.982948052631579">
We compare semi-supervised AL (SeSAL) with
its fully supervised counterpart (FuSAL), using
a passive learning scheme where examples are
randomly selected (RAND) as baseline. SeSAL
is first applied in a default configuration with a
very high confidence threshold (t = 0.99) with-
out any delay (d = 0). In further experiments,
these parameters are varied to study their impact
on SeSALs performance. All experiments were
run on both the newspaper (MUC7) and biological
(PENNBIOIE) corpus. When results are similar to
each other, only one data set will be discussed.
Distribution of Confidence Scores. The lead-
ing assumption for SeSAL is that only a small por-
tion of tokens within the selected sentences consti-
tute really hard decision problems, while the ma-
jority of tokens are easy to account for by the cur-
rent model. To test this stipulation we investigate
the distribution of the models confidence values
</bodyText>
<equation confidence="0.565803">
C~
(y
</equation>
<figureCaption confidence="0.751208666666667">
j ) over all tokens of the sentences (cf. Equa-
tion (9)) selected within one iteration of FuSAL.
Figure 1, as an example, depicts the histogram
</figureCaption>
<bodyText confidence="0.9542132">
for an early AL iteration round on the MUC7 cor-
pus. The vast majority of tokens has a confidence
score close to 1, the median lies at 0.9966. His-
tograms of subsequent AL iterations are very sim-
ilar with an even higher median. This is so because
</bodyText>
<page confidence="0.984348">
4
</page>
<bodyText confidence="0.9908315">
We removed sentences of considerable over and under
length (beyond +/- 3 standard deviations around the average
sentence length) so that the numbers in Table 1 differ from
those cited in the original sources.
</bodyText>
<page confidence="0.688902">
1043
</page>
<figure confidence="0.994362857142857">
\x0cconfidence score
frequency
0.2 0.4 0.6 0.8 1.0
0
500
1000
1500
</figure>
<figureCaption confidence="0.8117675">
Figure 1: Distribution of token-level confidence scores in the
5th iteration of FuSAL on MUC7 (number of tokens: 1,843)
</figureCaption>
<bodyText confidence="0.969036629629629">
the model gets continuously more confident when
trained on additional data and fewer hard cases re-
main in the shrinking pool.
Fully Supervised vs. Semi-Supervised AL.
Figure 2 compares the performance of FuSAL and
SeSAL on the two corpora. SeSAL is run with
a delay rate of d = 0 and a very high confi-
dence threshold of t = 0.99 so that only those
tokens are automatically labeled on which the cur-
rent model is almost certain. Figure 2 clearly
shows that SeSAL is much more efficient than
its fully supervised counterpart. Table 2 depicts
the exact numbers of manually labeled tokens to
reach the maximal (supervised) F-score on both
corpora. FuSAL saves about 50 % compared to
RAND, while SeSAL saves about 60 % compared
to FuSAL which constitutes an overall saving of
over 80 % compared to RAND.
These savings are calculated relative to the
number of tokens which have to be manually la-
beled. Yet, consider the following gedanken ex-
periment. Assume that, using SeSAL, every sec-
ond token in a sequence would have to be labeled.
Though this comes to a formal saving of 50 %,
the actual annotation effort in terms of the time
needed would hardly go down. It appears that
only when SeSAL splits a sentence into larger
</bodyText>
<table confidence="0.970955666666667">
Corpus Fmax RAND FuSAL SeSAL
MUC7 87.7 63,020 36,015 11,001
PENNBIOIE 82.3 194,019 83,017 27,201
</table>
<tableCaption confidence="0.982217">
Table 2: Tokens manually labeled to reach the maximal (su-
</tableCaption>
<figure confidence="0.997639">
pervised) F-score
0 10000 30000 50000
0.60
0.70
0.80
0.90
MUC7
manually labeled tokens
Fscore
SeSAL
FuSAL
RAND
0 10000 30000 50000
0.60
0.70
0.80
0.90
PennBioIE
manually labeled tokens
Fscore SeSAL
FuSAL
RAND
</figure>
<figureCaption confidence="0.930975666666667">
Figure 2: Learning curves for Semi-supervised AL (SeSAL),
Fully Supervised AL (FuSAL), and RAND(om) selection
well-packaged, chunk-like subsequences annota-
</figureCaption>
<bodyText confidence="0.987777">
tion time can really be saved. To demonstrate that
SeSAL comes close to this, we counted the num-
ber of base noun phrases (NPs) containing one or
more tokens to be manually labeled. On the MUC7
corpus, FuSAL requires 7,374 annotated NPs to
yield an F-score of 87 %, while SeSAL hit the
same F-score with only 4,017 NPs. Thus, also in
terms of the number of NPs, SeSAL saves about
45 % of the material to be considered.5
Detailed Analysis of SeSAL. As Figure 2 re-
veals, the learning curves of SeSAL stop early (on
MUC7 after 12,800 tokens, on PENNBIOIE after
27,600 tokens) because at that point the whole cor-
pus has been labeled exhaustively either manu-
ally, or automatically. So, using SeSAL the com-
plete corpus can be labeled with only a small
fraction of it actually being manually annotated
(MUC7: about 18 %, PENNBIOIE: about 13 %).
</bodyText>
<page confidence="0.961183">
5
</page>
<bodyText confidence="0.888108">
On PENNBIOIE, SeSAL also saves about 45 % com-
pared to FuSAL to achieve an F-score of 81 %.
</bodyText>
<page confidence="0.938455">
1044
</page>
<bodyText confidence="0.8930345">
\x0cTable 3 provides additional analysis results on
MUC7. In very early AL rounds, a large ratio of
tokens has to be manually labeled (70-80 %). This
number decreases increasingly as the classifier im-
proves (and the pool contains fewer informative
sentences). The number of tagging errors is quite
low, resulting in a high accuracy of the created cor-
pus of constantly over 99 %.
labeled tokens
manual automatic AR (%) errors ACC
</bodyText>
<table confidence="0.9986885">
1,000 253 1,253 79.82 6 99.51
5,000 6,207 11,207 44.61 82 99.27
10,000 25,506 34,406 28.16 174 99.51
12,800 57,371 70,171 18.24 259 99.63
</table>
<tableCaption confidence="0.999383">
Table 3: Analysis of SeSAL on MUC7: Manually and auto-
</tableCaption>
<bodyText confidence="0.973846470588235">
matically labeled tokens, annotation rate (AR) as the portion
of manually labeled tokens in the total amount of labeled to-
kens, errors and accuracy (ACC) of the created corpus.
The majority of the automatically labeled to-
kens (97-98 %) belong to the OUTSIDE class.
This coincides with the assumption that SeSAL
works especially well for labeling tasks where
some classes occur predominantly and can, in
most cases, easily be discriminated from the other
classes, as is the case in the NER scenario. An
analysis of the errors induced by the self-tagging
component reveals that most of the errors (90-
100 %) are due to missed entity classes, i.e., while
the correct class label for a token is one of the
entity classes, the OUTSIDE class was assigned.
This effect is more severe in early than in later AL
iterations (see Table 4 for the exact numbers).
</bodyText>
<table confidence="0.9039725">
labeled error types (%)
corpus tokens errors E2O O2E E2E
MUC7 10,000 75 100
70,000 259 96 1.3 2.7
</table>
<tableCaption confidence="0.998983">
Table 4: Distribution of errors of the self-tagging component.
</tableCaption>
<bodyText confidence="0.979580833333333">
Error types: OUTSIDE class assigned though an entity class
is correct (E2O), entity class assigned but OUTSIDE is cor-
rect (O2E), wrong entity class assigned (E2E).
Impact of the Confidence Threshold. We also
ran SeSAL with different confidence thresholds t
(0.99, 0.95, 0.90, and 0.70) and analyzed the re-
sults with respect to tagging errors and the model
performance. Figure 3 shows the learning and er-
ror curves for different thresholds on the MUC7
corpus. The supervised F-score of 87.7 % is only
reached by the highest and most restrictive thresh-
old of t = 0.99. With all other thresholds, SeSAL
</bodyText>
<figure confidence="0.999178583333333">
0 2000 6000 10000
0.60
0.70
0.80
0.90
learning curves
manually labeled tokens
Fscore
t=0.99
t=0.95
t=0.90
t=0.70
0 20000 40000 60000
0
500
1000
2000
error curves
all labeled tokens
errors
t=0.99
t=0.95
t=0.90
t=0.70
</figure>
<figureCaption confidence="0.999926">
Figure 3: Learning and error curves for SeSAL with different
</figureCaption>
<bodyText confidence="0.988780166666667">
thresholds on the MUC7 corpus
stops at much lower F-scores and produces labeled
training data of lower accuracy. Table 5 contains
the exact numbers and reveals that the poor model
performance of SeSAL with lower thresholds is
mainly due to dropping recall values.
</bodyText>
<table confidence="0.9053158">
threshold F R P Acc
0.99 87.7 85.9 89.9 99.6
0.95 85.4 82.3 88.7 98.8
0.90 84.3 80.6 88.3 98.1
0.70 69.9 61.8 81.1 96.5
</table>
<tableCaption confidence="0.972211">
Table 5: Maximum model performance on MUC7 in terms of
</tableCaption>
<bodyText confidence="0.991577222222222">
F-score (F), recall (R), precision (P) and accuracy (Acc) the
labeled corpus obtained by SeSAL with different thresholds
Impact of the Delay Rate. We also measured
the impact of delay rates on SeSALs efficiency
considering three delay rates (1,000, 5,000, and
10,000 tokens) in combination with three confi-
dence thresholds (0.99, 0.9, and 0.7). Figure 4 de-
picts the respective learning curves on the MUC7
corpus. For SeSAL with t = 0.99, the delay
</bodyText>
<figure confidence="0.991276022222222">
1045
\x0c0 5000 10000 15000 20000
0.60
0.70
0.80
0.90
threshold 0.99
manually labeled tokens
Fscore
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=0.877
0 5000 10000 15000 20000
0.60
0.70
0.80
0.90
threshold 0.9
manually labeled tokens
Fscore
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=0.843
F=0.877
0 2000 6000 10000
0.60
0.70
0.80
0.90
threshold 0.7
manually labeled tokens
Fscore
FuSAL
SeSAL, d=0
SeSAL, d=1000
SeSAL, d=5000
SeSAL, d=10000
F=69.9
F=0.877
</figure>
<figureCaption confidence="0.999812">
Figure 4: SeSAL with different delay rates and thresholds on MUC7. Horizontal lines mark the supervised F-score (upper line)
</figureCaption>
<bodyText confidence="0.9937905">
and the maximal F-score achieved by SeSAL with the respective threshold and d = 0 (lower line).
has no particularly beneficial effect. However,
in combination with lower thresholds, the delay
rates show positive effects as SeSAL yields F-
scores closer to the maximal F-score of 87.7 %,
thus clearly outperforming undelayed SeSAL.
</bodyText>
<sectionHeader confidence="0.992933" genericHeader="conclusions">
6 Summary and Discussion
</sectionHeader>
<bodyText confidence="0.999042152542373">
Our experiments in the context of the NER
scenario render evidence to the hypothesis that
the proposed approach to semi-supervised AL
(SeSAL) for sequence labeling indeed strongly re-
duces the amount of tokens to be manually anno-
tated in terms of numbers, about 60% compared
to its fully supervised counterpart (FuSAL), and
over 80% compared to a totally passive learning
scheme based on random selection.
For SeSAL to work well, a high and, by this,
restrictive threshold has been shown to be crucial.
Otherwise, large amounts of tagging errors lead to
a poorer overall model performance. In our ex-
periments, tagging errors in such a scenario were
OUTSIDE labelings, while an entity class would
have been correct with the effect that the result-
ing models showed low recall rates.
The delay rate is important when SeSAL is run
with a low threshold as early tagging errors can
be avoided which otherwise reinforce themselves.
Finding the right balance between the delay factor
and low thresholds requires experimental calibra-
tion. For the most restrictive threshold (t = 0.99)
though such a delay is unimportant so that it can
be set to d = 0 circumventing this calibration step.
In summary, the self-tagging component of
SeSAL gets more influential when the confidence
threshold and the delay factor are set to lower val-
ues. At the same time though, under these con-
ditions negative side-effects such as deteriorated
data quality and, by this, inferior models emerge.
These problems are major drawbacks of many
bootstrapping approaches. However, our experi-
ments indicate that as long as self-training is cau-
tiously applied (as is done for SeSAL with restric-
tive parameters), it can definitely outperform an
entirely supervised approach.
From an annotation point of view, SeSAL effi-
ciently guides the annotator to regions within the
selected sentence which are very useful for the
learning task. In our experiments on the NER sce-
nario, those regions were mentions of entity names
or linguistic units which had a surface appearance
similar to entity mentions but could not yet be cor-
rectly distinguished by the model.
While we evaluated SeSAL here in terms of
tokens to be manually labeled, an open issue re-
mains, namely how much of the real annotation
effort measured by the time needed is saved
by this approach. We here hypothesize that hu-
man annotators work much more efficiently when
pointed to the regions of immediate interest in-
stead of making them skim in a self-paced way
through larger passages of (probably) semantically
irrelevant but syntactically complex utterances
a tiring and error-prone task. Future research is
needed to empirically investigate into this area and
quantify the savings in terms of the time achiev-
able with SeSAL in the NER scenario.
</bodyText>
<sectionHeader confidence="0.9603" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.83030925">
This work was funded by the EC within the
BOOTStrep (FP6-028099) and CALBC (FP7-
231727) projects. We want to thank Roman Klin-
ger (Fraunhofer SCAI) for fruitful discussions.
</bodyText>
<page confidence="0.876179">
1046
</page>
<reference confidence="0.996457470588235">
\x0cReferences
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In COLT98
Proceedings of the 11th Annual Conference on Com-
putational Learning Theory, pages 92100.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):138.
S. Engelson and I. Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from cor-
pora. In ACL96 Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics, pages 319326.
T. Kristjansson, A. Culotta, and P. Viola. 2004. Inter-
active information extraction with constrained Con-
ditional Random Fields. In AAAI04 Proceed-
ings of 19th National Conference on Artificial Intel-
ligence, pages 412418.
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. T. Mc-
Donald, M. S. Palmer, and A. I. Schein. 2004. Inte-
grated annotation for biomedical information extrac-
tion. In Proceedings of the HLT-NAACL 2004 Work-
shop Linking Biological Literature, Ontologies and
Databases: Tools for Users, pages 6168.
J. D. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional Random Fields: Probabilistic models
for segmenting and labeling sequence data. In
ICML01 Proceedings of the 18th International
Conference on Machine Learning, pages 282289.
D. D. Lewis and J. Catlett. 1994. Heterogeneous
uncertainty sampling for supervised learning. In
ICML94 Proceedings of the 11th International
Conference on Machine Learning, pages 148156.
Linguistic Data Consortium. 2001. Message Under-
standing Conference (MUC) 7. LDC2001T02. FTP
FILE. Philadelphia: Linguistic Data Consortium.
A. McCallum and K. Nigam. 1998. Employing EM
and pool-based Active Learning for text classifica-
tion. In ICML98 Proceedings of the 15th Interna-
tional Conference on Machine Learning, pages 350
358.
I. A. Muslea, S. Minton, and C. A. Knoblock. 2002.
Active semi-supervised learning = Robust multi-
view learning. In ICML02 Proceedings of the
19th International Conference on Machine Learn-
ing, pages 435442.
G. Ngai and D. Yarowsky. 2000. Rule writing or anno-
tation: Cost-efficient resource usage for base noun
phrase chunking. In ACL00 Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 117125.
D. Pierce and C. Cardie. 2001. Limitations of co-
training for natural language learning from large
datasets. In EMNLP01 Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 19.
L. R. Rabiner. 1989. A tutorial on Hidden Markov
Models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2):257286.
B. Settles and M. Craven. 2008. An analysis of Active
Learning strategies for sequence labeling tasks. In
EMNLP08 Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 10691078.
B. Settles, M. Craven, and L. Friedland. 2008. Active
Learning with real annotation costs. In Proceedings
of the NIPS 2008 Workshop on Cost-Sensitive Ma-
chine Learning, pages 110.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT92 Proceedings of
the 5th Annual Workshop on Computational Learn-
ing Theory, pages 287294.
K. Tomanek, J. Wermter, and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts anno-
tation costs and maintains corpus reusability of an-
notated data. In EMNLP-CoNLL07 Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Language Learning, pages 486495.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In ACL95
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 189
196.
</reference>
<page confidence="0.702325">
1047
</page>
<figure confidence="0.30196">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.298194">
<note confidence="0.860925666666667">b&amp;apos;Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 10391047, Suntec, Singapore, 2-7 August 2009. c 2009 ACL and AFNLP</note>
<title confidence="0.964034">Semi-Supervised Active Learning for Sequence Labeling</title>
<author confidence="0.97069">Katrin Tomanek</author>
<author confidence="0.97069">Udo Hahn</author>
<affiliation confidence="0.949547">Jena University Language &amp; Information Engineering (JULIE) Lab</affiliation>
<address confidence="0.568212">Friedrich-Schiller-Universitat Jena, Germany</address>
<email confidence="0.998947">{katrin.tomanek|udo.hahn}@uni-jena.de</email>
<abstract confidence="0.988335722222222">While Active Learning (AL) has already been shown to markedly reduce the annotation efforts for many sequence labeling tasks compared to random selection, AL remains unconcerned about the internal structure of the selected sequences (typically, sentences). We propose a semisupervised AL approach for sequence labeling where only highly uncertain subsequences are presented to human annotators, while all others in the selected sequences are automatically labeled. For the task of entity recognition, our experiments reveal that this approach reduces annotation efforts in terms of manually labeled tokens by up to 60 % compared to the standard, fully supervised AL scheme.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In COLT98 Proceedings of the 11th Annual Conference on Computational Learning Theory,</booktitle>
<pages>92100</pages>
<contexts>
<context position="13429" citStr="Blum and Mitchell, 1998" startWordPosition="2295" endWordPosition="2298">s on which a single classifier is least confident. AL has been successfully applied to many NLP tasks; Settles and Craven (2008) compare the effectiveness of several AL approaches for sequence labeling tasks of NLP. Self-training (Yarowsky, 1995) is a form of semi-supervised learning. From a seed set of labeled examples a weak model is learned which subsequently gets incrementally refined. In each step, unlabeled examples on which the current model is very confident are labeled with their predictions, added to the training set, and a new model is learned. Similar to self-training, cotraining (Blum and Mitchell, 1998) augments the training set by automatically labeled examples. It is a multi-learner algorithm where the learners have independent views on the data and mutually produce labeled examples for each other. Bootstrapping approaches often fail when applied to NLP tasks where large amounts of training material are required to achieve acceptable performance levels. Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a highperforming model from being learned. Also, the size of the </context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>\x0cReferences A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT98 Proceedings of the 11th Annual Conference on Computational Learning Theory, pages 92100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="15855" citStr="Dempster et al., 1977" startWordPosition="2678" endWordPosition="2681"> bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). This shortcoming is also avoided by our SeSAL approach, as we intentionally select informative examples only. A combination of active and semi-supervised learning has first been proposed by McCallum and Nigam (1998) for text classification. Committeebased AL is used for the example selection. The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) (Dempster et al., 1977) including the unlabeled examples. The idea is to avoid manual labeling of examples whose labels can be reliably assigned by EM. Similarly, co-testing (Muslea et al., 2002), a multi-view AL algorithms, selects examples for the multi-view, semi-supervised Co-EM algorithm. In both works, semi-supervision is based on variants of the EM algorithm in combination with all unlabeled examples from the pool. Our approach to semisupervised AL is different as, firstly, we augment the training data using a self-tagging mechanism (McCallum and Nigam (1998) and Muslea et al. (2002) performed semi-supervisio</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 39(1):138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Engelson</author>
<author>I Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.</title>
<date>1996</date>
<booktitle>In ACL96 Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>319326</pages>
<contexts>
<context position="1867" citStr="Engelson and Dagan, 1996" startWordPosition="281" endWordPosition="285">llow-up problem, viz. the need for human annotators to supply large amounts of golden annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the tex</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>S. Engelson and I. Dagan. 1996. Minimizing manual annotation cost in supervised training from corpora. In ACL96 Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, pages 319326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kristjansson</author>
<author>A Culotta</author>
<author>P Viola</author>
</authors>
<title>Interactive information extraction with constrained Conditional Random Fields.</title>
<date>2004</date>
<booktitle>In AAAI04 Proceedings of 19th National Conference on Artificial Intelligence,</booktitle>
<pages>412418</pages>
<contexts>
<context position="16788" citStr="Kristjansson et al. (2004)" startWordPosition="2829" endWordPosition="2833">based on variants of the EM algorithm in combination with all unlabeled examples from the pool. Our approach to semisupervised AL is different as, firstly, we augment the training data using a self-tagging mechanism (McCallum and Nigam (1998) and Muslea et al. (2002) performed semi-supervision to augment the models using EM), and secondly, we operate in the sequence labeling scenario where an example is made up of several units each requiring 1042 \x0ca label partial labeling of sequence examples is a central characteristic of our approach. Another work also closely related to ours is that of Kristjansson et al. (2004). In an information extraction setting, the confidence per extracted field is calculated by a constrained variant of the ForwardBackward algorithm. Unreliable fields are highlighted so that the automatically annotated corpus can be corrected. In contrast, AL selection of examples together with partial manual labeling of the selected examples are the main foci of our work. 5 Experiments and Results In this section, we turn to the empirical assessment of semi-supervised AL (SeSAL) for sequence labeling on the NLP task of named entity recognition. By the nature of this task, the sequences in this</context>
</contexts>
<marker>Kristjansson, Culotta, Viola, 2004</marker>
<rawString>T. Kristjansson, A. Culotta, and P. Viola. 2004. Interactive information extraction with constrained Conditional Random Fields. In AAAI04 Proceedings of 19th National Conference on Artificial Intelligence, pages 412418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulick</author>
<author>A Bies</author>
<author>M Liberman</author>
<author>M Mandel</author>
<author>R T McDonald</author>
<author>M S Palmer</author>
<author>A I Schein</author>
</authors>
<title>Integrated annotation for biomedical information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>6168</pages>
<contexts>
<context position="19279" citStr="Kulick et al., 2004" startWordPosition="3223" endWordPosition="3226">-language newspaper domain, we took the training part of the MUC7 corpus (Linguistic Data Consortium, 2001) which incorporates seven different entity types, viz. per3 The OUTSIDE class is assigned to each token that does not denote an entity in the underlying domain of discourse. corpus entity classes sentences tokens MUC7 7 3,020 78,305 PENNBIOIE 3 10,570 267,320 Table 1: Quantitative characteristics of the chosen corpora sons, organizations, locations, times, dates, monetary expressions, and percentages. From the sublanguage biology domain, we used the oncology part of the PENNBIOIE corpus (Kulick et al., 2004) and removed all but three gene entity subtypes (generic, protein, and rna). Table 1 summarizes the quantitative characteristics of both corpora.4 The results reported below are averages of 20 independent runs. For each run, we randomly split each corpus into a pool of unlabeled examples to select from (90 % of the corpus), and a complementary evaluation set (10 % of the corpus). 5.2 Empirical Evaluation We compare semi-supervised AL (SeSAL) with its fully supervised counterpart (FuSAL), using a passive learning scheme where examples are randomly selected (RAND) as baseline. SeSAL is first app</context>
</contexts>
<marker>Kulick, Bies, Liberman, Mandel, McDonald, Palmer, Schein, 2004</marker>
<rawString>S. Kulick, A. Bies, M. Liberman, M. Mandel, R. T. McDonald, M. S. Palmer, and A. I. Schein. 2004. Integrated annotation for biomedical information extraction. In Proceedings of the HLT-NAACL 2004 Workshop Linking Biological Literature, Ontologies and Databases: Tools for Users, pages 6168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML01 Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282289</pages>
<contexts>
<context position="4411" citStr="Lafferty et al., 2001" startWordPosition="699" endWordPosition="702">ing AL techniques. Our experiments are laid out in Section 5 where we compare fully and semi-supervised AL for NER on two corpora, the newspaper selection of MUC7 and PENNBIOIE, a biological abstracts corpus. 1039 \x0c2 Conditional Random Fields for Sequence Labeling Many NLP tasks, such as POS tagging, chunking, or NER, are sequence labeling problems where a sequence of class labels ~ y = (y1, . . . ,yn) Yn are assigned to a sequence of input units ~ x = (x1, . . . ,xn) Xn. Input units xj are usually tokens, class labels yj can be POS tags or entity classes. Conditional Random Fields (CRFs) (Lafferty et al., 2001) are a probabilistic framework for labeling structured data and model P~ (~ y|~ x). We focus on first-order linear-chain CRFs, a special form of CRFs for sequential data, where P~ (~ y|~ x) = 1 Z~ (~ x) exp \x10 n X j=1 m X i=1 ifi(yj1,yj,~ x, j) \x11 (1) with normalization factor Z~ (~ x), feature functions fi(), and feature weights i. Parameter Estimation. The model parameters i are set to maximize the penalized log-likelihood L on some training data T : L(T ) = X (~ x,~ y)T log p(~ y|~ x) m X i=1 2 i 22 (2) The partial derivations of L(T ) are L(T ) i = E(fi) E(fi) i 2 (3) where E(fi) is th</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. D. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In ICML01 Proceedings of the 18th International Conference on Machine Learning, pages 282289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In ICML94 Proceedings of the 11th International Conference on Machine Learning,</booktitle>
<pages>148156</pages>
<contexts>
<context position="12567" citStr="Lewis and Catlett, 1994" startWordPosition="2162" endWordPosition="2165"> labeled examples available to train the initial model. If class boundaries are poorly defined by choosing the seed set too small, a bootstrapping system cannot learn anything reasonable due to high error rates. If, on the other hand, class boundaries are already too well defined due to an overly large seed set, nothing to be learned is left. Thus, together with low thresholds, a delay rate of d &amp;gt; 0 might be crucial to obtain models of high performance. 4 Related Work Common approaches to AL are variants of the Query-By-Committee approach (Seung et al., 1992) or based on uncertainty sampling (Lewis and Catlett, 1994). Query-by-Committee uses a committee of classifiers, and examples on which the classifiers disagree most regarding their predictions are considered highly informative and thus selected for annotation. Uncertainty sampling selects examples on which a single classifier is least confident. AL has been successfully applied to many NLP tasks; Settles and Craven (2008) compare the effectiveness of several AL approaches for sequence labeling tasks of NLP. Self-training (Yarowsky, 1995) is a form of semi-supervised learning. From a seed set of labeled examples a weak model is learned which subsequent</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>D. D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In ICML94 Proceedings of the 11th International Conference on Machine Learning, pages 148156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<date>2001</date>
<booktitle>Message Understanding Conference (MUC) 7. LDC2001T02. FTP FILE. Philadelphia: Linguistic Data Consortium.</booktitle>
<contexts>
<context position="18766" citStr="Consortium, 2001" startWordPosition="3146" endWordPosition="3147">notation costs to the performance achieved by the respective model in terms of F1-score. The unit of annotation costs are manually labeled tokens. Although the assumption of uniform costs per token has already been subject of legitimate criticism (Settles et al., 2008), we believe that the number of annotated tokens is still a reasonable approximation in the absence of an empirically more adequate task-specific annotation cost model. We ran the experiments on two entity-annotated corpora. From the general-language newspaper domain, we took the training part of the MUC7 corpus (Linguistic Data Consortium, 2001) which incorporates seven different entity types, viz. per3 The OUTSIDE class is assigned to each token that does not denote an entity in the underlying domain of discourse. corpus entity classes sentences tokens MUC7 7 3,020 78,305 PENNBIOIE 3 10,570 267,320 Table 1: Quantitative characteristics of the chosen corpora sons, organizations, locations, times, dates, monetary expressions, and percentages. From the sublanguage biology domain, we used the oncology part of the PENNBIOIE corpus (Kulick et al., 2004) and removed all but three gene entity subtypes (generic, protein, and rna). Table 1 su</context>
</contexts>
<marker>Consortium, 2001</marker>
<rawString>Linguistic Data Consortium. 2001. Message Understanding Conference (MUC) 7. LDC2001T02. FTP FILE. Philadelphia: Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>Employing EM and pool-based Active Learning for text classification.</title>
<date>1998</date>
<booktitle>In ICML98 Proceedings of the 15th International Conference on Machine Learning,</booktitle>
<pages>350--358</pages>
<contexts>
<context position="15628" citStr="McCallum and Nigam (1998)" startWordPosition="2642" endWordPosition="2645">require full annotation, regions of high confidence are automatically labeled and thus do not require any manual inspection. Self-training and co-training, in contradistinction, select examples of high confidence only. Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). This shortcoming is also avoided by our SeSAL approach, as we intentionally select informative examples only. A combination of active and semi-supervised learning has first been proposed by McCallum and Nigam (1998) for text classification. Committeebased AL is used for the example selection. The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) (Dempster et al., 1977) including the unlabeled examples. The idea is to avoid manual labeling of examples whose labels can be reliably assigned by EM. Similarly, co-testing (Muslea et al., 2002), a multi-view AL algorithms, selects examples for the multi-view, semi-supervised Co-EM algorithm. In both works, semi-supervision is based on variants of the EM algorithm in combination with all unla</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. Employing EM and pool-based Active Learning for text classification. In ICML98 Proceedings of the 15th International Conference on Machine Learning, pages 350 358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Muslea</author>
<author>S Minton</author>
<author>C A Knoblock</author>
</authors>
<title>Active semi-supervised learning = Robust multiview learning.</title>
<date>2002</date>
<booktitle>In ICML02 Proceedings of the 19th International Conference on Machine Learning,</booktitle>
<pages>435442</pages>
<contexts>
<context position="16027" citStr="Muslea et al., 2002" startWordPosition="2706" endWordPosition="2709">001). This shortcoming is also avoided by our SeSAL approach, as we intentionally select informative examples only. A combination of active and semi-supervised learning has first been proposed by McCallum and Nigam (1998) for text classification. Committeebased AL is used for the example selection. The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) (Dempster et al., 1977) including the unlabeled examples. The idea is to avoid manual labeling of examples whose labels can be reliably assigned by EM. Similarly, co-testing (Muslea et al., 2002), a multi-view AL algorithms, selects examples for the multi-view, semi-supervised Co-EM algorithm. In both works, semi-supervision is based on variants of the EM algorithm in combination with all unlabeled examples from the pool. Our approach to semisupervised AL is different as, firstly, we augment the training data using a self-tagging mechanism (McCallum and Nigam (1998) and Muslea et al. (2002) performed semi-supervision to augment the models using EM), and secondly, we operate in the sequence labeling scenario where an example is made up of several units each requiring 1042 \x0ca label p</context>
</contexts>
<marker>Muslea, Minton, Knoblock, 2002</marker>
<rawString>I. A. Muslea, S. Minton, and C. A. Knoblock. 2002. Active semi-supervised learning = Robust multiview learning. In ICML02 Proceedings of the 19th International Conference on Machine Learning, pages 435442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>D Yarowsky</author>
</authors>
<title>Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking.</title>
<date>2000</date>
<booktitle>In ACL00 Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>117125</pages>
<contexts>
<context position="1892" citStr="Ngai and Yarowsky, 2000" startWordPosition="286" endWordPosition="289">need for human annotators to supply large amounts of golden annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the text do not contain any targ</context>
</contexts>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>G. Ngai and D. Yarowsky. 2000. Rule writing or annotation: Cost-efficient resource usage for base noun phrase chunking. In ACL00 Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 117125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pierce</author>
<author>C Cardie</author>
</authors>
<title>Limitations of cotraining for natural language learning from large datasets.</title>
<date>2001</date>
<booktitle>In EMNLP01 Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>19</pages>
<contexts>
<context position="13813" citStr="Pierce and Cardie (2001)" startWordPosition="2353" endWordPosition="2356">refined. In each step, unlabeled examples on which the current model is very confident are labeled with their predictions, added to the training set, and a new model is learned. Similar to self-training, cotraining (Blum and Mitchell, 1998) augments the training set by automatically labeled examples. It is a multi-learner algorithm where the learners have independent views on the data and mutually produce labeled examples for each other. Bootstrapping approaches often fail when applied to NLP tasks where large amounts of training material are required to achieve acceptable performance levels. Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a highperforming model from being learned. Also, the size of the seed set is an important parameter. When it is chosen too small data quality gets deteriorated quickly, when it is chosen too large no improvement over the initial model can be expected. To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. In this mode, a human is put into the co-training loop to review and, if necessar</context>
<context position="15411" citStr="Pierce and Cardie, 2001" startWordPosition="2609" endWordPosition="2612">otation. In contrast, our SeSAL approach which also applies bootstrapping, aims at avoiding to deteriorate data quality by explicitly pointing human annotators to classification-critical regions. While those regions require full annotation, regions of high confidence are automatically labeled and thus do not require any manual inspection. Self-training and co-training, in contradistinction, select examples of high confidence only. Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). This shortcoming is also avoided by our SeSAL approach, as we intentionally select informative examples only. A combination of active and semi-supervised learning has first been proposed by McCallum and Nigam (1998) for text classification. Committeebased AL is used for the example selection. The committee members are first trained on the labeled examples and then augmented by means of Expectation Maximization (EM) (Dempster et al., 1977) including the unlabeled examples. The idea is to avoid manual labeling of examples whose labels can be reliably assigned by EM. Similarly, co-testing (Musl</context>
</contexts>
<marker>Pierce, Cardie, 2001</marker>
<rawString>D. Pierce and C. Cardie. 2001. Limitations of cotraining for natural language learning from large datasets. In EMNLP01 Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="5394" citStr="Rabiner, 1989" startWordPosition="905" endWordPosition="906">l parameters i are set to maximize the penalized log-likelihood L on some training data T : L(T ) = X (~ x,~ y)T log p(~ y|~ x) m X i=1 2 i 22 (2) The partial derivations of L(T ) are L(T ) i = E(fi) E(fi) i 2 (3) where E(fi) is the empirical expectation of feature fi and can be calculated by counting the occurrences of fi in T . E(fi) is the model expectation of fi and can be written as E(fi) = X (~ x,~ y)T X ~ y Yn P~ (~ y |~ x) n X j=1 fi(y j1, y j, ~ x,j) (4) Direct computation of E(fi) is intractable due to the sum over all possible label sequences ~ y Yn. The Forward-Backward algorithm (Rabiner, 1989) solves this problem efficiently. Forward () and backward () scores are defined by j(y|~ x) = X yT1 j (y) j1(y |~ x) j(~ x, y , y) j(y|~ x) = X yTj(y) j+1(y |~ x) j(~ x, y, y ) where j(~ x,a,b) = exp \x10 Pm i=1 ifi(a,b,~ x, j) \x11 , Tj(y) is the set of all successors of a state y at a specified position j, and, accordingly, T1 j (y) is the set of predecessors. Normalized forward and backward scores are inserted into Equation (4) to replace P ~ y Yn P~ (~ y |~ x) so that L(T ) can be optimized with gradient-based or iterative-scaling methods. Inference and Probabilities. The marginal probabil</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. R. Rabiner. 1989. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
<author>M Craven</author>
</authors>
<title>An analysis of Active Learning strategies for sequence labeling tasks.</title>
<date>2008</date>
<booktitle>In EMNLP08 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>10691078</pages>
<contexts>
<context position="1941" citStr="Settles and Craven, 2008" startWordPosition="294" endWordPosition="297">ts of golden annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the text do not contain any target entity mention at all. To further exploit this</context>
<context position="9489" citStr="Settles and Craven, 2008" startWordPosition="1641" endWordPosition="1644">quence. Within many sequences of natural language data, there are probably large subsequences on which the current model already does quite well and thus could automatically generate annotations with high quality. This might, in particular, apply to NER where larger stretches of sentences do not contain any entity mention at all, or merely trivial instances of an entity class easily predictable by the current model. 1 There are many more sophisticated utility functions for sequence labeling. We have chosen this straightforward one for simplicity and because it has proven to be very effective (Settles and Craven, 2008). For the sequence labeling scenario, we accordingly modify the fully supervised AL approach from Section 3.1. Only those tokens remain to be manually labeled on which the current model is highly uncertain regarding their class labels, while all other tokens (those on which the model is sufficiently certain how to label them correctly) are automatically tagged. To select the sequence examples the same utility function as for FuSAL (cf. Equation (8)) is applied. To identify tokens xj from the selected sequences which still have to be manually labeled, the models confidence in label y j is estim</context>
<context position="12933" citStr="Settles and Craven (2008)" startWordPosition="2216" endWordPosition="2219">low thresholds, a delay rate of d &amp;gt; 0 might be crucial to obtain models of high performance. 4 Related Work Common approaches to AL are variants of the Query-By-Committee approach (Seung et al., 1992) or based on uncertainty sampling (Lewis and Catlett, 1994). Query-by-Committee uses a committee of classifiers, and examples on which the classifiers disagree most regarding their predictions are considered highly informative and thus selected for annotation. Uncertainty sampling selects examples on which a single classifier is least confident. AL has been successfully applied to many NLP tasks; Settles and Craven (2008) compare the effectiveness of several AL approaches for sequence labeling tasks of NLP. Self-training (Yarowsky, 1995) is a form of semi-supervised learning. From a seed set of labeled examples a weak model is learned which subsequently gets incrementally refined. In each step, unlabeled examples on which the current model is very confident are labeled with their predictions, added to the training set, and a new model is learned. Similar to self-training, cotraining (Blum and Mitchell, 1998) augments the training set by automatically labeled examples. It is a multi-learner algorithm where the </context>
</contexts>
<marker>Settles, Craven, 2008</marker>
<rawString>B. Settles and M. Craven. 2008. An analysis of Active Learning strategies for sequence labeling tasks. In EMNLP08 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 10691078.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
<author>M Craven</author>
<author>L Friedland</author>
</authors>
<title>Active Learning with real annotation costs.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS 2008 Workshop on Cost-Sensitive Machine Learning,</booktitle>
<pages>110</pages>
<contexts>
<context position="18418" citStr="Settles et al., 2008" startWordPosition="3091" endWordPosition="3094"> and morphological (suffixes/prefixes, lemmatized tokens), and contextual (features of neighboring tokens) ones. All experiments start from a seed set of 20 randomly selected examples and, in each iteration, 50 new examples are selected using AL. The efficiency of the different selection mechanisms is determined by learning curves which relate the annotation costs to the performance achieved by the respective model in terms of F1-score. The unit of annotation costs are manually labeled tokens. Although the assumption of uniform costs per token has already been subject of legitimate criticism (Settles et al., 2008), we believe that the number of annotated tokens is still a reasonable approximation in the absence of an empirically more adequate task-specific annotation cost model. We ran the experiments on two entity-annotated corpora. From the general-language newspaper domain, we took the training part of the MUC7 corpus (Linguistic Data Consortium, 2001) which incorporates seven different entity types, viz. per3 The OUTSIDE class is assigned to each token that does not denote an entity in the underlying domain of discourse. corpus entity classes sentences tokens MUC7 7 3,020 78,305 PENNBIOIE 3 10,570 </context>
</contexts>
<marker>Settles, Craven, Friedland, 2008</marker>
<rawString>B. Settles, M. Craven, and L. Friedland. 2008. Active Learning with real annotation costs. In Proceedings of the NIPS 2008 Workshop on Cost-Sensitive Machine Learning, pages 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H S Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee.</title>
<date>1992</date>
<booktitle>In COLT92 Proceedings of the 5th Annual Workshop on Computational Learning Theory,</booktitle>
<pages>287294</pages>
<contexts>
<context position="12508" citStr="Seung et al., 1992" startWordPosition="2153" endWordPosition="2156">ally depends on the size of the seed set the amount of labeled examples available to train the initial model. If class boundaries are poorly defined by choosing the seed set too small, a bootstrapping system cannot learn anything reasonable due to high error rates. If, on the other hand, class boundaries are already too well defined due to an overly large seed set, nothing to be learned is left. Thus, together with low thresholds, a delay rate of d &amp;gt; 0 might be crucial to obtain models of high performance. 4 Related Work Common approaches to AL are variants of the Query-By-Committee approach (Seung et al., 1992) or based on uncertainty sampling (Lewis and Catlett, 1994). Query-by-Committee uses a committee of classifiers, and examples on which the classifiers disagree most regarding their predictions are considered highly informative and thus selected for annotation. Uncertainty sampling selects examples on which a single classifier is least confident. AL has been successfully applied to many NLP tasks; Settles and Craven (2008) compare the effectiveness of several AL approaches for sequence labeling tasks of NLP. Self-training (Yarowsky, 1995) is a form of semi-supervised learning. From a seed set o</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>H. S. Seung, M. Opper, and H. Sompolinsky. 1992. Query by committee. In COLT92 Proceedings of the 5th Annual Workshop on Computational Learning Theory, pages 287294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tomanek</author>
<author>J Wermter</author>
<author>U Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains corpus reusability of annotated data.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL07 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning,</booktitle>
<pages>486495</pages>
<contexts>
<context position="1914" citStr="Tomanek et al., 2007" startWordPosition="290" endWordPosition="293"> to supply large amounts of golden annotation data on which ML systems can be trained. In most annotation campaigns, the language material chosen for manual annotation is selected randomly from some reference corpus. Active Learning (AL) has recently shaped as a much more efficient alternative for the creation of precious training material. In the AL paradigm, only examples of high training utility are selected for manual annotation in an iterative manner. Different approaches to AL have been successfully applied to a wide range of NLP tasks (Engelson and Dagan, 1996; Ngai and Yarowsky, 2000; Tomanek et al., 2007; Settles and Craven, 2008). When used for sequence labeling tasks such as POS tagging, chunking, or named entity recognition (NER), the examples selected by AL are sequences of text, typically sentences. Approaches to AL for sequence labeling are usually unconcerned about the internal structure of the selected sequences. Although a high overall training utility might be attributed to a sequence as a whole, the subsequences it is composed of tend to exhibit different degrees of training utility. In the NER scenario, e.g., large portions of the text do not contain any target entity mention at a</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>K. Tomanek, J. Wermter, and U. Hahn. 2007. An approach to text corpus construction which cuts annotation costs and maintains corpus reusability of annotated data. In EMNLP-CoNLL07 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning, pages 486495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In ACL95 Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="13051" citStr="Yarowsky, 1995" startWordPosition="2234" endWordPosition="2235"> AL are variants of the Query-By-Committee approach (Seung et al., 1992) or based on uncertainty sampling (Lewis and Catlett, 1994). Query-by-Committee uses a committee of classifiers, and examples on which the classifiers disagree most regarding their predictions are considered highly informative and thus selected for annotation. Uncertainty sampling selects examples on which a single classifier is least confident. AL has been successfully applied to many NLP tasks; Settles and Craven (2008) compare the effectiveness of several AL approaches for sequence labeling tasks of NLP. Self-training (Yarowsky, 1995) is a form of semi-supervised learning. From a seed set of labeled examples a weak model is learned which subsequently gets incrementally refined. In each step, unlabeled examples on which the current model is very confident are labeled with their predictions, added to the training set, and a new model is learned. Similar to self-training, cotraining (Blum and Mitchell, 1998) augments the training set by automatically labeled examples. It is a multi-learner algorithm where the learners have independent views on the data and mutually produce labeled examples for each other. Bootstrapping approa</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In ACL95 Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189 196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>