<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<bodyText confidence="0.4682965">
b&amp;apos;Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 15571567,
Edinburgh, Scotland, UK, July 2731, 2011. c
</bodyText>
<sectionHeader confidence="0.350886" genericHeader="abstract">
2011 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.80556">
Bootstrapped Named Entity Recognition for Product Attribute Extraction
</title>
<author confidence="0.763403">
Duangmanee (Pew) Putthividhya
</author>
<affiliation confidence="0.634337">
eBay Inc.
</affiliation>
<address confidence="0.863083">
2065 Hamilton Ave
San Jose, CA 95125
</address>
<email confidence="0.982333">
dputthividhya@ebay.com
</email>
<author confidence="0.903221">
Junling Hu
</author>
<affiliation confidence="0.740287">
eBay Inc.
</affiliation>
<address confidence="0.8994685">
2065 Hamilton Ave
San Jose, CA 95125
</address>
<email confidence="0.99524">
juhu@ebay.com
</email>
<sectionHeader confidence="0.99075" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.99966247368421">
We present a named entity recognition (NER)
system for extracting product attributes and
values from listing titles. Information extrac-
tion from short listing titles present a unique
challenge, with the lack of informative con-
text and grammatical structure. In this work,
we combine supervised NER with bootstrap-
ping to expand the seed list, and output nor-
malized results. Focusing on listings from
eBays clothing and shoes categories, our
bootstrapped NER system is able to identify
new brands corresponding to spelling variants
and typographical errors of the known brands,
as well as identifying novel brands. Among
the top 300 new brands predicted, our system
achieves 90.33% precision. To output normal-
ized attribute values, we explore several string
comparison algorithms and found n-gram sub-
string matching to work well in practice.
</bodyText>
<sectionHeader confidence="0.998252" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998993270833333">
Traditional named entity recognition (NER) task has
expanded beyond identifying people, location, and
organization to book titles, email addresses, phone
numbers, and protein names (Nadeau and Sekine
2007). Recently there has been a surge of interest
in extracting product attributes from online data due
to the rapid growth of E-Commerce. Current work
in this domain focuses on mining product reviews
and descriptions from retailer websites. Such text
data tend to be long and generate enough context for
the target task (Brody and Elhadad 2010; Liu et al.
2005; Popescu and Etzioni 2005). In this paper, we
focus on mining short product listing titles, which
poses unique challenges.
Short listings are typical in classified ads where
each seller is given limited space (in terms of words)
to describe the product. On eBay, product listing ti-
tles cannot exceed 55 characters in length. Similarly,
on Craigslist and newspaper ads, the length of a list-
ing title is restricted. Extracting product attributes
from such short titles faces the following challenges:
Loss of grammatical structure in short listings
where many nouns are piled together.
Typographical errors, abbreviations, and
acronyms that must be normalized to the
standardized values.
Lack of contextual information to infer product
attribute value.
It can be argued that the use of short listings simpli-
fies the problem of attribute extraction, since short
listings can be easily annotated and one can apply
supervised learning approach to extract product at-
tributes. However, as the size of the data grows, ob-
taining labeled training set on the scale of millions
of listings becomes very expensive. In such a sce-
nario, incorporating unlabeled examples in a semi-
supervised fashion to scale up the solution becomes
a necessity rather than a luxury.
We formulate the product attribute extraction
problem as a named entity recognition (NER) task
and investigate supervised and semi-supervised ap-
proaches to this problem. In addition, we have in-
vestigated attribute discovery, and normalization to
standardized values. We use listings from eBays
clothing and shoes categories and develop an at-
tribute extraction system for 4 attribute types. We
have 105, 335 listings from mens clothing category
and 72, 628 listings from womens clothing category
</bodyText>
<page confidence="0.963595">
1557
</page>
<bodyText confidence="0.9994341875">
\x0con eBay, constituting a dataset of 1, 380, 337 word
tokens.
In the first part of this work, we outline a super-
vised learning approach to attribute value extraction
where we train a sequential classifier and evaluate
the extraction performance on a set of hand-labeled
listings. Using maximum entropy and SVM as the
base classifier (for classifying the individual word
tokens), a hidden Markov model (HMM) is trained
on the the probabilistic output of the base classifier,
and a sequential label prediction is obtained using a
Viterbi decoding. We show a performance compar-
ison of supervised HMM, MaxEnt, SVM, and CRF
for this task.
In the second part of our work, to grow our seed
list of attributes, we present a bootstrapped algo-
rithm for attribute value discovery and normaliza-
tion, honing in on one particular attribute (brand).
The goal is given an initial list of unambiguous
brands, we grow the seed dictionary by discover-
ing context patterns that are often associated with
such attribute type. First, we automatically parti-
tion data into a training/test set by labeling word to-
kens in each listing using exact matching to entries
in the dictionary. Brand phrases that can be confused
with other attributes, e.g. the word camel both a
brand and a color will not be a part of this ini-
tial seed list to create the training set. A classifier
is then trained to learn context patterns surrounding
the known brands from the training set, and is used
to discover new brands from the test set.
Finally, for known attribute values, we normalize
the results to match to words in our dictionary. Nor-
malizing the variants of a known brand to a single
normalized output value is an important aspect of
a successful information extraction system. To this
end, we investigate several string similarity/distance
measures for this task and found that n-gram sub-
string similarity (Kondrak 2005) yields accurate nor-
malization results.
The main contribution of this work is a product
attribute extraction system that addresses the unique
problems of information extraction from short list-
ing titles. We combine supervised NER with boot-
strapping to expand the seed list, and investigate sev-
eral methods to normalize the extracted results. Our
system has been tested on large-scale eBay listing
datasets to demonstrate its effectiveness.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998827106382978">
Recent work on product attribute extraction by
(Brody and Elhadad 2010) applies a Latent Dirich-
let Allocation (LDA) model to identify different as-
pects of products from user reviews. Similar work
is presented in (Liu et al. 2005). Topic models such
as LDA groups similar words together by identify-
ing topics (product aspects) from patterns of word-
occurrences. Such grouping can discover new as-
pects of a product such as portability (for net-
book computers), but it may generate aspects that
are vague and not easily interpretable. Indeed, how
to refine discovered aspects and clean up words in
each aspect remains an open question. The LDA
approach also treats documents as bags of words,
where important information in word sequences is
not taken into account in learning the model.
Our work is most closely related to (Ghani 2006),
where a set of product attributes of interests are pre-
defined and a supervised learning method is applied
to extract the correct attribute values for each class.
Starting out from a small set of training examples,
a bootstrapping technique is used to generate more
training data from unlabeled data. The main dif-
ference to our method lies in how bootstrapping is
used. (Ghani 2006) used EM to add more train-
ing data from unlabeled data, while in our approach
bootstrapping is used to expand the seed list. First,
we automatically generate labeled data by matching
seed list to unlabeled data. Then, these auto-labeled
training set is used to train a classifier to identify new
attribute values from a separate set of unlabeled data.
Thirdly, newly discovered product attribute values
are added back to our seed list. Thus our original
classifier for product attribute extraction can be im-
proved through an expanded seed list.
In (Ghani and Jones 2002; Jones 2005), several
bootstrapping methods are compared. These meth-
ods include self-training, co-EM and EM. All of
these approaches are different from ours, as de-
scribed in detail earlier. In (Probst et al. 2006),
a Naive Bayes learner is combined with Co-EM to
generate more training data from unlabeled data, and
attribute-value pairs are extracted on adjacent words.
The automatic bootstrapping in this paper was in-
spired by (Pakhomov 2002)an acronym expansion
algorithm for medical text documents. The underly-
ing assumption is that abbreviated forms and their
</bodyText>
<page confidence="0.979259">
1558
</page>
<bodyText confidence="0.999534921568627">
\x0ccorresponding expansions occur in similar contexts;
consequently, the surrounding context patterns can
be used in associating the correct expansion to its
acronym.
Our seed list expansion algorithm indeed bears
some similarity to the work of (Nadeau el al 2006)
and (Nadeau 2007). In (Nadeau el al 2006), automo-
bile brands are learned automatically from web page
context. First, a small set of 196 seed brands are ex-
tracted together with their associated web page con-
texts from popular news feed. The web context is
subsequently used to extract additional automobile
brands, which result in a total of 5701 brands. How-
ever, the reported results in (Nadeau el al 2006) have
low precision, in some case less than 50%. Eventu-
ally their approach needs to rely on rule-based ambi-
guity resolver to increase the precision. Our system
does not rely on manually created rules.
A more NLP-oriented approach is proposed in
(Popescu and Etzioni 2005), where noun phrases are
extracted from online user reviews. Their system
tries to identify product features and user opinions
from such noun phrases. A PMI (pointwise mutual
information) score is evaluated between each noun
phrase and discriminators associated with the prod-
uct class. The noun-phrase approach does not work
well in informal texts. In our case, user-generated
short product listings may have many nouns con-
catenated together without forming a phrase or
obeying correct grammatical rules.
Finally, another similar bootstrapping method is
presented in (Mintz et al. 2009), where instances
of known entity relations (or seed list in our paper)
are matched to sentences in a set of Wikipedia arti-
cles, and a learning algorithm is trained from the sur-
rounding features of the entities. The trained model
is then applied to a test set of Wikipedia articles,
and has been reported to be able to discover new in-
stances. In our case, we apply our learned model to
a new test set, and discover new brand names from
the listings.
The nature of non-grammatical text we face
makes our work similar to the NER work on infor-
mal texts. (Minkov et al. 2005) proposes an NER
system that extracts personal names from emails.
The work in (Gruhl et al 2009) identifies song titles
from online forums on popular music, where song
titles can be very ambiguous. By using real-world
constraints such as known song titles, (Gruhl et al
2009) restricts the set of possible entities and are
able to obtain reasonable recognition performance.
</bodyText>
<sectionHeader confidence="0.996456" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.99977971875">
The data used in all analysis in this paper is obtained
from eBays clothing and shoes category. Clothing
and shoes have been important revenue-generating
categories on the eBay site, and a successful at-
tribute extraction system will serve as an invaluable
tool for gathering important business and market-
ing intelligence. For these categories, the attributes
that we are interested in are brand (B), garment
type/style (G), size (S), and color (C). We gather
105, 335 listings from mens clothing category and
72, 628 listings from womens clothing category,
constituting a dataset of 1, 380, 337 word tokens. On
average, each listing title contains 7.76 words.
A few examples of listings from eBays clothing
and shoes categories are shown in Fig 1. When de-
signing an attribute extraction system to distinguish
between the 4 attribute types, we must take into ac-
count the fact that individual words alone with-
out considering context are ambiguous, as each
word can belong to multiple attribute types. To give
concrete examples, inc is a brand name of womens
apparel but many sellers use it as an acronym for
inch (brand vs. size). The word blazer can be a
brand entity or it can be a garment type (brand vs.
garment type). In addition, like other real-world
user-generated texts, eBay listings are littered with
site-specific acronyms, e.g. BNWT (brand new with
tag), NIB (new in box), and abbreviations introduced
by individual sellers, e.g. immac (immaculate), trs
(trousers). In designing an information extraction
system for our dataset, we need to account for the
general as well as specific properties of our dataset.
</bodyText>
<sectionHeader confidence="0.974477" genericHeader="method">
4 Supervised Named Entity Recognition
</sectionHeader>
<bodyText confidence="0.99849">
In the first part of this work, we adopt a supervised
named entity recognition (NER) framework for the
attribute extraction problem from eBay listing titles.
The goal is to correctly extract attribute values cor-
responding to the 4 attribute types, from each list-
ing. One key assumption of the supervised learn-
ing paradigm is the availability of a labeled training
data for training a classifier to distinguish between
different classes. We generate our training data in
</bodyText>
<page confidence="0.989714">
1559
</page>
<figureCaption confidence="0.810731">
\x0cFigure 1: Example listings and their corresponding labels from the clothing and shoes category.
</figureCaption>
<bodyText confidence="0.999119533333333">
the following manner. For each listing, we remove
extraneous punctuation symbols (*,(,),!,:,;) and tok-
enize each listing into a sequence of tokens. Given 4
dictionaries of seed values for the 4 attribute types,
we match n-gram tokens to the seed values in the
dictionaries, and create an initial round of labeled
training set, which must then be manually inspected
for correctness. In this work, we tagged and manu-
ally verified 1, 000 listings randomly sampled from
the 105, 335 listings from the mens clothing cate-
gory, resulting in a total of 7, 921 labeled tokens with
1, 521-word vocabulary. Fig. 1 shows examples of
labeled listings, with tags B corresponding to brand,
C for color, S for size, G for garment type/style, and
NA for none of the above.
</bodyText>
<subsectionHeader confidence="0.996795">
4.1 Classifiers
</subsectionHeader>
<bodyText confidence="0.998306882352941">
One of the most popular generative model based
classifiers for named entity recognition tasks is Hid-
den Markov Model (HMM), which explicitly cap-
tures temporal statistics in the data by modeling
state (label/tag) transitions over time. Discrimina-
tive classifiers, which directly model the posterior
distribution of class label given features, i.e. SVM
(Isozaki and Kazawa 2002) and Maximum Entropy
model for NER (Chieu and Ng 2003), have been
shown to outperform generative model based clas-
sifiers. More recently, Conditional Random Fields
(CRF) (Feng and McCallum 2004; McCallum 2003)
has been proposed for a sequence labeling problem
and has been established by many as the state-of-
the-art model for supervised named entity recogni-
tion task. In this section, we briefly summarize the
pros and cons of each approach.
</bodyText>
<subsubsectionHeader confidence="0.958378">
4.1.1 Hidden Markov Models
</subsubsectionHeader>
<bodyText confidence="0.997360833333333">
A hidden Markov model (HMM) is a probabilistic
generative model for sequential data. HMM is char-
acterized by 2 sets of model parameters emission
probabilities which produce the observation variable
given the hidden state, and the state transition prob-
ability matrix which captures the temporal correla-
tion in the hidden state sequences. Given a set of la-
beled training sequences as shown in Figure 1, one
can train an HMM to model temporal statistics in
the observation sequences. In our task, a sequence
of word tokens from listing titles are our observa-
tions. One simple approach to use HMM is to set a
hidden state to correspond to a tag class. In the train-
ing phase, since all the tags are given, the hidden
states indeed become visible and inference in this
model becomes much more simplified. The multino-
mial parameter for the emission probabilities p(w|s)
can be learned with a closed-form update (maximum
likelihood estimate). During testing, however, an ef-
ficient forward-backward algorithm must be used to
infer the most likely tag sequence that accounts for
the observation.
One main drawback of HMM is the type of fea-
tures that it can handle. Like other probabilistic gen-
erative models, in order to account for rich, over-
lapping feature sets, e.g. text formatting features,
the correlation structures in the overlapping features
must be explicitly modeled. Indeed, in the clas-
sic HMM based NER, the simple feature used is
the word identity itself, which might not be suffi-
ciently discriminative in distinguishing between dif-
ferent classes. In addition, because of data sparsity
(out-of-vocabulary) problem due to the long-tailed
distribution of words in natural language, sophisti-
cated unknown word models are generally needed
for good performance (Klein et al. 2003).
</bodyText>
<subsubsectionHeader confidence="0.988253">
4.1.2 Maximum Entropy models
</subsubsectionHeader>
<bodyText confidence="0.998550181818182">
The principle of maximum entropy states that
among all the distributions that satisfy feature con-
straints, we should pick the distribution with the
highest entropy, since it makes the least assumption
about the data and will have better generalization
capability to unseen data. Maximum entropy clas-
sifier, therefore, is the highest entropy conditional
distribution of the class label given features, which
has been shown to conveniently take an exponential
form. Maximum entropy classifier is thus closely
related to logistic regression model.
</bodyText>
<page confidence="0.982367">
1560
</page>
<listItem confidence="0.998433846153846">
\x0cPosition Features:
- Position from the beginning of listing
- Position to the end of listing
Orthographic Features:
- Identity of the current word
- Current word contains a digit
- Current word contains only digits
- Current word is capitalized
- Current word begins with a capitalized letter followed by
all non-cap letters.
- Current word is &amp;
- Current word is
- N-gram substring features of current word (N = 4, 5, 6)
Context Features:
- Identity of 2 words before the current word
- Identity of 2 words after the current word
- Previous word is from
- Previous word is by
- Previous word is and
- N-gram substring features of neighboring words (N = 4, 5, 6)
Dictionary Features:
- Membership to the 4 dictionaries of attributes
- Exclusive membership to dictionary of brand names
- Exclusive membership to dictionary of garment types
- Exclusive membership to dictionary of sizes
- Exclusive membership to dictionary of colors
</listItem>
<tableCaption confidence="0.679541">
Table 1: Feature set used in discriminative classifiers.
MaxEnt classifiers (Ratnaparkhi 1996; Ratna-
</tableCaption>
<bodyText confidence="0.994841125">
parkhi 1998) have been applied to various NLP ap-
plications. The attraction of the framework lies in
the ease with which different information sources
used in the modeling process are combined and the
good results that are reported with the use of these
models. The set of redundant features used for the
MaxEnt classifier is the same as those used for the
SVM classifier, which we outline in the next section.
</bodyText>
<subsubsectionHeader confidence="0.971873">
4.1.3 Support Vector Machines
</subsubsectionHeader>
<bodyText confidence="0.998467209302326">
Support Vector Machine (SVM) is yet another
popular classifier for a supervised NER task. In a
binary classification case, SVM finds parameters of
a linearly separating hyperplane that best separates
data from the 2 classes, in a sense that the margin
of separation is maximized. Since only the samples
closest to the decision boundary (the so-called sup-
port vectors) determine the location of the separating
hyperplane, SVM can be trained on very few train-
ing examples even for data in a high-dimensional
space. For our supervised NER system, we use the
following features, as described in detail in Table 1,
as input to the discriminative classifiers.
The use of char N-gram (N-gram substring) fea-
tures was inspired by the work of (Klein et al. 2003),
where the introduction of such features has been
shown to improve the overall F1 score by over 20%.
In (Kanaris et al. 2006), char N-gram features con-
sistently outperform word features in learning effec-
tive spam classifiers. Indeed the use of character N-
gram features as an input to the classifier subsumes
the use of prefix, suffix, and the entire word features.
Generally speaking, char N-gram features provide a
more robust representation against misspelling since
string s1 and its spelling variant s2 may share many
char N-gram substrings in common.
POS and punctuation features are not used in our
NER system. This is mainly due to the fact that eBay
listing titles are not complete sentences and the out-
put from running a POS tagger through such data
can indeed be unreliable. For punctuation features,
eBay sellers are known to abuse punctuation marks
excessively to draw attention of the potential buyers
to click on their listings. In addition, we find that
morphological features are less predictive of entity
names in eBay listing titles than they are in formal
documents. To give a concrete example, capitaliza-
tion is a good predictor of entity names in traditional
NER systems, but on the eBay site, many sellers
use all-cap or all-lowercase letters for every word
in their titles, bringing into question the discrimi-
native power of widely used features in traditional
NER systems.
</bodyText>
<subsubsectionHeader confidence="0.991174">
4.1.4 Viterbi Smoothing
</subsubsectionHeader>
<bodyText confidence="0.997843052631579">
The Viterbi algorithm can be used to smooth the
prediction output from SVM or MaxEnt. More
specifically, the Viterbi decoder enforces the tempo-
ral consistency on the individual label prediction as
inferred by the base classifier MaxEnt or SVM,
independently based on the feature representation
of each word token. The probabilistic ouput of the
base classifier is the observation or evidence, while
the temporal consistency is encoded in the empirical
state transition probability matrix inferred from the
training data. This scenario is analogous to compar-
ing MAP (maximum a priori) estimate with that of
ML (maximum likelihood) in that the former incor-
porates a prior belief when making a final estimate
of the parameter values (most likely label sequence
predicted by the Viterbi algorithm), while the latter
uses only the observation to infer the most likely pa-
rameter estimate (independently inferred predicted
labels of each word token from the base classifier).
</bodyText>
<page confidence="0.809827">
1561
</page>
<bodyText confidence="0.992097125">
\x0cWe adopt the approach from the work of (Chieu
and Ng 2003), which uses Viterbi to improve the
classification results from MaxEnt classifier for
NER tasks. Instead of computing the transition
probability matrix by recording the frequency of
how many times state i at time T transitions to state
j at time T + 1, we simply record that this state i
to j transition is admissible. This approach, indeed,
divides a set of all label sequences into ones that are
admissible and inadmissible, and assign equal prob-
abilities to all the admissible sequences. Such an ap-
proach therefore eliminates all the inadmissible se-
quences of labels (i.e. prohibit the scenario where
-in sub-tag is followed by -begin sub-tag), while al-
lowing the Viterbi algorithm to give more weight to
the classification outputs from SVM or MaxEnt.
</bodyText>
<subsubsectionHeader confidence="0.958432">
4.1.5 Conditional Random Field (CRF)
</subsubsectionHeader>
<bodyText confidence="0.997611272727273">
Conditional Random Field, since its conception in
the seminal work of (Lafferty et al. 2002), is a dis-
criminative classifier for sequential data that com-
bines the best of both worlds. Like SVM and Max-
Ent, CRF is a discriminative classifier that directly
models the conditional distribution of the target vari-
able given the observed variable, i.e. no modeling
resource is wasted in modeling complex correlation
structures in the observation sequences. Like HMM,
CRF makes prediction on the label sequence by in-
corporating the temporal smoothness. Indeed CRF
has been established by many as the state-of-the-
art supervised named entity recognition system for
traditional NER tasks (Feng and McCallum 2004;
McCallum 2003), for NER in biomedical texts (Set-
tles 2004), and in various languages besides English,
such as Bengali (Ekbal et al. 2008) and Chinese
(Mao et al 2008). Various modifications to CRF
have recently been introduced to take into account
of non-local dependencies (Krishnan and Manning
2006) or broader context beyond training data (Du
et al. 2010).
</bodyText>
<subsectionHeader confidence="0.951897">
4.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.995658428571429">
In this section, we compare the generative model
based and discriminative model classifiers for super-
vised NER tasks. Given 1, 000 manually tagged list-
ings from the clothing and shoes category in eBay,
we adopt a 90-10 split and use 90% of the data for
training and 10% for testing. Each listing title is to-
kenized into a sequence of word tokens, each manu-
</bodyText>
<table confidence="0.989737">
SVM MaxEnt HMM CRF
w/o Viterbi 89.05% 87.64% - -
w/ Viterbi 89.47% 88.13% 83.82 93.35%
</table>
<tableCaption confidence="0.996016">
Table 2: Classification accuracy (%) on 9-class NER on
</tableCaption>
<bodyText confidence="0.953132976190476">
mens clothing dataset, comparing SVM, MaxEnt, super-
vised HMM, and CRF.
ally assigned to one of the 5 tags: brand (B), size
(S), color (C), garment type (G), and none of the
above (NA). In order to more accurately capture the
boundary of multi-token attribute values, we further
sub-divide each tag into 2 classes using -beg and -in
sub-tags. This step increases the number of classes
that our classifier needs to handle from 5 to 9 classes
given as follows: {B-beg, B-in, C-beg, C-in, S-beg,
S-in, G-beg, G-in, and NA}.
Table 2 shows a comparison of classification ac-
curacy from 4 classifiers SVM, MaxEnt, HMM,
and CRF. Supervised HMM, with the most simplis-
tic feature, yields the baseline result at 83.82% accu-
racy. All the discriminative classifiers CRF, Max-
Ent, and SVM outperform the baseline by HMM,
with CRF improving on the baseline performance by
the largest margin, concurring to other reports of its
state-of-the-art results. Indeed, when using exactly
the same set of features as SVM and MaxEnt, the
performance of CRF indeed drops to 89.11%, which
is on par with that of SVM and MaxEnt. However,
when restricting to using dictionary and word iden-
tity features, the performance of CRF improves, in-
dicating the importance of feature selection to such
model. SVM and MaxEnt yield similar performance
with SVM slightly outperforming MaxEnt classifier
by 1.6%. The incorporation of temporal smoothness
constraint enforced by the Viterbi algorithm slightly
improves the label sequence prediction (comparing
row 1 and row 2 in Table 2).
The HMM implementation used in our experi-
ments is the Hunpos tagger in (Halacsy et al. 2007),
which captures the state transitional probabilities us-
ing second-order Markov model. For SVM, we use
the popular libSVM package (Chang and Lin 2001)
which produces probabilistic output from fitting a
sigmoid function to the distances between samples
and the separating hyperplane. We use linear kernel
in our experiments, although RBF kernel with grid
search for optimal parameters yield slightly superior
</bodyText>
<page confidence="0.960391">
1562
</page>
<bodyText confidence="0.8343515">
\x0cperformance, with a significantly higher computa-
tional cost. The MaxEnt implementation used in our
experiment is the version available from the NLTK
toolkit, with BFGS optimizer. For CRF, we use the
linear-chain CRF model available from the Mallet
package1.
</bodyText>
<sectionHeader confidence="0.971806" genericHeader="method">
5 Bootstrapping for Dictionary Expansion
</sectionHeader>
<bodyText confidence="0.9941193125">
The supervised learning approach assumes the ex-
istence of an annotated set of training data. Often
times, training data must be painstakingly marked
up and collecting large-scale labeled training exam-
ples can be very costly. In recent years, more and
more research effort has been focused on how to
leverage a vast amount of unlabeled data in a semi-
supervised or entirely unsupervised fashion for NER
as well as for other similar NLP tasks, e.g. POS
tagging, sentence boundary detection, and word
sense disambiguation (Riloff 1999; Ghani and Jones
2002; Probst et al. 2006; Brody and Elhadad 2010;
Haghighi 2010).
One way to incorporate a vast amount of unla-
beled data is to learn a clustering of words that as-
signs syntactically similar words to the same clus-
ters. Popular clustering algorithms used prevalently
in many NER systems are, for example, the combi-
nation of distributional and morphological similar-
ity work of (Clark 2003) or the classic N-gram lan-
guage model based clustering algorithm of (Brown
et al. 1992). In such a system, when training an
NER classifier, we introduce a word cluster id as an
additional feature in the input, with the hope that the
model will pick out clusters that are highly indica-
tive of each class. When encountering words that
are out-of-vocabulary (OOV) in the test set, if those
words are assigned the same cluster membership as
some other words in the training set, the cluster fea-
ture will fire, allowing for correct classification re-
sults to be obtained (Lin and Wu 2009; Faruqui and
Pado 2010).
</bodyText>
<subsectionHeader confidence="0.994186">
5.1 Growing Seed Dictionary
</subsectionHeader>
<bodyText confidence="0.99665">
In this work, we focus on the problem of how to
grow the seed dictionary and discovering new brand
names from eBay listing data. While the perfor-
mances of supervised NER classifiers as described
in sections 4.1.1-4.1.5 are satisfactory, in practice,
</bodyText>
<equation confidence="0.825251">
1
http://mallet.cs.umass.edu/
</equation>
<bodyText confidence="0.999016763157895">
however, especially with a small training set size,
we often find that the trained model puts too much
weight on the dictionary membership feature and
new attribute values are not properly detected. In
this section, instead of using the seed list of known
attribute values as a feature into a classifier, we
use the seed values to automatically generate la-
beled training data. For the specific case of brand
discovery, this initial list used to generate training
data must contain only names that are unambigu-
ously brands. We hence remove ambiguous names
or phrases that belong to multiple attribute types
from the list, such as jumpers(both a brand name
and a garment type), or (ii) camel is a short name
of brand Camel active as well as a color, or (iii) lrg
is an acronym for a brand as well as an acronym for
large which specifies size.
The training/test data is generated by matching
N-gram tokens in listing titles to all the entries in
the initial brand seed dictionary. Following the con-
vention in (Minkov et al. 2005), we use the follow-
ing set of 5 tags, (1) one-token entity (B1 tag) (2)
first token of a multi-token entity (Bo tag for Brand-
open) (3) last token of a multi-token entity (Bc tag
for Brand-close) (4) middle token of a multi-token
entity (Bi tag for Brand-inside) (5) token that is not
part of a brand entity (NA tag). The listings with
at least one non-NA tags are put in the training set,
and listings that contain only NA tags are in the test
set. Similar to the acronym expansion algorithm of
(Pakhomov 2002) which learns contexts that asso-
ciate acronyms to their correct expansions, the in-
tuition behind our work in this section is that the
classifier, trained on a labeled training set of known
brands, learns context patterns that can discriminate
the current word as being a brand (more precisely as
part of a brand) from the other attribute types, which
are now lumped together as NA.
</bodyText>
<subsectionHeader confidence="0.987116">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.99276125">
In the first experiment, a set of 72, 628 listings from
the womens clothing category is partitioned into a
training set of 39, 448 listings and test set of 33, 180
listings based on an initial seed list of known 6, 312
womens apparel brands manually prepared by our
fashion experts. The partitioning is done, as de-
scribed in great detail above, in such a way that
known brands in the seed list do not exist in the
</bodyText>
<page confidence="0.852376">
1563
</page>
<table confidence="0.369849571428571">
\x0cWomens Clothing Mens Clothing Garment Type
monsoon henleys nightshirt
riverislandtop abercrombie&amp;fitch cargoshorts
dorothyperkins lacost trenchcoat
river islanfd versace sweatpants
marks&amp;spencers sonnetti cardigans
river islands supremebeing boardshorts
river islan brookhaven tracksuite
monsoomn guiness swimshorts
dorothry perkins next trouses
principle suprerdry microfleece
?river island henbury boilersuit
bnwtmonsoon paul smiths snopants
marella ricci pjs
</table>
<tableCaption confidence="0.845998">
soulcal craghopper jkt
Table 3: Discovered attribute values, ranked order by
</tableCaption>
<bodyText confidence="0.9815144375">
their confidence scores. (Left) Discovered brands from
Womens clothing category. We use 6,312 brands as seed
values. (Middle) Discovered brands from Mens clothing
category, with 3,499 seed values used. (Right) Discov-
ered garment types (styles) from Mens clothing category,
learned from 203 seed values.
test data (using exact string matching criterion). We
train a 5-class MaxEnt classifier and adopt the same
feature sets as described in Section 4.1.3. During
the test phase, the classifier predicts the most likely
brand attribute from each listing, where we are only
interested in the predictions with confidence scores
exceeding a set threshold. We ranked order the pre-
dicted brands by their confidence scores (probabil-
ities) and the top 300 unique brands are selected.
We manually verify the 300 predicted brands and
found that 90.33% of the predicted brands are indeed
names of designers or womens apparel stores (true
positive), resulting a precision score of 90.33%.
Indeed, the precision score presented above is ob-
tained using an exact matching criterion where par-
tial extraction of a brand is regarded as a miss, i.e.
our extractor extracts only Calvin when Calvin Klein
is present in the listing (false positive). The left col-
umn of Table 3 shows examples of newly discovered
brands from Womens clothing category. Many of
these newly discovered brands are indeed misspelled
versions of the known brands in the seed dictionary.
The middle column of Table 3 shows a set of
Mens clothing brands learned automatically from a
similar experiment conducted on a set of 105, 335
listings from Mens clothing category. Using an ini-
</bodyText>
<table confidence="0.714456333333333">
Seed list Test set 1 Test set 2
Orig. seeds 83.56% 90.02%
Orig. seed + 200 new brands 92.75% 93.66%
</table>
<tableCaption confidence="0.956761">
Table 4: NER Accuracy on 2 test sets as the seed dictio-
</tableCaption>
<bodyText confidence="0.995082405405405">
nary for brands grows. Results shown here are obtained
the same Mens clothing category dataset, as used to show
the supervised NER results in Table 2.
tial set of 3, 499 known brand seeds, we partition
the dataset into a training set of 67, 307 listings and
a test set of 38, 028 listings (for later reference we
refer to this test set as set A). Based on the top 200
predicted brands, 179 of which are verified as being
true positive samples, resulting in 89.5% precision.
We carry out a similar experiment to grow the seed
dictionary for garment type, and are able to iden-
tify the top 60 new garment types. 54 out of 60 are
true positive samples, resulting in precision score =
90%. Examples of the newly discovered garment
types are shown in Table 3 (right column), where ab-
breviated forms of garment types such as jkt (short
for jacket) and pjs (short for pajamas) are also dis-
covered through our algorithm.
By adding these newly discovered attributes back
to the dictionary, we can now re-evaluate our super-
vised NER system from section 4 with the grown
seed list. To this end, we construct 2 test sets from
the same 105, 335 listings of Mens clothing cate-
gory as used in Section 4. Test set 1 is a set of
500 listings randomly sampled from the 38, 028-
listing subset known not to contain any brands in
the original brand seed dictionary (set A). As seen
in Table 4, an improvement of 9% in accuracy re-
sults from the use of the grown seed list. Since
this dataset is known to not contain any brands from
the original brand seed dictionary, the addition of
200 new brands solely accounts for all the accuracy
boost. Test set 2 is constructed slightly differently
by randomly sampling 500 listings from the entire
105, 335 listings of Mens clothing category. As
seen in Table 4, a smaller improvement of 3.7% is
observed.
</bodyText>
<sectionHeader confidence="0.989328" genericHeader="method">
6 Normalization
</sectionHeader>
<bodyText confidence="0.999781333333333">
With the above described brand discovery algorithm,
the newly discovered brands from the test set can be
grouped into 2 categories (i) misspelling, spelling
</bodyText>
<page confidence="0.910975">
1564
</page>
<bodyText confidence="0.998222192307692">
\x0cinvariants, abbreviated forms of known brands in the
seed list or (ii) novel brands or clothing/shoes de-
signers, which are not members of the original seed
list. Normalizing the variants of a known brand to
a single normalized output value is an important as-
pect of our attribute extraction algorithm, as these
variants account for over 20% of listings in the eBay
clothing and shoes category. When gathering busi-
ness/marketing intelligence, missing out on 20% of
the data could skew the calculation of supply, de-
mand, and pricing metrics, and eventually lead to
the wrong policy decision made.
The problem of alternate spellings of names has
been addressed in the database community success-
fully using fuzzy string matching algorithms e.g.
Soundex or string edit distance. In this work, since
the attribute values are often partially extracted, i.e.
a word in a multi-word phrase is extracted, in or-
der to match to the correct normalized value, we
must investigate robust substring matching algo-
rithms suitable for partial matching. To this end,
we explore 2 string similarity/distance measures for
normalizing the extracted attributes. First, we in-
vestigate n-gram similarity measures defined as the
number of shared character n-grams, i.e. substrings
of length n (Kondrak 2005). More specifically, a
string similarity measure between s1 and s2 is de-
fined as the percentage of common substrings of
length n (out of all substrings of length n). This
similarity measure is quite robust to partial match-
ing, as a two-word phrase can appear out of order
while most of the character n-grams, where n = 3,
remain virtually unchanged. Certainly, finding the
right value of n will greatly impact the matching per-
formance of the algorithm. In our experiment, we
find the optimal n for brands to be 3 and 4. Table 5
shows a few examples of normalized outputs as a re-
sult of finding the best match for the extracted brand
names from among a set of predefined normalized
values. When the best matching score falls below a
threshold, we declare no match is found and classify
the extracted brand as a new brand.
Another distance measure that we explore is the
Jaro-Winkler distance. Designed to be more suitable
for matching short strings such as peoples names,
Jaro-Winkler distance is defined based on the num-
ber of character transpositions and the number of
matching characters. In addition, a prefix scale p
Extracted brands Normalized values
river islands river island
fruit of loom fruit of the loom
fruit loom fruit of the loom
</bodyText>
<figure confidence="0.952322928571429">
ralph lauren ralph lauren
mark &amp; spencer marks &amp; spencer
yvessaintlaurent yves saint laurent
yves st laurent yves saint laurent
combats combat
kickers kickers
kickers kickers
armarni armani
abrecrombie abercrombie
life &amp; limb NEW BRAND
oliver baker NEW BRAND
haines &amp; bonner NEW BRAND
dehavilland NEW BRAND
nigel cabourn NEW BRAND
</figure>
<tableCaption confidence="0.844495">
Table 5: Extracted brands and their corresponding nor-
malized values.
</tableCaption>
<bodyText confidence="0.990818">
parameter is used and can be tuned to weigh more
favorably on strings that match from the beginning
for a set prefix length. In our experiments with brand
normalization, over 50% of the matches from the
Jaro-Winkler distance are, however, identified as be-
ing incorrect.
</bodyText>
<sectionHeader confidence="0.997195" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99936645">
In this work, we have described an information ex-
traction system for applications in the domain of in-
ventory/business Intelligence. The goal is given an
eBay listing title, our system correctly extracts the
defining attributes in order to associate each item to
a specific product. We investigate and compare sev-
eral supervised NER systems supervised HMM,
SVM, MaxEnt, and CRF and found SVM and
MaxEnt with Viterbi decoding to yield the best per-
formance. Focusing on the clothing and shoes cat-
egories on eBays site, we presented a bootstrapped
algorithm that can identify new brand names corre-
sponding to (1) spelling invariants or typographical
errors of the known brands in the seed list and (2)
novel brands or designers. Our attribute extractor
correctly discovers new brands with over 90% pre-
cision on multiple corpora of listings. To output nor-
malized attribute values, we explore several fuzzy
string comparison algorithms and found n-gram sub-
string matching to work well in practice.
</bodyText>
<page confidence="0.922093">
1565
</page>
<sectionHeader confidence="0.866785" genericHeader="acknowledgments">
\x0c8 Acknowledgment
</sectionHeader>
<bodyText confidence="0.993671333333333">
The authors would like to thank Nalini Johnas and
Padmanaban Ramasamy for their help in gathering
listing data used in all of our experiments.
</bodyText>
<sectionHeader confidence="0.983508" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99419887654321">
A. Berger, S. Pietra, V. Pietra, A Maximum Entropy Ap-
proach to Natural Language Processing, ACL 1996.
S. Brody, N. Elhadad, An Unsupervised Aspect-Sentiment
Model for Online Reviews, HLT-NAACL 2010.
P. Brown, P. deSouza, R. Mercer, V. Della Pietra, J.
Lai, Class-based n-gram Models of Natural Language,
ACL 1992.
C.-C Chang, C.-J. Lin, LibSVM: A Library for Support
Vector Machines (2001).
H. L. Chieu, H. T. Ng, Named Entity Recognition with a
Maximum Entropy Approach, ACL 2003.
A. Clark, Combining Distributional and Morphological
Information for Part of Speech Induction, EACL 2003
G. Demartini, C. S. Firan, M. Georgescu, T. Iofciu, R.
Krestel, and W. Nejdl, An Architecture for Finding En-
tities on the web, Latin American Web Congress 2009.
J. Du, Z. Zhang, J. Yan, Y. Cui, and Z. Chen. Using
search session context for named entity recognition in
query. In SIGIR10, Geneva, Switzerland, July 19-23
2010.
Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopadhyay.
2008. Named entity recognition in Bengali: A condi-
tional random field approach. In Proceedings of IJC-
NLP, pages 589594.
M. Faruqui, S. Pado, Training and Evaluating a German
Named Entity Recognizer with Semantic Generaliza-
tion, Proceedings of Konvens 2010, Saarbrucken, Ger-
many.
F. Feng, A. McCallum, Chinese segmentation and new
word detection using conditional random fields, in
COLING 2004.
J. R. Finkel, T. Grenager, and C. Manning, Incorporat-
ing Non-local Information into Information Extraction
Systems by Gibbs Sampling, ACL 2005.
J. R. Finkel, C. Manning, Nested Named Entity Recogni-
tion, EMNLP 2009.
R. Ghani, K. Probst, Y. Liu, M. Krema, A. Fano, Text
Mining for Product Attribute Extraction, SIGKDD,
2006.
R. Ghani, R. Jones, A comparison of efficacy and as-
sumptions of bootstrapping algorithms for training
information extraction systems, Workshop on Lin-
guistic Knowledge Acquisition and Representation at
the Third International Conference on Language Re-
sources and Evaluation (LREC), 2002.
T. Grenager, D. Klein, and C. D. Manning, Unsupervised
Learning of Field Segmentation Models for Informa-
tion Extraction, ACL 2005.
D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and A.
Sheth. Context and Domain Knowledge Enhanced En-
tity Spotting In Informal Text. In Proceedings of the 8th
International Semantic Web Conference (ISWC 2009).
Springer, 2009.
A. D. Haghighi, Unsupervised Models of Entity Refer-
ence Resolution, Ph. D. Thesis, University of Calfor-
nia, Berkeley, 2010.
P. Halacsy, A. Kornai, C. Oravecz, HunPos: an open
source trigram tagger, ACL 2007.
H. Isozaki and H. Kazawa, Efficient Support Vector Clas-
sifiers for Named Entity Recognition, ACL 2002.
R. Jones, Learning to Extract Entities from Labeled and
Unlabeled Text, PhD Thesis, 2005.
I. Kanaris, K. Kanaris, I. Houvardas, E. Stamatatos,
Words vs. Character N-grams for Anti-spam Filtering,
International Journal on Artificial Intelligence Tools,
2006.
D. Klein, J. Smarr, H. Nguyen, C. Manning, Named En-
tity Recognition with Character-level Models, CoNLL
2003.
R. Koeling, Chunking with Maximum Entropy Models,
Proc. of CoNLL-2000.
G. Kondrak, N-Gram Similarity and Distance, SPIRE
2005.
V. Krishnan and C. D. Manning, An effective two-stage
model for exploiting non-local dependencies in named
entity recognition, in ACL-COLING, 2006.
T. Kudo, Y. Matsumoto, Chunking with Support Vector
Machines, ACL 2001.
J. Lafferty, A. McCallum, F. Pereira, Conditional Ran-
dom Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data, ICML 2002.
V. I. Levenshtein, Binary code capable of correcting dele-
tions, insertions, and reversals. Phs. Dokl., 6:707-710.
D. Lin, X. Wu, Phrase Clustering for Discriminative
Learning, ACL 2009.
B. Liu, M. Hu, and J. Cheng, Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web, WWW
2005.
Xinnian Mao, Saike He, Sencheng Bao, Yuan Dong,
and Haila Wang, Chinese Word Segmentation and
Named Entity Recognition Based on Conditional Ran-
dom Fields, Sixth SIGHAN Workshop on Chinese
Language Processing, 2008
A. McCallum, Efficiently Inducing Features of Condi-
tional Random Fields, UAI 2003.
A. McCallum, D. Jensen, A Note on Unification
of Information Extraction and Data Mining using
Conditional-Probability, Relational Models, Proceed-
ings of IJCAI-2003 on Learning Statistical Models
from Relational Data, 2003.
1566
\x0cJ. F. McCarthy, A Trainable Approach to Coreference
Resolution for Information Extraction, Ph. D. Thesis,
University of Massachusetts at Amherst, 1996.
E. Minkov, R. C. Wang, and W. W. Cohen, Extracting
Personal Names from Email: Applying Named Entity
Recognition to Informal Text, ACL 2005.
Mike Mintz, Steven Bills, Rion Snow, Daniel Juraf-
sky. 2009. Distant Supervision for Relation Extraction
without Labeled Data, In Proceedings of ACL/AFNLP
2009.
S. Moghaddam, M. Ester, Opinion Digger: An Unsuper-
vised Opinion Miner from Unstructured Product Re-
views, CIKM 2010
David Nadeau, P. Turney, S.Matwin, Unsupervised
Named Entity Recognition: Generating Gazetteers
and Resolving Ambiguity. In Proc. Canadian Confer-
ence on Artificial Intelligence, 2006.
David Nadeau and Satoshi Sekine. A survey of named en-
tity recognition and classification. Linguisticae Inves-
tigationes, 30(1):326, 2007.
Nadeau, D., Semi-Supervised Named Entity Recognition:
Learning to Recognize 100 Entity Types with Little Su-
pervision, PhD thesis, University of Ottawa, 2007.
S. Pakhomov, Semi-supervised Maximum Entropy Based
Approach to Acronym and Abbreviation Normalization
in Medical Texts, ACL 2002.
A.-M. Popescu, O. Etzioni, Extracting Product Features
and Opinions from Reviews, EMNLP 2005.
K. Probst, R. Ghani, M. Krema, A. Fano, Semi-
Supervised Learning to Extract Attribute-Value Pairs
from Product Descriptions on the Web, ECML 2006.
V. Punyakanok, D. Roth, The use of classifiers in sequen-
tial inference, NIPS 2001.
H. Raghavan, J. Allan, Matching Inconsistently Spelled
Names in Automatic Speech Recognizer Output for In-
formation Retrieval, HLT-EMNLP 2005.
A. Ratnaparkhi, A Maximum Entropy Part of Speech Tag-
ger. In EMNLP 1996.
A. Ratnaparkhi, Maximum Entropy Models forNatural
Language Ambiguity Resolution, Ph. D. Thesis, Uni-
versity of Pennsylvania.
E. Riloff, R. Jones, Learning Dictionaries for Informa-
tion Extraction by Multi-Level Bootstrapping, AAAI
1999.
Settles, B. (2004), Biomedical named entity recognition
using conditional random fields and rich feature sets,
in Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (NLPBA), 2004, Geneva, Switzerland.
W. M. Soon, H. T. Ng, D. Chung, Y. Lim, A machine
learning approach to coreference resolution of noun
phrases, Computational Linguistics, 27(4): 521-544,
2001.
H. Wallach, Efficient Training of Conditional Random
Fields, M. Sc. Thesis, Division of Informatics, Uni-
versity of Edinburgh, 2002.
D. Wu, W. S. Lee, N. Ye, and H. L. Chieu, Domain
adaptive bootstrapping for named entity recognition,
EMNLP 2009.
Y. Zhao, B. Qin, S. Hu, T. Liu, Generalizing Syntactic
Structures for Product Attribute Candidate Extraction,
</reference>
<figure confidence="0.665675">
ACL 2010
1567
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.766353">
<note confidence="0.95868">b&amp;apos;Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 15571567, Edinburgh, Scotland, UK, July 2731, 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.90051">Bootstrapped Named Entity Recognition for Product Attribute Extraction</title>
<author confidence="0.980995">Duangmanee Putthividhya</author>
<affiliation confidence="0.999764">eBay Inc.</affiliation>
<address confidence="0.9997185">2065 Hamilton Ave San Jose, CA 95125</address>
<email confidence="0.999713">dputthividhya@ebay.com</email>
<author confidence="0.996467">Junling Hu</author>
<affiliation confidence="0.999939">eBay Inc.</affiliation>
<address confidence="0.9996655">2065 Hamilton Ave San Jose, CA 95125</address>
<email confidence="0.999764">juhu@ebay.com</email>
<abstract confidence="0.99929965">We present a named entity recognition (NER) system for extracting product attributes and values from listing titles. Information extraction from short listing titles present a unique challenge, with the lack of informative context and grammatical structure. In this work, we combine supervised NER with bootstrapping to expand the seed list, and output normalized results. Focusing on listings from eBays clothing and shoes categories, our bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Pietra</author>
<author>V Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing,</title>
<date>1996</date>
<location>ACL</location>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Pietra, V. Pietra, A Maximum Entropy Approach to Natural Language Processing, ACL 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brody</author>
<author>N Elhadad</author>
</authors>
<title>An Unsupervised Aspect-Sentiment Model for Online Reviews,</title>
<date>2010</date>
<location>HLT-NAACL</location>
<contexts>
<context position="1849" citStr="Brody and Elhadad 2010" startWordPosition="271" endWordPosition="274">and found n-gram substring matching to work well in practice. 1 Introduction Traditional named entity recognition (NER) task has expanded beyond identifying people, location, and organization to book titles, email addresses, phone numbers, and protein names (Nadeau and Sekine 2007). Recently there has been a surge of interest in extracting product attributes from online data due to the rapid growth of E-Commerce. Current work in this domain focuses on mining product reviews and descriptions from retailer websites. Such text data tend to be long and generate enough context for the target task (Brody and Elhadad 2010; Liu et al. 2005; Popescu and Etzioni 2005). In this paper, we focus on mining short product listing titles, which poses unique challenges. Short listings are typical in classified ads where each seller is given limited space (in terms of words) to describe the product. On eBay, product listing titles cannot exceed 55 characters in length. Similarly, on Craigslist and newspaper ads, the length of a listing title is restricted. Extracting product attributes from such short titles faces the following challenges: Loss of grammatical structure in short listings where many nouns are piled together</context>
<context position="6031" citStr="Brody and Elhadad 2010" startWordPosition="944" endWordPosition="947">ilarity/distance measures for this task and found that n-gram substring similarity (Kondrak 2005) yields accurate normalization results. The main contribution of this work is a product attribute extraction system that addresses the unique problems of information extraction from short listing titles. We combine supervised NER with bootstrapping to expand the seed list, and investigate several methods to normalize the extracted results. Our system has been tested on large-scale eBay listing datasets to demonstrate its effectiveness. 2 Related Work Recent work on product attribute extraction by (Brody and Elhadad 2010) applies a Latent Dirichlet Allocation (LDA) model to identify different aspects of products from user reviews. Similar work is presented in (Liu et al. 2005). Topic models such as LDA groups similar words together by identifying topics (product aspects) from patterns of wordoccurrences. Such grouping can discover new aspects of a product such as portability (for netbook computers), but it may generate aspects that are vague and not easily interpretable. Indeed, how to refine discovered aspects and clean up words in each aspect remains an open question. The LDA approach also treats documents a</context>
<context position="27017" citStr="Brody and Elhadad 2010" startWordPosition="4359" endWordPosition="4362"> for Dictionary Expansion The supervised learning approach assumes the existence of an annotated set of training data. Often times, training data must be painstakingly marked up and collecting large-scale labeled training examples can be very costly. In recent years, more and more research effort has been focused on how to leverage a vast amount of unlabeled data in a semisupervised or entirely unsupervised fashion for NER as well as for other similar NLP tasks, e.g. POS tagging, sentence boundary detection, and word sense disambiguation (Riloff 1999; Ghani and Jones 2002; Probst et al. 2006; Brody and Elhadad 2010; Haghighi 2010). One way to incorporate a vast amount of unlabeled data is to learn a clustering of words that assigns syntactically similar words to the same clusters. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the hope that the model will pick out clusters </context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>S. Brody, N. Elhadad, An Unsupervised Aspect-Sentiment Model for Online Reviews, HLT-NAACL 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>P deSouza</author>
<author>R Mercer</author>
<author>V Della Pietra</author>
<author>J Lai</author>
</authors>
<title>Class-based n-gram Models of Natural Language,</title>
<date>1992</date>
<location>ACL</location>
<contexts>
<context position="27442" citStr="Brown et al. 1992" startWordPosition="4430" endWordPosition="4433"> well as for other similar NLP tasks, e.g. POS tagging, sentence boundary detection, and word sense disambiguation (Riloff 1999; Ghani and Jones 2002; Probst et al. 2006; Brody and Elhadad 2010; Haghighi 2010). One way to incorporate a vast amount of unlabeled data is to learn a clustering of words that assigns syntactically similar words to the same clusters. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the hope that the model will pick out clusters that are highly indicative of each class. When encountering words that are out-of-vocabulary (OOV) in the test set, if those words are assigned the same cluster membership as some other words in the training set, the cluster feature will fire, allowing for correct classification results to be obtained (Lin and Wu 2009; Faruqui and Pado 2010). 5.1 Growing Seed Dictionary In this work, we focus on the problem of how to grow</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. Brown, P. deSouza, R. Mercer, V. Della Pietra, J. Lai, Class-based n-gram Models of Natural Language, ACL 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LibSVM: A Library for Support Vector Machines</title>
<date>2001</date>
<contexts>
<context position="25855" citStr="Chang and Lin 2001" startWordPosition="4179" endWordPosition="4182">atures, the performance of CRF improves, indicating the importance of feature selection to such model. SVM and MaxEnt yield similar performance with SVM slightly outperforming MaxEnt classifier by 1.6%. The incorporation of temporal smoothness constraint enforced by the Viterbi algorithm slightly improves the label sequence prediction (comparing row 1 and row 2 in Table 2). The HMM implementation used in our experiments is the Hunpos tagger in (Halacsy et al. 2007), which captures the state transitional probabilities using second-order Markov model. For SVM, we use the popular libSVM package (Chang and Lin 2001) which produces probabilistic output from fitting a sigmoid function to the distances between samples and the separating hyperplane. We use linear kernel in our experiments, although RBF kernel with grid search for optimal parameters yield slightly superior 1562 \x0cperformance, with a significantly higher computational cost. The MaxEnt implementation used in our experiment is the version available from the NLTK toolkit, with BFGS optimizer. For CRF, we use the linear-chain CRF model available from the Mallet package1. 5 Bootstrapping for Dictionary Expansion The supervised learning approach a</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C Chang, C.-J. Lin, LibSVM: A Library for Support Vector Machines (2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Chieu</author>
<author>H T Ng</author>
</authors>
<title>Named Entity Recognition with a Maximum Entropy Approach,</title>
<date>2003</date>
<location>ACL</location>
<contexts>
<context position="14233" citStr="Chieu and Ng 2003" startWordPosition="2285" endWordPosition="2288">vocabulary. Fig. 1 shows examples of labeled listings, with tags B corresponding to brand, C for color, S for size, G for garment type/style, and NA for none of the above. 4.1 Classifiers One of the most popular generative model based classifiers for named entity recognition tasks is Hidden Markov Model (HMM), which explicitly captures temporal statistics in the data by modeling state (label/tag) transitions over time. Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers. More recently, Conditional Random Fields (CRF) (Feng and McCallum 2004; McCallum 2003) has been proposed for a sequence labeling problem and has been established by many as the state-ofthe-art model for supervised named entity recognition task. In this section, we briefly summarize the pros and cons of each approach. 4.1.1 Hidden Markov Models A hidden Markov model (HMM) is a probabilistic generative model for sequential data. HMM is characterized by 2 sets of model parameters emission probabilities which produce the observati</context>
<context position="21693" citStr="Chieu and Ng 2003" startWordPosition="3496" endWordPosition="3499">tency is encoded in the empirical state transition probability matrix inferred from the training data. This scenario is analogous to comparing MAP (maximum a priori) estimate with that of ML (maximum likelihood) in that the former incorporates a prior belief when making a final estimate of the parameter values (most likely label sequence predicted by the Viterbi algorithm), while the latter uses only the observation to infer the most likely parameter estimate (independently inferred predicted labels of each word token from the base classifier). 1561 \x0cWe adopt the approach from the work of (Chieu and Ng 2003), which uses Viterbi to improve the classification results from MaxEnt classifier for NER tasks. Instead of computing the transition probability matrix by recording the frequency of how many times state i at time T transitions to state j at time T + 1, we simply record that this state i to j transition is admissible. This approach, indeed, divides a set of all label sequences into ones that are admissible and inadmissible, and assign equal probabilities to all the admissible sequences. Such an approach therefore eliminates all the inadmissible sequences of labels (i.e. prohibit the scenario wh</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>H. L. Chieu, H. T. Ng, Named Entity Recognition with a Maximum Entropy Approach, ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Combining Distributional and Morphological Information for Part of Speech Induction,</title>
<date>2003</date>
<location>EACL</location>
<contexts>
<context position="27355" citStr="Clark 2003" startWordPosition="4417" endWordPosition="4418">f unlabeled data in a semisupervised or entirely unsupervised fashion for NER as well as for other similar NLP tasks, e.g. POS tagging, sentence boundary detection, and word sense disambiguation (Riloff 1999; Ghani and Jones 2002; Probst et al. 2006; Brody and Elhadad 2010; Haghighi 2010). One way to incorporate a vast amount of unlabeled data is to learn a clustering of words that assigns syntactically similar words to the same clusters. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the hope that the model will pick out clusters that are highly indicative of each class. When encountering words that are out-of-vocabulary (OOV) in the test set, if those words are assigned the same cluster membership as some other words in the training set, the cluster feature will fire, allowing for correct classification results to be obtained (Lin and Wu 2009; Faruqui and Pado </context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>A. Clark, Combining Distributional and Morphological Information for Part of Speech Induction, EACL 2003 G. Demartini, C. S. Firan, M. Georgescu, T. Iofciu, R.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krestel</author>
<author>W Nejdl</author>
</authors>
<title>An Architecture for Finding Entities on the web, Latin American Web Congress</title>
<date>2009</date>
<marker>Krestel, Nejdl, 2009</marker>
<rawString>Krestel, and W. Nejdl, An Architecture for Finding Entities on the web, Latin American Web Congress 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Du</author>
<author>Z Zhang</author>
<author>J Yan</author>
<author>Y Cui</author>
<author>Z Chen</author>
</authors>
<title>Using search session context for named entity recognition in query.</title>
<date>2010</date>
<booktitle>In SIGIR10,</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="23546" citStr="Du et al. 2010" startWordPosition="3794" endWordPosition="3797">ces. Like HMM, CRF makes prediction on the label sequence by incorporating the temporal smoothness. Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCallum 2004; McCallum 2003), for NER in biomedical texts (Settles 2004), and in various languages besides English, such as Bengali (Ekbal et al. 2008) and Chinese (Mao et al 2008). Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010). 4.2 Experimental Results In this section, we compare the generative model based and discriminative model classifiers for supervised NER tasks. Given 1, 000 manually tagged listings from the clothing and shoes category in eBay, we adopt a 90-10 split and use 90% of the data for training and 10% for testing. Each listing title is tokenized into a sequence of word tokens, each manuSVM MaxEnt HMM CRF w/o Viterbi 89.05% 87.64% - - w/ Viterbi 89.47% 88.13% 83.82 93.35% Table 2: Classification accuracy (%) on 9-class NER on mens clothing dataset, comparing SVM, MaxEnt, supervised HMM, and CRF. ally</context>
</contexts>
<marker>Du, Zhang, Yan, Cui, Chen, 2010</marker>
<rawString>J. Du, Z. Zhang, J. Yan, Y. Cui, and Z. Chen. Using search session context for named entity recognition in query. In SIGIR10, Geneva, Switzerland, July 19-23 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asif Ekbal</author>
<author>Rejwanul Haque</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>Named entity recognition in Bengali: A conditional random field approach.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>589594</pages>
<contexts>
<context position="23325" citStr="Ekbal et al. 2008" startWordPosition="3759" endWordPosition="3762">native classifier that directly models the conditional distribution of the target variable given the observed variable, i.e. no modeling resource is wasted in modeling complex correlation structures in the observation sequences. Like HMM, CRF makes prediction on the label sequence by incorporating the temporal smoothness. Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCallum 2004; McCallum 2003), for NER in biomedical texts (Settles 2004), and in various languages besides English, such as Bengali (Ekbal et al. 2008) and Chinese (Mao et al 2008). Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010). 4.2 Experimental Results In this section, we compare the generative model based and discriminative model classifiers for supervised NER tasks. Given 1, 000 manually tagged listings from the clothing and shoes category in eBay, we adopt a 90-10 split and use 90% of the data for training and 10% for testing. Each listing title is tokenized into a sequence of word tokens, each </context>
</contexts>
<marker>Ekbal, Haque, Bandyopadhyay, 2008</marker>
<rawString>Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopadhyay. 2008. Named entity recognition in Bengali: A conditional random field approach. In Proceedings of IJCNLP, pages 589594.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Faruqui</author>
<author>S Pado</author>
</authors>
<title>Training and Evaluating a German Named Entity Recognizer with Semantic Generalization,</title>
<date>2010</date>
<booktitle>Proceedings of Konvens</booktitle>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="27960" citStr="Faruqui and Pado 2010" startWordPosition="4521" endWordPosition="4524">k of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the hope that the model will pick out clusters that are highly indicative of each class. When encountering words that are out-of-vocabulary (OOV) in the test set, if those words are assigned the same cluster membership as some other words in the training set, the cluster feature will fire, allowing for correct classification results to be obtained (Lin and Wu 2009; Faruqui and Pado 2010). 5.1 Growing Seed Dictionary In this work, we focus on the problem of how to grow the seed dictionary and discovering new brand names from eBay listing data. While the performances of supervised NER classifiers as described in sections 4.1.1-4.1.5 are satisfactory, in practice, 1 http://mallet.cs.umass.edu/ however, especially with a small training set size, we often find that the trained model puts too much weight on the dictionary membership feature and new attribute values are not properly detected. In this section, instead of using the seed list of known attribute values as a feature into</context>
</contexts>
<marker>Faruqui, Pado, 2010</marker>
<rawString>M. Faruqui, S. Pado, Training and Evaluating a German Named Entity Recognizer with Semantic Generalization, Proceedings of Konvens 2010, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Feng</author>
<author>A McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields, in COLING</title>
<date>2004</date>
<contexts>
<context position="14371" citStr="Feng and McCallum 2004" startWordPosition="2305" endWordPosition="2308">/style, and NA for none of the above. 4.1 Classifiers One of the most popular generative model based classifiers for named entity recognition tasks is Hidden Markov Model (HMM), which explicitly captures temporal statistics in the data by modeling state (label/tag) transitions over time. Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers. More recently, Conditional Random Fields (CRF) (Feng and McCallum 2004; McCallum 2003) has been proposed for a sequence labeling problem and has been established by many as the state-ofthe-art model for supervised named entity recognition task. In this section, we briefly summarize the pros and cons of each approach. 4.1.1 Hidden Markov Models A hidden Markov model (HMM) is a probabilistic generative model for sequential data. HMM is characterized by 2 sets of model parameters emission probabilities which produce the observation variable given the hidden state, and the state transition probability matrix which captures the temporal correlation in the hidden stat</context>
<context position="23186" citStr="Feng and McCallum 2004" startWordPosition="3736" endWordPosition="3739"> et al. 2002), is a discriminative classifier for sequential data that combines the best of both worlds. Like SVM and MaxEnt, CRF is a discriminative classifier that directly models the conditional distribution of the target variable given the observed variable, i.e. no modeling resource is wasted in modeling complex correlation structures in the observation sequences. Like HMM, CRF makes prediction on the label sequence by incorporating the temporal smoothness. Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCallum 2004; McCallum 2003), for NER in biomedical texts (Settles 2004), and in various languages besides English, such as Bengali (Ekbal et al. 2008) and Chinese (Mao et al 2008). Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010). 4.2 Experimental Results In this section, we compare the generative model based and discriminative model classifiers for supervised NER tasks. Given 1, 000 manually tagged listings from the clothing and shoes category in eBay, we adopt a </context>
</contexts>
<marker>Feng, McCallum, 2004</marker>
<rawString>F. Feng, A. McCallum, Chinese segmentation and new word detection using conditional random fields, in COLING 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling,</title>
<date>2005</date>
<location>ACL</location>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. R. Finkel, T. Grenager, and C. Manning, Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling, ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>C Manning</author>
</authors>
<title>Nested Named Entity Recognition,</title>
<date>2009</date>
<location>EMNLP</location>
<marker>Finkel, Manning, 2009</marker>
<rawString>J. R. Finkel, C. Manning, Nested Named Entity Recognition, EMNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ghani</author>
<author>K Probst</author>
<author>Y Liu</author>
<author>M Krema</author>
<author>A Fano</author>
</authors>
<title>Text Mining for Product Attribute Extraction,</title>
<date>2006</date>
<location>SIGKDD,</location>
<marker>Ghani, Probst, Liu, Krema, Fano, 2006</marker>
<rawString>R. Ghani, K. Probst, Y. Liu, M. Krema, A. Fano, Text Mining for Product Attribute Extraction, SIGKDD, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ghani</author>
<author>R Jones</author>
</authors>
<title>A comparison of efficacy and assumptions of bootstrapping algorithms for training information extraction systems,</title>
<date>2002</date>
<booktitle>Workshop on Linguistic Knowledge Acquisition and Representation at the Third International Conference on Language Resources and Evaluation (LREC),</booktitle>
<contexts>
<context position="7743" citStr="Ghani and Jones 2002" startWordPosition="1229" endWordPosition="1232">hod lies in how bootstrapping is used. (Ghani 2006) used EM to add more training data from unlabeled data, while in our approach bootstrapping is used to expand the seed list. First, we automatically generate labeled data by matching seed list to unlabeled data. Then, these auto-labeled training set is used to train a classifier to identify new attribute values from a separate set of unlabeled data. Thirdly, newly discovered product attribute values are added back to our seed list. Thus our original classifier for product attribute extraction can be improved through an expanded seed list. In (Ghani and Jones 2002; Jones 2005), several bootstrapping methods are compared. These methods include self-training, co-EM and EM. All of these approaches are different from ours, as described in detail earlier. In (Probst et al. 2006), a Naive Bayes learner is combined with Co-EM to generate more training data from unlabeled data, and attribute-value pairs are extracted on adjacent words. The automatic bootstrapping in this paper was inspired by (Pakhomov 2002)an acronym expansion algorithm for medical text documents. The underlying assumption is that abbreviated forms and their 1558 \x0ccorresponding expansions </context>
<context position="26973" citStr="Ghani and Jones 2002" startWordPosition="4351" endWordPosition="4354"> from the Mallet package1. 5 Bootstrapping for Dictionary Expansion The supervised learning approach assumes the existence of an annotated set of training data. Often times, training data must be painstakingly marked up and collecting large-scale labeled training examples can be very costly. In recent years, more and more research effort has been focused on how to leverage a vast amount of unlabeled data in a semisupervised or entirely unsupervised fashion for NER as well as for other similar NLP tasks, e.g. POS tagging, sentence boundary detection, and word sense disambiguation (Riloff 1999; Ghani and Jones 2002; Probst et al. 2006; Brody and Elhadad 2010; Haghighi 2010). One way to incorporate a vast amount of unlabeled data is to learn a clustering of words that assigns syntactically similar words to the same clusters. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the</context>
</contexts>
<marker>Ghani, Jones, 2002</marker>
<rawString>R. Ghani, R. Jones, A comparison of efficacy and assumptions of bootstrapping algorithms for training information extraction systems, Workshop on Linguistic Knowledge Acquisition and Representation at the Third International Conference on Language Resources and Evaluation (LREC), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Grenager</author>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Unsupervised Learning of Field Segmentation Models for Information Extraction,</title>
<date>2005</date>
<location>ACL</location>
<marker>Grenager, Klein, Manning, 2005</marker>
<rawString>T. Grenager, D. Klein, and C. D. Manning, Unsupervised Learning of Field Segmentation Models for Information Extraction, ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gruhl</author>
<author>M Nagarajan</author>
<author>J Pieper</author>
<author>C Robson</author>
<author>A Sheth</author>
</authors>
<title>Context and Domain Knowledge Enhanced Entity Spotting In Informal Text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 8th International Semantic Web Conference (ISWC</booktitle>
<contexts>
<context position="10509" citStr="Gruhl et al 2009" startWordPosition="1682" endWordPosition="1685">ed list in our paper) are matched to sentences in a set of Wikipedia articles, and a learning algorithm is trained from the surrounding features of the entities. The trained model is then applied to a test set of Wikipedia articles, and has been reported to be able to discover new instances. In our case, we apply our learned model to a new test set, and discover new brand names from the listings. The nature of non-grammatical text we face makes our work similar to the NER work on informal texts. (Minkov et al. 2005) proposes an NER system that extracts personal names from emails. The work in (Gruhl et al 2009) identifies song titles from online forums on popular music, where song titles can be very ambiguous. By using real-world constraints such as known song titles, (Gruhl et al 2009) restricts the set of possible entities and are able to obtain reasonable recognition performance. 3 Corpus The data used in all analysis in this paper is obtained from eBays clothing and shoes category. Clothing and shoes have been important revenue-generating categories on the eBay site, and a successful attribute extraction system will serve as an invaluable tool for gathering important business and marketing intel</context>
</contexts>
<marker>Gruhl, Nagarajan, Pieper, Robson, Sheth, 2009</marker>
<rawString>D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and A. Sheth. Context and Domain Knowledge Enhanced Entity Spotting In Informal Text. In Proceedings of the 8th International Semantic Web Conference (ISWC 2009).</rawString>
</citation>
<citation valid="false">
<date>2009</date>
<publisher>Springer,</publisher>
<marker>2009</marker>
<rawString>Springer, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Haghighi</author>
</authors>
<title>Unsupervised Models of Entity Reference Resolution,</title>
<date>2010</date>
<tech>Ph. D. Thesis,</tech>
<institution>University of Calfornia, Berkeley,</institution>
<contexts>
<context position="27033" citStr="Haghighi 2010" startWordPosition="4363" endWordPosition="4364">n The supervised learning approach assumes the existence of an annotated set of training data. Often times, training data must be painstakingly marked up and collecting large-scale labeled training examples can be very costly. In recent years, more and more research effort has been focused on how to leverage a vast amount of unlabeled data in a semisupervised or entirely unsupervised fashion for NER as well as for other similar NLP tasks, e.g. POS tagging, sentence boundary detection, and word sense disambiguation (Riloff 1999; Ghani and Jones 2002; Probst et al. 2006; Brody and Elhadad 2010; Haghighi 2010). One way to incorporate a vast amount of unlabeled data is to learn a clustering of words that assigns syntactically similar words to the same clusters. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the hope that the model will pick out clusters that are highly </context>
</contexts>
<marker>Haghighi, 2010</marker>
<rawString>A. D. Haghighi, Unsupervised Models of Entity Reference Resolution, Ph. D. Thesis, University of Calfornia, Berkeley, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Halacsy</author>
<author>A Kornai</author>
<author>C Oravecz</author>
</authors>
<title>HunPos: an open source trigram tagger,</title>
<date>2007</date>
<journal>ACL</journal>
<contexts>
<context position="25705" citStr="Halacsy et al. 2007" startWordPosition="4156" endWordPosition="4159">formance of CRF indeed drops to 89.11%, which is on par with that of SVM and MaxEnt. However, when restricting to using dictionary and word identity features, the performance of CRF improves, indicating the importance of feature selection to such model. SVM and MaxEnt yield similar performance with SVM slightly outperforming MaxEnt classifier by 1.6%. The incorporation of temporal smoothness constraint enforced by the Viterbi algorithm slightly improves the label sequence prediction (comparing row 1 and row 2 in Table 2). The HMM implementation used in our experiments is the Hunpos tagger in (Halacsy et al. 2007), which captures the state transitional probabilities using second-order Markov model. For SVM, we use the popular libSVM package (Chang and Lin 2001) which produces probabilistic output from fitting a sigmoid function to the distances between samples and the separating hyperplane. We use linear kernel in our experiments, although RBF kernel with grid search for optimal parameters yield slightly superior 1562 \x0cperformance, with a significantly higher computational cost. The MaxEnt implementation used in our experiment is the version available from the NLTK toolkit, with BFGS optimizer. For </context>
</contexts>
<marker>Halacsy, Kornai, Oravecz, 2007</marker>
<rawString>P. Halacsy, A. Kornai, C. Oravecz, HunPos: an open source trigram tagger, ACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>H Kazawa</author>
</authors>
<title>Efficient Support Vector Classifiers for Named Entity Recognition,</title>
<date>2002</date>
<location>ACL</location>
<contexts>
<context position="14179" citStr="Isozaki and Kazawa 2002" startWordPosition="2275" endWordPosition="2278">ulting in a total of 7, 921 labeled tokens with 1, 521-word vocabulary. Fig. 1 shows examples of labeled listings, with tags B corresponding to brand, C for color, S for size, G for garment type/style, and NA for none of the above. 4.1 Classifiers One of the most popular generative model based classifiers for named entity recognition tasks is Hidden Markov Model (HMM), which explicitly captures temporal statistics in the data by modeling state (label/tag) transitions over time. Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers. More recently, Conditional Random Fields (CRF) (Feng and McCallum 2004; McCallum 2003) has been proposed for a sequence labeling problem and has been established by many as the state-ofthe-art model for supervised named entity recognition task. In this section, we briefly summarize the pros and cons of each approach. 4.1.1 Hidden Markov Models A hidden Markov model (HMM) is a probabilistic generative model for sequential data. HMM is characterized by 2 sets of model paramet</context>
</contexts>
<marker>Isozaki, Kazawa, 2002</marker>
<rawString>H. Isozaki and H. Kazawa, Efficient Support Vector Classifiers for Named Entity Recognition, ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
</authors>
<title>Learning to Extract Entities from Labeled and Unlabeled Text,</title>
<date>2005</date>
<tech>PhD Thesis,</tech>
<contexts>
<context position="7756" citStr="Jones 2005" startWordPosition="1233" endWordPosition="1234">rapping is used. (Ghani 2006) used EM to add more training data from unlabeled data, while in our approach bootstrapping is used to expand the seed list. First, we automatically generate labeled data by matching seed list to unlabeled data. Then, these auto-labeled training set is used to train a classifier to identify new attribute values from a separate set of unlabeled data. Thirdly, newly discovered product attribute values are added back to our seed list. Thus our original classifier for product attribute extraction can be improved through an expanded seed list. In (Ghani and Jones 2002; Jones 2005), several bootstrapping methods are compared. These methods include self-training, co-EM and EM. All of these approaches are different from ours, as described in detail earlier. In (Probst et al. 2006), a Naive Bayes learner is combined with Co-EM to generate more training data from unlabeled data, and attribute-value pairs are extracted on adjacent words. The automatic bootstrapping in this paper was inspired by (Pakhomov 2002)an acronym expansion algorithm for medical text documents. The underlying assumption is that abbreviated forms and their 1558 \x0ccorresponding expansions occur in simi</context>
</contexts>
<marker>Jones, 2005</marker>
<rawString>R. Jones, Learning to Extract Entities from Labeled and Unlabeled Text, PhD Thesis, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kanaris</author>
<author>K Kanaris</author>
<author>I Houvardas</author>
<author>E Stamatatos</author>
</authors>
<title>Words vs. Character N-grams for Anti-spam Filtering,</title>
<date>2006</date>
<journal>International Journal on Artificial Intelligence Tools,</journal>
<contexts>
<context position="19361" citStr="Kanaris et al. 2006" startWordPosition="3122" endWordPosition="3125">aration is maximized. Since only the samples closest to the decision boundary (the so-called support vectors) determine the location of the separating hyperplane, SVM can be trained on very few training examples even for data in a high-dimensional space. For our supervised NER system, we use the following features, as described in detail in Table 1, as input to the discriminative classifiers. The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al. 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%. In (Kanaris et al. 2006), char N-gram features consistently outperform word features in learning effective spam classifiers. Indeed the use of character Ngram features as an input to the classifier subsumes the use of prefix, suffix, and the entire word features. Generally speaking, char N-gram features provide a more robust representation against misspelling since string s1 and its spelling variant s2 may share many char N-gram substrings in common. POS and punctuation features are not used in our NER system. This is mainly due to the fact that eBay listing titles are not complete sentences and the output from runni</context>
</contexts>
<marker>Kanaris, Kanaris, Houvardas, Stamatatos, 2006</marker>
<rawString>I. Kanaris, K. Kanaris, I. Houvardas, E. Stamatatos, Words vs. Character N-grams for Anti-spam Filtering, International Journal on Artificial Intelligence Tools, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>J Smarr</author>
<author>H Nguyen</author>
<author>C Manning</author>
</authors>
<title>Named Entity Recognition with Character-level Models,</title>
<date>2003</date>
<location>CoNLL</location>
<contexts>
<context position="16426" citStr="Klein et al. 2003" startWordPosition="2638" endWordPosition="2641">ther probabilistic generative models, in order to account for rich, overlapping feature sets, e.g. text formatting features, the correlation structures in the overlapping features must be explicitly modeled. Indeed, in the classic HMM based NER, the simple feature used is the word identity itself, which might not be sufficiently discriminative in distinguishing between different classes. In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al. 2003). 4.1.2 Maximum Entropy models The principle of maximum entropy states that among all the distributions that satisfy feature constraints, we should pick the distribution with the highest entropy, since it makes the least assumption about the data and will have better generalization capability to unseen data. Maximum entropy classifier, therefore, is the highest entropy conditional distribution of the class label given features, which has been shown to conveniently take an exponential form. Maximum entropy classifier is thus closely related to logistic regression model. 1560 \x0cPosition Featur</context>
<context position="19235" citStr="Klein et al. 2003" startWordPosition="3099" endWordPosition="3102">arameters of a linearly separating hyperplane that best separates data from the 2 classes, in a sense that the margin of separation is maximized. Since only the samples closest to the decision boundary (the so-called support vectors) determine the location of the separating hyperplane, SVM can be trained on very few training examples even for data in a high-dimensional space. For our supervised NER system, we use the following features, as described in detail in Table 1, as input to the discriminative classifiers. The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al. 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%. In (Kanaris et al. 2006), char N-gram features consistently outperform word features in learning effective spam classifiers. Indeed the use of character Ngram features as an input to the classifier subsumes the use of prefix, suffix, and the entire word features. Generally speaking, char N-gram features provide a more robust representation against misspelling since string s1 and its spelling variant s2 may share many char N-gram substrings in common. POS and punctuation features are not used i</context>
</contexts>
<marker>Klein, Smarr, Nguyen, Manning, 2003</marker>
<rawString>D. Klein, J. Smarr, H. Nguyen, C. Manning, Named Entity Recognition with Character-level Models, CoNLL 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Koeling</author>
</authors>
<title>Chunking with Maximum Entropy Models,</title>
<booktitle>Proc. of CoNLL-2000.</booktitle>
<marker>Koeling, </marker>
<rawString>R. Koeling, Chunking with Maximum Entropy Models, Proc. of CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kondrak</author>
</authors>
<title>N-Gram Similarity and Distance,</title>
<date>2005</date>
<location>SPIRE</location>
<contexts>
<context position="5505" citStr="Kondrak 2005" startWordPosition="866" endWordPosition="867"> a part of this initial seed list to create the training set. A classifier is then trained to learn context patterns surrounding the known brands from the training set, and is used to discover new brands from the test set. Finally, for known attribute values, we normalize the results to match to words in our dictionary. Normalizing the variants of a known brand to a single normalized output value is an important aspect of a successful information extraction system. To this end, we investigate several string similarity/distance measures for this task and found that n-gram substring similarity (Kondrak 2005) yields accurate normalization results. The main contribution of this work is a product attribute extraction system that addresses the unique problems of information extraction from short listing titles. We combine supervised NER with bootstrapping to expand the seed list, and investigate several methods to normalize the extracted results. Our system has been tested on large-scale eBay listing datasets to demonstrate its effectiveness. 2 Related Work Recent work on product attribute extraction by (Brody and Elhadad 2010) applies a Latent Dirichlet Allocation (LDA) model to identify different a</context>
<context position="36242" citStr="Kondrak 2005" startWordPosition="5908" endWordPosition="5909">tabase community successfully using fuzzy string matching algorithms e.g. Soundex or string edit distance. In this work, since the attribute values are often partially extracted, i.e. a word in a multi-word phrase is extracted, in order to match to the correct normalized value, we must investigate robust substring matching algorithms suitable for partial matching. To this end, we explore 2 string similarity/distance measures for normalizing the extracted attributes. First, we investigate n-gram similarity measures defined as the number of shared character n-grams, i.e. substrings of length n (Kondrak 2005). More specifically, a string similarity measure between s1 and s2 is defined as the percentage of common substrings of length n (out of all substrings of length n). This similarity measure is quite robust to partial matching, as a two-word phrase can appear out of order while most of the character n-grams, where n = 3, remain virtually unchanged. Certainly, finding the right value of n will greatly impact the matching performance of the algorithm. In our experiment, we find the optimal n for brands to be 3 and 4. Table 5 shows a few examples of normalized outputs as a result of finding the be</context>
</contexts>
<marker>Kondrak, 2005</marker>
<rawString>G. Kondrak, N-Gram Similarity and Distance, SPIRE 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Krishnan</author>
<author>C D Manning</author>
</authors>
<title>An effective two-stage model for exploiting non-local dependencies in named entity recognition, in ACL-COLING,</title>
<date>2006</date>
<contexts>
<context position="23489" citStr="Krishnan and Manning 2006" startWordPosition="3784" endWordPosition="3787">in modeling complex correlation structures in the observation sequences. Like HMM, CRF makes prediction on the label sequence by incorporating the temporal smoothness. Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCallum 2004; McCallum 2003), for NER in biomedical texts (Settles 2004), and in various languages besides English, such as Bengali (Ekbal et al. 2008) and Chinese (Mao et al 2008). Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010). 4.2 Experimental Results In this section, we compare the generative model based and discriminative model classifiers for supervised NER tasks. Given 1, 000 manually tagged listings from the clothing and shoes category in eBay, we adopt a 90-10 split and use 90% of the data for training and 10% for testing. Each listing title is tokenized into a sequence of word tokens, each manuSVM MaxEnt HMM CRF w/o Viterbi 89.05% 87.64% - - w/ Viterbi 89.47% 88.13% 83.82 93.35% Table 2: Classification accuracy (%) on 9-class NER on mens clothing data</context>
</contexts>
<marker>Krishnan, Manning, 2006</marker>
<rawString>V. Krishnan and C. D. Manning, An effective two-stage model for exploiting non-local dependencies in named entity recognition, in ACL-COLING, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with Support Vector Machines,</title>
<date>2001</date>
<location>ACL</location>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo, Y. Matsumoto, Chunking with Support Vector Machines, ACL 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data,</title>
<date>2002</date>
<journal>Phs. Dokl.,</journal>
<pages>6--707</pages>
<location>ICML</location>
<contexts>
<context position="22577" citStr="Lafferty et al. 2002" startWordPosition="3641" endWordPosition="3644">record that this state i to j transition is admissible. This approach, indeed, divides a set of all label sequences into ones that are admissible and inadmissible, and assign equal probabilities to all the admissible sequences. Such an approach therefore eliminates all the inadmissible sequences of labels (i.e. prohibit the scenario where -in sub-tag is followed by -begin sub-tag), while allowing the Viterbi algorithm to give more weight to the classification outputs from SVM or MaxEnt. 4.1.5 Conditional Random Field (CRF) Conditional Random Field, since its conception in the seminal work of (Lafferty et al. 2002), is a discriminative classifier for sequential data that combines the best of both worlds. Like SVM and MaxEnt, CRF is a discriminative classifier that directly models the conditional distribution of the target variable given the observed variable, i.e. no modeling resource is wasted in modeling complex correlation structures in the observation sequences. Like HMM, CRF makes prediction on the label sequence by incorporating the temporal smoothness. Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCa</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2002</marker>
<rawString>J. Lafferty, A. McCallum, F. Pereira, Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, ICML 2002. V. I. Levenshtein, Binary code capable of correcting deletions, insertions, and reversals. Phs. Dokl., 6:707-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>X Wu</author>
</authors>
<title>Phrase Clustering for Discriminative Learning,</title>
<date>2009</date>
<location>ACL</location>
<contexts>
<context position="27936" citStr="Lin and Wu 2009" startWordPosition="4517" endWordPosition="4520">al similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the hope that the model will pick out clusters that are highly indicative of each class. When encountering words that are out-of-vocabulary (OOV) in the test set, if those words are assigned the same cluster membership as some other words in the training set, the cluster feature will fire, allowing for correct classification results to be obtained (Lin and Wu 2009; Faruqui and Pado 2010). 5.1 Growing Seed Dictionary In this work, we focus on the problem of how to grow the seed dictionary and discovering new brand names from eBay listing data. While the performances of supervised NER classifiers as described in sections 4.1.1-4.1.5 are satisfactory, in practice, 1 http://mallet.cs.umass.edu/ however, especially with a small training set size, we often find that the trained model puts too much weight on the dictionary membership feature and new attribute values are not properly detected. In this section, instead of using the seed list of known attribute </context>
</contexts>
<marker>Lin, Wu, 2009</marker>
<rawString>D. Lin, X. Wu, Phrase Clustering for Discriminative Learning, ACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>M Hu</author>
<author>J Cheng</author>
</authors>
<title>Opinion Observer: Analyzing and Comparing Opinions on the Web,</title>
<date>2005</date>
<location>WWW</location>
<contexts>
<context position="1866" citStr="Liu et al. 2005" startWordPosition="275" endWordPosition="278">ng matching to work well in practice. 1 Introduction Traditional named entity recognition (NER) task has expanded beyond identifying people, location, and organization to book titles, email addresses, phone numbers, and protein names (Nadeau and Sekine 2007). Recently there has been a surge of interest in extracting product attributes from online data due to the rapid growth of E-Commerce. Current work in this domain focuses on mining product reviews and descriptions from retailer websites. Such text data tend to be long and generate enough context for the target task (Brody and Elhadad 2010; Liu et al. 2005; Popescu and Etzioni 2005). In this paper, we focus on mining short product listing titles, which poses unique challenges. Short listings are typical in classified ads where each seller is given limited space (in terms of words) to describe the product. On eBay, product listing titles cannot exceed 55 characters in length. Similarly, on Craigslist and newspaper ads, the length of a listing title is restricted. Extracting product attributes from such short titles faces the following challenges: Loss of grammatical structure in short listings where many nouns are piled together. Typographical e</context>
<context position="6189" citStr="Liu et al. 2005" startWordPosition="971" endWordPosition="974">s work is a product attribute extraction system that addresses the unique problems of information extraction from short listing titles. We combine supervised NER with bootstrapping to expand the seed list, and investigate several methods to normalize the extracted results. Our system has been tested on large-scale eBay listing datasets to demonstrate its effectiveness. 2 Related Work Recent work on product attribute extraction by (Brody and Elhadad 2010) applies a Latent Dirichlet Allocation (LDA) model to identify different aspects of products from user reviews. Similar work is presented in (Liu et al. 2005). Topic models such as LDA groups similar words together by identifying topics (product aspects) from patterns of wordoccurrences. Such grouping can discover new aspects of a product such as portability (for netbook computers), but it may generate aspects that are vague and not easily interpretable. Indeed, how to refine discovered aspects and clean up words in each aspect remains an open question. The LDA approach also treats documents as bags of words, where important information in word sequences is not taken into account in learning the model. Our work is most closely related to (Ghani 200</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>B. Liu, M. Hu, and J. Cheng, Opinion Observer: Analyzing and Comparing Opinions on the Web, WWW 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinnian Mao</author>
<author>Saike He</author>
<author>Sencheng Bao</author>
<author>Yuan Dong</author>
<author>Haila Wang</author>
</authors>
<title>Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields,</title>
<date>2008</date>
<booktitle>Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>UAI</location>
<contexts>
<context position="23354" citStr="Mao et al 2008" startWordPosition="3765" endWordPosition="3768">models the conditional distribution of the target variable given the observed variable, i.e. no modeling resource is wasted in modeling complex correlation structures in the observation sequences. Like HMM, CRF makes prediction on the label sequence by incorporating the temporal smoothness. Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCallum 2004; McCallum 2003), for NER in biomedical texts (Settles 2004), and in various languages besides English, such as Bengali (Ekbal et al. 2008) and Chinese (Mao et al 2008). Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010). 4.2 Experimental Results In this section, we compare the generative model based and discriminative model classifiers for supervised NER tasks. Given 1, 000 manually tagged listings from the clothing and shoes category in eBay, we adopt a 90-10 split and use 90% of the data for training and 10% for testing. Each listing title is tokenized into a sequence of word tokens, each manuSVM MaxEnt HMM CRF w/o Vi</context>
</contexts>
<marker>Mao, He, Bao, Dong, Wang, 2008</marker>
<rawString>Xinnian Mao, Saike He, Sencheng Bao, Yuan Dong, and Haila Wang, Chinese Word Segmentation and Named Entity Recognition Based on Conditional Random Fields, Sixth SIGHAN Workshop on Chinese Language Processing, 2008 A. McCallum, Efficiently Inducing Features of Conditional Random Fields, UAI 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>D Jensen</author>
</authors>
<title>A Note on Unification of Information Extraction and Data Mining using Conditional-Probability, Relational Models,</title>
<date>2003</date>
<booktitle>Proceedings of IJCAI-2003 on Learning Statistical Models from Relational Data,</booktitle>
<marker>McCallum, Jensen, 2003</marker>
<rawString>A. McCallum, D. Jensen, A Note on Unification of Information Extraction and Data Mining using Conditional-Probability, Relational Models, Proceedings of IJCAI-2003 on Learning Statistical Models from Relational Data, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F McCarthy</author>
</authors>
<title>A Trainable Approach to Coreference Resolution for Information Extraction,</title>
<date>1996</date>
<tech>Ph. D. Thesis,</tech>
<institution>University of Massachusetts at Amherst,</institution>
<marker>McCarthy, 1996</marker>
<rawString>\x0cJ. F. McCarthy, A Trainable Approach to Coreference Resolution for Information Extraction, Ph. D. Thesis, University of Massachusetts at Amherst, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Minkov</author>
<author>R C Wang</author>
<author>W W Cohen</author>
</authors>
<title>Extracting Personal Names from Email: Applying Named Entity Recognition to Informal Text,</title>
<date>2005</date>
<location>ACL</location>
<contexts>
<context position="10413" citStr="Minkov et al. 2005" startWordPosition="1665" endWordPosition="1668">pping method is presented in (Mintz et al. 2009), where instances of known entity relations (or seed list in our paper) are matched to sentences in a set of Wikipedia articles, and a learning algorithm is trained from the surrounding features of the entities. The trained model is then applied to a test set of Wikipedia articles, and has been reported to be able to discover new instances. In our case, we apply our learned model to a new test set, and discover new brand names from the listings. The nature of non-grammatical text we face makes our work similar to the NER work on informal texts. (Minkov et al. 2005) proposes an NER system that extracts personal names from emails. The work in (Gruhl et al 2009) identifies song titles from online forums on popular music, where song titles can be very ambiguous. By using real-world constraints such as known song titles, (Gruhl et al 2009) restricts the set of possible entities and are able to obtain reasonable recognition performance. 3 Corpus The data used in all analysis in this paper is obtained from eBays clothing and shoes category. Clothing and shoes have been important revenue-generating categories on the eBay site, and a successful attribute extract</context>
<context position="29296" citStr="Minkov et al. 2005" startWordPosition="4747" endWordPosition="4750"> discovery, this initial list used to generate training data must contain only names that are unambiguously brands. We hence remove ambiguous names or phrases that belong to multiple attribute types from the list, such as jumpers(both a brand name and a garment type), or (ii) camel is a short name of brand Camel active as well as a color, or (iii) lrg is an acronym for a brand as well as an acronym for large which specifies size. The training/test data is generated by matching N-gram tokens in listing titles to all the entries in the initial brand seed dictionary. Following the convention in (Minkov et al. 2005), we use the following set of 5 tags, (1) one-token entity (B1 tag) (2) first token of a multi-token entity (Bo tag for Brandopen) (3) last token of a multi-token entity (Bc tag for Brand-close) (4) middle token of a multi-token entity (Bi tag for Brand-inside) (5) token that is not part of a brand entity (NA tag). The listings with at least one non-NA tags are put in the training set, and listings that contain only NA tags are in the test set. Similar to the acronym expansion algorithm of (Pakhomov 2002) which learns contexts that associate acronyms to their correct expansions, the intuition </context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>E. Minkov, R. C. Wang, and W. W. Cohen, Extracting Personal Names from Email: Applying Named Entity Recognition to Informal Text, ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Distant Supervision for Relation Extraction without Labeled Data,</title>
<date>2009</date>
<booktitle>In Proceedings of ACL/AFNLP</booktitle>
<contexts>
<context position="9842" citStr="Mintz et al. 2009" startWordPosition="1559" endWordPosition="1562">ed in (Popescu and Etzioni 2005), where noun phrases are extracted from online user reviews. Their system tries to identify product features and user opinions from such noun phrases. A PMI (pointwise mutual information) score is evaluated between each noun phrase and discriminators associated with the product class. The noun-phrase approach does not work well in informal texts. In our case, user-generated short product listings may have many nouns concatenated together without forming a phrase or obeying correct grammatical rules. Finally, another similar bootstrapping method is presented in (Mintz et al. 2009), where instances of known entity relations (or seed list in our paper) are matched to sentences in a set of Wikipedia articles, and a learning algorithm is trained from the surrounding features of the entities. The trained model is then applied to a test set of Wikipedia articles, and has been reported to be able to discover new instances. In our case, we apply our learned model to a new test set, and discover new brand names from the listings. The nature of non-grammatical text we face makes our work similar to the NER work on informal texts. (Minkov et al. 2005) proposes an NER system that </context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, Daniel Jurafsky. 2009. Distant Supervision for Relation Extraction without Labeled Data, In Proceedings of ACL/AFNLP 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Moghaddam</author>
<author>M Ester</author>
</authors>
<title>Opinion Digger: An Unsupervised Opinion Miner from Unstructured Product Reviews,</title>
<date>2010</date>
<booktitle>In Proc. Canadian Conference on Artificial Intelligence,</booktitle>
<location>CIKM</location>
<marker>Moghaddam, Ester, 2010</marker>
<rawString>S. Moghaddam, M. Ester, Opinion Digger: An Unsupervised Opinion Miner from Unstructured Product Reviews, CIKM 2010 David Nadeau, P. Turney, S.Matwin, Unsupervised Named Entity Recognition: Generating Gazetteers and Resolving Ambiguity. In Proc. Canadian Conference on Artificial Intelligence, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Nadeau</author>
<author>Satoshi Sekine</author>
</authors>
<title>A survey of named entity recognition and classification.</title>
<date>2007</date>
<journal>Linguisticae Investigationes,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="1509" citStr="Nadeau and Sekine 2007" startWordPosition="215" endWordPosition="218">ur bootstrapped NER system is able to identify new brands corresponding to spelling variants and typographical errors of the known brands, as well as identifying novel brands. Among the top 300 new brands predicted, our system achieves 90.33% precision. To output normalized attribute values, we explore several string comparison algorithms and found n-gram substring matching to work well in practice. 1 Introduction Traditional named entity recognition (NER) task has expanded beyond identifying people, location, and organization to book titles, email addresses, phone numbers, and protein names (Nadeau and Sekine 2007). Recently there has been a surge of interest in extracting product attributes from online data due to the rapid growth of E-Commerce. Current work in this domain focuses on mining product reviews and descriptions from retailer websites. Such text data tend to be long and generate enough context for the target task (Brody and Elhadad 2010; Liu et al. 2005; Popescu and Etzioni 2005). In this paper, we focus on mining short product listing titles, which poses unique challenges. Short listings are typical in classified ads where each seller is given limited space (in terms of words) to describe t</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>David Nadeau and Satoshi Sekine. A survey of named entity recognition and classification. Linguisticae Investigationes, 30(1):326, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
</authors>
<title>Semi-Supervised Named Entity Recognition: Learning to Recognize 100 Entity Types with Little Supervision,</title>
<date>2007</date>
<tech>PhD thesis,</tech>
<institution>University of Ottawa,</institution>
<contexts>
<context position="8597" citStr="Nadeau 2007" startWordPosition="1361" endWordPosition="1362">ombined with Co-EM to generate more training data from unlabeled data, and attribute-value pairs are extracted on adjacent words. The automatic bootstrapping in this paper was inspired by (Pakhomov 2002)an acronym expansion algorithm for medical text documents. The underlying assumption is that abbreviated forms and their 1558 \x0ccorresponding expansions occur in similar contexts; consequently, the surrounding context patterns can be used in associating the correct expansion to its acronym. Our seed list expansion algorithm indeed bears some similarity to the work of (Nadeau el al 2006) and (Nadeau 2007). In (Nadeau el al 2006), automobile brands are learned automatically from web page context. First, a small set of 196 seed brands are extracted together with their associated web page contexts from popular news feed. The web context is subsequently used to extract additional automobile brands, which result in a total of 5701 brands. However, the reported results in (Nadeau el al 2006) have low precision, in some case less than 50%. Eventually their approach needs to rely on rule-based ambiguity resolver to increase the precision. Our system does not rely on manually created rules. A more NLP-</context>
</contexts>
<marker>Nadeau, 2007</marker>
<rawString>Nadeau, D., Semi-Supervised Named Entity Recognition: Learning to Recognize 100 Entity Types with Little Supervision, PhD thesis, University of Ottawa, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pakhomov</author>
</authors>
<title>Semi-supervised Maximum Entropy Based Approach to Acronym and Abbreviation Normalization in Medical Texts,</title>
<date>2002</date>
<location>ACL</location>
<contexts>
<context position="8188" citStr="Pakhomov 2002" startWordPosition="1301" endWordPosition="1302">es are added back to our seed list. Thus our original classifier for product attribute extraction can be improved through an expanded seed list. In (Ghani and Jones 2002; Jones 2005), several bootstrapping methods are compared. These methods include self-training, co-EM and EM. All of these approaches are different from ours, as described in detail earlier. In (Probst et al. 2006), a Naive Bayes learner is combined with Co-EM to generate more training data from unlabeled data, and attribute-value pairs are extracted on adjacent words. The automatic bootstrapping in this paper was inspired by (Pakhomov 2002)an acronym expansion algorithm for medical text documents. The underlying assumption is that abbreviated forms and their 1558 \x0ccorresponding expansions occur in similar contexts; consequently, the surrounding context patterns can be used in associating the correct expansion to its acronym. Our seed list expansion algorithm indeed bears some similarity to the work of (Nadeau el al 2006) and (Nadeau 2007). In (Nadeau el al 2006), automobile brands are learned automatically from web page context. First, a small set of 196 seed brands are extracted together with their associated web page contex</context>
<context position="29806" citStr="Pakhomov 2002" startWordPosition="4844" endWordPosition="4845">o all the entries in the initial brand seed dictionary. Following the convention in (Minkov et al. 2005), we use the following set of 5 tags, (1) one-token entity (B1 tag) (2) first token of a multi-token entity (Bo tag for Brandopen) (3) last token of a multi-token entity (Bc tag for Brand-close) (4) middle token of a multi-token entity (Bi tag for Brand-inside) (5) token that is not part of a brand entity (NA tag). The listings with at least one non-NA tags are put in the training set, and listings that contain only NA tags are in the test set. Similar to the acronym expansion algorithm of (Pakhomov 2002) which learns contexts that associate acronyms to their correct expansions, the intuition behind our work in this section is that the classifier, trained on a labeled training set of known brands, learns context patterns that can discriminate the current word as being a brand (more precisely as part of a brand) from the other attribute types, which are now lumped together as NA. 5.2 Experiments In the first experiment, a set of 72, 628 listings from the womens clothing category is partitioned into a training set of 39, 448 listings and test set of 33, 180 listings based on an initial seed list</context>
</contexts>
<marker>Pakhomov, 2002</marker>
<rawString>S. Pakhomov, Semi-supervised Maximum Entropy Based Approach to Acronym and Abbreviation Normalization in Medical Texts, ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting Product Features and Opinions from Reviews,</title>
<date>2005</date>
<location>EMNLP</location>
<contexts>
<context position="1893" citStr="Popescu and Etzioni 2005" startWordPosition="279" endWordPosition="282">rk well in practice. 1 Introduction Traditional named entity recognition (NER) task has expanded beyond identifying people, location, and organization to book titles, email addresses, phone numbers, and protein names (Nadeau and Sekine 2007). Recently there has been a surge of interest in extracting product attributes from online data due to the rapid growth of E-Commerce. Current work in this domain focuses on mining product reviews and descriptions from retailer websites. Such text data tend to be long and generate enough context for the target task (Brody and Elhadad 2010; Liu et al. 2005; Popescu and Etzioni 2005). In this paper, we focus on mining short product listing titles, which poses unique challenges. Short listings are typical in classified ads where each seller is given limited space (in terms of words) to describe the product. On eBay, product listing titles cannot exceed 55 characters in length. Similarly, on Craigslist and newspaper ads, the length of a listing title is restricted. Extracting product attributes from such short titles faces the following challenges: Loss of grammatical structure in short listings where many nouns are piled together. Typographical errors, abbreviations, and a</context>
<context position="9256" citStr="Popescu and Etzioni 2005" startWordPosition="1471" endWordPosition="1474">le brands are learned automatically from web page context. First, a small set of 196 seed brands are extracted together with their associated web page contexts from popular news feed. The web context is subsequently used to extract additional automobile brands, which result in a total of 5701 brands. However, the reported results in (Nadeau el al 2006) have low precision, in some case less than 50%. Eventually their approach needs to rely on rule-based ambiguity resolver to increase the precision. Our system does not rely on manually created rules. A more NLP-oriented approach is proposed in (Popescu and Etzioni 2005), where noun phrases are extracted from online user reviews. Their system tries to identify product features and user opinions from such noun phrases. A PMI (pointwise mutual information) score is evaluated between each noun phrase and discriminators associated with the product class. The noun-phrase approach does not work well in informal texts. In our case, user-generated short product listings may have many nouns concatenated together without forming a phrase or obeying correct grammatical rules. Finally, another similar bootstrapping method is presented in (Mintz et al. 2009), where instan</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A.-M. Popescu, O. Etzioni, Extracting Product Features and Opinions from Reviews, EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Probst</author>
<author>R Ghani</author>
<author>M Krema</author>
<author>A Fano</author>
</authors>
<title>SemiSupervised Learning to Extract Attribute-Value Pairs from Product Descriptions on the Web,</title>
<date>2006</date>
<location>ECML</location>
<contexts>
<context position="7957" citStr="Probst et al. 2006" startWordPosition="1263" endWordPosition="1266">d data by matching seed list to unlabeled data. Then, these auto-labeled training set is used to train a classifier to identify new attribute values from a separate set of unlabeled data. Thirdly, newly discovered product attribute values are added back to our seed list. Thus our original classifier for product attribute extraction can be improved through an expanded seed list. In (Ghani and Jones 2002; Jones 2005), several bootstrapping methods are compared. These methods include self-training, co-EM and EM. All of these approaches are different from ours, as described in detail earlier. In (Probst et al. 2006), a Naive Bayes learner is combined with Co-EM to generate more training data from unlabeled data, and attribute-value pairs are extracted on adjacent words. The automatic bootstrapping in this paper was inspired by (Pakhomov 2002)an acronym expansion algorithm for medical text documents. The underlying assumption is that abbreviated forms and their 1558 \x0ccorresponding expansions occur in similar contexts; consequently, the surrounding context patterns can be used in associating the correct expansion to its acronym. Our seed list expansion algorithm indeed bears some similarity to the work </context>
<context position="26993" citStr="Probst et al. 2006" startWordPosition="4355" endWordPosition="4358">ge1. 5 Bootstrapping for Dictionary Expansion The supervised learning approach assumes the existence of an annotated set of training data. Often times, training data must be painstakingly marked up and collecting large-scale labeled training examples can be very costly. In recent years, more and more research effort has been focused on how to leverage a vast amount of unlabeled data in a semisupervised or entirely unsupervised fashion for NER as well as for other similar NLP tasks, e.g. POS tagging, sentence boundary detection, and word sense disambiguation (Riloff 1999; Ghani and Jones 2002; Probst et al. 2006; Brody and Elhadad 2010; Haghighi 2010). One way to incorporate a vast amount of unlabeled data is to learn a clustering of words that assigns syntactically similar words to the same clusters. Popular clustering algorithms used prevalently in many NER systems are, for example, the combination of distributional and morphological similarity work of (Clark 2003) or the classic N-gram language model based clustering algorithm of (Brown et al. 1992). In such a system, when training an NER classifier, we introduce a word cluster id as an additional feature in the input, with the hope that the model</context>
</contexts>
<marker>Probst, Ghani, Krema, Fano, 2006</marker>
<rawString>K. Probst, R. Ghani, M. Krema, A. Fano, SemiSupervised Learning to Extract Attribute-Value Pairs from Product Descriptions on the Web, ECML 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
</authors>
<title>The use of classifiers in sequential inference,</title>
<date>2001</date>
<location>NIPS</location>
<marker>Punyakanok, Roth, 2001</marker>
<rawString>V. Punyakanok, D. Roth, The use of classifiers in sequential inference, NIPS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Raghavan</author>
<author>J Allan</author>
</authors>
<title>Matching Inconsistently Spelled Names in Automatic Speech Recognizer Output for Information Retrieval,</title>
<date>2005</date>
<location>HLT-EMNLP</location>
<marker>Raghavan, Allan, 2005</marker>
<rawString>H. Raghavan, J. Allan, Matching Inconsistently Spelled Names in Automatic Speech Recognizer Output for Information Retrieval, HLT-EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part of Speech Tagger.</title>
<date>1996</date>
<booktitle>In EMNLP</booktitle>
<tech>Ph. D. Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="18036" citStr="Ratnaparkhi 1996" startWordPosition="2900" endWordPosition="2901">N = 4, 5, 6) Context Features: - Identity of 2 words before the current word - Identity of 2 words after the current word - Previous word is from - Previous word is by - Previous word is and - N-gram substring features of neighboring words (N = 4, 5, 6) Dictionary Features: - Membership to the 4 dictionaries of attributes - Exclusive membership to dictionary of brand names - Exclusive membership to dictionary of garment types - Exclusive membership to dictionary of sizes - Exclusive membership to dictionary of colors Table 1: Feature set used in discriminative classifiers. MaxEnt classifiers (Ratnaparkhi 1996; Ratnaparkhi 1998) have been applied to various NLP applications. The attraction of the framework lies in the ease with which different information sources used in the modeling process are combined and the good results that are reported with the use of these models. The set of redundant features used for the MaxEnt classifier is the same as those used for the SVM classifier, which we outline in the next section. 4.1.3 Support Vector Machines Support Vector Machine (SVM) is yet another popular classifier for a supervised NER task. In a binary classification case, SVM finds parameters of a line</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi, A Maximum Entropy Part of Speech Tagger. In EMNLP 1996. A. Ratnaparkhi, Maximum Entropy Models forNatural Language Ambiguity Resolution, Ph. D. Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping,</title>
<date>1999</date>
<location>AAAI</location>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff, R. Jones, Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping, AAAI 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Biomedical named entity recognition using conditional random fields and rich feature sets,</title>
<date>2004</date>
<booktitle>in Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="23246" citStr="Settles 2004" startWordPosition="3747" endWordPosition="3749">t combines the best of both worlds. Like SVM and MaxEnt, CRF is a discriminative classifier that directly models the conditional distribution of the target variable given the observed variable, i.e. no modeling resource is wasted in modeling complex correlation structures in the observation sequences. Like HMM, CRF makes prediction on the label sequence by incorporating the temporal smoothness. Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCallum 2004; McCallum 2003), for NER in biomedical texts (Settles 2004), and in various languages besides English, such as Bengali (Ekbal et al. 2008) and Chinese (Mao et al 2008). Various modifications to CRF have recently been introduced to take into account of non-local dependencies (Krishnan and Manning 2006) or broader context beyond training data (Du et al. 2010). 4.2 Experimental Results In this section, we compare the generative model based and discriminative model classifiers for supervised NER tasks. Given 1, 000 manually tagged listings from the clothing and shoes category in eBay, we adopt a 90-10 split and use 90% of the data for training and 10% for</context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>Settles, B. (2004), Biomedical named entity recognition using conditional random fields and rich feature sets, in Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA), 2004, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Soon</author>
<author>H T Ng</author>
<author>D Chung</author>
<author>Y Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases,</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<pages>521--544</pages>
<marker>Soon, Ng, Chung, Lim, 2001</marker>
<rawString>W. M. Soon, H. T. Ng, D. Chung, Y. Lim, A machine learning approach to coreference resolution of noun phrases, Computational Linguistics, 27(4): 521-544, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
</authors>
<title>Efficient Training of Conditional Random Fields,</title>
<date>2002</date>
<tech>M. Sc. Thesis,</tech>
<institution>Division of Informatics, University of Edinburgh,</institution>
<marker>Wallach, 2002</marker>
<rawString>H. Wallach, Efficient Training of Conditional Random Fields, M. Sc. Thesis, Division of Informatics, University of Edinburgh, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
<author>W S Lee</author>
<author>N Ye</author>
<author>H L Chieu</author>
</authors>
<title>Domain adaptive bootstrapping for named entity recognition, EMNLP</title>
<date>2009</date>
<marker>Wu, Lee, Ye, Chieu, 2009</marker>
<rawString>D. Wu, W. S. Lee, N. Ye, and H. L. Chieu, Domain adaptive bootstrapping for named entity recognition, EMNLP 2009. Y. Zhao, B. Qin, S. Hu, T. Liu, Generalizing Syntactic Structures for Product Attribute Candidate Extraction,</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>