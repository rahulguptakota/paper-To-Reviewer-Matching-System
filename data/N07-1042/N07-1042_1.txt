b'Proceedings of NAACL HLT 2007, pages 332339,
Rochester, NY, April 2007. c

2007 Association for Computational Linguistics
Multi-document Relationship Fusion via
Constraints on Probabilistic Databases
Gideon Mann
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
gideon.mann@gmail.com
Abstract
Previous multi-document relationship ex-
traction and fusion research has focused
on single relationships. Shifting the fo-
cus to multiple relationships allows for the
use of mutual constraints to aid extrac-
tion. This paper presents a fusion method
which uses a probabilistic database model
to pick relationships which violate few
constraints. This model allows improved
performance on constructing corporate
succession timelines from multiple doc-
uments with respect to a multi-document
fusion baseline.
1 Introduction
Single document information extraction of named
entities and relationships has received much atten-
tion since the MUC evaluations1 in the mid-90s (Ap-
pelt et al., 1993; Grishman and Sundheim, 1996).
Recently, there has been increased interest in the
extraction of named entities and relationships from
multiple documents, since the redundancy of infor-
mation across documents has been shown to be a
powerful resource for obtaining high quality infor-
mation even when the extractors have access to little
or no training data (Etzioni et al., 2004; Hasegawa
et al., 2004). Much of the recent work in multi-
document relationship extraction has focused on
the extraction of isolated relationships (Agichtein,
2005; Pasca et al., 2006), but often the goal, as in
1
http://www.itl.nist.gov/iaui/894.02/related projects/muc/
single document tasks like MUC, is to extract a tem-
plate or a relational database composed of related
facts.
With databases containing multiple relationships,
the semantics of the database impose constraints
on possible database configurations. This paper
presents a statistical method which picks relation-
ships which violate few constraints as measured by
a probabilistic database model. The constraints are
hard constraints, and robust estimates are achieved
by accounting for the underlying extraction/fusion
uncertainty.
This method is applied to the problem of con-
structing management succession timelines which
have a rich set of semantic constraints. Using con-
straints on probabilistic databases yields F-Measure
improvements of 5 to 18 points on a per-relationship
basis over a state-of-the-art multi-document extrac-
tion/fusion baseline. The constraints proposed in
this paper are used in a context of minimally super-
vised information extractors and present an alterna-
tive to costly manual annotation.
2 Semantic Constraints on Databases
This paper considers management succession
databases where each record has three fields: a
CEOs name and the start and end years for that
persons tenure as CEO (Table 1 Column 1). Each
record is represented by three binary logical pred-
icates: ceo(c,x), start(x,y1), end(x,y2), where c is
a company, x is a CEOs name, and y1 and y2 are
years.2
2
All of the relationships in this paper are defined to be bi-
nary relationships. When extracting relationships of higher ar-
332
\x0cExplicit Implicit Logical Constraints
Relationships Relationships (a partial list)
ceo(c, x) precedes(x1,x2) ceo(c,x1), precedes(x1,x2)  ceo(c,x2), end(x1,y), start(x2, y)
start(x, y) inoffice(x, y) start(x,y1), inoffice(x,y2), end(x, y3)  y1  y2  y3
end(x, y) predates(x1, x2) inoffice(x1, y1), inoffice(x2, y2) , y1 < y2  predates(x1,x2)
precedes(x1,x2) , inoffice(x1,y1), inoffice(x2,y2)  y1  y2
Table 1: Database semantics can provide 1) a method to augment the explicit relationships in the database
with implicit relationships and 2) logical constraints on these explicit and implicit relationships. In the above
table, c is a company, xi is a person, and yi is a time.
In this setting, database semantics allow for the
derivation of other implicit relationships from the
database: the immediate predecessor of a given CEO
(precedes(x1,x2)), all predecessors of a given CEO
(predates(x1,x2)) and all years the CEO was in of-
fice (inoffice(x,y3)), where x2 is a CEOs name and
t3 a year (Table 1 Column 2).
These implicit relationships and the original ex-
plicit relationships are governed by a series of se-
mantic relations which impose constraints on the
permissible database configurations (Table 1 Col-
umn 3). For example, it will always be true that a
CEOs start date precedes their end date:
x : start(x, y1
), end(x, y2
)  y1
 y2
.
Multi-document extraction of single relationships
exploits redundancy and variety of expression to ex-
tract accurate information from across many docu-
ments. However, these are not the only benefits of
extraction from a large document collection. As well
as being rich in redundant information, large docu-
ment collections also contain a wealth of secondary
relationships which are related to the relationship of
interest via constraints as described above. These
secondary relationships can yield benefits to aug-
ment those achieved by redundancy.
3 Multi-Document Database Fusion
There are typically two steps in the extraction of sin-
gle relationships from multiple documents. In the
first step, a relationship extractor goes through the
corpus, finds all possible relationships r in all sen-
tences s and gives them a score p(r|s). Next, the
ity, typically binary relationships are combined (McDonald et
al., 2005).
relationships are fused across sentences to generate
one score r for each relationship.
This paper proposes a third step which combines
the fusion scores across relationships. This section
first presents a probabilistic database model gener-
ated from fusion scores and then shows how to use
this model for multi-document fusion.
3.1 A Probabilistic Database Model
A relationship r is defined to be a 3-tuple rt,a,b =
r(t, a, b), where t is the type of the relationship (e.g.
start), and a and b are the arguments of the binary
relationship.3
To construct a probabilistic database for a given
corpus, the weights generated in relationship fusion
are normalized to provide the conditional probability
of a relationship given its type:
p(rt,a,b
1 |t1) =
rt,a,b
1
P
ri:rt
i =t rt,a,b
i
,
where r is the fusion score generated by the extrac-
tion/fusion system.4 By applying a prior over types
p(t), a distribution p(r1, t1) can be derived. Given
strong independence assumptions, the probability of
an ordered database configuration R = r1..n of types
t1..n is:
p(r1..n, t1..n) =
n
Y
i=1
p(ri, ti). (1)
3
For readibility in future examples, a and b are replaced
by the types of their arguments. For example, for start the year
in which the CEO starts is referred to as ryear
.
4
The following fusion method does not depend on a par-
ticular extraction/fusion architecture or training methodology,
merely this conditional probability distribution.
333
\x0cAs proposed, the model in Equation 1 is faulty
since the relationships in a database are not inde-
pendent. Given a set of database constraints, certain
database configurations are illegal and should be as-
signed zero probability. To address this, the model
in Equation 1 is augmented with constraints that ex-
plicitly set the probability of a database configura-
tion to zero when they are violated.
A database constraint is a logical formula
(r1..()), where () is the arity of the constraint
. For the constraints presented in this paper, all
constraints  are modeled with two terms  and 
where:
(r1..()) =
\x10

(r1..())  
(r1..())
\x11
.
For a set of relationships, a constraint holds if ()
is true, and the constraint applies if () is true. A
constraint () can only be violated (false) when the
constraint applies, since: (false  X) = true.
In application to a database, each constraint  is
quantified over the database to become a quantified
constraint r1..n . For example, the constraints that a
persons start date must come before their end date is
universally quantified over all pairs of relationships
in a configuration R = r1..n:
r1..n = r1,r2  R : (r1, r2) =
(rt
1 = start, rt
2 = end, rceo
1 = rceo
2 )
 (ryear
1 < ryear
2 ).
This constraint applies to start and end relationships
whose CEO argument matches and is violated when
the years are not in order. If the quantified constraint
r1..n is true for a given database configuration r1..n
then it holds.
To ensure that only legal database configurations
are assigned positive probabilities, Equation 1 is
augmented with a factor

r1..n
=
(
1 if r1..n holds
0 otherwise .
To include a constraint , the database model in
Equation 1 is extended to be:
p(r1..n, t1..n) =
1
Z
Y
i
p(ri, ti)
!

r1..n
,
where Z is the partition function and corresponds to
the total probability of all database configurations. A
set of constraints 1..Q = 1..Q can be integrated
similarly:
p1..Q (r1..n, t1..n) =
1
Z
Y
i
p(ri, ti)
!
Y
q
q
r1..n
(2)
With these added constraints, the probabilistic
database model assigns non-zero probability only to
databases which dont violate any constraints.
3.2 Constraints on Probabilistic Databases for
Relationship Rescoring
Though the constrained probabilistic database
model in Equation 2 is theoretically appealing, it
would be infeasible to calculate its partition func-
tion which requires enumeration of all legal 2n
databases. This section proposes two methods for
re-scoring relationships with regards to how likely
they are to be present in a legal database configu-
ration using the model proposed above. The first
method is a confidence estimate based on how likely
it is that  holds for a given relationship r1:
(r1, t1) = Ep(r2..n,t2..n)
h

r1..()
i
=
P
r2..()
\x10Q()
i=2 p(ri, ti)
\x11

r1..()
P
r2..()
\x10Q()
i=2 p(ri, ti)
\x11
=
P
r2..()
p(r1..n, t1..n)
P
r2..()
p(r1..n, t1..n)
,
where the expectation that the constraint holds
is equivalent to the likelihood ratio between the
database probability models with and without con-
straints. In effect, this model measures the expec-
tation that the constraint holds for a finite database
look-ahead of size ()  1.
With this method, for a constraint to reduce the
confidence in a particular relationship by half, half
of all configurations would have to violate the con-
straint.5 Since inconsistencies are relatively rare, for
a given relationship (r, t)  1 (i.e. almost all
small databases are legal).
5
Assuming equal probability for all relationships.
334
\x0cTo remedy this, another factor  is defined simi-
larly to , except that it takes a value of 1 only if the
constraint applies to that database configuration. An
applicability probability model is then defined as:
p(r1..n, t1..n) =
1
Z
Y
i
p(ri, ti)
!

r1..n
.
The second confidence estimate is based on how
likely it is that the constraint is holds in cases where
it applies (i.e. is not violated):
,(r1, t1)
= Ep(r2..n,t2..n)
h

r1..()
i
=
P
r2..()
\x10Q()
i=2 p(ri, ti)
\x11

r1..()

r1..()
P
r2..()
\x10Q()
i=2 p(ri, ti)
\x11

r1..()
.
When the constraint doesnt apply it cannot be vio-
lated, so this confidence estimate ignores those con-
figurations that cant be affected by the constraint.
Recall that (r, t) is the likelihood ratio be-
tween the probability of configurations in which r
holds for constraint  and all configurations. In con-
trast, ,(r, t) is the likelihood ratio between the
database configurations where r applies and holds
for  and the database configurations where  ap-
plies. In the later ratio, for confidence in a particular
relationship to be cut in half, only half of the con-
figurations which might actually contain an incon-
sistency would be required to produce a violation.6
As a result, ,(r, t) gives a much higher penalty to
relationships which create inconsistencies than does
(r, t).
In order to apply multiple constraints, indepen-
dent database look-aheads are generated for each
constraint q:
1..Q,1..Q (r1, t1) =
Y
q
q,q (r1, t1).
For a particular relationship type, these confidence
scores are calculated and then used to rank the rela-
6
For example, for a start relationship and the constraint that
a CEO must start before they end, this method would only ex-
amine configurations of one start and one end relationship for
the same CEO. The confidence in a particular start date would
be halved if half of the proposed end dates for a given CEO
occurred before it.
tionships via:
c1..Q,1..Q (r1, t1) = p(r1, t1)
Y
q
q,q (r1, t1)
(3)
Databases with different precision/recall trade offs
can be selected by descending the ranked list.7
4 Experiments
In order to test the fusion method proposed above,
human annotators manually constructed truth data
of complete chief executive histories for 18 Fortune-
500 companies using online resources. Extraction
from these documents is particularly difficult be-
cause these data have vast differences in genre and
style and are considerably noisy. Furthermore, the
task is complicated to start with.8
A corpus was created for each company by is-
suing a Google query for CEO-of-Company OR
Company-CEO, and collecting the top ranked doc-
uments, generating up to 1000 documents per com-
pany. The data was then split randomly into training,
development and testing sets of 6, 4, and 8 compa-
nies.
Training : Anheuser-Busch, Hewlett-Packard,
Lenner, McGraw-Hill, Pfizer, Raytheon
Dev. : Boeing, Heinz, Staples, Textron
Test : General Electric, General Motors,
Gannett, The Home Depot, IBM,
Kroger, Sears, UPS
Ground truth was created from the entire web, but
since the corpus for each company is only a small
web snapshot, the experimental results are not simi-
lar to extraction tasks like MUC and ACE in that the
corpus is not guaranteed to contain the information
necessary to build the entire database. In particular,
7
One thing to note is that since all relationships are given
confidence estimates separately, this process may result ulti-
mately in a database where constraints are violated. A potential
solution, which is not explored here, would be to incrementally
add relationships to the database from the ranked list only if
their addition doesnt make the database inconsistent.
8
For example, in certain companies, the title of the chief
executive has changed over the years, often going from Presi-
dent to Chief Executive Officer. To make things more com-
plicated, after the change, the role of President may still hang
on as a subordinate to the CEO!
335
\x0c1) Only one start or end per person.
r1, r2 : (r1, r2) = (rtype
1 = rtype
2 = (start  end), rceo
1 = rceo
2 )  (ryear
1 = ryear
2 )
2) Only a CEOs start or end dates belong in the database.
r1r2 : (r1, r2) = (rtype
1 = start  end, rtype
2 = ceo)  (rceo
1 = rceo
2 )
3) Start dates come before end dates.
r1, r2 : (r1, r2) = (rtype
1 = start, rtype
2 = end, rceo
1 = rceo
2 )  (ryear
1  ryear
2 )
4) Cant be in the middle of someone elses tenure.
r1, r2, r3 : (r1, r2, r3) = (rtype
1 = start  inoffice, rtype
2 = end  inoffice, rtype
3 = start  inoffice  end,
rceo
1 = rceo
2 6= rceo
3 , ryear
1 < ryear
2 )  (ryear
3  ryear
1  ryear
3  ryear
2 )
5) CEOs are only in office after their start.
r1, r2 : (r1, r2) = (rtype
1 = start, rtype
2 = inoffice, rceo
1 = rceo
2 )  (ryear
1  ryear
2 )
6) CEOs are only in office before their end.
r1, r2 : (r1, r2) = (rtype
1 = inoffice, rtype
2 = end, rceo
1 = rceo
2 )  (ryear
1  ryear
2 )
7) Someones end is the same as their successors start.
r1, r2, r3 : (r1, r2, r3) =
(rtype
1 = end, rtype
2 = start, rtype
3 = precedes, rceo
1 = rfirst
3 , rceo
2 = rsecond
3 )  (ryear
1 = ryear
2 )
8) All of the someones dates (start, inoffice, end) are before their successors.
r1, r2, r3 : (r1, r2, r3) = (rtype
1 = start  end  inoffice, rtype
2 = start  inoffice  end, rtype
3 = precedes,
rceo
1 = rfirst
3 , rceo
2 = rsecond
3 )  (ryear
1  ryear
2 )
9) Only CEO succession in the database.
r1r1, r2 : (r1, r2, r3) = (rtype
1 = precedes, rtype
2 = rtype
3 = ceo)  (rfirst
1 = rceo
2 , rsecond
1 = rceo
3 )
Table 2: For a CEO succession database like the one presented in Table 1, the above constraints must hold
if the database is consistent.
many CEOs from pre-Internet years were either in-
frequently mentioned or not mentioned at all in the
database.9 In the following experiments, recall is re-
ported for facts that were retrieved by the extraction
system.
4.1 Relationship Extraction and Fusion
A two-class maximum-entropy classifier was trained
for each relationship type. Each classifier takes a
sentence and two marked entities (e.g. a person and
a year)10 and gives the probability that a relation-
ship between the two entities is supported by the
sentence. For each relationship type, one of the ele-
ments is designated as the hook in order to gener-
ate likely negative examples.11 In training, all entity
pairs are collected from the corpus. The pairs whose
hook element doesnt appear in the database are
thrown out. The remaining pairs are then marked
9
Another consequence is that assessing the effectiveness of
the relationships extraction on a per-extraction basis is difficult.
Because there are no training sentences where it is known that
the sentence contains the relationship of interest, grading per-
extraction results can be deceptive.
10
The person tagger used is the named-entity tagger from
OpenNLP tools and the year tagger simply finds any four digit
numbers between 1950 and 2010.
11
For the CEO relationship, the company was taken to be
the hook. For the other relationships the hook was the primary
CEO.
by exact match to the database. In testing, the rela-
tionship extractor yields the probability p(r|s) of an
entity pair relationship r in a particular sentence s.
The features used in the classifier are: unigrams
between the given information and the target, dis-
tance in words between the given information and
the target, and the exact string between the given in-
formation and the target (if less that 3 words long).
After extraction from individual sentences, the re-
lationships are fused together such that there is one
score for each unique entity pair. In the case of per-
son names, normalization was performed to merge
coreferent but lexically distinct names (e.g. Phil
Condit and Philip M. Condit).
In the following experiments, the baseline fusion
score is:
r =
X
s
p(r|s) (4)
4.2 Experimental Results
Given the management succession database pro-
posed in Section 2, Table 2 enumerates a set of quan-
tified constraints. Information extraction and fusion
were run separately for each company to create a
probabilistic database. In this section, various con-
straint sets are applied, either individually or jointly,
and evaluated in two ways. The first measures per-
relationship precision/recall using the model pro-
336
\x0cSecond (8)
First Before
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 0.2 0.4 0.6 0.8 1
Baseline
Only One (1)
CEOs Only (2)
Start Before End (3)
Inoffice After Start (5)
2,5
2,3,5,
Figure 1: Precision/Recall curve for start(x,t) rela-
tionships. The joint constraint 2,3,5,8 is the best
performing, even though constraints 3 and 8
(not pictured) alone dont perform well.
posed and the second looks at the precision/recall
of a heterogeneous database with many relationship
types. Both evaluations examine the ranked lists of
relationships, where the relationships are ranked by
rescoring via constraints on probabilistic databases
(Equation 3) and compared to the baseline fusion
score (Equation 4). The evaluations use two stan-
dard metrics, interpolated precision at recall level i
(PRi ), and MaxF1:
PRi = max
ji
PRj ,
MaxF1 = max
i
2
1
PRi
+ 1
Ri
.
Figures 1, 2, and 3 show precision/recall curves
for the application of various sets of constraints. Ta-
ble 3 lists the MaxF1 scores for each of the con-
straint variants. For start and end, the majority of
constraints are beneficial. For precedes, only the
constraint that improved performance constraints
both people in the relationship to be CEOs. Across
all relationships, performance is hurt when using the
constraint that there could only be one relationship
of each type for a given CEO. The reason behind
this is that the confidence estimate based on this
constraint favors relationships with few competitors,
and those relationships are typically for people who
are infrequent in the corpus (and therefore unlikely
to be CEOs).
The best-performing constraint sets yield between
5 and 18 points of improvement on Max F1 (Ta-
ble 3). Surprisingly, the gains from joint con-
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 0.2 0.4 0.6 0.8 1
Baseline
Only One (1)
CEOs Only (2)
Inoffice Before End (6)
2,6
Figure 2: Precision/Recall curve for end(x,t) rela-
tionships alone. The joint constraint 2,6 is the best
performing.
0
0.1
0.2
0.3
0.4
0.5
0.6
0 0.2 0.4 0.6 0.8 1
Baseline
End is Start (7)
First Before Second (8)
Only CEO Succession (9)
Figure 3: Precision/Recall for precedes(x,y) rela-
tionships alone. Though the constraint First Before
Second (8) helps performance on start(x,t) relation-
ships, the only constraint which aids here is Only
CEOs Succession (9).
straints are sometimes more than their additive
gains. 2,3,5,6,8 is 6 points better for the start rela-
tionship than 2,3,5,6, but the gains from 8 alone
are negligible.
These performance gains on the individual rela-
tionship types also lead to gains when generating an
entire database (Figure 4). The highest performing
constraint is the CEOs Only (2) constraint, which
outperforms the joint constraints of the previous sec-
tion. One reason the joint constraints dont do as
well here is that each constraint makes the confi-
dence estimate smaller and smaller. This doesnt
have an effect when judging the relationship types
individually, but when combining the relationships
results, the fused relationships types (start, end) be-
337
\x0cMax F1
Constraint Set Start End Pre. DB
 (baseline) 31.2 35.8 34.5 37.9
Only One (1) 10.5 7.2 - 38.1
CEOs Only (2) or (9) 43.3 39.4 39.4 42.9
Start Before End (3) 40.8 32.8 - 40.9
No Overlaps (4) 31.5 35.9 - 36.8
Inoffice After Start (5) 32.5 - - 38.2
Inoffice Before End (6) - 36.5 - 37.4
End is Start (7) 7.3 8.0 20.7 39.2
First before Second (8) 31.4 35.6 26.3 38.1
2,5,6 43.3 40.8 - 42.7
2,3,5,6 43.9 43.3 - 42.2
2,3,5,6,8 49.3 43.9 26.3 40.9
Table 3: Max F1 scores for three relationships
Start(x,t), End(x,t) and Precedes(x,y)) in isolation
and within the context of whole database DB. The
joint constraints perform best for the explicit rela-
tionships in isolation. Using constraints on implicit
derived fields (Inoffice and Precedes) provides ad-
ditional benefit above constraints strictly on explicit
database fields (start, end, ceo).
come artificially lower ranked than the unfused rela-
tionship type (ceo). The best performing contrained
probabilistic database approach beats the baseline
by 5 points.
5 Related Work
Techniques for information extraction from min-
imally supervised data have been explored by
Brin (1998), Agichtein and Gravano (2000), and
Ravichandran and Hovy (2002). Those techniques
propose methods for estimating extractors from ex-
ample relationships and a corpus which contains in-
stances of those relationships.
Nahm and Mooney (2002) explore techniques for
extracting multiple relationships in single document
extraction. They learn rules for predicting certain
fields given other extracted fields (i.e. a someone
who lists Windows as a specialty is likely to know
Microsoft Word).
Perhaps the most related work to what is pre-
sented here is previous research which uses database
information as co-occurrence features for informa-
tion extraction in a multi-document setting. Mann
Inoffice Before End (6)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.2 0.4 0.6 0.8 1
Baseline
CEOs Only (2)
Start Before End (3)
2, Inoffice After Start (5)
2,3,5,6
Figure 4: Precision/Recall curve for whole database
reconstruction. Performance curves using con-
straints dominate the baseline.
and Yarowsky (2005) present an incremental ap-
proach where co-occurrence with a known relation-
ship is a feature added in training and test. Cu-
lotta et al. (2006) introduce a data mining approach
where discovered relationships from a database are
used as features in extracting new relationships. The
database constraints presented in this paper provide
a more general framework for jointly conditioning
multiple relationships. Additionally, this constraint-
based approach can be applied without special train-
ing of the extraction/fusion system.
In the context of information fusion of single rela-
tionships across multiple documents, Downey et al.
(2005) propose a method that models the probabili-
ties of positive and negative extracted classifications.
More distantly related, Sutton and McCallum (2004)
and Finkel et al. (2005) propose graphical models
for combining information about a given entity from
multiple mentions.
In the field of question answering, Prager et al.
(2004) answer a question about the list of composi-
tions produced by a given subject by looking for re-
lated information about the subjects birth and death.
Their method treats supporting information as fixed
hard constraints on the original questions and are ap-
plied in an ad-hoc fashion. This paper proposes a
probabilistic method for using constraints in the con-
text of database extraction and applies this method
over a larger set of relations.
Richardson and Domingos (2006) propose a
method for reasoning about databases and logical
constraints using Markov Random Fields. Their
338
\x0cmodel applies reasoning starting from a known
database. In this paper the database is built from ex-
traction/fusion of relationships from web pages and
contains a significant amount of noise.
6 Conclusion
This paper has presented a probabilistic method for
fusing extracted facts in the context of database ex-
traction when there exist logical constraints between
the fields in the database. The method estimates the
probability than the inclusion of a given relationship
will violate database constraints by taking into ac-
count the uncertainty of the other extracted relation-
ships. Along with the relationships explicitly listed
in the database, constraints are formed over implicit
fields directly recoverable from the explicit listed re-
lationships.
The construction of CEO succession timelines us-
ing minimally trained extractors from web text is a
particularly challenging problem because of noise
resulting from the wide variation in genre in the cor-
pora and errors in extraction. The use of constraints
on probabilistic databases is effective in resolving
many of these errors, leading to improved precision
and recall of retrieved facts, with F-measure gains of
5 to 18 points.
The method presented in this paper combines
symbolic and statistical approaches to natural lan-
guage processing. Logical constraints are made
more robust by taking into account the uncertainty
of the extracted information. An interesting area
of future work is the application of data mining to
search for appropriate constraints to integrate into
this model.
Acknowledgements
This work was supported in part by DoD contract #HM1582-
06-1-2013. Any opinions, findings and conclusions or recom-
mendations expressed in this material belong to the author and
do not necessarily reflect those of the sponsor.
References
E. Agichtein and L. Gravano. 2000. Snowball: Extracting re-
lations from large plain-text collections. In Proceedings of
ICDL, pages 8594.
E. Agichtein. 2005. Extracting Relations from Large Text Col-
lections. Ph.D. thesis, Columbia University.
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993.
FASTUS: a finite-state processor for information extraction
from real-world text. In Proceedings of IJCAI.
S. Brin. 1998. Extracting patterns and relations from the world
wide web. In WebDB Workshop at 6th International Confer-
ence on Extending Database Technology, EDBT98, pages
172183.
A. Culotta, A. McCallum, and J. Betz. 2006. Integrating prob-
abilistic extraction models and data mining to discover rela-
tions and patterns in text. In HLT-NAACL, pages 296303,
New York, NY, June.
D. Downey, O. Etzioni, and S. Soderland. 2005. A probabilistic
model of redundancy in information extraction. In IJCAI.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A-M. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2004. Web-
scale information extraction in knowitall. In WWW.
J. Finkel, T. Grenager, , and C. Manning. 2005. Incorporating
non-local information into information extraction systems by
gibbs sampling. In ACL.
R. Grishman and B. Sundheim. 1996. Message understanding
conference-6: A brief history. In Proceedings of COLING.
T. Hasegawa, S. Sekine, and R. Grishman. 2004. Discovering
relations amoung named entities from large corpora. In ACL.
G. Mann and D. Yarowsky. 2005. Multi-field information ex-
traction and cross-document fusion. In ACL.
R. McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin, and
P. White. 2005. Simple algorithms for complex relationship
extraction with applications to biomedical ie. In Proceed-
ings of ACL.
U. Nahm and R. Mooney. 2002. Text mining with information
extraction. In Proceedings of the AAAI 2220 Spring Sympo-
sium on Mining Answers from Texts and Knowledge Bases,
pages 6067.
M. Pasca, D. Lin, J. Bigham, A. Lifchits, and A. Jain. 2006.
Organizing and searching the world wide web of facts - step
one: The one-million fact extracion challenge. In AAAI.
J. Prager, J. Chu-Carroll, and K. Czuba. 2004. Question an-
swering by constraint satisfaction: Qa-by-dossier with con-
straints. In Proceedings of ACL, pages 574581.
D. Ravichandran and E. Hovy. 2002. Learning surface text
patterns for a question answering system. In Proceedings of
ACL, pages 4147.
M. Richardson and P. Domingos. 2006. Markov logic net-
works. Machine Learning, 62:107136.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information extraction.
Technical Report TR # 04-49, University of Massachusetts,
July. Presented at ICML Workshop on Statistical Relational
Learning and Its Connections to Other Fields.
339
\x0c'