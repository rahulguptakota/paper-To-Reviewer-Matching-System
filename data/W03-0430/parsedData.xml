<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<sectionHeader confidence="0.802147" genericHeader="abstract">
b&amp;apos;Early Results for
</sectionHeader>
<title confidence="0.7373635">
Named Entity Recognition with Conditional Random Fields,
Feature Induction and Web-Enhanced Lexicons
</title>
<author confidence="0.991262">
Andrew McCallum and Wei Li
</author>
<affiliation confidence="0.998606">
Department of Computer Science
University of Massachusetts Amherst
</affiliation>
<address confidence="0.959681">
Amherst, MA 01003
</address>
<email confidence="0.998596">
{mccallum,weili}@cs.umass.edu
</email>
<sectionHeader confidence="0.998822" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998880116883117">
Models for many natural language tasks benefit from the
flexibility to use overlapping, non-independent features.
For example, the need for labeled data can be drastically
reduced by taking advantage of domain knowledge in
the form of word lists, part-of-speech tags, character n-
grams, and capitalization patterns. While it is difficult to
capture such inter-dependent features with a generative
probabilistic model, conditionally-trained models, such
as conditional maximum entropy models, handle them
well. There has been significant work with such mod-
els for greedy sequence modeling in NLP (Ratnaparkhi,
1996; Borthwick et al., 1998).
Conditional Random Fields (CRFs) (Lafferty et al.,
2001) are undirected graphical models, a special case of
which correspond to conditionally-trained finite state ma-
chines. While based on the same exponential form as
maximum entropy models, they have efficient procedures
for complete, non-greedy finite-state inference and train-
ing. CRFs have shown empirical successes recently in
POS tagging (Lafferty et al., 2001), noun phrase segmen-
tation (Sha and Pereira, 2003) and Chinese word segmen-
tation (McCallum and Feng, 2003).
Given these models great flexibility to include a wide
array of features, an important question that remains is
what features should be used? For example, in some
cases capturing a word tri-gram is important, however,
there is not sufficient memory or computation to include
all word tri-grams. As the number of overlapping atomic
features increases, the difficulty and importance of con-
structing only certain feature combinations grows.
This paper presents a feature induction method for
CRFs. Founded on the principle of constructing only
those feature conjunctions that significantly increase log-
likelihood, the approach builds on that of Della Pietra et
al (1997), but is altered to work with conditional rather
than joint probabilities, and with a mean-field approxi-
mation and other additional modifications that improve
efficiency specifically for a sequence model. In compari-
son with traditional approaches, automated feature induc-
tion offers both improved accuracy and significant reduc-
tion in feature count; it enables the use of richer, higher-
order Markov models, and offers more freedom to liber-
ally guess about which atomic features may be relevant
to a task.
Feature induction methods still require the user to cre-
ate the building-block atomic features. Lexicon member-
ship tests are particularly powerful features in natural lan-
guage tasks. The question is where to get lexicons that are
relevant for the particular task at hand?
This paper describes WebListing, a method that obtains
seeds for the lexicons from the labeled data, then uses the
Web, HTML formatting regularities and a search engine
service to significantly augment those lexicons. For ex-
ample, based on the appearance of Arnold Palmer in the
labeled data, we gather from the Web a large list of other
golf players, including Tiger Woods (a phrase that is dif-
ficult to detect as a name without a good lexicon).
We present results on the CoNLL-2003 named entity
recognition (NER) shared task, consisting of news arti-
cles with tagged entities PERSON, LOCATION, ORGANI-
ZATION and MISC. The data is quite complex; for exam-
ple the English data includes foreign person names (such
as Yayuk Basuki and Innocent Butare), a wide diversity of
locations (including sports venues such as The Oval, and
rare location names such as Nirmal Hriday), many types
of organizations (from company names such as 3M, to
acronyms for political parties such as KDP, to location
names used to refer to sports teams such as Cleveland),
and a wide variety of miscellaneous named entities (from
software such as Java, to nationalities such as Basque, to
sporting competitions such as 1,000 Lakes Rally).
On this, our first attempt at a NER task, with just a few
person-weeks of effort and little work on development-
set error analysis, our method currently obtains overall
English F1 of 84.04% on the test set by using CRFs, fea-
ture induction and Web-augmented lexicons. German F1
using very limited lexicons is 68.11%.
</bodyText>
<sectionHeader confidence="0.97857" genericHeader="method">
2 Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.99773145">
Conditional Random Fields (CRFs) (Lafferty et al., 2001)
are undirected graphical models used to calculate the con-
ditional probability of values on designated output nodes
given values assigned to other designated input nodes.
In the special case in which the output nodes of the
graphical model are linked by edges in a linear chain,
CRFs make a first-order Markov independence assump-
tion, and thus can be understood as conditionally-trained
\x0cfinite state machines (FSMs). In the remainder of this
section we introduce the likelihood model, inference and
estimation procedures for CRFs.
Let o = ho1, o2, ...oT i be some observed input data
sequence, such as a sequence of words in text in a doc-
ument, (the values on n input nodes of the graphical
model). Let S be a set of FSM states, each of which
is associated with a label, l L, (such as ORG). Let
s = hs1, s2, ...sT i be some sequence of states, (the val-
ues on T output nodes). By the Hammersley-Clifford the-
orem, CRFs define the conditional probability of a state
sequence given an input sequence to be
</bodyText>
<equation confidence="0.996245916666667">
P(s|o) =
1
Zo
exp
T
X
t=1
X
k
kfk(st1, st, o, t)
!
,
</equation>
<bodyText confidence="0.997373588235294">
where Zo is a normalization factor over all state se-
quences, fk(st1, st, o, t) is an arbitrary feature func-
tion over its arguments, and k is a learned weight for
each feature function. A feature function may, for exam-
ple, be defined to have value 0 in most cases, and have
value 1 if and only if st1 is state #1 (which may have
label OTHER), and st is state #2 (which may have la-
bel LOCATION), and the observation at position t in o
is a word appearing in a list of country names. Higher
weights make their corresponding FSM transitions more
likely, so the weight k in this example should be pos-
itive. More generally, feature functions can ask pow-
erfully arbitrary questions about the input sequence, in-
cluding queries about previous words, next words, and
conjunctions of all these, and fk() can range ....
CRFs define the conditional probability of a label
sequence based on total probability over the state se-
</bodyText>
<equation confidence="0.821838">
quences, P(l|o) =
P
s:l(s)=l P(s|o), where l(s) is
</equation>
<bodyText confidence="0.99407325">
the sequence of labels corresponding to the labels of the
states in sequence s.
Note that the normalization factor, Zo, is the sum
of the scores of all possible state sequences, Zo =
</bodyText>
<equation confidence="0.989619142857143">
P
sST exp
\x10PT
t=1
P
k kfk(st1, st, o, t)
\x11
</equation>
<bodyText confidence="0.944496928571429">
, and that
the number of state sequences is exponential in the in-
put sequence length, T. In arbitrarily-structured CRFs,
calculating the normalization factor in closed form is
intractable, but in linear-chain-structured CRFs, as in
forward-backward for hidden Markov models (HMMs),
the probability that a particular transition was taken be-
tween two CRF states at a particular position in the input
sequence can be calculated efficiently by dynamic pro-
gramming. We define slightly modified forward values,
t(si), to be the unnormalized probability of arriving
in state si given the observations ho1, ...oti. We set 0(s)
equal to the probability of starting in each state s, and
recurse:
</bodyText>
<equation confidence="0.993896454545454">
t+1(s) =
X
s0
t(s0
) exp
X
k
kfk(s0
, s, o, t)
!
.
</equation>
<bodyText confidence="0.9942125">
The backward procedure and the remaining details of
Baum-Welch are defined similarly. Zo is then
</bodyText>
<equation confidence="0.495558">
P
</equation>
<bodyText confidence="0.9827115">
s T (s).
The Viterbi algorithm for finding the most likely state
sequence given the observation sequence can be corre-
spondingly modified from its HMM form.
</bodyText>
<subsectionHeader confidence="0.988945">
2.1 Training CRFs
</subsectionHeader>
<bodyText confidence="0.9992285">
The weights of a CRF, ={, ...}, are set to maximize the
conditional log-likelihood of labeled sequences in some
</bodyText>
<equation confidence="0.9766717">
training set, D = {ho, li(1)
, ...ho, li(j)
, ...ho, li(N)
}:
L =
N
X
j=1
log
\x10
P(l(j)
|o(j)
)
\x11
X
k
2
k
22
,
</equation>
<bodyText confidence="0.996852571428571">
where the second sum is a Gaussian prior over parameters
(with variance ) that provides smoothing to help cope
with sparsity in the training data.
When the training labels make the state sequence un-
ambiguous (as they often do in practice), the likelihood
function in exponential models such as CRFs is con-
vex, so there are no local maxima, and thus finding the
global optimum is guaranteed. It has recently been shown
that quasi-Newton methods, such as L-BFGS, are signifi-
cantly more efficient than traditional iterative scaling and
even conjugate gradient (Malouf, 2002; Sha and Pereira,
2003). This method approximates the second-derivative
of the likelihood by keeping a running, finite-sized win-
dow of previous first-derivatives.
L-BFGS can simply be treated as a black-box opti-
mization procedure, requiring only that one provide the
first-derivative of the function to be optimized. Assum-
ing that the training labels on instance j make its state
path unambiguous, let s(j)
denote that path, and then the
first-derivative of the log-likelihood is
</bodyText>
<equation confidence="0.975121">
L
k
=
N
X
j=1
Ck(s(j)
, o(j)
)
N
X
j=1
X
s
P(s|o(j)
)Ck(s, o(j)
)
k
2
</equation>
<bodyText confidence="0.9516205">
where Ck(s, o) is the count for feature k given s
and o, equal to
</bodyText>
<equation confidence="0.675163">
PT
t=1 fk(st1, st, o, t), the sum of
fk(st1, st, o, t) values for all positions, t, in the se-
</equation>
<bodyText confidence="0.92368">
quence s. The first two terms correspond to the differ-
ence between the empirical expected value of feature fk
and the models expected value: (E[fk]E[fk])N. The
last term is the derivative of the Gaussian prior.
</bodyText>
<sectionHeader confidence="0.91414" genericHeader="method">
3 Efficient Feature Induction for CRFs
</sectionHeader>
<bodyText confidence="0.997269333333333">
Typically the features, fk, are based on some number of
hand-crafted atomic observational tests (such as word is
capitalized or word is said, or word appears in lexi-
con of country names), and a large collection of features
is formed by making conjunctions of the atomic tests in
certain user-defined patterns; (for example, the conjunc-
tions consisting of all tests at the current sequence po-
sition conjoined with all tests at the position one step
\x0caheadspecifically, for instance, current word is capi-
talized and next word is Inc). There can easily be
over 100,000 atomic tests (mostly based on tests for the
identity of words in the vocabulary), and ten or more
shifted-conjunction patternsresulting in several million
features (Sha and Pereira, 2003). This large number of
features can be prohibitively expensive in memory and
computation; furthermore many of these features are ir-
relevant, and others that are relevant are excluded.
In response, we wish to use just those time-shifted
conjunctions that will significantly improve performance.
We start with no features, and over several rounds of fea-
ture induction: (1) consider a set of proposed new fea-
tures, (2) select for inclusion those candidate features that
will most increase the log-likelihood of the correct state
path s(j)
, and (3) train weights for all features. The pro-
posed new features are based on the hand-crafted obser-
vational testsconsisting of singleton tests, and binary
conjunctions of tests with each other and with features
currently in the model. The later allows arbitrary-length
conjunctions to be built. The fact that not all singleton
tests are included in the model gives the designer great
freedom to use a very large variety of observational tests,
and a large window of time shifts.
To consider the effect of adding a new feature, define
the new sequence model with additional feature, g, hav-
ing weight , to be
</bodyText>
<equation confidence="0.998908235294118">
P+g,(s|o) =
P(s|o) exp
\x10PT
t=1 g(st1, st, o, t)
\x11
Zo(, g, )
;
Zo(, g, )
def
=
P
s0 P(s0
|o) exp(
PT
t=1 g(s0
t1, s0
t, o, t))
</equation>
<bodyText confidence="0.919068625">
in the denominator is simply the additional portion of
normalization required to make the new function sum to
1 over all state sequences.
Following (Della Pietra et al., 1997), we efficiently as-
sess many candidate features in parallel by assuming that
the parameters on all included features remain fixed
while estimating the gain, G(g), of a candidate feature, g,
based on the improvement in log-likelihood it provides,
</bodyText>
<equation confidence="0.548370333333333">
G(g) = max
G(g, ) = max
L+g L.
</equation>
<bodyText confidence="0.758619">
where L+g includes 2
</bodyText>
<page confidence="0.924184">
/22
</page>
<bodyText confidence="0.994459863636364">
.
In addition, we make this approach tractable for CRFs
with two further reasonable and mutually-supporting ap-
proximations specific to CRFs. (1) We avoid dynamic
programming for inference in the gain calculation with
a mean-field approximation, removing the dependence
among states. (Thus we transform the gain from a se-
quence problem to a token classification problem. How-
ever, the original posterior distribution over states given
each token, P(s|o) = t(s|o)t+1(s|o)/Zo, is still
calculated by dynamic programming without approxima-
tion.) Furthermore, we can calculate the gain of aggre-
gate features irrespective of transition source, g(st, o, t),
and expand them after they are selected. (2) In many
sequence problems, the great majority of the tokens are
correctly labeled even in the early stages of training. We
significantly gain efficiency by including in the gain cal-
culation only those tokens that are mislabeled by the cur-
rent model. Let {o(i) : i = 1...M} be those tokens, and
o(i) be the input sequence in which the ith error token
occurs at position t(i). Then algebraic simplification us-
ing these approximations and previous definitions gives
</bodyText>
<figure confidence="0.906296705882353">
G(g, ) =
M
X
i=1
log
exp g(st(i), o(i), t(i))
\x01
Zo(i)(, g, )
!
2
22
= M E[g]
M
X
i=1
log(E[exp( g)|o(i)]
2
</figure>
<page confidence="0.701613">
22
</page>
<equation confidence="0.4948525">
,
where Zo(i)(, g, ) (with non-bold o) is simply
P
s P(s|o(i)) exp(g(s, o(i), t(i))). The optimal val-
</equation>
<bodyText confidence="0.987231416666667">
ues of the s cannot be solved in closed form, but New-
tons method finds them all in about 12 quick iterations.
There are two additional important modeling choices:
(1) Because we expect our models to still require sev-
eral thousands of features, we save time by adding many
of the features with highest gain each round of induction
rather than just one; (including a few redundant features
is not harmful). (2) Because even models with a small se-
lect number of features can still severely overfit, we train
the model with just a few BFGS iterations (not to con-
vergence) before performing the next round of feature in-
duction. Details are in (McCallum, 2003).
</bodyText>
<sectionHeader confidence="0.999536" genericHeader="method">
4 Web-augmented Lexicons
</sectionHeader>
<bodyText confidence="0.999824227272727">
Some general-purpose lexicons, such a surnames and lo-
cation names, are widely available, however, many nat-
ural language tasks will benefit from more task-specific
lexicons, such as lists of soccer teams, political parties,
NGOs and English counties. Creating new lexicons en-
tirely by hand is tedious and time consuming.
Using a technique we call WebListing, we build lexi-
cons automatically from HTML data on the Web. Previ-
ous work has built lexicons from fixed corpora by deter-
mining linguistic patterns for the context in which rele-
vant words appear (Collins and Singer, 1999; Jones et al.,
1999). Rather than mining a small corpus, we gather data
from nearly the entire Web; rather than relying on fragile
linguistic context patterns, we leverage robust formatting
regularities on the Web. WebListing finds co-occurrences
of seed terms that appear in an identical HTML format-
ting pattern, and augments a lexicon with other terms on
the page that share the same formatting. Our current im-
plementation uses GoogleSets, which we understand to
be a simple implementation of this approach based on us-
ing HTML list items as the formatting regularity. We are
currently building a more sophisticated replacement.
</bodyText>
<sectionHeader confidence="0.996478" genericHeader="evaluation">
\x0c5 Results
</sectionHeader>
<bodyText confidence="0.999506393939394">
To perform named entity extraction on the news articles
in the CoNLL-2003 English shared task, several families
of features are used, all time-shifted by -2, -1, 0, 1, 2: (a)
the word itself, (b) 16 character-level regular expressions,
mostly concerning capitalization and digit patterns, such
as A, A+, Aa+, Aa+Aa*, A., D+, where A, a and D indi-
cate the regular expressions [A-Z], [a-z] and [0-9],
(c) 8 lexicons entered by hand, such as honorifics, days
and months, (d) 15 lexicons obtained from specific web
sites, such as countries, publicly-traded companies, sur-
names, stopwords, and universities, (e) 25 lexicons ob-
tained by WebListing (including people names, organi-
zations, NGOs and nationalities), (f) all the above tests
with prefix firstmention from any previous duplicate of
the current word, (if capitalized). A small amount of
hand-filtering was performed on some of the WebList-
ing lexicons. Since GoogleSets support for non-English
is severely limited, only 5 small lexicons were used for
German; but character bi- and tri-grams were added.
A Java-implemented, first-order CRF was trained for
about 12 hours on a 1GHz Pentium with a Gaussian prior
variance of 0.5, inducing 1000 or fewer features (down
to a gain threshold of 5.0) each round of 10 iterations of
L-BFGS. Candidate conjunctions are limited to the 1000
atomic and existing features with highest gain. Perfor-
mance results for each of the entity classes can be found
in Figure 1. The model achieved an overall F1 of 84.04%
on the English test set using 6423 features. (Using a set
of fixed conjunction patterns instead of feature induction
results in F1 73.34%, with about 1 million features; trial-
and-error tuning the fixed patterns would likely improve
this.) Accuracy gains are expected from experimentation
with the induction parameters and improved WebListing.
</bodyText>
<sectionHeader confidence="0.966931" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9813835">
We thank John Lafferty, Fernando Pereira, Andres Corrada-
Emmanuel, Drew Bagnell and Guy Lebanon, for helpful
input. This work was supported in part by the Center
for Intelligent Information Retrieval, SPAWARSYSCEN-SD
grant numbers N66001-99-1-8912 and N66001-02-1-8903, Ad-
vanced Research and Development Activity under contract
number MDA904-01-C-0984, and DARPA contract F30602-
01-2-0566.
</bodyText>
<sectionHeader confidence="0.972331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98498975">
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998.
Exploiting diverse knowledge sources via maximum entropy
in named entity recognition. In Proceedings of the Sixth
Workshop on Very Large Corpora, Association for Compu-
tational Linguistics.
M. Collins and Y. Singer. 1999. Unsupervised models for
named entity classification. In Proceedings of the Joint SIG-
DAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora.
Stephen Della Pietra, Vincent J. Della Pietra, and John D. Laf-
ferty. 1997. Inducing Features of Random Fields. IEEE
English devel. Precision Recall F=1
</reference>
<table confidence="0.999350347826087">
LOC 93.82% 91.78% 92.79
MISC 83.99% 78.52% 81.17
ORG 84.23% 82.03% 83.11
PER 92.64% 93.65% 93.14
Overall 89.84% 88.10% 88.96
English test Precision Recall F=1
LOC 87.23% 87.65% 87.44
MISC 74.44% 71.37% 72.87
ORG 79.52% 78.33% 78.92
PER 91.05% 89.98% 90.51
Overall 84.52% 83.55% 84.04
German devel. Precision Recall F=1
LOC 68.55% 68.84% 68.69
MISC 72.66% 45.25% 55.77
ORG 70.64% 54.88% 61.77
PER 82.21% 64.31% 72.17
Overall 73.60% 59.01% 65.50
German test Precision Recall F=1
LOC 71.92% 69.28% 70.57
MISC 69.59% 42.69% 52.91
ORG 63.85% 48.90% 55.38
PER 90.04% 74.14% 81.32
Overall 75.97% 61.72% 68.11
</table>
<tableCaption confidence="0.994519">
Table 1: English and German named entity extraction.
</tableCaption>
<reference confidence="0.992313666666667">
Transactions on Pattern Analysis and Machine Intelligence,
19(4):380393.
Rosie Jones, Andrew McCallum, Kamal Nigam, and Ellen
Riloff. 1999. Bootstrapping for Text Learning Tasks. In
IJCAI-99 Workshop on Text Mining: Foundations, Tech-
niques and Applications.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. ICML.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Sixth Workshop on
Computational Language Learning (CoNLL-2002).
Andrew McCallum and Fang-Fang Feng. 2003. Chinese
Word Segmentation with Conditional Random Fields and In-
tegrated Domain Knowledge. In Unpublished Manuscript.
Andrew McCallum. 2003. Efficiently Inducing Features of
Conditional Random Fields. In Nineteenth Conference on
Uncertainty in Artificial Intelligence (UAI03). (Submitted).
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for
Part-of-Speech Tagging. In Eric Brill and Kenneth Church,
editors, Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 133142. Asso-
ciation for Computational Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing with
Conditional Random Fields. In Proceedings of Human Lan-
guage Technology, NAACL.
\x0c&amp;apos;
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980412">
<title confidence="0.998112">b&amp;apos;Early Results for Named Entity Recognition with Conditional Random Fields, Feature Induction and Web-Enhanced Lexicons</title>
<author confidence="0.999979">Andrew McCallum</author>
<author confidence="0.999979">Wei Li</author>
<affiliation confidence="0.9987455">Department of Computer Science University of Massachusetts Amherst</affiliation>
<address confidence="0.999432">Amherst, MA 01003</address>
<email confidence="0.988908">mccallum@cs.umass.edu</email>
<email confidence="0.988908">weili@cs.umass.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<title>Exploiting diverse knowledge sources via maximum entropy in named entity recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="915" citStr="Borthwick et al., 1998" startWordPosition="122" endWordPosition="125">y natural language tasks benefit from the flexibility to use overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character ngrams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models great flexibility to include a wide array of features,</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998. Exploiting diverse knowledge sources via maximum entropy in named entity recognition. In Proceedings of the Sixth Workshop on Very Large Corpora, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="14656" citStr="Collins and Singer, 1999" startWordPosition="2446" endWordPosition="2449">e in (McCallum, 2003). 4 Web-augmented Lexicons Some general-purpose lexicons, such a surnames and location names, are widely available, however, many natural language tasks will benefit from more task-specific lexicons, such as lists of soccer teams, political parties, NGOs and English counties. Creating new lexicons entirely by hand is tedious and time consuming. Using a technique we call WebListing, we build lexicons automatically from HTML data on the Web. Previous work has built lexicons from fixed corpora by determining linguistic patterns for the context in which relevant words appear (Collins and Singer, 1999; Jones et al., 1999). Rather than mining a small corpus, we gather data from nearly the entire Web; rather than relying on fragile linguistic context patterns, we leverage robust formatting regularities on the Web. WebListing finds co-occurrences of seed terms that appear in an identical HTML formatting pattern, and augments a lexicon with other terms on the page that share the same formatting. Our current implementation uses GoogleSets, which we understand to be a simple implementation of this approach based on using HTML list items as the formatting regularity. We are currently building a m</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>John D Lafferty</author>
</authors>
<title>Inducing Features of Random Fields.</title>
<date>1997</date>
<booktitle>IEEE English devel. Precision Recall F=1 Transactions on Pattern Analysis and Machine Intelligence,</booktitle>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="2109" citStr="Pietra et al (1997)" startWordPosition="301" endWordPosition="304">a wide array of features, an important question that remains is what features should be used? For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams. As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows. This paper presents a feature induction method for CRFs. Founded on the principle of constructing only those feature conjunctions that significantly increase loglikelihood, the approach builds on that of Della Pietra et al (1997), but is altered to work with conditional rather than joint probabilities, and with a mean-field approximation and other additional modifications that improve efficiency specifically for a sequence model. In comparison with traditional approaches, automated feature induction offers both improved accuracy and significant reduction in feature count; it enables the use of richer, higherorder Markov models, and offers more freedom to liberally guess about which atomic features may be relevant to a task. Feature induction methods still require the user to create the building-block atomic features. </context>
<context position="11726" citStr="Pietra et al., 1997" startWordPosition="1954" endWordPosition="1957"> The fact that not all singleton tests are included in the model gives the designer great freedom to use a very large variety of observational tests, and a large window of time shifts. To consider the effect of adding a new feature, define the new sequence model with additional feature, g, having weight , to be P+g,(s|o) = P(s|o) exp \x10PT t=1 g(st1, st, o, t) \x11 Zo(, g, ) ; Zo(, g, ) def = P s0 P(s0 |o) exp( PT t=1 g(s0 t1, s0 t, o, t)) in the denominator is simply the additional portion of normalization required to make the new function sum to 1 over all state sequences. Following (Della Pietra et al., 1997), we efficiently assess many candidate features in parallel by assuming that the parameters on all included features remain fixed while estimating the gain, G(g), of a candidate feature, g, based on the improvement in log-likelihood it provides, G(g) = max G(g, ) = max L+g L. where L+g includes 2 /22 . In addition, we make this approach tractable for CRFs with two further reasonable and mutually-supporting approximations specific to CRFs. (1) We avoid dynamic programming for inference in the gain calculation with a mean-field approximation, removing the dependence among states. (Thus we transf</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent J. Della Pietra, and John D. Lafferty. 1997. Inducing Features of Random Fields. IEEE English devel. Precision Recall F=1 Transactions on Pattern Analysis and Machine Intelligence, 19(4):380393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
<author>Andrew McCallum</author>
<author>Kamal Nigam</author>
<author>Ellen Riloff</author>
</authors>
<title>Bootstrapping for Text Learning Tasks.</title>
<date>1999</date>
<booktitle>In IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications.</booktitle>
<contexts>
<context position="14677" citStr="Jones et al., 1999" startWordPosition="2450" endWordPosition="2453">eb-augmented Lexicons Some general-purpose lexicons, such a surnames and location names, are widely available, however, many natural language tasks will benefit from more task-specific lexicons, such as lists of soccer teams, political parties, NGOs and English counties. Creating new lexicons entirely by hand is tedious and time consuming. Using a technique we call WebListing, we build lexicons automatically from HTML data on the Web. Previous work has built lexicons from fixed corpora by determining linguistic patterns for the context in which relevant words appear (Collins and Singer, 1999; Jones et al., 1999). Rather than mining a small corpus, we gather data from nearly the entire Web; rather than relying on fragile linguistic context patterns, we leverage robust formatting regularities on the Web. WebListing finds co-occurrences of seed terms that appear in an identical HTML formatting pattern, and augments a lexicon with other terms on the page that share the same formatting. Our current implementation uses GoogleSets, which we understand to be a simple implementation of this approach based on using HTML list items as the formatting regularity. We are currently building a more sophisticated rep</context>
</contexts>
<marker>Jones, McCallum, Nigam, Riloff, 1999</marker>
<rawString>Rosie Jones, Andrew McCallum, Kamal Nigam, and Ellen Riloff. 1999. Bootstrapping for Text Learning Tasks. In IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In</title>
<date>2001</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="973" citStr="Lafferty et al., 2001" startWordPosition="130" endWordPosition="133">e overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character ngrams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models great flexibility to include a wide array of features, an important question that remains is what features shoul</context>
<context position="4494" citStr="Lafferty et al., 2001" startWordPosition="689" endWordPosition="692"> names used to refer to sports teams such as Cleveland), and a wide variety of miscellaneous named entities (from software such as Java, to nationalities such as Basque, to sporting competitions such as 1,000 Lakes Rally). On this, our first attempt at a NER task, with just a few person-weeks of effort and little work on developmentset error analysis, our method currently obtains overall English F1 of 84.04% on the test set by using CRFs, feature induction and Web-augmented lexicons. German F1 using very limited lexicons is 68.11%. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models used to calculate the conditional probability of values on designated output nodes given values assigned to other designated input nodes. In the special case in which the output nodes of the graphical model are linked by edges in a linear chain, CRFs make a first-order Markov independence assumption, and thus can be understood as conditionally-trained \x0cfinite state machines (FSMs). In the remainder of this section we introduce the likelihood model, inference and estimation procedures for CRFs. Let o = ho1, o2, ...oT i be some observed input data sequence, su</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Sixth Workshop on Computational Language Learning (CoNLL-2002).</booktitle>
<contexts>
<context position="8556" citStr="Malouf, 2002" startWordPosition="1413" endWordPosition="1414">log \x10 P(l(j) |o(j) ) \x11 X k 2 k 22 , where the second sum is a Gaussian prior over parameters (with variance ) that provides smoothing to help cope with sparsity in the training data. When the training labels make the state sequence unambiguous (as they often do in practice), the likelihood function in exponential models such as CRFs is convex, so there are no local maxima, and thus finding the global optimum is guaranteed. It has recently been shown that quasi-Newton methods, such as L-BFGS, are significantly more efficient than traditional iterative scaling and even conjugate gradient (Malouf, 2002; Sha and Pereira, 2003). This method approximates the second-derivative of the likelihood by keeping a running, finite-sized window of previous first-derivatives. L-BFGS can simply be treated as a black-box optimization procedure, requiring only that one provide the first-derivative of the function to be optimized. Assuming that the training labels on instance j make its state path unambiguous, let s(j) denote that path, and then the first-derivative of the log-likelihood is L k = N X j=1 Ck(s(j) , o(j) ) N X j=1 X s P(s|o(j) )Ck(s, o(j) ) k 2 where Ck(s, o) is the count for feature k given s</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Sixth Workshop on Computational Language Learning (CoNLL-2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Fang-Fang Feng</author>
</authors>
<title>Chinese Word Segmentation with Conditional Random Fields and Integrated Domain Knowledge. In Unpublished Manuscript.</title>
<date>2003</date>
<contexts>
<context position="1440" citStr="McCallum and Feng, 2003" startWordPosition="199" endWordPosition="202">rk with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models great flexibility to include a wide array of features, an important question that remains is what features should be used? For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams. As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows. This paper presents a feature induction method for CRFs. Founded on the principle of constructing only those feature conjunctions that significantly increase lo</context>
</contexts>
<marker>McCallum, Feng, 2003</marker>
<rawString>Andrew McCallum and Fang-Fang Feng. 2003. Chinese Word Segmentation with Conditional Random Fields and Integrated Domain Knowledge. In Unpublished Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
</authors>
<title>Efficiently Inducing Features of Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03).</booktitle>
<location>(Submitted).</location>
<contexts>
<context position="14053" citStr="McCallum, 2003" startWordPosition="2352" endWordPosition="2353"> form, but Newtons method finds them all in about 12 quick iterations. There are two additional important modeling choices: (1) Because we expect our models to still require several thousands of features, we save time by adding many of the features with highest gain each round of induction rather than just one; (including a few redundant features is not harmful). (2) Because even models with a small select number of features can still severely overfit, we train the model with just a few BFGS iterations (not to convergence) before performing the next round of feature induction. Details are in (McCallum, 2003). 4 Web-augmented Lexicons Some general-purpose lexicons, such a surnames and location names, are widely available, however, many natural language tasks will benefit from more task-specific lexicons, such as lists of soccer teams, political parties, NGOs and English counties. Creating new lexicons entirely by hand is tedious and time consuming. Using a technique we call WebListing, we build lexicons automatically from HTML data on the Web. Previous work has built lexicons from fixed corpora by determining linguistic patterns for the context in which relevant words appear (Collins and Singer, 1</context>
</contexts>
<marker>McCallum, 2003</marker>
<rawString>Andrew McCallum. 2003. Efficiently Inducing Features of Conditional Random Fields. In Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI03). (Submitted).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133142</pages>
<editor>In Eric Brill and Kenneth Church, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<contexts>
<context position="890" citStr="Ratnaparkhi, 1996" startWordPosition="120" endWordPosition="121">tion Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features. For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character ngrams, and capitalization patterns. While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models great flexibility to include </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Eric Brill and Kenneth Church, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133142. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow Parsing with Conditional Random Fields.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology, NAACL. \x0c&amp;apos;</booktitle>
<contexts>
<context position="1384" citStr="Sha and Pereira, 2003" startWordPosition="190" endWordPosition="193">odels, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models great flexibility to include a wide array of features, an important question that remains is what features should be used? For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams. As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows. This paper presents a feature induction method for CRFs. Founded on the principle of constructing only t</context>
<context position="8580" citStr="Sha and Pereira, 2003" startWordPosition="1415" endWordPosition="1418">) |o(j) ) \x11 X k 2 k 22 , where the second sum is a Gaussian prior over parameters (with variance ) that provides smoothing to help cope with sparsity in the training data. When the training labels make the state sequence unambiguous (as they often do in practice), the likelihood function in exponential models such as CRFs is convex, so there are no local maxima, and thus finding the global optimum is guaranteed. It has recently been shown that quasi-Newton methods, such as L-BFGS, are significantly more efficient than traditional iterative scaling and even conjugate gradient (Malouf, 2002; Sha and Pereira, 2003). This method approximates the second-derivative of the likelihood by keeping a running, finite-sized window of previous first-derivatives. L-BFGS can simply be treated as a black-box optimization procedure, requiring only that one provide the first-derivative of the function to be optimized. Assuming that the training labels on instance j make its state path unambiguous, let s(j) denote that path, and then the first-derivative of the log-likelihood is L k = N X j=1 Ck(s(j) , o(j) ) N X j=1 X s P(s|o(j) )Ck(s, o(j) ) k 2 where Ck(s, o) is the count for feature k given s and o, equal to PT t=1 </context>
<context position="10272" citStr="Sha and Pereira, 2003" startWordPosition="1704" endWordPosition="1707">s said, or word appears in lexicon of country names), and a large collection of features is formed by making conjunctions of the atomic tests in certain user-defined patterns; (for example, the conjunctions consisting of all tests at the current sequence position conjoined with all tests at the position one step \x0caheadspecifically, for instance, current word is capitalized and next word is Inc). There can easily be over 100,000 atomic tests (mostly based on tests for the identity of words in the vocabulary), and ten or more shifted-conjunction patternsresulting in several million features (Sha and Pereira, 2003). This large number of features can be prohibitively expensive in memory and computation; furthermore many of these features are irrelevant, and others that are relevant are excluded. In response, we wish to use just those time-shifted conjunctions that will significantly improve performance. We start with no features, and over several rounds of feature induction: (1) consider a set of proposed new features, (2) select for inclusion those candidate features that will most increase the log-likelihood of the correct state path s(j) , and (3) train weights for all features. The proposed new featu</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow Parsing with Conditional Random Fields. In Proceedings of Human Language Technology, NAACL. \x0c&amp;apos;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>