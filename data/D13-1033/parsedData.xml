<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.860725">
b&apos;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 333344,
Seattle, Washington, USA, 18-21 October 2013. c
</bodyText>
<sectionHeader confidence="0.551731" genericHeader="abstract">
2013 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.897128">
The Effects of Syntactic Features in Automatic Prediction of Morphology
</title>
<author confidence="0.890088">
Wolfgang Seeker and Jonas Kuhn
</author>
<affiliation confidence="0.9320195">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.994468">
{seeker,jonas}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.990508" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.997906545454545">
Morphology and syntax interact considerably
in many languages and language processing
should pay attention to these interdependen-
cies. We analyze the effect of syntactic fea-
tures when used in automatic morphology pre-
diction on four typologically different lan-
guages. We show that predicting morphology
for languages with highly ambiguous word
forms profits from taking the syntactic context
of words into account and results in state-of-
the-art models.
</bodyText>
<sectionHeader confidence="0.998101" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999007125">
In this paper, we investigate the interplay between
syntax and morphology with respect to the task of
assigning morphological descriptions (or tags) to
each token of a sentence. Specifically, we examine
the effect of syntactic information when it is inte-
grated into the feature model of a morphological tag-
ger. We test the effect of syntactic features on four
languages Czech, German, Hungarian, and Span-
ish and find that syntactic features improve our tag-
ger considerably for Czech and German, but not for
Hungarian and Spanish. Our analysis of construc-
tions that show morpho-syntactic agreement sug-
gests that syntactic features are important if the lan-
guage shows frequent word form syncretisms1 that
can be disambiguated by the syntactic context.
The meaning of a sentence is structurally encoded
</bodyText>
<page confidence="0.91891">
1
</page>
<bodyText confidence="0.9990959375">
Syncretism describes the situation where a word form is
ambiguous between several different morphological descrip-
tions within its inflection paradigm.
by morphological and syntactic means.2 Different
languages, however, use them to a different extent.
Languages like English encode grammatical infor-
mation (like the subject vs object status of an argu-
ment) via word order, whereas languages like Czech
or Hungarian use different word forms. Automatic
analysis of languages with rich morphology needs
to pay attention to the interaction between morphol-
ogy and syntax in order to arrive at suitable com-
putational models. Linguistic theory (e. g., Bresnan
(2001), Melcuk (2009)) suggests many interactions
between morphology and syntax. For example, lan-
guages with a case system use different forms of the
same word to mark different syntactic (or seman-
tic) relations (Blake, 2001). In many languages, two
words that participate in a syntactic relation show
covariance in some or all of their morphological fea-
tures (so-called agreement, Corbett (2006)).3
Automatic annotation of morphology assigns
morphological descriptions (e. g., nominative-
singular-masculine) to word forms. It is usually
modeled as a sequence model, often in combination
with part-of-speech tagging and lemmatization
(Collins, 2002; Hajic, 2004; Smith et al., 2005;
Chrupaa et al., 2008, and others). Sequence models
achieve high accuracy and coverage but since they
only use linear context they only approximate some
of the underlying hierarchical relationships. As
an example for these hierarchical relationships,
</bodyText>
<page confidence="0.965066">
2
</page>
<bodyText confidence="0.9923965">
And also by prosodic means, which we will not discuss
since text-based tools rarely have access to this information.
</bodyText>
<page confidence="0.980464">
3
</page>
<bodyText confidence="0.984240666666667">
For example, in English, the subject of a sentence and the
finite verb agree with respect to their number and person fea-
ture.
</bodyText>
<page confidence="0.997837">
333
</page>
<figure confidence="0.676498153846154">
\x0cdie wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepragten Regionen
nom/acc.pl.fem nom/acc.pl.fem
the economic - most developed , modern and to part catholic influenced regions
NK
MO
PM MO
NK
CJ
CD
MO
NK MO
CJ
the regions that are economically most developed, modern, and partly catholic
</figure>
<figureCaption confidence="0.998872">
Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value.
</figureCaption>
<bodyText confidence="0.998850171428572">
Figure 1 shows a German noun phrase taken from
the German TiGer corpus (Brants et al., 2002).
The two bold-faced words are the determiner and
the head noun of the phrase, and they agree in
their gender, number, and case values. The word
Regionen (regions) is four-way ambiguous for its
case value, which is reduced to a two-way ambi-
guity between nominative and accusative by the
determiner. Further disambiguation would require
information about the syntactic role of the noun
phrase in a sentence. There are 11 tokens between
these two words, which would require a context
window of at least 13 to capture the agreement
relation within a sequence model. Syntactically,
however, as indicated by the dependency tree,
the determiner and the head are linked directly.
The interdependency between morphology and
syntax in the example thus manifests itself in the
morphological disambiguation of a highly syncretic
word form because of its government or agreement
relation to its respective syntactic head/dependents.
Of course, the sequence model is most of the
time a reasonable approximation, because the ma-
jority of noun phrases in the TiGer corpus are not
as long as the example in Figure 1.4 Furthermore,
not all languages show this kind of relationship be-
tween morphological forms and syntactic relation as
demonstrated for German. But taking advantage of
the morphosyntactic dependencies in a language can
give us better models that may even be capable of
handling the more difficult or rare cases. We there-
fore advocate that models for predicting morphology
should be designed with the typological characteris-
tics of a language and its morphosyntactic properties
in mind, and should, where appropriate, integrate
</bodyText>
<page confidence="0.976137">
4
</page>
<bodyText confidence="0.998642">
We find 57,551 noun phrases with less than three tokens
between determiner and noun and 4,670 with three or more.
syntactic information in order to better model the
morphosyntactic interdependencies of the language.
In the remainder of the paper, we show empiri-
cally that taking syntactic information into account
produces state-of-the-art models for languages with
a high interdependency between morphology and
syntax. We use a simple setup, where we combine
a morphological tagger and a dependency parser in
a bootstrapping architecture in order to analyze the
effect of syntactic information on the performance
of the morphological tagger (Section 2). Using syn-
tactic features in morphology prediction requires a
syntactically annotated corpus for training a statisti-
cal parser, which may not be available for languages
with few resources. We show in Section 3 that only
very little syntactically annotated data is required to
achieve the improvements. We furthermore expect
that the improved morphological information also
improves parsing performance and present a prelim-
inary experiment in Section 4.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="method">
2 Experiments
</sectionHeader>
<bodyText confidence="0.999635">
In this section, we present a series of experiments
that investigate the effect of syntactic information on
the prediction of morphological features. We start
by describing our data sets and the system that we
used for the experiments.
</bodyText>
<subsectionHeader confidence="0.999168">
2.1 Languages and Data Sets
</subsectionHeader>
<bodyText confidence="0.999338571428571">
We test our hypotheses on four different languages:
Czech, German, Hungarian, and Spanish.
Spanish, a Romance language, and German, a
Germanic language, constitute inflecting languages
that show verbal and nominal morphology, but not
as sophisticated as Czech and Hungarian. As we
will see in the experiments, it is relatively easy to
</bodyText>
<page confidence="0.999109">
334
</page>
<bodyText confidence="0.999291138888889">
\x0cpredict the morphological information annotated in
the Spanish data set.
Czech and Hungarian represent languages with
very rich morphological systems both in verbal and
nominal morphological paradigms. They differ sig-
nificantly in the way in which morphological infor-
mation is encoded in word forms. Czech, a Slavic
language, is an inflecting language, where one suf-
fix may signal several different morphological cate-
gories simultaneously (e. g., number, gender, case).
In contrast, Hungarian, a Finno-Ugric language, is
of the agglutinating type, where each morphological
category is marked by its own morpheme.
Both German and Czech show various form syn-
cretisms in their inflection paradigms. Form syn-
cretisms emerge when the same word form is am-
biguous between several different morphological de-
scriptions, and they are a major challenge to auto-
matic morphological analysis. Spanish shows syn-
cretism in the verbal inflection paradigms. In Hun-
garian, form syncretisms are much less frequent.
The case paradigm of Hungarian only shows one
form syncretism between dative and genitive case
(out of about 18 case suffixes).
All languages show agreement between subject
and verb, and within the noun phrase. The word or-
der in Czech and Hungarian is very variable whereas
it is more restrictive in Spanish and German.
As our data, we use the CoNLL 2009 Shared
Task data sets (Hajic et al., 2009) for Czech and
Spanish. For German, we use the dependency
conversion of the TiGer treebank by Seeker and
Kuhn (2012), splitting it into 40k/5k/5k sentences
for training/development/test. For Hungarian, we
use the Szeged Dependency Treebank (Vincze et al.,
2010), with the split of Farkas et al. (2012).
</bodyText>
<subsectionHeader confidence="0.998718">
2.2 System Description
</subsectionHeader>
<bodyText confidence="0.999828">
To test our hypotheses, we implemented a tagger
that assigns full morphological descriptions to each
token in a sentence. The system was inspired by the
morphological tagger included in mate-tools.5 Like
the tagger provided with mate-tools, it is a classifier
that tags each token using the surrounding tokens in
</bodyText>
<page confidence="0.980637">
5
</page>
<bodyText confidence="0.999880742857143">
A collection of language independent, data-driven analysis
tools for lemmatization, pos-tagging, morphological analysis,
and dependency parsing: http://code.google.com/p/mate-tools
its feature model. Models are trained using passive-
aggressive online training (Crammer et al., 2003).
The system makes two passes over each sentence:
The first pass provides predicted tags that are used
as features during the second pass. We also adopted
the idea of a tag filter, which deterministically as-
signs tags for words that always occur with the same
tag in the training data.
For all matters of syntactic annotation in this pa-
per, we use the graph-based dependency parser by
Bohnet (2010), also included in mate-tools. All data
sets are annotated with gold syntactic information,
which is used to train the parsing models.
For our experiments, we use a bootstrapping ap-
proach: the parser uses the output of the morphology
in its feature set, and the morphological tagger we
want to analyze uses the output of the parser as syn-
tactic features. Since it is best to keep the training
setting as similar as possible to the test setting, we
use 10-fold jackknifing to annotate our training data
with predicted morphology or syntax respectively.
Jackknifing differs from cross-validation only in
its purpose. Cross-validation is used for evaluating
data, jackknifing is used to annotate data. The data
set is split into n parts, and n-1 parts are used to train
a model for annotating the nth part. This is then
rotated n times such that each part is annotated by
the automatic tool without training it on its own test
data. Jackknifing is important for creating a realis-
tic training scenario that provides automatic prepro-
cessing. For annotating development and test sets,
models are trained on the jackknifed training set.
</bodyText>
<subsectionHeader confidence="0.996176">
2.3 The Effects of Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999850142857143">
In the first experiment, we use the system described
in Section 2.2 to predict morphological information
on all four languages. We start with describing the
general setup and the feature set, and continue with
a discussion of the results.
The experimental setup is as follows: the German
and Spanish data sets are annotated with lemma and
part-of-speech information using 10-fold jackknif-
ing. The annotation is done with mate-tools lem-
matizer and pos-tagger. For Czech and Hungarian,
we keep the annotation provided with the data sets.
Note that our experimental setup does not include
lemmas or part-of-speech tags as part of the predic-
tion of the morphology but annotates them in a pre-
</bodyText>
<page confidence="0.998154">
335
</page>
<bodyText confidence="0.925557777777778">
\x0cprocessing step. It is not necessary to separate part-
of-speech and lemma from the prediction of mor-
phology and, in fact, many systems perform these
steps simultaneously (e. g. Spoustova et al. (2009)).
Doing morphology prediction as a separate step al-
lows us to use lemma and part-of-speech informa-
tion in the feature set.6
static features
form form1b form2b
</bodyText>
<figure confidence="0.823767166666667">
form3b form1a lemma2a
pos1b pos2b pos1a
form+pos pos+s1 pos+s2
pos+s3 pos+s4 lemma+p2
lemma+p3 pos+number form+form1b
pos+pos1a pos+pos1b+pos2b s1+s1 1b
s1+s1 1a s2+s2 1a last-verb-lemma
last-verb-pos next-verb-lemma next-verb-pos
dynamic features
tag1b+tag2b tag2b+tag3b tag1a
tag1a+tag1b tag1a+tag2a tag2a+tag3a
pos1b+case1b last-verb-tag next-verb-tag
pos1b+case1b+pos2b+case2b
Hungarian only features
pos+uppercase
Czech only features
pos+p2
Spanish only features
</figure>
<equation confidence="0.720189666666667">
s5 p1 p4
p5 s2 1a s3 1a
s4 1a
</equation>
<tableCaption confidence="0.96975">
Table 1: Baseline feature set. form means word form,
</tableCaption>
<bodyText confidence="0.993699642857143">
lemma is lemma, pos is part-of-speech, s1/p1 stand for
suffix and prefix of length 1 (characters), tag is the mor-
phological tag predicted by the system, 1b/1a means 1
token before/after the current token, and + marks feature
conjunctions. number marks if the form contains a digit.
After preprocessing the data, our baseline system
is trained using the feature set shown in Table 1. The
baseline system does not make use of any syntactic
information but predicts morphological information
based solely on tokens and their linear context. The
features are divided into static features, which can be
computed on the input, and dynamic features, which
are computed also on previous output of the system
(cf. two passes in Section 2.2).
</bodyText>
<page confidence="0.993216">
6
</page>
<bodyText confidence="0.98114319047619">
Lemma and part-of-speech prediction may also profit from
syntactic information, see e.g. Prins (2004) or Bohnet and Nivre
(2012).
The feature sets in Table 1 were developed specif-
ically for our experiments and are the result of an
automatic forward/backward feature selection pro-
cess. The purpose of the feature selection was to ar-
rive at a baseline system that performs well without
any syntactic information. With such an optimized
baseline system, we can measure the contribution of
syntactic features more reliably.
The last-verb/next-verb and pos+case features are
variants of the features proposed in Votrubec (2006).
They extract information about the first verb within
the last 10/the next 30 tokens in the sentence. The
case feature extracts the case value from previously
assigned morphological tags. Note that the verb
features are approximating syntactic information by
making the assumption that the closest verbs are
likely to be syntactic heads for many words.
static features
</bodyText>
<construct confidence="0.4692152">
h lemma h s2 h s3 pos+h pos s1+h s1
h dir h dir+h pos
ld s1 ld s2 ld p1 ld p4
dynamic features
h tag ld tag
</construct>
<tableCaption confidence="0.90509">
Table 2: Syntactic features. h and ld mark features from
</tableCaption>
<bodyText confidence="0.996644227272727">
the head and the left-most daughter, dir is a binary fea-
ture marking the direction of the head with respect to the
current token.
After training the baseline models, we use them to
annotate the whole data set with morphological in-
formation (using 10-fold jackknifing for the training
portions). We then use 10-fold jackknifing again to
annotate the data sets with the dependency parser.
At this point, all our data sets are annotated with
predicted morphology from our baseline system and
with syntactic information from the parser, which
uses the morphological information from our base-
line system in its feature set. We can now retrain our
morphological tagger using features that are derived
from the dependency trees provided by the parser.
Note that this is not a stacking architecture, since
the second system does not use the predicted mor-
phology output from the baseline system. The loop
simply ensures that we get the best possible syntac-
tic features.
We extract two kinds of syntactic features: fea-
tures of the syntactic head of the current token, and
</bodyText>
<page confidence="0.999367">
336
</page>
<table confidence="0.987422590909091">
\x0cdev set test set
all oov all oov
Czech
morfette 90.37 68.66 90.01 67.25
our baseline 92.51 73.12 92.29 72.58
pred syntax *93.18 74.04 *92.82 73.11
gold syntax *93.64 75.20 *93.30 74.96
German
morfette 86.78 66.37 84.58 61.05
our baseline 90.92 72.52 89.11 69.67
pred syntax *92.07 75.06 *90.10 71.18
gold syntax *92.70 *76.29 *90.87 *73.20
Hungarian
morfette *96.19 *85.82 95.99 *85.43
our baseline 96.08 84.49 95.94 83.76
pred syntax 96.18 84.70 96.11 83.85
gold syntax *96.46 85.30 *96.35 84.50
Spanish
morfette 97.83 89.67 97.76 91.00
our baseline 97.83 89.05 97.59 90.88
pred syntax 97.84 89.08 97.67 90.91
gold syntax 98.11 90.34 97.88 91.61
</table>
<tableCaption confidence="0.999241">
Table 3: The effect of syntactic features when predicting
</tableCaption>
<bodyText confidence="0.962436615384615">
morphological information. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with = 0.05).
features of the left-most daughter of the current to-
ken. We also experimented with other types, e. g.
the right-most daughter, but these features did not
improve the model. This is likely due to the way
these languages encode morphological information
and may be different for other languages. From the
head and the left-most daughter, we construct fea-
tures about form, lemma, affixes, and tags. Table 2
lists the syntactic features that we use in the model.
With the syntactic features available due to the
parsing step, we train new models with the full sys-
tem. For each language, we run four experiments.
The first two are baseline experiments, where we
use the off-the-shelf morphological tagger morfette
(Chrupaa et al., 2008) and our own baseline sys-
tem, both of which do not use any syntactic features.
In the third experiment, we evaluate our full system
using the syntactic features provided by the depen-
dency parser. As an oracle experiment, we also re-
port results on the full system when using the gold
standard syntax from the treebank. Table 3 presents
all results in terms of accuracy on all tokens (all)
dev set test set
</bodyText>
<table confidence="0.978109210526316">
all oov all oov
Czech
featurama 94.75 84.12 94.78 84.23
our baseline 93.80 80.47 93.57 80.53
pred syntax *94.40 81.51 *94.24 81.61
gold syntax *94.80 82.45 *94.64 82.80
German
RFTagger 90.63 72.11 89.04 70.80
our baseline 92.59 80.73 91.48 78.83
pred syntax *93.70 82.71 *92.51 80.20
gold syntax *94.28 *84.12 *93.32 *82.35
Hungarian
our baseline 97.27 92.61 97.03 91.28
pred syntax 97.38 92.39 97.19 91.50
gold syntax *97.63 92.79 *97.45 91.92
Spanish
our baseline 98.23 92.46 98.02 93.15
pred syntax 98.24 92.30 98.07 93.03
gold syntax 98.40 92.82 *98.22 93.64
</table>
<tableCaption confidence="0.999414">
Table 4: The effect of syntactic features when predicting
</tableCaption>
<bodyText confidence="0.98717756">
morphology using lexicons. * mark statistically signifi-
cantly better models compared to our baseline (sentence-
based t-test with = 0.05).
and out-of-vocabulary tokens only (oov). Out-of-
vocabulary tokens do not occur in the training data.
We find trends along several axes: Generally, the
syntactic features work well for Czech and Ger-
man, whereas for Hungarian and Spanish, they do
not yield any significant improvement. The im-
provements for German and Czech are between 0.5
(Czech) and 1.0 (German) percentage points abso-
lute in token accuracy, and between 0.2 (Czech test
set) and 2.5 (German dev set) percentage points ab-
solute in accuracy of unknown words. There are no
obvious differences between the development and
the test set in any of the languages.
Compared to the morfette baseline, we find our
systems to be either superior or equal to morfette in
terms of token accuracy. Regarding accuracy on un-
known words, morfette outperforms our systems for
Hungarian, but is outperformed on Czech and Ger-
man. For Spanish, all systems yield similar results.
Looking at the oracle experiment, we see that for
all languages, the system can learn something from
syntax. For Czech and German, this is clearly the
</bodyText>
<page confidence="0.991082">
337
</page>
<bodyText confidence="0.997003166666667">
\x0ccase, for Hungarian and Spanish, the differences are
small but visible. There are pronounced differences
between the predicted and the gold syntax experi-
ments in Czech and German. Clearly, the parser
makes mistakes that propagate through to the pre-
diction of the morphology.
</bodyText>
<subsectionHeader confidence="0.993085">
2.4 Syntax vs Lexicon
</subsectionHeader>
<bodyText confidence="0.999286702702703">
The current state-of-the-art in predicting morpho-
logical features makes use of morphological lexi-
cons (e.g. Hajic (2000), Hakkani-Tur et al. (2002),
Hajic (2004)). Lexicons define the possible morpho-
logical descriptions of a word and a statistical model
selects the most probable one among them. In the
following experiment, we test whether the contribu-
tion of syntactic features is similar or different to the
contribution of morphological lexicons.
Lexicons encode important knowledge that is dif-
ficult to pick up in a purely statistical system, e. g.
the gender of nouns, which often cannot be deduced
from the word form (Corbett, 1991).7
We extend our system from the previous experi-
ment to include information from a morphological
dictionaries. For Czech, we use the morphologi-
cal analyzer distributed with the Prague Dependency
Treebank 2 (Hajic et al., 2006). For German, we
use DMor (Schiller, 1994). For Hungarian, we use
(Tron et al., 2006), and for Spanish, we use the mor-
phological analyzer included in Freeling (Carreras et
al., 2004). The output of the analyzers is given to the
system as features that simply record the presence of
a particular morphological analysis for the current
word. The system can thus use the output of any
tool regardless of its annotation scheme, especially
if the annotation scheme of the treebank is different
from the one of the morphological analyzer.
Table 4 presents the results of experiments where
we add the output of the morphological analyzers
to our system. Again, we run experiments with and
without syntactic features. For Czech, we also show
results from featurama8 with the feature set devel-
oped by Votrubec (2006). For German, we show re-
sults for RFTagger (Schmid and Laws, 2008).
As expected, the information from the morpho-
logical lexicon improves the overall performance
</bodyText>
<page confidence="0.989538">
7
</page>
<bodyText confidence="0.91603">
Lexicons are also often used to speed up processing con-
siderably by restricting the search space of the statistical model.
</bodyText>
<page confidence="0.915284">
8
</page>
<bodyText confidence="0.961619666666667">
http://sourceforge.net/projects/featurama/
considerably compared to the results in Table 3, es-
pecially on unknown tokens. This shows that even
with the considerable amounts of training data avail-
able nowadays, rule-based morphological analyzers
are important resources for morphological descrip-
tion (cf. Hajic (2000)). The contribution of syn-
tactic features in German and Czech is almost the
same as in the previous experiment, indicating that
the syntactic features contribute information that is
orthogonal to that of the morphological lexicon. The
lexicon provides lexical knowledge about a word
form, while the syntactic features provide the syn-
tactic context that is needed in German and Czech
to decide on the right morphological tag.
</bodyText>
<subsectionHeader confidence="0.969318">
2.5 Language Differences
</subsectionHeader>
<bodyText confidence="0.96328296875">
From the previous experiments, we conclude that
syntactic features help in the prediction of morphol-
ogy for Czech and German, but not for Hungarian
and Spanish. To further investigate the difference
between Czech and German on the one hand, and
Hungarian and Spanish on the other, we take a closer
look at the output of the tagger.
We find an interesting difference between the
two pairs of languages, namely the performance
with respect to agreement. Agreement is a phe-
nomenon where morphology and syntax strongly in-
teract. Morphological features co-vary between two
items in the sentence, but the relation between these
items can occur at various linguistic levels (Corbett,
2006). If the syntactic information helps with pre-
dicting morphological information, we expect this
to be particularly helpful with getting agreement
right. All languages show agreement to some ex-
tent. Specifically, all languages show agreement in
number (and person) between the subject and the
verb of a clause. Czech, German, and Spanish show
agreement in number, gender, and case (not Span-
ish) within a noun phrase. Hungarian shows case
agreement within the noun phrase only rarely, e.g.
for attributively used demonstrative pronouns.
In order to test the effect on agreement, we mea-
sure the accuracy on tokens that are in an agreement
relation with their syntactic head. We counted sub-
ject verb agreement as well as agreement with re-
spect to number, gender, and case (where applicable)
between a noun and its dependent adjective and de-
terminer. Table 5 displays the counts from the devel-
</bodyText>
<page confidence="0.992267">
338
</page>
<bodyText confidence="0.959876666666667">
\x0copment sets of each language. We compare the base-
line system that does not use any syntactic informa-
tion with the output of the morphological tagger that
uses the gold syntax. We use the gold syntax rather
than the predicted one in order to eliminate any in-
fluence from parsing errors. As can be seen from the
results, the level of agreement relations in Czech and
German improves when using syntactic information,
whereas in Spanish and Hungarian, only very tiny
</bodyText>
<figure confidence="0.475293315789474">
changes occur.
agreement baseline gold syntax
Czech
sbj-verb 3199/4044 = 79.10 3264/4044 = 80.71
NP case 8719/9132 = 95.48 8821/9132 = 96.59
NP num 8933/9132 = 97.82 9016/9132 = 98.73
NP gen 8493/9132 = 93.00 8768/9132 = 96.01
German
sbj-verb 4412/4696 = 93.95 4562/4696 = 97.15
NP case 13340/13951 = 95.62 13510/13951 = 96.84
NP num 13631/13951 = 97.71 13788/13951 = 98.83
NP gen 13253/13951 = 95.00 13528/13951 = 96.97
Hungarian
sbj-verb 8653/10219 = 84.68 8655/10219 = 84.70
NP case 402/891 = 45.12 412/891 = 46.24
Spanish
sbj-verb 1930/2004 = 96.31 1932/2004 = 96.41
NP num 8810/8849 = 99.56 8816/8849 = 99.63
NP gen 8810/8849 = 99.56 8821/8849 = 99.68
</figure>
<tableCaption confidence="0.960693">
Table 5: Agreement counts in morphological annotation
</tableCaption>
<bodyText confidence="0.962525739130435">
compared between the baseline system and the oracle
system using gold syntax.
For Czech and German, these results sugguest
that syntactic information helps with agreement. We
believe that the reasons why it does not help for
Hungarian and Spanish are the following: for Span-
ish, we see that also the baseline model achieves
very high accuracies (cf. Table 3) and also high rates
of correct agreement. It seems that for Spanish, syn-
tactic context is simply not necessary to make the
correct prediction. For Hungarian, the reason lies
within the inflectional paradigms of the language,
which do not show any form syncretism, mean-
ing that word forms in Hungarian are usually not
ambiguous within one morphological category (e.g.
case). Making a morphological tag prediction, how-
ever, is difficult only if the word form itself is am-
biguous between several morphological tags. In this
case, using the agreement relation between the word
and its syntactic head can help the system making
the proper prediction. This is the situation that we
find in Czech and German, where form syncretism
is pervasive in the inflectional paradigms.
</bodyText>
<subsectionHeader confidence="0.941063">
2.6 Syntactic Features in Czech
</subsectionHeader>
<bodyText confidence="0.953632454545454">
In Section 2.4 we compared the performance of our
system on Czech to another system, featurama (see
Table 4). Featurama outperforms our baseline sys-
tem by a percentage point in token accuracy (and
even more for unknown tokens). Syntactic informa-
tion closes that gap to a large extent but only using
gold syntax gets our system on a par with featurama.
The question then arises whether the syntactic
features actually contribute something new to the
task, or whether the same effect could also be
achieved with linear context features alone as in fea-
turama. In order to test this we run an additional
experiment, where we add some of the syntax fea-
tures to the feature set of featurama. Specifically,
we add the static features from Table 2 that do not
use lemma or part-of-speech information. Due to the
way featurama works, we cannot use features from
the morphological tags (the dynamic features).
The results in Table 6 show that also featurama
profits from syntactic features, which corroborates
the findings from the previous experiments. We also
note again that better syntax would improve results
</bodyText>
<figureCaption confidence="0.577044333333333">
even more.
dev set test set
all oov all oov
</figureCaption>
<table confidence="0.989539333333333">
featurama 94.75 84.12 94.78 84.23
pred syntax 95.18 84.65 95.09 84.52
gold syntax *95.39 84.62 *95.34 85.03
</table>
<tableCaption confidence="0.995876">
Table 6: Syntactic features for featurama (Czech). * mark
</tableCaption>
<bodyText confidence="0.586576">
statistically significantly better models compared to feat-
urama (sentence-based t-test with = 0.05).
</bodyText>
<sectionHeader confidence="0.572256" genericHeader="method">
3 How Much Syntax is Needed?
</sectionHeader>
<bodyText confidence="0.988617428571429">
Syntactic features require syntactically annotated
corpora. Without a treebank to train the parser, the
morphology cannot profit from syntactic features.9
This may be problematic for languages for which
there is no treebank, because creating a treebank is
expensive. Fortunately, it turns out that very small
amounts of syntactically annotated data are enough
</bodyText>
<page confidence="0.674247">
9
</page>
<bodyText confidence="0.390734">
Which is of course only a problem for statistical parsers.
</bodyText>
<page confidence="0.597634">
339
</page>
<figure confidence="0.971581034482759">
\x0cGerman Czech
88
89
90
91
92
93
94
0 5000 10000 15000 20000 25000 30000 35000 40000
accuracy
of
morphology
# of sentences in training data of syntactic parser
dev
test
88
89
90
91
92
93
94
0 5000 10000 15000 20000 25000 30000 35000 40000
accuracy
of
morphology
# of sentences in training data of syntactic parser
dev
test
</figure>
<figureCaption confidence="0.999725">
Figure 2: Dependency between amount of training data for syntactic parser and quality of morphological prediction.
</figureCaption>
<bodyText confidence="0.98580812195122">
to provide a parsing quality that is sufficient for the
morphological tagger.
In order to test what amount of training data is
needed, we train several parsing models on increas-
ing amounts of syntactically annotated data. For ex-
ample, the first experiment uses the first 1,000 sen-
tences of the treebank. We perform 5-fold jackknif-
ing with the parser on these sentences to annotate
them with syntax. Then we train one parsing model
on these 1,000 sentences and use it to annotate the
rest of the training data as well as the development
and the test set. This gives us the full data set an-
notated with syntax that was learned from the first
1,000 sentences of the treebank. The morphologi-
cal tagger is then trained on the full training set and
applied to development and test set.
Figure 2 shows the dependency between the
amount of training data given to the parser and the
quality of the morphological tagger using syntac-
tic features provided by this parser. The left-most
point corresponds to a model that does not use syn-
tactic information. For both languages, German
and Czech, we find that already 1,000 sentences are
enough training data for the parser to provide useful
syntactic information to the morphological tagger.
After 5,000 sentences, both curves flatten out and
stay on the same level. We conclude that using syn-
tactic features for morphological prediction is viable
even if there is only small amounts of syntactic data
available to train the parser.
As a related experiment, we also test if we can get
the same effect with a very simple and thus much
faster parser. We use the brute-force algorithm de-
scribed in Covington (2001), which selects for each
token in the sentence another token as the head. It
does not have any tree requirements, so it is not even
guaranteed to yield a cycle-free tree structure. In Ta-
ble 7, we compare the simple parser with the mate-
parser, both trained on the first 5,000 sentences of
the treebank. Evaluation is done in terms of labeled
(LAS) and unlabeled attachment score (UAS).10
</bodyText>
<table confidence="0.677208125">
dev set test set
LAS UAS LAS UAS
Czech
simple parser (5k) 71.57 78.96 69.09 77.23
full parser (5k) 76.77 84.38 74.70 83.00
German
simple parser (5k) 83.06 85.23 78.56 81.18
full parser (5k) 87.56 90.08 83.69 86.58
</table>
<tableCaption confidence="0.996955">
Table 7: Simple parser vs full parser syntactic quality.
</tableCaption>
<bodyText confidence="0.972138066666667">
Trained on first 5,000 sentences of the training set.
As expected, the simple parser performs much
worse in terms of syntactic quality. Table 8 shows
the performance of the morphological tagger when
using the output of both parsers as syntactic fea-
tures. For Czech, both parsers seem to supply sim-
ilar information to the morphological tagger, while
for German, using the full parser is clearly better.
In both cases, the morphological tagger outperforms
the models that do not use syntactic information (cf.
Table 3). The performance on unknown words is
however much worse for both languages. We con-
clude that even with a simple parser and little train-
ing data, the morphology can make use of syntactic
information to some extent.
</bodyText>
<page confidence="0.965463">
10
</page>
<bodyText confidence="0.601904">
LAS: correct edges with correct labels
</bodyText>
<table confidence="0.813054428571429">
all edges
, UAS: correct edges
all edges
340
\x0cdev set test set
all oov all oov
Czech
no syntax 92.51 73.12 92.29 72.58
simple syntax 92.96 73.45 92.53 72.66
full syntax 93.08 73.64 92.69 73.39
German
no syntax 90.92 72.52 89.11 69.67
simple syntax 91.52 73.34 89.66 70.52
full syntax 91.92 83.46 89.91 80.50
</table>
<tableCaption confidence="0.998444">
Table 8: Simple parser vs full parser morphological
</tableCaption>
<bodyText confidence="0.964543083333333">
quality. The parsing models were trained on the first
5,000 sentences of the training data, the morphological
tagger was trained on the full training set.
4 Does Better Morphology lead to Better
Parses?
In the previous sections, we show that syntactic in-
formation improves a model for predicting morphol-
ogy for Czech and German, where syntax and mor-
phology interact considerably. A natural question
then is whether the improvement also occurs in the
other direction, namely whether the improved mor-
phology also leads to better parsing models.
In the previous experiments, we run a 10-fold
jackknifing process to annotate the training data with
morphological information using no syntactic fea-
tures and afterwards use jackknifing with the parser
to annotate syntax. The syntax is subsequently used
as features for our predicted-syntax experiments.
We can apply the same process once more with the
morphology prediction in order to annotate the train-
ing data with morphological information that is pre-
dicted using the syntactic features. A parser trained
on this data will then use the improved morphology
as features. If the improved morphology has an im-
pact on the parser, the quality of the second parsing
model should then be superior to the first parsing
model, which uses the morphology predicted with-
out syntactic information. Note that for the follow-
ing experiments, neither morphology model uses the
morphological lexicon.
Table 9 presents the evaluation of the two pars-
ing models (one using morphology without syntactic
features, the other one using the improved morphol-
ogy). The results show no improvement in parsing
performance when using the improved morphology.
Looking closer at the output, we find differences be-
</bodyText>
<table confidence="0.922835625">
dev set test set
LAS UAS LAS UAS
Czech
baseline morph 81.73 88.45 81.02 87.77
morph w/ syntax 81.63 88.37 80.83 87.61
German
baseline morph 91.16 92.97 88.06 90.24
morph w/ syntax 91.20 92.97 88.15 90.34
</table>
<tableCaption confidence="0.997148">
Table 9: Impact of the improved morphology on the qual-
</tableCaption>
<bodyText confidence="0.999755454545455">
ity of the dependency parser for Czech and German.
tween the two parsing models with respect to gram-
matical functions that are morphologically marked.
For example, in German, performance on subjects
and accusative objects improves while performance
for dative objects and genitives decreases. This sug-
gests different strengths in the two parsing models.
However, the question how to make use of the im-
proved morphology in parsing clearly needs more
research in the future. A promising avenue may be
the approach by Hohensee and Bender (2012).
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9998205">
Morphological taggers have been developed for
many languages. The most common approach is the
combination of a morphological lexicon with a sta-
tistical disambiguation model (Hakkani-Tur et al.,
2002; Hajic, 2004; Smith et al., 2005; Spoustova
et al., 2009; Zsibrita et al., 2013).
Our work has been inspired by Versley et al.
(2010), who annotate a treebank with morphologi-
cal information after the syntax had been annotated
already. The system used a finite-state morphology
to propose a set of candidate tags for each word,
which is then further restricted using hand-crafted
rules over the already available syntax tree.
Lee et al. (2011) pursue the idea of jointly predict-
ing syntax and morphology, out of the motivation
that joint models should model the problem more
faithfully. They demonstrate that both sides can use
information from each other. However, their model
is computationally quite demanding and its overall
performance falls far behind the standard pipeline
approach where both tasks are done in sequence.
The problem of modeling the interaction between
morphology and syntax has recently attracted some
attention in the SPMRL workshops (Tsarfaty et al.,
</bodyText>
<page confidence="0.978864">
341
</page>
<bodyText confidence="0.9894955">
\x0c2010). Modeling morphosyntactic relations explic-
itly has been shown to improve statistical parsing
models (Tsarfaty and Simaan, 2010; Goldberg and
Elhadad, 2010; Seeker and Kuhn, 2013), but the co-
dependency between morphology and syntax makes
it a difficult problem, and linguistic intuition is often
contradicted by the empirical findings. For example,
Marton et al. (2013) show that case information is
the most helpful morphological feature for parsing
Arabic, but only if it is given as gold information,
whereas using case information from an automatic
system may even harm the performance.
Morphologically rich languages pose different
challenges for automatic systems. In this paper, we
work with European languages, where the problem
of predicting morphology can be reduced to a tag-
ging problem. In languages like Arabic, Hebrew,
or Turkish, widespread ambiguity in segmentation
of single words into meaningful morphemes adds an
additional complexity. Given a good segmentation
tool that takes care of this, our approach is appli-
cable to these languages as well. For Hebrew, this
problem has also been addressed by jointly mod-
eling segmentation, morphological prediction, and
syntax (Cohen and Smith, 2007; Goldberg and Tsar-
faty, 2008; Goldberg and Elhadad, 2013).
</bodyText>
<sectionHeader confidence="0.997675" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999722576923077">
In this paper, we have demonstrated that using syn-
tactic information for predicting morphological in-
formation is helpful if the language shows form syn-
cretism in combination with morphosyntactic phe-
nomena like agreement. A model that uses syntactic
information is superior to a sequence model because
it leverages the syntactic dependencies that may hold
between morphologically dependent words as sug-
gested by linguistic theory. We also showed that
only small amounts of training data for a statistical
parser would be needed to improve the morphologi-
cal tagger. Making use of the improved morphology
in the dependency parser is not straight-forward and
requires more investigation in the future.
Modeling the interaction between morphology
and syntax is important for building successful pars-
ing pipelines for languages with free word order and
rich morphology. Moreover, our experiments show
that paying attention to the individual properties of a
language can help us explain and predict the behav-
ior of automatic tools. Thus, the term morpholog-
ically rich language should be viewed as a broad
term that covers many different languages, whose
differences among each other may be as important as
the difference with languages with a less rich mor-
phology.
</bodyText>
<sectionHeader confidence="0.966981" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.971180666666667">
We would like to thank Jan Hajic and Jan Stepanek
for their kind help with the Czech morphology and
featurama. We would also like to thank Thomas
Muller for sharing resources and thoughts with us,
and Anders Bjorkelund for commenting on earlier
versions of this paper. This work was funded by
the Deutsche Forschungsgemeinschaft (DFG) via
SFB 732 Incremental Specification in Context,
project D8.
</bodyText>
<sectionHeader confidence="0.964257" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997951464285714">
Barry J. Blake. 2001. Case. Cambridge University
Press, Cambridge, New York, 2nd edition.
Bernd Bohnet and Joakim Nivre. 2012. A Transition-
Based System for Joint Part-of-Speech Tagging and
Labeled Non-Projective Dependency Parsing. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 1455
1465, Jeju, South Korea. Association for Computa-
tional Linguistics.
Bernd Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 8997, Beijing, China. International
Committee on Computational Linguistics.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In Proceedings of the 1st Workshop
on Treebanks and Linguistic Theories, pages 2441,
Sozopol, Bulgaria.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well Publishers.
Xavier Carreras, Isaac Chao, Llus Padr, and Muntsa Padr.
2004. Freeling: An open-source suite of language
analyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC04), pages 239242. European Language Re-
sources Association (ELRA).
</reference>
<page confidence="0.977856">
342
</page>
<reference confidence="0.931059093457944">
\x0cGrzegorz Chrupaa, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC08), pages 23622367, Marrakech, Morocco.
European Language Resources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
logical and syntactic disambiguation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 208217, Prague,
Czech Republic. Association for Computational Lin-
guistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 18. Association for
Computational Linguistics, July.
Greville G. Corbett. 1991. Gender. Cambridge Text-
books in Linguistics. Cambridge University Press.
Greville G. Corbett. 2006. Agreement. Cambridge Text-
books in Linguistics. Cambridge University Press.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing (with corrections). In Pro-
ceedings of the 39th Annual ACM Southeast Confer-
ence, Athens, Gorgia. ACM.
Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and
Yoram Singer. 2003. Online passive-aggressive algo-
rithms. In Proceedings of the 16th Annual Conference
on Neural Information Processing Systems, volume 7,
pages 12171224, Cambridge, Massachusetts, USA.
MIT Press.
Richard Farkas, Veronika Vincze, and Helmut Schmid.
2012. Dependency parsing of hungarian: Baseline re-
sults and challenges. In Proceedings of the 13th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 5565, Avignon,
France. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. Easy first
dependency parsing of modern Hebrew. In Proceed-
ings of the NAACL HLT 2010 First Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages,
pages 103107, Los Angeles, California, USA. Asso-
ciation for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2013. Word seg-
mentation, unknown-word resolution, and morpholog-
ical agreement in a hebrew parsing system. Computa-
tional Linguistics, 39(1):121160.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics, pages 371379, Columbus, Ohio. Association for
Computational Linguistics.
Jan Hajic. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 6th ANLP Conference
/ 1st NAACL Meeting, pages 94101, Seattle, Wash-
ington. Association for Computational Linguistics.
Jan Hajic. 2004. Disambiguation of Rich Inflection
(Computational Morphology of Czech). Nakladatel-
stv Karolinum, Prague, Czech Republic.
Jan Hajic, Jarmila Panevova, Eva Hajicova, Petr Sgall,
Petr Pajas, Jan Stepanek, Ji Havelka, and Marie
Mikulova. 2006. Prague Dependency Treebank 2.0.
Jan Hajic, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antonia Mart, Llus
Marquez, Adam Meyers, Joakim Nivre, Sebastian
Pado, Jan Stepanek, Pavel Stranak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and Semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning: Shared Task, pages 118, Boulder, Col-
orado, USA. Association for Computational Linguis-
tics.
Dilek Z. Hakkani-Tur, Kemal Oflazer, and Gokhan Tur.
2002. Statistical morphological disambiguation for
agglutinative languages. Computers and the Humani-
ties, 36(4):381410.
Matt Hohensee and Emily M. Bender. 2012. Getting
more from morphology in multilingual dependency
parsing. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 315326, Montreal, Canada. Association
for Computational Linguistics.
John Lee, Jason Naradowsky, and David A. Smith. 2011.
A discriminative model for joint morphological disam-
biguation and dependency parsing. In Proceedings of
the 49th annual meeting of the Association for Compu-
tational Linguistics, pages 885894, Portland, USA.
Association for Computational Linguistics.
Yuval Marton, Nizar Habash, and Owen Rambow. 2013.
Dependency parsing of modern standard arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161194.
Igor Melcuk. 2009. Dependency in linguistic descrip-
tion.
Robbert Prins. 2004. Beyond N in N-gram tagging. In
Leonoor Van Der Beek, Dmitriy Genzel, and Daniel
Midgley, editors, Proceedings of the ACL 2004 Student
Research Workshop, pages 6166, Barcelona, Spain.
Association for Computational Linguistics.
Anne Schiller. 1994. Dmor - users guide. Technical
report, University of Stuttgart.
</reference>
<page confidence="0.984063">
343
</page>
<reference confidence="0.983683630136987">
\x0cHelmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 777784, Morristown, NJ,
USA. Association for Computational Linguistics.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 31323139, Istanbul, Turkey. European
Language Resources Association (ELRA).
Wolfgang Seeker and Jonas Kuhn. 2013. Morphologi-
cal and syntactic case in statistical dependency pars-
ing. Computational Linguistics, 39(1):2355.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 475482, Vancouver, British Columbia, Canada,
October. Association for Computational Linguistics.
Drahomra Johanka Spoustova, Jan Hajic, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised train-
ing for the averaged perceptron POS tagger. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 763771, Athens, Greece. Association for
Computational Linguistics.
Viktor Tron, Peter Halacsy, Peter Rebrus, Andras Rung,
Peter Vajda, and Eszter Simon. 2006. Morphdb.hu:
Hungarian lexical database and morphological gram-
mar. In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation, pages
16701673, Genoa, Italy.
Reut Tsarfaty and Khalil Simaan. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 4048, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
Kubler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages (SPMRL):
what, how and whither. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 112, Los
Angeles, California, USA. Association for Computa-
tional Linguistics.
Yannick Versley, Kathrin Beck, Erhard Hinrichs, and
Heike Telljohann. 2010. A syntax-first approach to
high-quality morphological analysis and lemma dis-
ambiguation for the tba-d/z treebank. In 9th Confer-
ence on Treebanks and Linguistic Theories (TLT9),
pages 233244.
Veronika Vincze, Dora Szauter, Attila Almasi, Gyorgy
Mora, Zoltan Alexin, and Janos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of the 7th
Conference on International Language Resources and
Evaluation, pages 18551862, Valletta, Malta. Euro-
pean Language Resources Association (ELRA).
Jan Votrubec. 2006. Morphological tagging based on
averaged perceptron. In WDS06 Proceedings of Con-
tributed Papers, pages 191195, Praha, Czechia. Mat-
fyzpress, Charles University.
Janos Zsibrita, Veronika Vincze, and Richard Farkas.
2013. magyarlanc 2.0: szintaktikai elemzes es felgy-
orstott szofaji egyertelmstes. In Attila Tanacs and
Veronika Vincze, editors, IX. Magyar Szamtogepes
Nyelveszeti Konferencia, pages 368374, Szeged,
Hungary.
</reference>
<page confidence="0.995254">
344
</page>
<figure confidence="0.245523">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.680750">
<note confidence="0.946251666666667">b&apos;Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 333344, Seattle, Washington, USA, 18-21 October 2013. c 2013 Association for Computational Linguistics</note>
<title confidence="0.825786">The Effects of Syntactic Features in Automatic Prediction of Morphology</title>
<author confidence="0.996529">Wolfgang Seeker</author>
<author confidence="0.996529">Jonas Kuhn</author>
<affiliation confidence="0.998317">Institute for Natural Language Processing University of Stuttgart</affiliation>
<email confidence="0.992672">seeker@ims.uni-stuttgart.de</email>
<email confidence="0.992672">jonas@ims.uni-stuttgart.de</email>
<abstract confidence="0.995942166666666">Morphology and syntax interact considerably in many languages and language processing should pay attention to these interdependencies. We analyze the effect of syntactic features when used in automatic morphology prediction on four typologically different languages. We show that predicting morphology for languages with highly ambiguous word forms profits from taking the syntactic context of words into account and results in state-ofthe-art models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Barry J Blake</author>
</authors>
<date>2001</date>
<publisher>Case. Cambridge University Press,</publisher>
<location>Cambridge, New York,</location>
<note>2nd edition.</note>
<contexts>
<context position="2560" citStr="Blake, 2001" startWordPosition="378" endWordPosition="379"> English encode grammatical information (like the subject vs object status of an argument) via word order, whereas languages like Czech or Hungarian use different word forms. Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models. Linguistic theory (e. g., Bresnan (2001), Melcuk (2009)) suggests many interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajic, 2004; Smith et al., 2005; Chrupaa et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approxim</context>
</contexts>
<marker>Blake, 2001</marker>
<rawString>Barry J. Blake. 2001. Case. Cambridge University Press, Cambridge, New York, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A TransitionBased System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1455--1465</pages>
<institution>Jeju, South Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="13740" citStr="Bohnet and Nivre (2012)" startWordPosition="2122" endWordPosition="2125">umber marks if the form contains a digit. After preprocessing the data, our baseline system is trained using the feature set shown in Table 1. The baseline system does not make use of any syntactic information but predicts morphological information based solely on tokens and their linear context. The features are divided into static features, which can be computed on the input, and dynamic features, which are computed also on previous output of the system (cf. two passes in Section 2.2). 6 Lemma and part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next 30 tokens in the sentence. The</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A TransitionBased System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1455 1465, Jeju, South Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<journal>International Committee on Computational Linguistics.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>8997</pages>
<location>Beijing,</location>
<contexts>
<context position="10104" citStr="Bohnet (2010)" startWordPosition="1550" endWordPosition="1551">or lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools its feature model. Models are trained using passiveaggressive online training (Crammer et al., 2003). The system makes two passes over each sentence: The first pass provides predicted tags that are used as features during the second pass. We also adopted the idea of a tag filter, which deterministically assigns tags for words that always occur with the same tag in the training data. For all matters of syntactic annotation in this paper, we use the graph-based dependency parser by Bohnet (2010), also included in mate-tools. All data sets are annotated with gold syntactic information, which is used to train the parsing models. For our experiments, we use a bootstrapping approach: the parser uses the output of the morphology in its feature set, and the morphological tagger we want to analyze uses the output of the parser as syntactic features. Since it is best to keep the training setting as similar as possible to the test setting, we use 10-fold jackknifing to annotate our training data with predicted morphology or syntax respectively. Jackknifing differs from cross-validation only i</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 8997, Beijing, China. International Committee on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen-Shirra</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the 1st Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>2441</pages>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="4037" citStr="Brants et al., 2002" startWordPosition="607" endWordPosition="610">subject of a sentence and the finite verb agree with respect to their number and person feature. 333 \x0cdie wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepragten Regionen nom/acc.pl.fem nom/acc.pl.fem the economic - most developed , modern and to part catholic influenced regions NK MO PM MO NK CJ CD MO NK MO CJ the regions that are economically most developed, modern, and partly catholic Figure 1: Example of a German noun phrase. First and last word agree in number, gender, and case value. Figure 1 shows a German noun phrase taken from the German TiGer corpus (Brants et al., 2002). The two bold-faced words are the determiner and the head noun of the phrase, and they agree in their gender, number, and case values. The word Regionen (regions) is four-way ambiguous for its case value, which is reduced to a two-way ambiguity between nominative and accusative by the determiner. Further disambiguation would require information about the syntactic role of the noun phrase in a sentence. There are 11 tokens between these two words, which would require a context window of at least 13 to capture the agreement relation within a sequence model. Syntactically, however, as indicated </context>
</contexts>
<marker>Brants, Dipper, Hansen-Shirra, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Proceedings of the 1st Workshop on Treebanks and Linguistic Theories, pages 2441, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Lexical-Functional Syntax.</title>
<date>2001</date>
<publisher>Blackwell Publishers.</publisher>
<contexts>
<context position="2341" citStr="Bresnan (2001)" startWordPosition="344" endWordPosition="345">rd form is ambiguous between several different morphological descriptions within its inflection paradigm. by morphological and syntactic means.2 Different languages, however, use them to a different extent. Languages like English encode grammatical information (like the subject vs object status of an argument) via word order, whereas languages like Czech or Hungarian use different word forms. Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models. Linguistic theory (e. g., Bresnan (2001), Melcuk (2009)) suggests many interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech</context>
</contexts>
<marker>Bresnan, 2001</marker>
<rawString>Joan Bresnan. 2001. Lexical-Functional Syntax. Blackwell Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Isaac Chao</author>
<author>Llus Padr</author>
<author>Muntsa Padr</author>
</authors>
<title>Freeling: An open-source suite of language analyzers.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC04),</booktitle>
<pages>239242</pages>
<contexts>
<context position="21000" citStr="Carreras et al., 2004" startWordPosition="3307" endWordPosition="3310">f morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajic et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tron et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set dev</context>
</contexts>
<marker>Carreras, Chao, Padr, Padr, 2004</marker>
<rawString>Xavier Carreras, Isaac Chao, Llus Padr, and Muntsa Padr. 2004. Freeling: An open-source suite of language analyzers. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC04), pages 239242. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cGrzegorz Chrupaa</author>
<author>Georgiana Dinu</author>
<author>Josef van Genabith</author>
</authors>
<title>Learning morphology with morfette.</title>
<date>2008</date>
<journal>European Language Resources Association</journal>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC08),</booktitle>
<pages>23622367</pages>
<location>Marrakech,</location>
<marker>Chrupaa, Dinu, van Genabith, 2008</marker>
<rawString>\x0cGrzegorz Chrupaa, Georgiana Dinu, and Josef van Genabith. 2008. Learning morphology with morfette. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC08), pages 23622367, Marrakech, Morocco. European Language Resources Association (ELRA). http://www.lrec-conf.org/proceedings/lrec2008/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Noah A Smith</author>
</authors>
<title>Joint morphological and syntactic disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>208217</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="37279" citStr="Cohen and Smith, 2007" startWordPosition="5953" endWordPosition="5956">ce. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed </context>
</contexts>
<marker>Cohen, Smith, 2007</marker>
<rawString>Shay B. Cohen and Noah A. Smith. 2007. Joint morphological and syntactic disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 208217, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="2982" citStr="Collins, 2002" startWordPosition="436" endWordPosition="437">ny interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajic, 2004; Smith et al., 2005; Chrupaa et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2 And also by prosodic means, which we will not discuss since text-based tools rarely have access to this information. 3 For example, in English, the subject of a sentence and the finite verb agree with respect to their number and person feature. 333 \x0cdie wirtschaftlich am weitesten entwickelten , modernen und </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 18. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greville G Corbett</author>
</authors>
<title>Gender. Cambridge Textbooks in Linguistics.</title>
<date>1991</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="20592" citStr="Corbett, 1991" startWordPosition="3243" endWordPosition="3244">art in predicting morphological features makes use of morphological lexicons (e.g. Hajic (2000), Hakkani-Tur et al. (2002), Hajic (2004)). Lexicons define the possible morphological descriptions of a word and a statistical model selects the most probable one among them. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajic et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tron et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output</context>
</contexts>
<marker>Corbett, 1991</marker>
<rawString>Greville G. Corbett. 1991. Gender. Cambridge Textbooks in Linguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greville G Corbett</author>
</authors>
<title>Agreement. Cambridge Textbooks in Linguistics.</title>
<date>2006</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2731" citStr="Corbett (2006)" startWordPosition="404" endWordPosition="405">orms. Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models. Linguistic theory (e. g., Bresnan (2001), Melcuk (2009)) suggests many interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajic, 2004; Smith et al., 2005; Chrupaa et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2 And also by prosodic means, which we will not discuss since te</context>
<context position="23361" citStr="Corbett, 2006" startWordPosition="3680" endWordPosition="3681">elp in the prediction of morphology for Czech and German, but not for Hungarian and Spanish. To further investigate the difference between Czech and German on the one hand, and Hungarian and Spanish on the other, we take a closer look at the output of the tagger. We find an interesting difference between the two pairs of languages, namely the performance with respect to agreement. Agreement is a phenomenon where morphology and syntax strongly interact. Morphological features co-vary between two items in the sentence, but the relation between these items can occur at various linguistic levels (Corbett, 2006). If the syntactic information helps with predicting morphological information, we expect this to be particularly helpful with getting agreement right. All languages show agreement to some extent. Specifically, all languages show agreement in number (and person) between the subject and the verb of a clause. Czech, German, and Spanish show agreement in number, gender, and case (not Spanish) within a noun phrase. Hungarian shows case agreement within the noun phrase only rarely, e.g. for attributively used demonstrative pronouns. In order to test the effect on agreement, we measure the accuracy </context>
</contexts>
<marker>Corbett, 2006</marker>
<rawString>Greville G. Corbett. 2006. Agreement. Cambridge Textbooks in Linguistics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A fundamental algorithm for dependency parsing (with corrections).</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual ACM Southeast Conference,</booktitle>
<publisher>ACM.</publisher>
<location>Athens, Gorgia.</location>
<contexts>
<context position="30546" citStr="Covington (2001)" startWordPosition="4874" endWordPosition="4875">information. For both languages, German and Czech, we find that already 1,000 sentences are enough training data for the parser to provide useful syntactic information to the morphological tagger. After 5,000 sentences, both curves flatten out and stay on the same level. We conclude that using syntactic features for morphological prediction is viable even if there is only small amounts of syntactic data available to train the parser. As a related experiment, we also test if we can get the same effect with a very simple and thus much faster parser. We use the brute-force algorithm described in Covington (2001), which selects for each token in the sentence another token as the head. It does not have any tree requirements, so it is not even guaranteed to yield a cycle-free tree structure. In Table 7, we compare the simple parser with the mateparser, both trained on the first 5,000 sentences of the treebank. Evaluation is done in terms of labeled (LAS) and unlabeled attachment score (UAS).10 dev set test set LAS UAS LAS UAS Czech simple parser (5k) 71.57 78.96 69.09 77.23 full parser (5k) 76.77 84.38 74.70 83.00 German simple parser (5k) 83.06 85.23 78.56 81.18 full parser (5k) 87.56 90.08 83.69 86.58</context>
</contexts>
<marker>Covington, 2001</marker>
<rawString>Michael A. Covington. 2001. A fundamental algorithm for dependency parsing (with corrections). In Proceedings of the 39th Annual ACM Southeast Conference, Athens, Gorgia. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2003</date>
<booktitle>In Proceedings of the 16th Annual Conference on Neural Information Processing Systems,</booktitle>
<volume>7</volume>
<pages>12171224</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="9706" citStr="Crammer et al., 2003" startWordPosition="1479" endWordPosition="1482">. 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5 A collection of language independent, data-driven analysis tools for lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools its feature model. Models are trained using passiveaggressive online training (Crammer et al., 2003). The system makes two passes over each sentence: The first pass provides predicted tags that are used as features during the second pass. We also adopted the idea of a tag filter, which deterministically assigns tags for words that always occur with the same tag in the training data. For all matters of syntactic annotation in this paper, we use the graph-based dependency parser by Bohnet (2010), also included in mate-tools. All data sets are annotated with gold syntactic information, which is used to train the parsing models. For our experiments, we use a bootstrapping approach: the parser us</context>
</contexts>
<marker>Crammer, Dekel, Shalev-Shwartz, Singer, 2003</marker>
<rawString>Koby Crammer, Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer. 2003. Online passive-aggressive algorithms. In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, volume 7, pages 12171224, Cambridge, Massachusetts, USA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Farkas</author>
<author>Veronika Vincze</author>
<author>Helmut Schmid</author>
</authors>
<title>Dependency parsing of hungarian: Baseline results and challenges.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>5565</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France.</location>
<contexts>
<context position="9085" citStr="Farkas et al. (2012)" startWordPosition="1394" endWordPosition="1397">nd genitive case (out of about 18 case suffixes). All languages show agreement between subject and verb, and within the noun phrase. The word order in Czech and Hungarian is very variable whereas it is more restrictive in Spanish and German. As our data, we use the CoNLL 2009 Shared Task data sets (Hajic et al., 2009) for Czech and Spanish. For German, we use the dependency conversion of the TiGer treebank by Seeker and Kuhn (2012), splitting it into 40k/5k/5k sentences for training/development/test. For Hungarian, we use the Szeged Dependency Treebank (Vincze et al., 2010), with the split of Farkas et al. (2012). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5 A collection of language independent, data-driven analysis tools for lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools its feature model. Models are trained using passiveaggressive online training (</context>
</contexts>
<marker>Farkas, Vincze, Schmid, 2012</marker>
<rawString>Richard Farkas, Veronika Vincze, and Helmut Schmid. 2012. Dependency parsing of hungarian: Baseline results and challenges. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 5565, Avignon, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Easy first dependency parsing of modern Hebrew.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>103107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="36226" citStr="Goldberg and Elhadad, 2010" startWordPosition="5792" endWordPosition="5795">ivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 341 \x0c2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Simaan, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morp</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. Easy first dependency parsing of modern Hebrew. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103107, Los Angeles, California, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>Word segmentation, unknown-word resolution, and morphological agreement in a hebrew parsing system.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="37337" citStr="Goldberg and Elhadad, 2013" startWordPosition="5962" endWordPosition="5965">allenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed to improve the morphological tagger. Making use of the imp</context>
</contexts>
<marker>Goldberg, Elhadad, 2013</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2013. Word segmentation, unknown-word resolution, and morphological agreement in a hebrew parsing system. Computational Linguistics, 39(1):121160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>371379</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="37308" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="5957" endWordPosition="5961">h languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good segmentation tool that takes care of this, our approach is applicable to these languages as well. For Hebrew, this problem has also been addressed by jointly modeling segmentation, morphological prediction, and syntax (Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2013). 6 Conclusion In this paper, we have demonstrated that using syntactic information for predicting morphological information is helpful if the language shows form syncretism in combination with morphosyntactic phenomena like agreement. A model that uses syntactic information is superior to a sequence model because it leverages the syntactic dependencies that may hold between morphologically dependent words as suggested by linguistic theory. We also showed that only small amounts of training data for a statistical parser would be needed to improve the morphological </context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, pages 371379, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
</authors>
<title>Morphological Tagging: Data vs. Dictionaries.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ANLP Conference / 1st NAACL Meeting,</booktitle>
<pages>94101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington.</location>
<contexts>
<context position="20073" citStr="Hajic (2000)" startWordPosition="3160" endWordPosition="3161"> all systems yield similar results. Looking at the oracle experiment, we see that for all languages, the system can learn something from syntax. For Czech and German, this is clearly the 337 \x0ccase, for Hungarian and Spanish, the differences are small but visible. There are pronounced differences between the predicted and the gold syntax experiments in Czech and German. Clearly, the parser makes mistakes that propagate through to the prediction of the morphology. 2.4 Syntax vs Lexicon The current state-of-the-art in predicting morphological features makes use of morphological lexicons (e.g. Hajic (2000), Hakkani-Tur et al. (2002), Hajic (2004)). Lexicons define the possible morphological descriptions of a word and a statistical model selects the most probable one among them. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from </context>
<context position="22229" citStr="Hajic (2000)" startWordPosition="3500" endWordPosition="3501">c (2006). For German, we show results for RFTagger (Schmid and Laws, 2008). As expected, the information from the morphological lexicon improves the overall performance 7 Lexicons are also often used to speed up processing considerably by restricting the search space of the statistical model. 8 http://sourceforge.net/projects/featurama/ considerably compared to the results in Table 3, especially on unknown tokens. This shows that even with the considerable amounts of training data available nowadays, rule-based morphological analyzers are important resources for morphological description (cf. Hajic (2000)). The contribution of syntactic features in German and Czech is almost the same as in the previous experiment, indicating that the syntactic features contribute information that is orthogonal to that of the morphological lexicon. The lexicon provides lexical knowledge about a word form, while the syntactic features provide the syntactic context that is needed in German and Czech to decide on the right morphological tag. 2.5 Language Differences From the previous experiments, we conclude that syntactic features help in the prediction of morphology for Czech and German, but not for Hungarian an</context>
</contexts>
<marker>Hajic, 2000</marker>
<rawString>Jan Hajic. 2000. Morphological Tagging: Data vs. Dictionaries. In Proceedings of the 6th ANLP Conference / 1st NAACL Meeting, pages 94101, Seattle, Washington. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
</authors>
<title>Disambiguation of Rich Inflection (Computational Morphology of Czech). Nakladatelstv Karolinum,</title>
<date>2004</date>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2995" citStr="Hajic, 2004" startWordPosition="438" endWordPosition="439"> between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajic, 2004; Smith et al., 2005; Chrupaa et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2 And also by prosodic means, which we will not discuss since text-based tools rarely have access to this information. 3 For example, in English, the subject of a sentence and the finite verb agree with respect to their number and person feature. 333 \x0cdie wirtschaftlich am weitesten entwickelten , modernen und zum Teil kath</context>
<context position="20114" citStr="Hajic (2004)" startWordPosition="3166" endWordPosition="3167">ng at the oracle experiment, we see that for all languages, the system can learn something from syntax. For Czech and German, this is clearly the 337 \x0ccase, for Hungarian and Spanish, the differences are small but visible. There are pronounced differences between the predicted and the gold syntax experiments in Czech and German. Clearly, the parser makes mistakes that propagate through to the prediction of the morphology. 2.4 Syntax vs Lexicon The current state-of-the-art in predicting morphological features makes use of morphological lexicons (e.g. Hajic (2000), Hakkani-Tur et al. (2002), Hajic (2004)). Lexicons define the possible morphological descriptions of a word and a statistical model selects the most probable one among them. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, </context>
<context position="35094" citStr="Hajic, 2004" startWordPosition="5619" endWordPosition="5620">mple, in German, performance on subjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both</context>
</contexts>
<marker>Hajic, 2004</marker>
<rawString>Jan Hajic. 2004. Disambiguation of Rich Inflection (Computational Morphology of Czech). Nakladatelstv Karolinum, Prague, Czech Republic.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajic</author>
<author>Jarmila Panevova</author>
<author>Eva Hajicova</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan Stepanek</author>
<author>Ji Havelka</author>
<author>Marie Mikulova</author>
</authors>
<title>Prague Dependency Treebank 2.0. Jan Hajic,</title>
<date>2006</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>118</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Mart, Llus Marquez, Adam Meyers, Joakim Nivre, Sebastian Pado,</location>
<contexts>
<context position="20818" citStr="Hajic et al., 2006" startWordPosition="3276" endWordPosition="3279">l model selects the most probable one among them. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajic et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tron et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output</context>
</contexts>
<marker>Hajic, Panevova, Hajicova, Sgall, Pajas, Stepanek, Havelka, Mikulova, 2006</marker>
<rawString>Jan Hajic, Jarmila Panevova, Eva Hajicova, Petr Sgall, Petr Pajas, Jan Stepanek, Ji Havelka, and Marie Mikulova. 2006. Prague Dependency Treebank 2.0. Jan Hajic, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Mart, Llus Marquez, Adam Meyers, Joakim Nivre, Sebastian Pado, Jan Stepanek, Pavel Stranak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and Semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning: Shared Task, pages 118, Boulder, Colorado, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-Tur</author>
<author>Kemal Oflazer</author>
<author>Gokhan Tur</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages. Computers and the Humanities,</title>
<date>2002</date>
<pages>36--4</pages>
<contexts>
<context position="20100" citStr="Hakkani-Tur et al. (2002)" startWordPosition="3162" endWordPosition="3165">ield similar results. Looking at the oracle experiment, we see that for all languages, the system can learn something from syntax. For Czech and German, this is clearly the 337 \x0ccase, for Hungarian and Spanish, the differences are small but visible. There are pronounced differences between the predicted and the gold syntax experiments in Czech and German. Clearly, the parser makes mistakes that propagate through to the prediction of the morphology. 2.4 Syntax vs Lexicon The current state-of-the-art in predicting morphological features makes use of morphological lexicons (e.g. Hajic (2000), Hakkani-Tur et al. (2002), Hajic (2004)). Lexicons define the possible morphological descriptions of a word and a statistical model selects the most probable one among them. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionarie</context>
<context position="35081" citStr="Hakkani-Tur et al., 2002" startWordPosition="5615" endWordPosition="5618">ologically marked. For example, in German, performance on subjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstr</context>
</contexts>
<marker>Hakkani-Tur, Oflazer, Tur, 2002</marker>
<rawString>Dilek Z. Hakkani-Tur, Kemal Oflazer, and Gokhan Tur. 2002. Statistical morphological disambiguation for agglutinative languages. Computers and the Humanities, 36(4):381410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Hohensee</author>
<author>Emily M Bender</author>
</authors>
<title>Getting more from morphology in multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>315326</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<contexts>
<context position="34866" citStr="Hohensee and Bender (2012)" startWordPosition="5583" endWordPosition="5586">syntax 91.20 92.97 88.15 90.34 Table 9: Impact of the improved morphology on the quality of the dependency parser for Czech and German. tween the two parsing models with respect to grammatical functions that are morphologically marked. For example, in German, performance on subjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules</context>
</contexts>
<marker>Hohensee, Bender, 2012</marker>
<rawString>Matt Hohensee and Emily M. Bender. 2012. Getting more from morphology in multilingual dependency parsing. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 315326, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
</authors>
<title>A discriminative model for joint morphological disambiguation and dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>885894</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, USA.</location>
<contexts>
<context position="35524" citStr="Lee et al. (2011)" startWordPosition="5688" endWordPosition="5691">ve been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 341 \x0c2010). Modeling morphosyntactic relations explicitly has b</context>
</contexts>
<marker>Lee, Naradowsky, Smith, 2011</marker>
<rawString>John Lee, Jason Naradowsky, and David A. Smith. 2011. A discriminative model for joint morphological disambiguation and dependency parsing. In Proceedings of the 49th annual meeting of the Association for Computational Linguistics, pages 885894, Portland, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Dependency parsing of modern standard arabic with lexical and inflectional features.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="36440" citStr="Marton et al. (2013)" startWordPosition="5825" endWordPosition="5828">e falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 341 \x0c2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Simaan, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to a tagging problem. In languages like Arabic, Hebrew, or Turkish, widespread ambiguity in segmentation of single words into meaningful morphemes adds an additional complexity. Given a good s</context>
</contexts>
<marker>Marton, Habash, Rambow, 2013</marker>
<rawString>Yuval Marton, Nizar Habash, and Owen Rambow. 2013. Dependency parsing of modern standard arabic with lexical and inflectional features. Computational Linguistics, 39(1):161194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Melcuk</author>
</authors>
<date>2009</date>
<note>Dependency in linguistic description.</note>
<contexts>
<context position="2356" citStr="Melcuk (2009)" startWordPosition="346" endWordPosition="347">uous between several different morphological descriptions within its inflection paradigm. by morphological and syntactic means.2 Different languages, however, use them to a different extent. Languages like English encode grammatical information (like the subject vs object status of an argument) via word order, whereas languages like Czech or Hungarian use different word forms. Automatic analysis of languages with rich morphology needs to pay attention to the interaction between morphology and syntax in order to arrive at suitable computational models. Linguistic theory (e. g., Bresnan (2001), Melcuk (2009)) suggests many interactions between morphology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and le</context>
</contexts>
<marker>Melcuk, 2009</marker>
<rawString>Igor Melcuk. 2009. Dependency in linguistic description.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbert Prins</author>
</authors>
<title>Beyond N in N-gram tagging.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL 2004 Student Research Workshop,</booktitle>
<pages>6166</pages>
<editor>In Leonoor Van Der Beek, Dmitriy Genzel, and Daniel Midgley, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain.</location>
<contexts>
<context position="13713" citStr="Prins (2004)" startWordPosition="2119" endWordPosition="2120"> conjunctions. number marks if the form contains a digit. After preprocessing the data, our baseline system is trained using the feature set shown in Table 1. The baseline system does not make use of any syntactic information but predicts morphological information based solely on tokens and their linear context. The features are divided into static features, which can be computed on the input, and dynamic features, which are computed also on previous output of the system (cf. two passes in Section 2.2). 6 Lemma and part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next 30 </context>
</contexts>
<marker>Prins, 2004</marker>
<rawString>Robbert Prins. 2004. Beyond N in N-gram tagging. In Leonoor Van Der Beek, Dmitriy Genzel, and Daniel Midgley, editors, Proceedings of the ACL 2004 Student Research Workshop, pages 6166, Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Schiller</author>
</authors>
<title>Dmor - users guide.</title>
<date>1994</date>
<tech>Technical report,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="20860" citStr="Schiller, 1994" startWordPosition="3285" endWordPosition="3286">em. In the following experiment, we test whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajic et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tron et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our sys</context>
</contexts>
<marker>Schiller, 1994</marker>
<rawString>Anne Schiller. 1994. Dmor - users guide. Technical report, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cHelmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>777784</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="21691" citStr="Schmid and Laws, 2008" startWordPosition="3422" endWordPosition="3425"> simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set developed by Votrubec (2006). For German, we show results for RFTagger (Schmid and Laws, 2008). As expected, the information from the morphological lexicon improves the overall performance 7 Lexicons are also often used to speed up processing considerably by restricting the search space of the statistical model. 8 http://sourceforge.net/projects/featurama/ considerably compared to the results in Table 3, especially on unknown tokens. This shows that even with the considerable amounts of training data available nowadays, rule-based morphological analyzers are important resources for morphological description (cf. Hajic (2000)). The contribution of syntactic features in German and Czech </context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>\x0cHelmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an application to fine-grained POS tagging. In Proceedings of the 22nd International Conference on Computational Linguistics, pages 777784, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Making Ellipses Explicit in Dependency Conversion for a German Treebank.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation,</booktitle>
<pages>31323139</pages>
<location>Istanbul,</location>
<contexts>
<context position="8900" citStr="Seeker and Kuhn (2012)" startWordPosition="1367" endWordPosition="1370"> shows syncretism in the verbal inflection paradigms. In Hungarian, form syncretisms are much less frequent. The case paradigm of Hungarian only shows one form syncretism between dative and genitive case (out of about 18 case suffixes). All languages show agreement between subject and verb, and within the noun phrase. The word order in Czech and Hungarian is very variable whereas it is more restrictive in Spanish and German. As our data, we use the CoNLL 2009 Shared Task data sets (Hajic et al., 2009) for Czech and Spanish. For German, we use the dependency conversion of the TiGer treebank by Seeker and Kuhn (2012), splitting it into 40k/5k/5k sentences for training/development/test. For Hungarian, we use the Szeged Dependency Treebank (Vincze et al., 2010), with the split of Farkas et al. (2012). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5 A collection of language independent, data-driven analysis tools for lemmat</context>
</contexts>
<marker>Seeker, Kuhn, 2012</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German Treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 31323139, Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Morphological and syntactic case in statistical dependency parsing.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="36250" citStr="Seeker and Kuhn, 2013" startWordPosition="5796" endWordPosition="5799">ould model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 341 \x0c2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Simaan, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where the problem of predicting morphology can be reduced to</context>
</contexts>
<marker>Seeker, Kuhn, 2013</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2013. Morphological and syntactic case in statistical dependency parsing. Computational Linguistics, 39(1):2355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>David A Smith</author>
<author>Roy W Tromble</author>
</authors>
<title>Context-based morphological disambiguation with random fields.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>475482</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="3015" citStr="Smith et al., 2005" startWordPosition="440" endWordPosition="443">hology and syntax. For example, languages with a case system use different forms of the same word to mark different syntactic (or semantic) relations (Blake, 2001). In many languages, two words that participate in a syntactic relation show covariance in some or all of their morphological features (so-called agreement, Corbett (2006)).3 Automatic annotation of morphology assigns morphological descriptions (e. g., nominativesingular-masculine) to word forms. It is usually modeled as a sequence model, often in combination with part-of-speech tagging and lemmatization (Collins, 2002; Hajic, 2004; Smith et al., 2005; Chrupaa et al., 2008, and others). Sequence models achieve high accuracy and coverage but since they only use linear context they only approximate some of the underlying hierarchical relationships. As an example for these hierarchical relationships, 2 And also by prosodic means, which we will not discuss since text-based tools rarely have access to this information. 3 For example, in English, the subject of a sentence and the finite verb agree with respect to their number and person feature. 333 \x0cdie wirtschaftlich am weitesten entwickelten , modernen und zum Teil katholisch gepragten Reg</context>
<context position="35114" citStr="Smith et al., 2005" startWordPosition="5621" endWordPosition="5624">an, performance on subjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use infor</context>
</contexts>
<marker>Smith, Smith, Tromble, 2005</marker>
<rawString>Noah A. Smith, David A. Smith, and Roy W. Tromble. 2005. Context-based morphological disambiguation with random fields. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 475482, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomra Johanka Spoustova</author>
<author>Jan Hajic</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>763771</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="12166" citStr="Spoustova et al. (2009)" startWordPosition="1887" endWordPosition="1890">lows: the German and Spanish data sets are annotated with lemma and part-of-speech information using 10-fold jackknifing. The annotation is done with mate-tools lemmatizer and pos-tagger. For Czech and Hungarian, we keep the annotation provided with the data sets. Note that our experimental setup does not include lemmas or part-of-speech tags as part of the prediction of the morphology but annotates them in a pre335 \x0cprocessing step. It is not necessary to separate partof-speech and lemma from the prediction of morphology and, in fact, many systems perform these steps simultaneously (e. g. Spoustova et al. (2009)). Doing morphology prediction as a separate step allows us to use lemma and part-of-speech information in the feature set.6 static features form form1b form2b form3b form1a lemma2a pos1b pos2b pos1a form+pos pos+s1 pos+s2 pos+s3 pos+s4 lemma+p2 lemma+p3 pos+number form+form1b pos+pos1a pos+pos1b+pos2b s1+s1 1b s1+s1 1a s2+s2 1a last-verb-lemma last-verb-pos next-verb-lemma next-verb-pos dynamic features tag1b+tag2b tag2b+tag3b tag1a tag1a+tag1b tag1a+tag2a tag2a+tag3a pos1b+case1b last-verb-tag next-verb-tag pos1b+case1b+pos2b+case2b Hungarian only features pos+uppercase Czech only features p</context>
<context position="35138" citStr="Spoustova et al., 2009" startWordPosition="5625" endWordPosition="5628">ubjects and accusative objects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. </context>
</contexts>
<marker>Spoustova, Hajic, Raab, Spousta, 2009</marker>
<rawString>Drahomra Johanka Spoustova, Jan Hajic, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised training for the averaged perceptron POS tagger. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 763771, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Tron</author>
<author>Peter Halacsy</author>
<author>Peter Rebrus</author>
<author>Andras Rung</author>
<author>Peter Vajda</author>
<author>Eszter Simon</author>
</authors>
<title>Morphdb.hu: Hungarian lexical database and morphological grammar.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>16701673</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="20903" citStr="Tron et al., 2006" startWordPosition="3291" endWordPosition="3294"> whether the contribution of syntactic features is similar or different to the contribution of morphological lexicons. Lexicons encode important knowledge that is difficult to pick up in a purely statistical system, e. g. the gender of nouns, which often cannot be deduced from the word form (Corbett, 1991).7 We extend our system from the previous experiment to include information from a morphological dictionaries. For Czech, we use the morphological analyzer distributed with the Prague Dependency Treebank 2 (Hajic et al., 2006). For German, we use DMor (Schiller, 1994). For Hungarian, we use (Tron et al., 2006), and for Spanish, we use the morphological analyzer included in Freeling (Carreras et al., 2004). The output of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and wit</context>
</contexts>
<marker>Tron, Halacsy, Rebrus, Rung, Vajda, Simon, 2006</marker>
<rawString>Viktor Tron, Peter Halacsy, Peter Rebrus, Andras Rung, Peter Vajda, and Eszter Simon. 2006. Morphdb.hu: Hungarian lexical database and morphological grammar. In Proceedings of the 5th International Conference on Language Resources and Evaluation, pages 16701673, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Khalil Simaan</author>
</authors>
<title>Modeling morphosyntactic agreement in constituency-based parsing of Modern Hebrew.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>4048</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="36198" citStr="Tsarfaty and Simaan, 2010" startWordPosition="5788" endWordPosition="5791"> morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall performance falls far behind the standard pipeline approach where both tasks are done in sequence. The problem of modeling the interaction between morphology and syntax has recently attracted some attention in the SPMRL workshops (Tsarfaty et al., 341 \x0c2010). Modeling morphosyntactic relations explicitly has been shown to improve statistical parsing models (Tsarfaty and Simaan, 2010; Goldberg and Elhadad, 2010; Seeker and Kuhn, 2013), but the codependency between morphology and syntax makes it a difficult problem, and linguistic intuition is often contradicted by the empirical findings. For example, Marton et al. (2013) show that case information is the most helpful morphological feature for parsing Arabic, but only if it is given as gold information, whereas using case information from an automatic system may even harm the performance. Morphologically rich languages pose different challenges for automatic systems. In this paper, we work with European languages, where th</context>
</contexts>
<marker>Tsarfaty, Simaan, 2010</marker>
<rawString>Reut Tsarfaty and Khalil Simaan. 2010. Modeling morphosyntactic agreement in constituency-based parsing of Modern Hebrew. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 4048, Los Angeles, California, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Reut Tsarfaty</author>
<author>Djame Seddah</author>
<author>Yoav Goldberg</author>
<author>Sandra Kubler</author>
<author>Marie Candito</author>
<author>Jennifer Foster</author>
<author>Yannick Versley</author>
<author>Ines Rehbein</author>
<author>Lamia Tounsi</author>
</authors>
<title>Statistical parsing of morphologically rich languages (SPMRL): what, how and whither.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,</booktitle>
<pages>112</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California, USA.</location>
<marker>Tsarfaty, Seddah, Goldberg, Kubler, Candito, Foster, Versley, Rehbein, Tounsi, 2010</marker>
<rawString>Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra Kubler, Marie Candito, Jennifer Foster, Yannick Versley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical parsing of morphologically rich languages (SPMRL): what, how and whither. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 112, Los Angeles, California, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Kathrin Beck</author>
<author>Erhard Hinrichs</author>
<author>Heike Telljohann</author>
</authors>
<title>A syntax-first approach to high-quality morphological analysis and lemma disambiguation for the tba-d/z treebank.</title>
<date>2010</date>
<booktitle>In 9th Conference on Treebanks and Linguistic Theories (TLT9),</booktitle>
<pages>233244</pages>
<contexts>
<context position="35215" citStr="Versley et al. (2010)" startWordPosition="5639" endWordPosition="5642">nd genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is computationally quite demanding and its overall perfo</context>
</contexts>
<marker>Versley, Beck, Hinrichs, Telljohann, 2010</marker>
<rawString>Yannick Versley, Kathrin Beck, Erhard Hinrichs, and Heike Telljohann. 2010. A syntax-first approach to high-quality morphological analysis and lemma disambiguation for the tba-d/z treebank. In 9th Conference on Treebanks and Linguistic Theories (TLT9), pages 233244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Dora Szauter</author>
<author>Attila Almasi</author>
<author>Gyorgy Mora</author>
<author>Zoltan Alexin</author>
<author>Janos Csirik</author>
</authors>
<title>Hungarian Dependency Treebank.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the 7th Conference on International Language Resources and Evaluation,</booktitle>
<pages>18551862</pages>
<location>Valletta,</location>
<contexts>
<context position="9045" citStr="Vincze et al., 2010" startWordPosition="1386" endWordPosition="1389">ows one form syncretism between dative and genitive case (out of about 18 case suffixes). All languages show agreement between subject and verb, and within the noun phrase. The word order in Czech and Hungarian is very variable whereas it is more restrictive in Spanish and German. As our data, we use the CoNLL 2009 Shared Task data sets (Hajic et al., 2009) for Czech and Spanish. For German, we use the dependency conversion of the TiGer treebank by Seeker and Kuhn (2012), splitting it into 40k/5k/5k sentences for training/development/test. For Hungarian, we use the Szeged Dependency Treebank (Vincze et al., 2010), with the split of Farkas et al. (2012). 2.2 System Description To test our hypotheses, we implemented a tagger that assigns full morphological descriptions to each token in a sentence. The system was inspired by the morphological tagger included in mate-tools.5 Like the tagger provided with mate-tools, it is a classifier that tags each token using the surrounding tokens in 5 A collection of language independent, data-driven analysis tools for lemmatization, pos-tagging, morphological analysis, and dependency parsing: http://code.google.com/p/mate-tools its feature model. Models are trained u</context>
</contexts>
<marker>Vincze, Szauter, Almasi, Mora, Alexin, Csirik, 2010</marker>
<rawString>Veronika Vincze, Dora Szauter, Attila Almasi, Gyorgy Mora, Zoltan Alexin, and Janos Csirik. 2010. Hungarian Dependency Treebank. In Proceedings of the 7th Conference on International Language Resources and Evaluation, pages 18551862, Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Votrubec</author>
</authors>
<title>Morphological tagging based on averaged perceptron.</title>
<date>2006</date>
<booktitle>In WDS06 Proceedings of Contributed Papers,</booktitle>
<pages>191195</pages>
<location>Praha, Czechia. Matfyzpress, Charles University.</location>
<contexts>
<context position="14234" citStr="Votrubec (2006)" startWordPosition="2200" endWordPosition="2201">nd part-of-speech prediction may also profit from syntactic information, see e.g. Prins (2004) or Bohnet and Nivre (2012). The feature sets in Table 1 were developed specifically for our experiments and are the result of an automatic forward/backward feature selection process. The purpose of the feature selection was to arrive at a baseline system that performs well without any syntactic information. With such an optimized baseline system, we can measure the contribution of syntactic features more reliably. The last-verb/next-verb and pos+case features are variants of the features proposed in Votrubec (2006). They extract information about the first verb within the last 10/the next 30 tokens in the sentence. The case feature extracts the case value from previously assigned morphological tags. Note that the verb features are approximating syntactic information by making the assumption that the closest verbs are likely to be syntactic heads for many words. static features h lemma h s2 h s3 pos+h pos s1+h s1 h dir h dir+h pos ld s1 ld s2 ld p1 ld p4 dynamic features h tag ld tag Table 2: Syntactic features. h and ld mark features from the head and the left-most daughter, dir is a binary feature mark</context>
<context position="21625" citStr="Votrubec (2006)" startWordPosition="3412" endWordPosition="3413">ut of the analyzers is given to the system as features that simply record the presence of a particular morphological analysis for the current word. The system can thus use the output of any tool regardless of its annotation scheme, especially if the annotation scheme of the treebank is different from the one of the morphological analyzer. Table 4 presents the results of experiments where we add the output of the morphological analyzers to our system. Again, we run experiments with and without syntactic features. For Czech, we also show results from featurama8 with the feature set developed by Votrubec (2006). For German, we show results for RFTagger (Schmid and Laws, 2008). As expected, the information from the morphological lexicon improves the overall performance 7 Lexicons are also often used to speed up processing considerably by restricting the search space of the statistical model. 8 http://sourceforge.net/projects/featurama/ considerably compared to the results in Table 3, especially on unknown tokens. This shows that even with the considerable amounts of training data available nowadays, rule-based morphological analyzers are important resources for morphological description (cf. Hajic (2</context>
</contexts>
<marker>Votrubec, 2006</marker>
<rawString>Jan Votrubec. 2006. Morphological tagging based on averaged perceptron. In WDS06 Proceedings of Contributed Papers, pages 191195, Praha, Czechia. Matfyzpress, Charles University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janos Zsibrita</author>
<author>Veronika Vincze</author>
<author>Richard Farkas</author>
</authors>
<title>magyarlanc 2.0: szintaktikai elemzes es felgyorstott szofaji egyertelmstes.</title>
<date>2013</date>
<booktitle>In Attila Tanacs and Veronika Vincze, editors, IX. Magyar Szamtogepes Nyelveszeti Konferencia,</booktitle>
<pages>368374</pages>
<location>Szeged, Hungary.</location>
<contexts>
<context position="35162" citStr="Zsibrita et al., 2013" startWordPosition="5629" endWordPosition="5632">bjects improves while performance for dative objects and genitives decreases. This suggests different strengths in the two parsing models. However, the question how to make use of the improved morphology in parsing clearly needs more research in the future. A promising avenue may be the approach by Hohensee and Bender (2012). 5 Related Work Morphological taggers have been developed for many languages. The most common approach is the combination of a morphological lexicon with a statistical disambiguation model (Hakkani-Tur et al., 2002; Hajic, 2004; Smith et al., 2005; Spoustova et al., 2009; Zsibrita et al., 2013). Our work has been inspired by Versley et al. (2010), who annotate a treebank with morphological information after the syntax had been annotated already. The system used a finite-state morphology to propose a set of candidate tags for each word, which is then further restricted using hand-crafted rules over the already available syntax tree. Lee et al. (2011) pursue the idea of jointly predicting syntax and morphology, out of the motivation that joint models should model the problem more faithfully. They demonstrate that both sides can use information from each other. However, their model is </context>
</contexts>
<marker>Zsibrita, Vincze, Farkas, 2013</marker>
<rawString>Janos Zsibrita, Veronika Vincze, and Richard Farkas. 2013. magyarlanc 2.0: szintaktikai elemzes es felgyorstott szofaji egyertelmstes. In Attila Tanacs and Veronika Vincze, editors, IX. Magyar Szamtogepes Nyelveszeti Konferencia, pages 368374, Szeged, Hungary.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>