<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.576926">
b&apos;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 132141,
</bodyText>
<address confidence="0.452652">
Portland, Oregon, June 19-24, 2011. c
</address>
<title confidence="0.833609333333333">
2011 Association for Computational Linguistics
Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus
for Cross-Domain Sentiment Classification
</title>
<author confidence="0.947763">
Danushka Bollegala
</author>
<affiliation confidence="0.997591">
The University of Tokyo
</affiliation>
<address confidence="0.501074">
7-3-1, Hongo, Tokyo,
113-8656, Japan
danushka@
</address>
<email confidence="0.268104">
iba.t.u-tokyo.ac.jp
</email>
<author confidence="0.893234">
David Weir
</author>
<affiliation confidence="0.970956">
School of Informatics
University of Sussex
</affiliation>
<address confidence="0.8673945">
Falmer, Brighton,
BN1 9QJ, UK
</address>
<email confidence="0.3393155">
d.j.weir@
sussex.ac.uk
</email>
<author confidence="0.743561">
John Carroll
</author>
<affiliation confidence="0.9474325">
School of Informatics
University of Sussex
</affiliation>
<address confidence="0.864247">
Falmer, Brighton,
BN1 9QJ, UK
</address>
<email confidence="0.427093">
j.a.carroll@
sussex.ac.uk
</email>
<sectionHeader confidence="0.957963" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998826714285714">
We describe a sentiment classification method
that is applicable when we do not have any la-
beled data for a target domain but have some
labeled data for multiple other domains, des-
ignated as the source domains. We automat-
ically create a sentiment sensitive thesaurus
using both labeled and unlabeled data from
multiple source domains to find the associa-
tion between words that express similar senti-
ments in different domains. The created the-
saurus is then used to expand feature vectors
to train a binary classifier. Unlike previous
cross-domain sentiment classification meth-
ods, our method can efficiently learn from
multiple source domains. Our method signif-
icantly outperforms numerous baselines and
returns results that are better than or com-
parable to previous cross-domain sentiment
classification methods on a benchmark dataset
containing Amazon user reviews for different
types of products.
</bodyText>
<sectionHeader confidence="0.995429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993470866666667">
Users express opinions about products or services
they consume in blog posts, shopping sites, or re-
view sites. It is useful for both consumers as well
as for producers to know what general public think
about a particular product or service. Automatic
document level sentiment classification (Pang et al.,
2002; Turney, 2002) is the task of classifying a given
review with respect to the sentiment expressed by
the author of the review. For example, a sentiment
classifier might classify a user review about a movie
as positive or negative depending on the sentiment
expressed in the review. Sentiment classification
has been applied in numerous tasks such as opinion
mining (Pang and Lee, 2008), opinion summariza-
tion (Lu et al., 2009), contextual advertising (Fan
and Chang, 2010), and market analysis (Hu and Liu,
2004).
Supervised learning algorithms that require la-
beled data have been successfully used to build sen-
timent classifiers for a specific domain (Pang et al.,
2002). However, sentiment is expressed differently
in different domains, and it is costly to annotate
data for each new domain in which we would like
to apply a sentiment classifier. For example, in the
domain of reviews about electronics products, the
words durable and light are used to express pos-
itive sentiment, whereas expensive and short bat-
tery life often indicate negative sentiment. On the
other hand, if we consider the books domain the
words exciting and thriller express positive sen-
timent, whereas the words boring and lengthy
usually express negative sentiment. A classifier
trained on one domain might not perform well on
a different domain because it would fail to learn the
sentiment of the unseen words.
Work in cross-domain sentiment classification
(Blitzer et al., 2007) focuses on the challenge of
training a classifier from one or more domains
(source domains) and applying the trained classi-
fier in a different domain (target domain). A cross-
domain sentiment classification system must over-
come two main challenges. First, it must identify
which source domain features are related to which
target domain features. Second, it requires a learn-
ing framework to incorporate the information re-
</bodyText>
<page confidence="0.997251">
132
</page>
<bodyText confidence="0.999513794117647">
\x0cgarding the relatedness of source and target domain
features. Following previous work, we define cross-
domain sentiment classification as the problem of
learning a binary classifier (i.e. positive or negative
sentiment) given a small set of labeled data for the
source domain, and unlabeled data for both source
and target domains. In particular, no labeled data is
provided for the target domain.
In this paper, we describe a cross-domain senti-
ment classification method using an automatically
created sentiment sensitive thesaurus. We use la-
beled data from multiple source domains and unla-
beled data from source and target domains to rep-
resent the distribution of features. We represent a
lexical element (i.e. a unigram or a bigram of word
lemma) in a review using a feature vector. Next, for
each lexical element we measure its relatedness to
other lexical elements and group related lexical ele-
ments to create a thesaurus. The thesaurus captures
the relatedness among lexical elements that appear
in source and target domains based on the contexts
in which the lexical elements appear (their distribu-
tional context). A distinctive aspect of our approach
is that, in addition to the usual co-occurrence fea-
tures typically used in characterizing a words dis-
tributional context, we make use, where possible, of
the sentiment label of a document: i.e. sentiment la-
bels form part of our context features. This is what
makes the distributional thesaurus sensitive to senti-
ment. Unlabeled data is cheaper to collect compared
to labeled data and is often available in large quan-
tities. The use of unlabeled data enables us to ac-
curately estimate the distribution of words in source
and target domains. Our method can learn from a
large amount of unlabeled data to leverage a robust
cross-domain sentiment classifier.
We model the cross-domain sentiment classifica-
tion problem as one of feature expansion, where we
append additional related features to feature vectors
that represent source and target domain reviews in
order to reduce the mismatch of features between the
two domains. Methods that use related features have
been successfully used in numerous tasks such as
query expansion (Fang, 2008), and document classi-
fication (Shen et al., 2009). However, feature expan-
sion techniques have not previously been applied to
the task of cross-domain sentiment classification.
In our method, we use the automatically created
thesaurus to expand feature vectors in a binary clas-
sifier at train and test times by introducing related
lexical elements from the thesaurus. We use L1 reg-
ularized logistic regression as the classification al-
gorithm. (However, the method is agnostic to the
properties of the classifier and can be used to expand
feature vectors for any binary classifier). L1 regular-
ization enables us to select a small subset of features
for the classifier. Unlike previous work which at-
tempts to learn a cross-domain classifier using a sin-
gle source domain, we leverage data from multiple
source domains to learn a robust classifier that gen-
eralizes across multiple domains. Our contributions
can be summarized as follows.
We describe a fully automatic method to create
a thesaurus that is sensitive to the sentiment of
words expressed in different domains.
We describe a method to use the created the-
saurus to expand feature vectors at train and test
times in a binary classifier.
</bodyText>
<sectionHeader confidence="0.979615" genericHeader="method">
2 A Motivating Example
</sectionHeader>
<bodyText confidence="0.99982816">
To explain the problem of cross-domain sentiment
classification, consider the reviews shown in Ta-
ble 1 for the domains books and kitchen appliances.
Table 1 shows two positive and one negative re-
view from each domain. We have emphasized in
boldface the words that express the sentiment of
the authors of the reviews. We see that the words
excellent, broad, high quality, interesting, and
well researched are used to express positive senti-
ment in the books domain, whereas the word disap-
pointed indicates negative sentiment. On the other
hand, in the kitchen appliances domain the words
thrilled, high quality, professional, energy sav-
ing, lean, and delicious express positive sentiment,
whereas the words rust and disappointed express
negative sentiment. Although high quality would
express positive sentiment in both domains, and dis-
appointed negative sentiment, it is unlikely that we
would encounter well researched in kitchen appli-
ances reviews, or rust or delicious in book reviews.
Therefore, a model that is trained only using book
reviews might not have any weights learnt for deli-
cious or rust, which would make it difficult for this
model to accurately classify reviews of kitchen ap-
pliances.
</bodyText>
<page confidence="0.998871">
133
</page>
<bodyText confidence="0.963582933333333">
\x0cbooks kitchen appliances
+ Excellent and broad survey of the development of
civilization with all the punch of high quality fiction.
I was so thrilled when I unpack my processor. It is
so high quality and professional in both looks and
performance.
+ This is an interesting and well researched book. Energy saving grill. My husband loves the burgers
that I make from this grill. They are lean and deli-
cious.
- Whenever a new book by Philippa Gregory comes
out, I buy it hoping to have the same experience, and
lately have been sorely disappointed.
These knives are already showing spots of rust de-
spite washing by hand and drying. Very disap-
pointed.
</bodyText>
<tableCaption confidence="0.962386">
Table 1: Positive (+) and negative (-) sentiment reviews in two different domains.
</tableCaption>
<bodyText confidence="0.5459165">
sentence Excellent and broad survey of
the development of civilization.
</bodyText>
<equation confidence="0.559667333333333">
POS tags Excellent/JJ and/CC broad/JJ
survey/NN1 of/IO the/AT
development/NN1 of/IO civi-
lization/NN1
lexical elements
(unigrams)
</equation>
<bodyText confidence="0.436983">
excellent, broad, survey, devel-
</bodyText>
<figure confidence="0.716426285714286">
opment, civilization
lexical elements
(bigrams)
excellent+broad, broad+survey,
survey+development, develop-
ment+civilization
sentiment fea-
tures (lemma)
excellent*P, broad*P, sur-
vey*P, excellent+broad*P,
broad+survey*P
sentiment fea-
tures (POS)
JJ*P, NN1*P, JJ+NN1*P
</figure>
<tableCaption confidence="0.8183375">
Table 2: Generating lexical elements and sentiment fea-
tures from a positive review sentence.
</tableCaption>
<sectionHeader confidence="0.906443" genericHeader="method">
3 Sentiment Sensitive Thesaurus
</sectionHeader>
<bodyText confidence="0.9995553">
One solution to the feature mismatch problem out-
lined above is to use a thesaurus that groups differ-
ent words that express the same sentiment. For ex-
ample, if we know that both excellent and delicious
are positive sentiment words, then we can use this
knowledge to expand a feature vector that contains
the word delicious using the word excellent, thereby
reducing the mismatch between features in a test in-
stance and a trained model. Below we describe a
method to construct a sentiment sensitive thesaurus
for feature expansion.
Given a labeled or an unlabeled review, we first
split the review into individual sentences. We carry
out part-of-speech (POS) tagging and lemmatiza-
tion on each review sentence using the RASP sys-
tem (Briscoe et al., 2006). Lemmatization reduces
the data sparseness and has been shown to be effec-
tive in text classification tasks (Joachims, 1998). We
then apply a simple word filter based on POS tags to
select content words (nouns, verbs, adjectives, and
adverbs). In particular, previous work has identified
adjectives as good indicators of sentiment (Hatzi-
vassiloglou and McKeown, 1997; Wiebe, 2000).
Following previous work in cross-domain sentiment
classification, we model a review as a bag of words.
We select unigrams and bigrams from each sentence.
For the remainder of this paper, we will refer to un-
igrams and bigrams collectively as lexical elements.
Previous work on sentiment classification has shown
that both unigrams and bigrams are useful for train-
ing a sentiment classifier (Blitzer et al., 2007). We
note that it is possible to create lexical elements both
from source domain labeled reviews as well as from
unlabeled reviews in source and target domains.
Next, we represent each lexical element u using a
set of features as follows. First, we select other lex-
ical elements that co-occur with u in a review sen-
tence as features. Second, from each source domain
labeled review sentence in which u occurs, we cre-
ate sentiment features by appending the label of the
review to each lexical element we generate from that
review. For example, consider the sentence selected
from a positive review of a book shown in Table 2.
In Table 2, we use the notation *P to indicate posi-
tive sentiment features and *N to indicate negative
sentiment features. The example sentence shown in
Table 2 is selected from a positively labeled review,
and generates positive sentiment features as shown
in Table 2. In addition to word-level sentiment fea-
tures, we replace words with their POS tags to create
</bodyText>
<page confidence="0.998594">
134
</page>
<bodyText confidence="0.997385647058824">
\x0cPOS-level sentiment features. POS tags generalize
the word-level sentiment features, thereby reducing
feature sparseness.
Let us denote the value of a feature w in the fea-
ture vector u representing a lexical element u by
f(u, w). The vector u can be seen as a compact rep-
resentation of the distribution of a lexical element u
over the set of features that co-occur with u in the re-
views. From the construction of the feature vector u
described in the previous paragraph, it follows that
w can be either a sentiment feature or another lexical
element that co-occurs with u in some review sen-
tence. The distributional hypothesis (Harris, 1954)
states that words that have similar distributions are
semantically similar. We compute f(u, w) as the
pointwise mutual information between a lexical ele-
ment u and a feature w as follows:
</bodyText>
<equation confidence="0.988118454545455">
f(u, w) = log
c(u,w)
N
Pn
i=1 c(i,w)
N
Pm
j=1 c(u,j)
N
!
(1)
</equation>
<bodyText confidence="0.997659">
Here, c(u, w) denotes the number of review sen-
tences in which a lexical element u and a feature
w co-occur, n and m respectively denote the total
number of lexical elements and the total number of
</bodyText>
<equation confidence="0.8472844">
features, and N =
Pn
i=1
Pm
j=1 c(i, j). Pointwise
</equation>
<bodyText confidence="0.99417075">
mutual information is known to be biased towards
infrequent elements and features. We follow the dis-
counting approach of Pantel &amp; Ravichandran (2004)
to overcome this bias.
Next, for two lexical elements u and v (repre-
sented by feature vectors u and v, respectively), we
compute the relatedness (v, u) of the feature v to
the feature u as follows,
</bodyText>
<equation confidence="0.998223833333333">
(v, u) =
P
w{x|f(v,x)&amp;gt;0} f(u, w)
P
w{x|f(u,x)&amp;gt;0} f(u, w)
. (2)
</equation>
<bodyText confidence="0.897357909090909">
Here, we use the set notation {x|f(v, x) &amp;gt; 0} to
denote the set of features that co-occur with v. Re-
latedness of a lexical element u to another lexical
element v is the fraction of feature weights in the
feature vector for the element u that also co-occur
with the features in the feature vector for the ele-
ment v. If there are no features that co-occur with
both u and v, then the relatedness reaches its min-
imum value of 0. On the other hand if all features
that co-occur with u also co-occur with v, then the
relatedness , (v, u), reaches its maximum value of
</bodyText>
<listItem confidence="0.921352">
1. Note that relatedness is an asymmetric measure
by the definition given in Equation 2, and the relat-
</listItem>
<bodyText confidence="0.972353536585366">
edness (v, u) of an element v to another element u
is not necessarily equal to (u, v), the relatedness of
u to v.
We use the relatedness measure defined in Equa-
tion 2 to construct a sentiment sensitive thesaurus in
which, for each lexical element u we list lexical el-
ements v that co-occur with u (i.e. f(u, v) &amp;gt; 0) in
descending order of relatedness values (v, u). In
the remainder of the paper, we use the term base en-
try to refer to a lexical element u for which its related
lexical elements v (referred to as the neighbors of u)
are listed in the thesaurus. Note that relatedness val-
ues computed according to Equation 2 are sensitive
to sentiment labels assigned to reviews in the source
domain, because co-occurrences are computed over
both lexical and sentiment elements extracted from
reviews. In other words, the relatedness of an ele-
ment u to another element v depends upon the sen-
timent labels assigned to the reviews that generate u
and v. This is an important fact that differentiates
our sentiment-sensitive thesaurus from other distri-
butional thesauri which do not consider sentiment
information.
Moreover, we only need to retain lexical elements
in the sentiment sensitive thesaurus because when
predicting the sentiment label for target reviews (at
test time) we cannot generate sentiment elements
from those (unlabeled) reviews, therefore we are
not required to find expansion candidates for senti-
ment elements. However, we emphasize the fact that
the relatedness values between the lexical elements
listed in the sentiment-sensitive thesaurus are com-
puted using co-occurrences with both lexical and
sentiment features, and therefore the expansion can-
didates selected for the lexical elements in the tar-
get domain reviews are sensitive to sentiment labels
assigned to reviews in the source domain. Using
a sparse matrix format and approximate similarity
matching techniques (Sarawagi and Kirpal, 2004),
we can efficiently create a thesaurus from a large set
of reviews.
</bodyText>
<sectionHeader confidence="0.994611" genericHeader="method">
4 Feature Expansion
</sectionHeader>
<bodyText confidence="0.9988955">
Our feature expansion phase augments a feature vec-
tor with additional related features selected from the
</bodyText>
<page confidence="0.993809">
135
</page>
<bodyText confidence="0.997120076923077">
\x0csentiment-sensitive thesaurus created in Section 3 to
overcome the feature mismatch problem. First, fol-
lowing the bag-of-words model, we model a review
d using the set {w1, . . . , wN }, where the elements
wi are either unigrams or bigrams that appear in the
review d. We then represent a review d by a real-
valued term-frequency vector d RN , where the
value of the j-th element dj is set to the total number
of occurrences of the unigram or bigram wj in the
review d. To find the suitable candidates to expand a
vector d for the review d, we define a ranking score
score(ui, d) for each base entry in the thesaurus as
follows:
</bodyText>
<equation confidence="0.995573166666667">
score(ui, d) =
PN
j=1 dj(wj, ui)
PN
l=1 dl
(3)
</equation>
<bodyText confidence="0.98215516">
According to this definition, given a review d, a base
entry ui will have a high ranking score if there are
many words wj in the review d that are also listed
as neighbors for the base entry ui in the sentiment-
sensitive thesaurus. Moreover, we weight the re-
latedness scores for each word wj by its normal-
ized term-frequency to emphasize the salient uni-
grams and bigrams in a review. Recall that related-
ness is defined as an asymmetric measure in Equa-
tion 2, and we use (wj, ui) in the computation of
score(ui, d) in Equation 3. This is particularly im-
portant because we would like to score base entries
ui considering all the unigrams and bigrams that ap-
pear in a review d, instead of considering each uni-
gram or bigram individually.
To expand a vector, d, for a review d, we first
rank the base entries, ui using the ranking score
in Equation 3 and select the top k ranked base en-
tries. Let us denote the r-th ranked (1 r k)
base entry for a review d by vr
d. We then extend the
original set of unigrams and bigrams {w1, . . . , wN }
by the base entries v1
d, . . . , vk
d to create a new vec-
</bodyText>
<equation confidence="0.73421925">
tor d0
R(N+k) with dimensions corresponding to
w1, . . . , wN , v1
d, . . . , vk
</equation>
<bodyText confidence="0.9994172">
d for a review d. The values
of the extended vector d0
are set as follows. The
values of the first N dimensions that correspond to
unigrams and bigrams wi that occur in the review d
are set to di, their frequency in d. The subsequent k
dimensions that correspond to the top ranked based
entries for the review d are weighted according to
their ranking score. Specifically, we set the value of
the r-th ranked base entry vr
d to 1/r. Alternatively,
one could use the ranking score, score(vr
d, d), itself
as the value of the appended base entries. However,
both relatedness scores as well as normalized term-
frequencies can be small in practice, which leads to
very small absolute ranking scores. By using the
inverse rank, we only take into account the rela-
tive ranking of base entries and ignore their absolute
scores.
Note that the score of a base entry depends on a
review d. Therefore, we select different base en-
tries as additional features for expanding different
reviews. Furthermore, we do not expand each wi
individually when expanding a vector d for a re-
view. Instead, we consider all unigrams and bi-
grams in d when selecting the base entries for ex-
pansion. One can think of the feature expansion pro-
cess as a lower dimensional latent mapping of fea-
tures onto the space spanned by the base entries in
the sentiment-sensitive thesaurus. The asymmetric
property of the relatedness (Equation 2) implicitly
prefers common words that co-occur with numerous
other words as expansion candidates. Such words
act as domain independent pivots and enable us to
transfer the information regarding sentiment from
one domain to another.
Using the extended vectors d0
to represent re-
views, we train a binary classifier from the source
domain labeled reviews to predict positive and neg-
ative sentiment in reviews. We differentiate the ap-
pended base entries vr
d from wi that existed in the
original vector d (prior to expansion) by assigning
different feature identifiers to the appended base en-
tries. For example, a unigram excellent in a feature
vector is differentiated from the base entry excellent
by assigning the feature id, BASE=excellent to the
latter. This enables us to learn different weights for
base entries depending on whether they are useful
for expanding a feature vector. We use L1 regu-
larized logistic regression as the classification algo-
rithm (Ng, 2004), which produces a sparse model in
which most irrelevant features are assigned a zero
weight. This enables us to select useful features for
classification in a systematic way without having to
preselect features using heuristic approaches. The
regularization parameter is set to its default value
of 1 for all the experiments described in this paper.
</bodyText>
<page confidence="0.999004">
136
</page>
<sectionHeader confidence="0.816291" genericHeader="method">
\x0c5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.77586">
5.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9963470625">
To evaluate our method we use the cross-domain
sentiment classification dataset prepared by Blitzer
et al. (2007). This dataset consists of Amazon prod-
uct reviews for four different product types: books
(B), DVDs (D), electronics (E) and kitchen appli-
ances (K). There are 1000 positive and 1000 neg-
ative labeled reviews for each domain. Moreover,
the dataset contains some unlabeled reviews (on av-
erage 17, 547) for each domain. This benchmark
dataset has been used in much previous work on
cross-domain sentiment classification and by eval-
uating on it we can directly compare our method
against existing approaches.
Following previous work, we randomly select 800
positive and 800 negative labeled reviews from each
domain as training instances (i.e. 16004 = 6400);
the remainder is used for testing (i.e. 400 4 =
1600). In our experiments, we select each domain in
turn as the target domain, with one or more other do-
mains as sources. Note that when we combine more
than one source domain we limit the total number
of source domain labeled reviews to 1600, balanced
between the domains. For example, if we combine
two source domains, then we select 400 positive and
400 negative labeled reviews from each domain giv-
ing (400 + 400) 2 = 1600. This enables us to
perform a fair evaluation when combining multiple
source domains. The evaluation metric is classifica-
tion accuracy on a target domain, computed as the
percentage of correctly classified target domain re-
views out of the total number of reviews in the target
domain.
</bodyText>
<subsectionHeader confidence="0.999871">
5.2 Effect of Feature Expansion
</subsectionHeader>
<bodyText confidence="0.992751818181818">
To study the effect of feature expansion at train time
compared to test time, we used Amazon reviews for
two further domains, music and video, which were
also collected by Blitzer et al. (2007) but are not
part of the benchmark dataset. Each validation do-
main has 1000 positive and 1000 negative labeled
reviews, and 15000 unlabeled reviews. Using the
validation domains as targets, we vary the number
of top k ranked base entries (Equation 3) used for
feature expansion during training (Traink) and test-
ing (Testk), and measure the average classification
</bodyText>
<figure confidence="0.995132066666667">
0 200 400 600 800 1000
0
200
400
600
800
1000
Traink
Testk
0.776
0.778
0.78
0.782
0.784
0.786
</figure>
<figureCaption confidence="0.99394">
Figure 1: Feature expansion at train vs. test times.
</figureCaption>
<figure confidence="0.973630571428571">
B D K B+D B+K D+K B+D+K
50
55
60
65
70
75
80
85
Source Domains
Accuracy
on
electronics
domain
</figure>
<figureCaption confidence="0.999693">
Figure 2: Effect of using multiple source domains.
</figureCaption>
<bodyText confidence="0.997965384615385">
accuracy. Figure 1 illustrates the results using a heat
map, where dark colors indicate low accuracy val-
ues and light colors indicate high accuracy values.
We see that expanding features only at test time (the
left-most column) does not work well because we
have not learned proper weights for the additional
features. Similarly, expanding features only at train
time (the bottom-most row) also does not perform
well because the expanded features are not used dur-
ing testing. The maximum classification accuracy is
obtained when Testk = 400 and Traink = 800, and
we use these values for the remainder of the experi-
ments described in the paper.
</bodyText>
<subsectionHeader confidence="0.999253">
5.3 Combining Multiple Sources
</subsectionHeader>
<bodyText confidence="0.9980164">
Figure 2 shows the effect of combining multiple
source domains to build a sentiment classifier for
the electronics domain. We see that the kitchen do-
main is the single best source domain when adapt-
ing to the electronics target domain. This behavior
</bodyText>
<page confidence="0.970887">
137
</page>
<figure confidence="0.962022071428571">
\x0c0 200 400 600 800
40
45
50
55
60
65
70
75
80
85
Positive/Negative instances
Accuracy
B E K B+E B+K E+K B+E+K
</figure>
<figureCaption confidence="0.992159">
Figure 3: Effect of source domain labeled data.
</figureCaption>
<figure confidence="0.98043">
0 0.2 0.4 0.6 0.8 1
50
55
60
65
70
Source unlabeled dataset size
Accuracy
B E K B+E B+K E+K B+E+K
</figure>
<figureCaption confidence="0.999942">
Figure 4: Effect of source domain unlabeled data.
</figureCaption>
<bodyText confidence="0.984539181818182">
is explained by the fact that in general kitchen appli-
ances and electronic items have similar aspects. But
a more interesting observation is that the accuracy
that we obtain when we use two source domains is
always greater than the accuracy if we use those do-
mains individually. The highest accuracy is achieved
when we use all three source domains. Although
not shown here for space limitations, we observed
similar trends with other domains in the benchmark
dataset.
To investigate the impact of the quantity of source
domain labeled data on our method, we vary the
amount of data from zero to 800 reviews, with equal
amounts of positive and negative labeled data. Fig-
ure 3 shows the accuracy with the DVD domain as
the target. Note that source domain labeled data is
used both to create the sentiment sensitive thesaurus
as well as to train the sentiment classifier. When
there are multiple source domains we limit and bal-
ance the number of labeled instances as outlined in
Section 5.1. The amount of unlabeled data is held
constant, so that any change in classification accu-
</bodyText>
<figure confidence="0.979984111111111">
0 0.2 0.4 0.6 0.8 1
50
55
60
65
70
Target unlabeled dataset size
Accuracy
B E K B+E B+K E+K B+E+K
</figure>
<figureCaption confidence="0.999889">
Figure 5: Effect of target domain unlabeled data.
</figureCaption>
<bodyText confidence="0.995399852941176">
racy is directly attributable to the source domain la-
beled instances. Because this is a binary classifica-
tion task (i.e. positive vs. negative sentiment), a ran-
dom classifier that does not utilize any labeled data
would report a 50% classification accuracy. From
Figure 3, we see that when we increase the amount
of source domain labeled data the accuracy increases
quickly. In fact, by selecting only 400 (i.e. 50% of
the total 800) labeled instances per class, we achieve
the maximum performance in most of the cases.
To study the effect of source and target domain
unlabeled data on the performance of our method,
we create sentiment sensitive thesauri using differ-
ent proportions of unlabeled data. The amount of
labeled data is held constant and is balanced across
multiple domains as outlined in Section 5.1, so any
changes in classification accuracy can be directly at-
tributed to the contribution of unlabeled data. Figure
4 shows classification accuracy on the DVD target
domain when we vary the proportion of source do-
main unlabeled data (target domains unlabeled data
is fixed).
Likewise, Figure 5 shows the classification ac-
curacy on the DVD target domain when we vary
the proportion of the target domains unlabeled data
(source domains unlabeled data is fixed). From Fig-
ures 4 and 5, we see that irrespective of the amount
being used, there is a clear performance gain when
we use unlabeled data from multiple source domains
compared to using a single source domain. How-
ever, we could not observe a clear gain in perfor-
mance when we increase the amount of the unla-
beled data used to create the sentiment sensitive the-
saurus.
</bodyText>
<page confidence="0.99661">
138
</page>
<table confidence="0.9992183">
\x0cMethod K D E B
No Thesaurus 72.61 68.97 70.53 62.72
SCL 80.83 74.56 78.43 72.76
SCL-MI 82.06 76.30 78.93 74.56
SFA 81.48 76.31 75.30 77.73
LSA 79.00 73.50 77.66 70.83
FALSA 80.83 76.33 77.33 73.33
NSS 77.50 73.50 75.50 71.46
Proposed 85.18 78.77 83.63 76.32
Within-Domain 87.70 82.40 84.40 80.40
</table>
<tableCaption confidence="0.99757">
Table 3: Cross-domain sentiment classification accuracy.
</tableCaption>
<subsectionHeader confidence="0.97135">
5.4 Cross-Domain Sentiment Classification
</subsectionHeader>
<bodyText confidence="0.999304982758621">
Table 3 compares our method against a number of
baselines and previous cross-domain sentiment clas-
sification techniques using the benchmark dataset.
For all previous techniques we give the results re-
ported in the original papers. The No Thesaurus
baseline simulates the effect of not performing any
feature expansion. We simply train a binary clas-
sifier using unigrams and bigrams as features from
the labeled reviews in the source domains and ap-
ply the trained classifier on the target domain. This
can be considered to be a lower bound that does
not perform domain adaptation. SCL is the struc-
tural correspondence learning technique of Blitzer
et al. (2006). In SCL-MI, features are selected us-
ing the mutual information between a feature (uni-
gram or bigram) and a domain label. After selecting
salient features, the SCL algorithm is used to train a
binary classifier. SFA is the spectral feature align-
ment technique of Pan et al. (2010). Both the LSA
and FALSA techniques are based on latent semantic
analysis (Pan et al., 2010). For the Within-Domain
baseline, we train a binary classifier using the la-
beled data from the target domain. This upper base-
line represents the classification accuracy we could
hope to obtain if we were to have labeled data for the
target domain. Note that this is not a cross-domain
classification setting. To evaluate the benefit of us-
ing sentiment features on our method, we give a NSS
(non-sentiment sensitive) baseline in which we cre-
ate a thesaurus without using any sentiment features.
Proposed is our method.
From Table 3, we see that our proposed method
returns the best cross-domain sentiment classifica-
tion accuracy (shown in boldface) for the three do-
mains kitchen appliances, DVDs, and electronics.
For the books domain, the best results are returned
by SFA. The books domain has the lowest number
of unlabeled reviews (around 5000) in the dataset.
Because our method relies upon the availability of
unlabeled data for the construction of a sentiment
sensitive thesaurus, we believe that this accounts for
our lack of performance on the books domain. How-
ever, given that it is much cheaper to obtain unla-
beled than labeled data for a target domain, there is
strong potential for improving the performance of
our method in this domain. The analysis of vari-
ance (ANOVA) and Tukeys honestly significant dif-
ferences (HSD) tests on the classification accuracies
for the four domains show that our method is sta-
tistically significantly better than both the No The-
saurus and NSS baselines, at confidence level 0.05.
We therefore conclude that using the sentiment sen-
sitive thesaurus for feature expansion is useful for
cross-domain sentiment classification. The results
returned by our method are comparable to state-of-
the-art techniques such as SCL-MI and SFA. In par-
ticular, the differences between those techniques and
our method are not statistically significant.
</bodyText>
<sectionHeader confidence="0.999722" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99934375">
Compared to single-domain sentiment classifica-
tion, which has been studied extensively in previous
work (Pang and Lee, 2008; Turney, 2002), cross-
domain sentiment classification has only recently re-
ceived attention in response to advances in the area
of domain adaptation. Aue and Gammon (2005) re-
port a number of empirical tests into domain adap-
tation of sentiment classifiers using an ensemble of
classifiers. However, most of these tests were un-
able to outperform a simple baseline classifier that
is trained using all labeled data for all domains.
Blitzer et al. (2007) apply the structural corre-
spondence learning (SCL) algorithm to train a cross-
domain sentiment classifier. They first chooses a set
of pivot features using pointwise mutual informa-
tion between a feature and a domain label. Next,
linear predictors are learnt to predict the occur-
rences of those pivots. Finally, they use singular
value decomposition (SVD) to construct a lower-
dimensional feature space in which a binary classi-
</bodyText>
<page confidence="0.992686">
139
</page>
<bodyText confidence="0.997700344827586">
\x0cfier is trained. The selection of pivots is vital to the
performance of SCL and heuristically selected pivot
features might not guarantee the best performance
on target domains. In contrast, our method uses all
features when creating the thesaurus and selects a
subset of features during training using L1 regular-
ization. Moreover, we do not require SVD, which
has cubic time complexity so can be computation-
ally expensive for large datasets.
Pan et al. (2010) use structural feature alignment
(SFA) to find an alignment between domain spe-
cific and domain independent features. The mu-
tual information of a feature with domain labels is
used to classify domain specific and domain inde-
pendent features. Next, spectral clustering is per-
formed on a bipartite graph that represents the re-
lationship between the two sets of features. Fi-
nally, the top eigenvectors are selected to construct
a lower-dimensional projection. However, not all
words can be cleanly classified into domain spe-
cific or domain independent, and this process is con-
ducted prior to training a classifier. In contrast, our
method lets a particular lexical entry to be listed as
a neighour for multiple base entries. Moreover, we
expand each feature vector individually and do not
require any clustering. Furthermore, unlike SCL and
SFA, which consider a single source domain, our
method can efficiently adapt from multiple source
domains.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9997141">
We have described and evaluated a method to
construct a sentiment-sensitive thesaurus to bridge
the gap between source and target domains in
cross-domain sentiment classification using multi-
ple source domains. Experimental results using a
benchmark dataset for cross-domain sentiment clas-
sification show that our proposed method can im-
prove classification accuracy in a sentiment classi-
fier. In future, we intend to apply the proposed
method to other domain adaptation tasks.
</bodyText>
<sectionHeader confidence="0.962618" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9993945">
This research was conducted while the first author
was a visiting research fellow at Sussex university
under the postdoctoral fellowship of the Japan Soci-
ety for the Promotion of Science (JSPS).
</bodyText>
<sectionHeader confidence="0.991792" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99885798">
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
Technical report, Microsoft Research.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In EMNLP 2006.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
ACL 2007, pages 440447.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the rasp system. In COL-
ING/ACL 2006 Interactive Presentation Sessions.
Teng-Kai Fan and Chia-Hui Chang. 2010. Sentiment-
oriented contextual advertising. Knowledge and Infor-
mation Systems, 23(3):321344.
Hui Fang. 2008. A re-examination of query expansion
using lexical resources. In ACL 2008, pages 139147.
Z. Harris. 1954. Distributional structure. Word, 10:146
162.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In ACL 1997, pages 174181.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In KDD 2004, pages 168177.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In ECML 1998, pages 137142.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009.
Rated aspect summarization of short comments. In
WWW 2009, pages 131140.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In ICML 2004.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
WWW 2010.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In EMNLP 2002, pages 79
86.
Patrick Pantel and Deepak Ravichandran. 2004. Au-
tomatically labeling semantic classes. In NAACL-
HLT04, pages 321 328.
Sunita Sarawagi and Alok Kirpal. 2004. Efficient set
joins on similarity predicates. In SIGMOD 04, pages
743754.
</reference>
<page confidence="0.904807">
140
</page>
<reference confidence="0.998049444444444">
\x0cDou Shen, Jianmin Wu, Bin Cao, Jian-Tao Sun, Qiang
Yang, Zheng Chen, and Ying Li. 2009. Exploit-
ing term relationship to boost text classification. In
CIKM09, pages 1637 1640.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In ACL 2002, pages 417424.
Janyce M. Wiebe. 2000. Learning subjective adjective
from corpora. In AAAI 2000, pages 735740.
</reference>
<page confidence="0.981546">
141
</page>
<figure confidence="0.250429">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272097">
<note confidence="0.990418666666667">b&apos;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 132141, Portland, Oregon, June 19-24, 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.9971175">Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</title>
<author confidence="0.998604">Danushka Bollegala</author>
<affiliation confidence="0.999905">The University of Tokyo</affiliation>
<address confidence="0.99899">7-3-1, Hongo, Tokyo, 113-8656, Japan</address>
<email confidence="0.79473">danushka@iba.t.u-tokyo.ac.jp</email>
<author confidence="0.999949">David Weir</author>
<affiliation confidence="0.999816">School of Informatics University of Sussex</affiliation>
<address confidence="0.880043">Falmer, Brighton, BN1 9QJ, UK</address>
<email confidence="0.9392885">d.j.weir@sussex.ac.uk</email>
<author confidence="0.996079">John Carroll</author>
<affiliation confidence="0.9996695">School of Informatics University of Sussex</affiliation>
<address confidence="0.8800335">Falmer, Brighton, BN1 9QJ, UK</address>
<email confidence="0.9527495">j.a.carroll@sussex.ac.uk</email>
<abstract confidence="0.999683363636364">We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automatically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: a case study.</title>
<date>2005</date>
<tech>Technical report, Microsoft Research.</tech>
<marker>Aue, Gamon, 2005</marker>
<rawString>Anthony Aue and Michael Gamon. 2005. Customizing sentiment classifiers to new domains: a case study. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="28768" citStr="Blitzer et al. (2006)" startWordPosition="4801" endWordPosition="4804">number of baselines and previous cross-domain sentiment classification techniques using the benchmark dataset. For all previous techniques we give the results reported in the original papers. The No Thesaurus baseline simulates the effect of not performing any feature expansion. We simply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. This can be considered to be a lower bound that does not perform domain adaptation. SCL is the structural correspondence learning technique of Blitzer et al. (2006). In SCL-MI, features are selected using the mutual information between a feature (unigram or bigram) and a domain label. After selecting salient features, the SCL algorithm is used to train a binary classifier. SFA is the spectral feature alignment technique of Pan et al. (2010). Both the LSA and FALSA techniques are based on latent semantic analysis (Pan et al., 2010). For the Within-Domain baseline, we train a binary classifier using the labeled data from the target domain. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In EMNLP 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<date>2007</date>
<contexts>
<context position="3318" citStr="Blitzer et al., 2007" startWordPosition="502" endWordPosition="505"> classifier. For example, in the domain of reviews about electronics products, the words durable and light are used to express positive sentiment, whereas expensive and short battery life often indicate negative sentiment. On the other hand, if we consider the books domain the words exciting and thriller express positive sentiment, whereas the words boring and lengthy usually express negative sentiment. A classifier trained on one domain might not perform well on a different domain because it would fail to learn the sentiment of the unseen words. Work in cross-domain sentiment classification (Blitzer et al., 2007) focuses on the challenge of training a classifier from one or more domains (source domains) and applying the trained classifier in a different domain (target domain). A crossdomain sentiment classification system must overcome two main challenges. First, it must identify which source domain features are related to which target domain features. Second, it requires a learning framework to incorporate the information re132 \x0cgarding the relatedness of source and target domain features. Following previous work, we define crossdomain sentiment classification as the problem of learning a binary c</context>
<context position="11268" citStr="Blitzer et al., 2007" startWordPosition="1760" endWordPosition="1763">S tags to select content words (nouns, verbs, adjectives, and adverbs). In particular, previous work has identified adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). Following previous work in cross-domain sentiment classification, we model a review as a bag of words. We select unigrams and bigrams from each sentence. For the remainder of this paper, we will refer to unigrams and bigrams collectively as lexical elements. Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier (Blitzer et al., 2007). We note that it is possible to create lexical elements both from source domain labeled reviews as well as from unlabeled reviews in source and target domains. Next, we represent each lexical element u using a set of features as follows. First, we select other lexical elements that co-occur with u in a review sentence as features. Second, from each source domain labeled review sentence in which u occurs, we create sentiment features by appending the label of the review to each lexical element we generate from that review. For example, consider the sentence selected from a positive review of a</context>
<context position="21312" citStr="Blitzer et al. (2007)" startWordPosition="3529" endWordPosition="3532">useful for expanding a feature vector. We use L1 regularized logistic regression as the classification algorithm (Ng, 2004), which produces a sparse model in which most irrelevant features are assigned a zero weight. This enables us to select useful features for classification in a systematic way without having to preselect features using heuristic approaches. The regularization parameter is set to its default value of 1 for all the experiments described in this paper. 136 \x0c5 Experiments 5.1 Dataset To evaluate our method we use the cross-domain sentiment classification dataset prepared by Blitzer et al. (2007). This dataset consists of Amazon product reviews for four different product types: books (B), DVDs (D), electronics (E) and kitchen appliances (K). There are 1000 positive and 1000 negative labeled reviews for each domain. Moreover, the dataset contains some unlabeled reviews (on average 17, 547) for each domain. This benchmark dataset has been used in much previous work on cross-domain sentiment classification and by evaluating on it we can directly compare our method against existing approaches. Following previous work, we randomly select 800 positive and 800 negative labeled reviews from e</context>
<context position="22951" citStr="Blitzer et al. (2007)" startWordPosition="3806" endWordPosition="3809">ins, then we select 400 positive and 400 negative labeled reviews from each domain giving (400 + 400) 2 = 1600. This enables us to perform a fair evaluation when combining multiple source domains. The evaluation metric is classification accuracy on a target domain, computed as the percentage of correctly classified target domain reviews out of the total number of reviews in the target domain. 5.2 Effect of Feature Expansion To study the effect of feature expansion at train time compared to test time, we used Amazon reviews for two further domains, music and video, which were also collected by Blitzer et al. (2007) but are not part of the benchmark dataset. Each validation domain has 1000 positive and 1000 negative labeled reviews, and 15000 unlabeled reviews. Using the validation domains as targets, we vary the number of top k ranked base entries (Equation 3) used for feature expansion during training (Traink) and testing (Testk), and measure the average classification 0 200 400 600 800 1000 0 200 400 600 800 1000 Traink Testk 0.776 0.778 0.78 0.782 0.784 0.786 Figure 1: Feature expansion at train vs. test times. B D K B+D B+K D+K B+D+K 50 55 60 65 70 75 80 85 Source Domains Accuracy on electronics dom</context>
<context position="31602" citStr="Blitzer et al. (2007)" startWordPosition="5261" endWordPosition="5264">re not statistically significant. 6 Related Work Compared to single-domain sentiment classification, which has been studied extensively in previous work (Pang and Lee, 2008; Turney, 2002), crossdomain sentiment classification has only recently received attention in response to advances in the area of domain adaptation. Aue and Gammon (2005) report a number of empirical tests into domain adaptation of sentiment classifiers using an ensemble of classifiers. However, most of these tests were unable to outperform a simple baseline classifier that is trained using all labeled data for all domains. Blitzer et al. (2007) apply the structural correspondence learning (SCL) algorithm to train a crossdomain sentiment classifier. They first chooses a set of pivot features using pointwise mutual information between a feature and a domain label. Next, linear predictors are learnt to predict the occurrences of those pivots. Finally, they use singular value decomposition (SVD) to construct a lowerdimensional feature space in which a binary classi139 \x0cfier is trained. The selection of pivots is vital to the performance of SCL and heuristically selected pivot features might not guarantee the best performance on targe</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>bollywood Biographies</author>
</authors>
<title>boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL</title>
<date>2007</date>
<pages>440447</pages>
<marker>Biographies, 2007</marker>
<rawString>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In ACL 2007, pages 440447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<date>2006</date>
<contexts>
<context position="10475" citStr="Briscoe et al., 2006" startWordPosition="1636" endWordPosition="1639">e sentiment. For example, if we know that both excellent and delicious are positive sentiment words, then we can use this knowledge to expand a feature vector that contains the word delicious using the word excellent, thereby reducing the mismatch between features in a test instance and a trained model. Below we describe a method to construct a sentiment sensitive thesaurus for feature expansion. Given a labeled or an unlabeled review, we first split the review into individual sentences. We carry out part-of-speech (POS) tagging and lemmatization on each review sentence using the RASP system (Briscoe et al., 2006). Lemmatization reduces the data sparseness and has been shown to be effective in text classification tasks (Joachims, 1998). We then apply a simple word filter based on POS tags to select content words (nouns, verbs, adjectives, and adverbs). In particular, previous work has identified adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). Following previous work in cross-domain sentiment classification, we model a review as a bag of words. We select unigrams and bigrams from each sentence. For the remainder of this paper, we will refer to unigrams and b</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006.</rawString>
</citation>
<citation valid="true">
<title>The second release of the rasp system.</title>
<date>2006</date>
<booktitle>In COLING/ACL</booktitle>
<contexts>
<context position="28768" citStr="(2006)" startWordPosition="4804" endWordPosition="4804">ines and previous cross-domain sentiment classification techniques using the benchmark dataset. For all previous techniques we give the results reported in the original papers. The No Thesaurus baseline simulates the effect of not performing any feature expansion. We simply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. This can be considered to be a lower bound that does not perform domain adaptation. SCL is the structural correspondence learning technique of Blitzer et al. (2006). In SCL-MI, features are selected using the mutual information between a feature (unigram or bigram) and a domain label. After selecting salient features, the SCL algorithm is used to train a binary classifier. SFA is the spectral feature alignment technique of Pan et al. (2010). Both the LSA and FALSA techniques are based on latent semantic analysis (Pan et al., 2010). For the Within-Domain baseline, we train a binary classifier using the labeled data from the target domain. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for</context>
</contexts>
<marker>2006</marker>
<rawString>The second release of the rasp system. In COLING/ACL 2006 Interactive Presentation Sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teng-Kai Fan</author>
<author>Chia-Hui Chang</author>
</authors>
<title>Sentimentoriented contextual advertising.</title>
<date>2010</date>
<journal>Knowledge and Information Systems,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2333" citStr="Fan and Chang, 2010" startWordPosition="344" endWordPosition="347">ducers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used to build sentiment classifiers for a specific domain (Pang et al., 2002). However, sentiment is expressed differently in different domains, and it is costly to annotate data for each new domain in which we would like to apply a sentiment classifier. For example, in the domain of reviews about electronics products, the words durable and light are used to express positive sentiment, whereas expensive and short battery life often indicate negative sentiment. On the other </context>
</contexts>
<marker>Fan, Chang, 2010</marker>
<rawString>Teng-Kai Fan and Chia-Hui Chang. 2010. Sentimentoriented contextual advertising. Knowledge and Information Systems, 23(3):321344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Fang</author>
</authors>
<title>A re-examination of query expansion using lexical resources.</title>
<date>2008</date>
<booktitle>In ACL</booktitle>
<pages>139147</pages>
<contexts>
<context position="5944" citStr="Fang, 2008" startWordPosition="922" endWordPosition="923">use of unlabeled data enables us to accurately estimate the distribution of words in source and target domains. Our method can learn from a large amount of unlabeled data to leverage a robust cross-domain sentiment classifier. We model the cross-domain sentiment classification problem as one of feature expansion, where we append additional related features to feature vectors that represent source and target domain reviews in order to reduce the mismatch of features between the two domains. Methods that use related features have been successfully used in numerous tasks such as query expansion (Fang, 2008), and document classification (Shen et al., 2009). However, feature expansion techniques have not previously been applied to the task of cross-domain sentiment classification. In our method, we use the automatically created thesaurus to expand feature vectors in a binary classifier at train and test times by introducing related lexical elements from the thesaurus. We use L1 regularized logistic regression as the classification algorithm. (However, the method is agnostic to the properties of the classifier and can be used to expand feature vectors for any binary classifier). L1 regularization e</context>
</contexts>
<marker>Fang, 2008</marker>
<rawString>Hui Fang. 2008. A re-examination of query expansion using lexical resources. In ACL 2008, pages 139147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--146</pages>
<contexts>
<context position="12904" citStr="Harris, 1954" startWordPosition="2043" endWordPosition="2044">. POS tags generalize the word-level sentiment features, thereby reducing feature sparseness. Let us denote the value of a feature w in the feature vector u representing a lexical element u by f(u, w). The vector u can be seen as a compact representation of the distribution of a lexical element u over the set of features that co-occur with u in the reviews. From the construction of the feature vector u described in the previous paragraph, it follows that w can be either a sentiment feature or another lexical element that co-occurs with u in some review sentence. The distributional hypothesis (Harris, 1954) states that words that have similar distributions are semantically similar. We compute f(u, w) as the pointwise mutual information between a lexical element u and a feature w as follows: f(u, w) = log c(u,w) N Pn i=1 c(i,w) N Pm j=1 c(u,j) N ! (1) Here, c(u, w) denotes the number of review sentences in which a lexical element u and a feature w co-occur, n and m respectively denote the total number of lexical elements and the total number of features, and N = Pn i=1 Pm j=1 c(i, j). Pointwise mutual information is known to be biased towards infrequent elements and features. We follow the discou</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional structure. Word, 10:146 162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Predicting the semantic orientation of adjectives.</title>
<date>1997</date>
<booktitle>In ACL</booktitle>
<pages>174181</pages>
<contexts>
<context position="10841" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="1691" endWordPosition="1695">ent sensitive thesaurus for feature expansion. Given a labeled or an unlabeled review, we first split the review into individual sentences. We carry out part-of-speech (POS) tagging and lemmatization on each review sentence using the RASP system (Briscoe et al., 2006). Lemmatization reduces the data sparseness and has been shown to be effective in text classification tasks (Joachims, 1998). We then apply a simple word filter based on POS tags to select content words (nouns, verbs, adjectives, and adverbs). In particular, previous work has identified adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). Following previous work in cross-domain sentiment classification, we model a review as a bag of words. We select unigrams and bigrams from each sentence. For the remainder of this paper, we will refer to unigrams and bigrams collectively as lexical elements. Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier (Blitzer et al., 2007). We note that it is possible to create lexical elements both from source domain labeled reviews as well as from unlabeled reviews in source and target domains. Next, we rep</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen R. McKeown. 1997. Predicting the semantic orientation of adjectives. In ACL 1997, pages 174181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews. In KDD</title>
<date>2004</date>
<pages>168177</pages>
<contexts>
<context position="2373" citStr="Hu and Liu, 2004" startWordPosition="351" endWordPosition="354">out a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used to build sentiment classifiers for a specific domain (Pang et al., 2002). However, sentiment is expressed differently in different domains, and it is costly to annotate data for each new domain in which we would like to apply a sentiment classifier. For example, in the domain of reviews about electronics products, the words durable and light are used to express positive sentiment, whereas expensive and short battery life often indicate negative sentiment. On the other hand, if we consider the books domain th</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In KDD 2004, pages 168177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In ECML</booktitle>
<pages>137142</pages>
<contexts>
<context position="10599" citStr="Joachims, 1998" startWordPosition="1657" endWordPosition="1658">e to expand a feature vector that contains the word delicious using the word excellent, thereby reducing the mismatch between features in a test instance and a trained model. Below we describe a method to construct a sentiment sensitive thesaurus for feature expansion. Given a labeled or an unlabeled review, we first split the review into individual sentences. We carry out part-of-speech (POS) tagging and lemmatization on each review sentence using the RASP system (Briscoe et al., 2006). Lemmatization reduces the data sparseness and has been shown to be effective in text classification tasks (Joachims, 1998). We then apply a simple word filter based on POS tags to select content words (nouns, verbs, adjectives, and adverbs). In particular, previous work has identified adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). Following previous work in cross-domain sentiment classification, we model a review as a bag of words. We select unigrams and bigrams from each sentence. For the remainder of this paper, we will refer to unigrams and bigrams collectively as lexical elements. Previous work on sentiment classification has shown that both unigrams and bigrams </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In ECML 1998, pages 137142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Lu</author>
<author>ChengXiang Zhai</author>
<author>Neel Sundaresan</author>
</authors>
<date>2009</date>
<contexts>
<context position="2287" citStr="Lu et al., 2009" startWordPosition="338" endWordPosition="341">eful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used to build sentiment classifiers for a specific domain (Pang et al., 2002). However, sentiment is expressed differently in different domains, and it is costly to annotate data for each new domain in which we would like to apply a sentiment classifier. For example, in the domain of reviews about electronics products, the words durable and light are used to express positive sentiment, whereas expensive and short battery life of</context>
</contexts>
<marker>Lu, Zhai, Sundaresan, 2009</marker>
<rawString>Yue Lu, ChengXiang Zhai, and Neel Sundaresan. 2009.</rawString>
</citation>
<citation valid="false">
<title>Rated aspect summarization of short comments.</title>
<booktitle>In WWW 2009,</booktitle>
<pages>131140</pages>
<marker></marker>
<rawString>Rated aspect summarization of short comments. In WWW 2009, pages 131140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
</authors>
<title>Feature selection, l1 vs. l2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In ICML</booktitle>
<contexts>
<context position="20814" citStr="Ng, 2004" startWordPosition="3454" endWordPosition="3455">ict positive and negative sentiment in reviews. We differentiate the appended base entries vr d from wi that existed in the original vector d (prior to expansion) by assigning different feature identifiers to the appended base entries. For example, a unigram excellent in a feature vector is differentiated from the base entry excellent by assigning the feature id, BASE=excellent to the latter. This enables us to learn different weights for base entries depending on whether they are useful for expanding a feature vector. We use L1 regularized logistic regression as the classification algorithm (Ng, 2004), which produces a sparse model in which most irrelevant features are assigned a zero weight. This enables us to select useful features for classification in a systematic way without having to preselect features using heuristic approaches. The regularization parameter is set to its default value of 1 for all the experiments described in this paper. 136 \x0c5 Experiments 5.1 Dataset To evaluate our method we use the cross-domain sentiment classification dataset prepared by Blitzer et al. (2007). This dataset consists of Amazon product reviews for four different product types: books (B), DVDs (D</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regularization, and rotational invariance. In ICML 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Xiaochuan Ni</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Cross-domain sentiment classification via spectral feature alignment.</title>
<date>2010</date>
<booktitle>In WWW</booktitle>
<contexts>
<context position="29048" citStr="Pan et al. (2010)" startWordPosition="4849" endWordPosition="4852">imply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. This can be considered to be a lower bound that does not perform domain adaptation. SCL is the structural correspondence learning technique of Blitzer et al. (2006). In SCL-MI, features are selected using the mutual information between a feature (unigram or bigram) and a domain label. After selecting salient features, the SCL algorithm is used to train a binary classifier. SFA is the spectral feature alignment technique of Pan et al. (2010). Both the LSA and FALSA techniques are based on latent semantic analysis (Pan et al., 2010). For the Within-Domain baseline, we train a binary classifier using the labeled data from the target domain. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for the target domain. Note that this is not a cross-domain classification setting. To evaluate the benefit of using sentiment features on our method, we give a NSS (non-sentiment sensitive) baseline in which we create a thesaurus without using any sentiment features. Proposed is ou</context>
<context position="32495" citStr="Pan et al. (2010)" startWordPosition="5404" endWordPosition="5407">urrences of those pivots. Finally, they use singular value decomposition (SVD) to construct a lowerdimensional feature space in which a binary classi139 \x0cfier is trained. The selection of pivots is vital to the performance of SCL and heuristically selected pivot features might not guarantee the best performance on target domains. In contrast, our method uses all features when creating the thesaurus and selects a subset of features during training using L1 regularization. Moreover, we do not require SVD, which has cubic time complexity so can be computationally expensive for large datasets. Pan et al. (2010) use structural feature alignment (SFA) to find an alignment between domain specific and domain independent features. The mutual information of a feature with domain labels is used to classify domain specific and domain independent features. Next, spectral clustering is performed on a bipartite graph that represents the relationship between the two sets of features. Finally, the top eigenvectors are selected to construct a lower-dimensional projection. However, not all words can be cleanly classified into domain specific or domain independent, and this process is conducted prior to training a </context>
</contexts>
<marker>Pan, Ni, Sun, Yang, Chen, 2010</marker>
<rawString>Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, and Zheng Chen. 2010. Cross-domain sentiment classification via spectral feature alignment. In WWW 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="2246" citStr="Pang and Lee, 2008" startWordPosition="331" endWordPosition="334">s, shopping sites, or review sites. It is useful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used to build sentiment classifiers for a specific domain (Pang et al., 2002). However, sentiment is expressed differently in different domains, and it is costly to annotate data for each new domain in which we would like to apply a sentiment classifier. For example, in the domain of reviews about electronics products, the words durable and light are used to express positive sentiment, wh</context>
<context position="31153" citStr="Pang and Lee, 2008" startWordPosition="5188" endWordPosition="5191">w that our method is statistically significantly better than both the No Thesaurus and NSS baselines, at confidence level 0.05. We therefore conclude that using the sentiment sensitive thesaurus for feature expansion is useful for cross-domain sentiment classification. The results returned by our method are comparable to state-ofthe-art techniques such as SCL-MI and SFA. In particular, the differences between those techniques and our method are not statistically significant. 6 Related Work Compared to single-domain sentiment classification, which has been studied extensively in previous work (Pang and Lee, 2008; Turney, 2002), crossdomain sentiment classification has only recently received attention in response to advances in the area of domain adaptation. Aue and Gammon (2005) report a number of empirical tests into domain adaptation of sentiment classifiers using an ensemble of classifiers. However, most of these tests were unable to outperform a simple baseline classifier that is trained using all labeled data for all domains. Blitzer et al. (2007) apply the structural correspondence learning (SCL) algorithm to train a crossdomain sentiment classifier. They first chooses a set of pivot features u</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In EMNLP 2002,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1861" citStr="Pang et al., 2002" startWordPosition="268" endWordPosition="271"> learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products. 1 Introduction Users express opinions about products or services they consume in blog posts, shopping sites, or review sites. It is useful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used t</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In EMNLP 2002, pages 79 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes. In</title>
<date>2004</date>
<booktitle>NAACLHLT04,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="13550" citStr="Pantel &amp; Ravichandran (2004)" startWordPosition="2159" endWordPosition="2162"> that have similar distributions are semantically similar. We compute f(u, w) as the pointwise mutual information between a lexical element u and a feature w as follows: f(u, w) = log c(u,w) N Pn i=1 c(i,w) N Pm j=1 c(u,j) N ! (1) Here, c(u, w) denotes the number of review sentences in which a lexical element u and a feature w co-occur, n and m respectively denote the total number of lexical elements and the total number of features, and N = Pn i=1 Pm j=1 c(i, j). Pointwise mutual information is known to be biased towards infrequent elements and features. We follow the discounting approach of Pantel &amp; Ravichandran (2004) to overcome this bias. Next, for two lexical elements u and v (represented by feature vectors u and v, respectively), we compute the relatedness (v, u) of the feature v to the feature u as follows, (v, u) = P w{x|f(v,x)&amp;gt;0} f(u, w) P w{x|f(u,x)&amp;gt;0} f(u, w) . (2) Here, we use the set notation {x|f(v, x) &amp;gt; 0} to denote the set of features that co-occur with v. Relatedness of a lexical element u to another lexical element v is the fraction of feature weights in the feature vector for the element u that also co-occur with the features in the feature vector for the element v. If there are no feature</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In NAACLHLT04, pages 321 328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>Alok Kirpal</author>
</authors>
<title>Efficient set joins on similarity predicates.</title>
<date>2004</date>
<booktitle>In SIGMOD 04,</booktitle>
<pages>743754</pages>
<contexts>
<context position="16395" citStr="Sarawagi and Kirpal, 2004" startWordPosition="2649" endWordPosition="2652">te sentiment elements from those (unlabeled) reviews, therefore we are not required to find expansion candidates for sentiment elements. However, we emphasize the fact that the relatedness values between the lexical elements listed in the sentiment-sensitive thesaurus are computed using co-occurrences with both lexical and sentiment features, and therefore the expansion candidates selected for the lexical elements in the target domain reviews are sensitive to sentiment labels assigned to reviews in the source domain. Using a sparse matrix format and approximate similarity matching techniques (Sarawagi and Kirpal, 2004), we can efficiently create a thesaurus from a large set of reviews. 4 Feature Expansion Our feature expansion phase augments a feature vector with additional related features selected from the 135 \x0csentiment-sensitive thesaurus created in Section 3 to overcome the feature mismatch problem. First, following the bag-of-words model, we model a review d using the set {w1, . . . , wN }, where the elements wi are either unigrams or bigrams that appear in the review d. We then represent a review d by a realvalued term-frequency vector d RN , where the value of the j-th element dj is set to the to</context>
</contexts>
<marker>Sarawagi, Kirpal, 2004</marker>
<rawString>Sunita Sarawagi and Alok Kirpal. 2004. Efficient set joins on similarity predicates. In SIGMOD 04, pages 743754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cDou Shen</author>
<author>Jianmin Wu</author>
<author>Bin Cao</author>
<author>Jian-Tao Sun</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
<author>Ying Li</author>
</authors>
<title>Exploiting term relationship to boost text classification.</title>
<date>2009</date>
<booktitle>In CIKM09,</booktitle>
<pages>1637--1640</pages>
<contexts>
<context position="5993" citStr="Shen et al., 2009" startWordPosition="928" endWordPosition="931">tely estimate the distribution of words in source and target domains. Our method can learn from a large amount of unlabeled data to leverage a robust cross-domain sentiment classifier. We model the cross-domain sentiment classification problem as one of feature expansion, where we append additional related features to feature vectors that represent source and target domain reviews in order to reduce the mismatch of features between the two domains. Methods that use related features have been successfully used in numerous tasks such as query expansion (Fang, 2008), and document classification (Shen et al., 2009). However, feature expansion techniques have not previously been applied to the task of cross-domain sentiment classification. In our method, we use the automatically created thesaurus to expand feature vectors in a binary classifier at train and test times by introducing related lexical elements from the thesaurus. We use L1 regularized logistic regression as the classification algorithm. (However, the method is agnostic to the properties of the classifier and can be used to expand feature vectors for any binary classifier). L1 regularization enables us to select a small subset of features fo</context>
</contexts>
<marker>Shen, Wu, Cao, Sun, Yang, Chen, Li, 2009</marker>
<rawString>\x0cDou Shen, Jianmin Wu, Bin Cao, Jian-Tao Sun, Qiang Yang, Zheng Chen, and Ying Li. 2009. Exploiting term relationship to boost text classification. In CIKM09, pages 1637 1640.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<pages>417424</pages>
<contexts>
<context position="1876" citStr="Turney, 2002" startWordPosition="272" endWordPosition="273">e source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products. 1 Introduction Users express opinions about products or services they consume in blog posts, shopping sites, or review sites. It is useful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used to build sentime</context>
<context position="31168" citStr="Turney, 2002" startWordPosition="5192" endWordPosition="5193"> statistically significantly better than both the No Thesaurus and NSS baselines, at confidence level 0.05. We therefore conclude that using the sentiment sensitive thesaurus for feature expansion is useful for cross-domain sentiment classification. The results returned by our method are comparable to state-ofthe-art techniques such as SCL-MI and SFA. In particular, the differences between those techniques and our method are not statistically significant. 6 Related Work Compared to single-domain sentiment classification, which has been studied extensively in previous work (Pang and Lee, 2008; Turney, 2002), crossdomain sentiment classification has only recently received attention in response to advances in the area of domain adaptation. Aue and Gammon (2005) report a number of empirical tests into domain adaptation of sentiment classifiers using an ensemble of classifiers. However, most of these tests were unable to outperform a simple baseline classifier that is trained using all labeled data for all domains. Blitzer et al. (2007) apply the structural correspondence learning (SCL) algorithm to train a crossdomain sentiment classifier. They first chooses a set of pivot features using pointwise </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In ACL 2002, pages 417424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce M Wiebe</author>
</authors>
<title>Learning subjective adjective from corpora.</title>
<date>2000</date>
<booktitle>In AAAI</booktitle>
<pages>735740</pages>
<contexts>
<context position="10855" citStr="Wiebe, 2000" startWordPosition="1696" endWordPosition="1697">expansion. Given a labeled or an unlabeled review, we first split the review into individual sentences. We carry out part-of-speech (POS) tagging and lemmatization on each review sentence using the RASP system (Briscoe et al., 2006). Lemmatization reduces the data sparseness and has been shown to be effective in text classification tasks (Joachims, 1998). We then apply a simple word filter based on POS tags to select content words (nouns, verbs, adjectives, and adverbs). In particular, previous work has identified adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). Following previous work in cross-domain sentiment classification, we model a review as a bag of words. We select unigrams and bigrams from each sentence. For the remainder of this paper, we will refer to unigrams and bigrams collectively as lexical elements. Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier (Blitzer et al., 2007). We note that it is possible to create lexical elements both from source domain labeled reviews as well as from unlabeled reviews in source and target domains. Next, we represent each le</context>
</contexts>
<marker>Wiebe, 2000</marker>
<rawString>Janyce M. Wiebe. 2000. Learning subjective adjective from corpora. In AAAI 2000, pages 735740.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>