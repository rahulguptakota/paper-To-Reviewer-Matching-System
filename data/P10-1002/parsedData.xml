<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.4361865">
b&apos;Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220,
Uppsala, Sweden, 11-16 July 2010. c
</bodyText>
<sectionHeader confidence="0.366123" genericHeader="abstract">
2010 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.795635">
Dependency Parsing and Projection Based on Word-Pair Classification
</title>
<author confidence="0.806419">
Wenbin Jiang and Qun Liu
</author>
<affiliation confidence="0.860881333333333">
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
</affiliation>
<address confidence="0.894953">
P.O. Box 2704, Beijing 100190, China
</address>
<email confidence="0.963302">
{jiangwenbin, liuqun}@ict.ac.cn
</email>
<sectionHeader confidence="0.98833" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.97787247368421">
In this paper we describe an intuitionistic
method for dependency parsing, where a
classifier is used to determine whether a
pair of words forms a dependency edge.
And we also propose an effective strategy
for dependency projection, where the de-
pendency relationships of the word pairs
in the source language are projected to the
word pairs of the target language, leading
to a set of classification instances rather
than a complete tree. Experiments show
that, the classifier trained on the projected
classification instances significantly out-
performs previous projected dependency
parsers. More importantly, when this clas-
sifier is integrated into a maximum span-
ning tree (MST) dependency parser, ob-
vious improvement is obtained over the
MST baseline.
</bodyText>
<sectionHeader confidence="0.99479" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999267533333334">
Supervised dependency parsing achieves the state-
of-the-art in recent years (McDonald et al., 2005a;
McDonald and Pereira, 2006; Nivre et al., 2006).
Since it is costly and difficult to build human-
annotated treebanks, a lot of works have also been
devoted to the utilization of unannotated text. For
example, the unsupervised dependency parsing
(Klein and Manning, 2004) which is totally based
on unannotated data, and the semisupervised de-
pendency parsing (Koo et al., 2008) which is
based on both annotated and unannotated data.
Considering the higher complexity and lower per-
formance in unsupervised parsing, and the need of
reliable priori knowledge in semisupervised pars-
ing, it is a promising strategy to project the de-
pendency structures from a resource-rich language
to a resource-scarce one across a bilingual corpus
(Hwa et al., 2002; Hwa et al., 2005; Ganchev et al.,
2009; Smith and Eisner, 2009; Jiang et al., 2009).
For dependency projection, the relationship be-
tween words in the parsed sentences can be sim-
ply projected across the word alignment to words
in the unparsed sentences, according to the DCA
assumption (Hwa et al., 2005). Such a projec-
tion procedure suffers much from the word align-
ment errors and syntactic isomerism between lan-
guages, which usually lead to relationship projec-
tion conflict and incomplete projected dependency
structures. To tackle this problem, Hwa et al.
(2005) use some filtering rules to reduce noise,
and some hand-designed rules to handle language
heterogeneity. Smith and Eisner (2009) perform
dependency projection and annotation adaptation
with quasi-synchronous grammar features. Jiang
and Liu (2009) resort to a dynamic programming
procedure to search for a completed projected tree.
However, these strategies are all confined to the
same category that dependency projection must
produce completed projected trees. Because of the
free translation, the syntactic isomerism between
languages and word alignment errors, it would
be strained to completely project the dependency
structure from one language to another.
We propose an effective method for depen-
dency projection, which does not have to pro-
duce complete projected trees. Given a word-
aligned bilingual corpus with source language sen-
tences parsed, the dependency relationships of the
word pairs in the source language are projected to
the word pairs of the target language. A depen-
dency relationship is a boolean value that repre-
sents whether this word pair forms a dependency
edge. Thus a set of classification instances are ob-
tained. Meanwhile, we propose an intuitionistic
model for dependency parsing, which uses a clas-
sifier to determine whether a pair of words form
a dependency edge. The classifier can then be
trained on the projected classification instance set,
so as to build a projected dependency parser with-
out the need of complete projected trees.
</bodyText>
<page confidence="0.932187">
12
</page>
<equation confidence="0.687205666666667">
\x0ci
j j
i
</equation>
<figureCaption confidence="0.994527">
Figure 1: Illegal (a) and incomplete (b) dependency tree produced by the simple-collection method.
</figureCaption>
<bodyText confidence="0.999736730769231">
Experimental results show that, the classifier
trained on the projected classification instances
significantly outperforms the projected depen-
dency parsers in previous works. The classifier
trained on the Chinese projected classification in-
stances achieves a precision of 58.59% on the CTB
standard test set. More importantly, when this
classifier is integrated into a 2nd-ordered max-
imum spanning tree (MST) dependency parser
(McDonald and Pereira, 2006) in a weighted aver-
age manner, significant improvement is obtained
over the MST baselines. For the 2nd-order MST
parser trained on Penn Chinese Treebank (CTB)
5.0, the classifier give an precision increment of
0.5 points. Especially for the parser trained on the
smaller CTB 1.0, more than 1 points precision in-
crement is obtained.
In the rest of this paper, we first describe
the word-pair classification model for dependency
parsing (section 2) and the generation method
of projected classification instances (section 3).
Then we describe an application of the projected
parser: boosting a state-of-the-art 2nd-ordered
MST parser (section 4). After the comparisons
with previous works on dependency parsing and
projection, we finally five the experimental results.
</bodyText>
<sectionHeader confidence="0.992205" genericHeader="method">
2 Word-Pair Classification Model
</sectionHeader>
<subsectionHeader confidence="0.829216">
2.1 Model Definition
</subsectionHeader>
<bodyText confidence="0.978215487804878">
Following (McDonald et al., 2005a), x is used to
denote the sentence to be parsed, and xi to denote
the i-th word in the sentence. y denotes the de-
pendency tree for sentence x, and (i, j) y rep-
resents a dependency edge from word xi to word
xj, where xi is the parent of xj.
The task of the word-pair classification model
is to determine whether any candidate word pair,
xi and xj s.t. 1 i, j |x |and i 6= j, forms a
dependency edge. The classification result C(i, j)
can be a boolean value:
C(i, j) = p p {0, 1} (1)
as produced by a support vector machine (SVM)
classifier (Vapnik, 1998). p = 1 indicates that the
classifier supports the candidate edge (i, j), and
p = 0 the contrary. C(i, j) can also be a real-
valued probability:
C(i, j) = p 0 p 1 (2)
as produced by an maximum entropy (ME) classi-
fier (Berger et al., 1996). p is a probability which
indicates the degree the classifier support the can-
didate edge (i, j). Ideally, given the classifica-
tion results for all candidate word pairs, the depen-
dency parse tree can be composed of the candidate
edges with higher score (1 for the boolean-valued
classifier, and large p for the real-valued classi-
fier). However, more robust strategies should be
investigated since the ambiguity of the language
syntax and the classification errors usually lead to
illegal or incomplete parsing result, as shown in
Figure 1.
Follow the edge based factorization method
(Eisner, 1996), we factorize the score of a de-
pendency tree s(x, y) into its dependency edges,
and design a dynamic programming algorithm
to search for the candidate parse with maximum
score. This strategy alleviate the classification er-
rors to some degree and ensure a valid, complete
dependency parsing tree. If a boolean-valued clas-
sifier is used, the search algorithm can be formal-
ized as:
</bodyText>
<equation confidence="0.995198444444444">
y = argmax
y
s(x, y)
= argmax
y
X
(i,j)y
C(i, j)
(3)
</equation>
<bodyText confidence="0.985327">
And if a probability-valued classifier is used in-
stead, we replace the accumulation with cumula-
</bodyText>
<page confidence="0.999707">
13
</page>
<table confidence="0.975181083333333">
\x0cType Features
Unigram wordi posi wordi posi
wordj posj wordj posj
Bigram wordi posi wordj posj posi wordj posj wordi wordj posj
wordi posi posj wordi posi wordj wordi wordj
posi posj wordi posj posi wordj
Surrounding posi posi+1 posj1 posj posi1 posi posj1 posj posi posi+1 posj posj+1
posi1 posi posj posj+1 posi1 posi posj1 posi1 posi posj+1
posi posi+1 posj1 posi posi+1 posj+1 posi1 posj1 posj
posi1 posj posj+1 posi+1 posj1 posj posi+1 posj posj+1
posi posj1 posj posi posj posj+1 posi1 posi posj
posi posi+1 posj
</table>
<tableCaption confidence="0.8632815">
Table 1: Feature templates for the word-pair classification model.
tive product:
</tableCaption>
<equation confidence="0.997066555555556">
y = argmax
y
s(x, y)
= argmax
y
Y
(i,j)y
C(i, j)
(4)
</equation>
<bodyText confidence="0.963973466666667">
Where y is searched from the set of well-formed
dependency trees.
In our work we choose a real-valued ME clas-
sifier. Here we give the calculation of dependency
probability C(i, j). We use w to denote the param-
eter vector of the ME model, and f(i, j, r) to de-
note the feature vector for the assumption that the
word pair i and j has a dependency relationship r.
The symbol r indicates the supposed classification
result, where r = + means we suppose it as a de-
pendency edge and r = means the contrary. A
feature fk(i, j, r) f(i, j, r) equals 1 if it is ac-
tivated by the assumption and equals 0 otherwise.
The dependency probability can then be defined
as:
</bodyText>
<equation confidence="0.990086">
C(i, j) =
exp(w f(i, j, +))
P
r exp(w f(i, j, r))
=
exp(
P
k wk fk(i, j, +))
P
r exp(
P
k wk fk(i, j, r))
(5)
</equation>
<subsectionHeader confidence="0.724063">
2.2 Features for Classification
</subsectionHeader>
<bodyText confidence="0.999190444444444">
The feature templates for the classifier are simi-
lar to those of 1st-ordered MST model (McDon-
ald et al., 2005a). 1 Each feature is composed
of some words and POS tags surrounded word i
and/or word j, as well as an optional distance rep-
resentations between this two words. Table shows
the feature templates we use.
Previous graph-based dependency models usu-
ally use the index distance of word i and word j
</bodyText>
<page confidence="0.893178">
1
</page>
<bodyText confidence="0.998595708333333">
We exclude the in between features of McDonald et al.
(2005a) since preliminary experiments show that these fea-
tures bring no improvement to the word-pair classification
model.
to enrich the features with word distance infor-
mation. However, in order to utilize some syntax
information between the pair of words, we adopt
the syntactic distance representation of (Collins,
1996), named Collins distance for convenience. A
Collins distance comprises the answers of 6 ques-
tions:
Does word i precede or follow word j?
Are word i and word j adjacent?
Is there a verb between word i and word j?
Are there 0, 1, 2 or more than 2 commas be-
tween word i and word j?
Is there a comma immediately following the
first of word i and word j?
Is there a comma immediately preceding the
second of word i and word j?
Besides the original features generated according
to the templates in Table 1, the enhanced features
with Collins distance as postfixes are also used in
training and decoding of the word-pair classifier.
</bodyText>
<subsectionHeader confidence="0.999363">
2.3 Parsing Algorithm
</subsectionHeader>
<bodyText confidence="0.992531">
We adopt logarithmic dependency probabilities
in decoding, therefore the cumulative product of
probabilities in formula 6 can be replaced by ac-
cumulation of logarithmic probabilities:
</bodyText>
<equation confidence="0.998263">
y = argmax
y
s(x, y)
= argmax
y
Y
(i,j)y
C(i, j)
= argmax
y
X
(i,j)y
log(C(i, j))
(6)
</equation>
<bodyText confidence="0.999379">
Thus, the decoding algorithm for 1st-ordered MST
model, such as the Chu-Liu-Edmonds algorithm
</bodyText>
<page confidence="0.998495">
14
</page>
<listItem confidence="0.935530642857143">
\x0cAlgorithm 1 Dependency Parsing Algorithm.
1: Input: sentence x to be parsed
2: for hi, ji h1, |x|i in topological order do
3: buf
4: for k i..j 1 do all partitions
5: for l V[i, k] and r V[k + 1, j] do
6: insert DERIV(l, r) into buf
7: insert DERIV(r, l) into buf
8: V[i, j] top K derivations of buf
9: Output: the best derivation of V[1, |x|]
10: function DERIV(p, c)
11: d p c {(p root, c root)} new derivation
12: d evl EVAL(d) evaluation function
13: return d
</listItem>
<bodyText confidence="0.970781818181818">
used in McDonald et al. (2005b), is also appli-
cable here. In this work, however, we still adopt
the more general, bottom-up dynamic program-
ming algorithm Algorithm 1 in order to facilitate
the possible expansions. Here, V[i, j] contains the
candidate parsing segments of the span [i, j], and
the function EVAL(d) accumulates the scores of
all the edges in dependency segment d. In prac-
tice, the cube-pruning strategy (Huang and Chi-
ang, 2005) is used to speed up the enumeration of
derivations (loops started by line 4 and 5).
</bodyText>
<sectionHeader confidence="0.968896" genericHeader="method">
3 Projected Classification Instance
</sectionHeader>
<bodyText confidence="0.990629647058824">
After the introduction of the word-pair classifica-
tion model, we now describe the extraction of pro-
jected dependency instances. In order to allevi-
ate the effect of word alignment errors, we base
the projection on the alignment matrix, a compact
representation of multiple GIZA++ (Och and Ney,
2000) results, rather than a single word alignment
in previous dependency projection works. Figure
2 shows an example.
Suppose a bilingual sentence pair, composed of
a source sentence e and its target translation f. ye
is the parse tree of the source sentence. A is the
alignment matrix between them, and each element
Ai,j denotes the degree of the alignment between
word ei and word fj. We define a boolean-valued
function (y, i, j, r) to investigate the dependency
relationship of word i and word j in parse tree y:
</bodyText>
<equation confidence="0.979514875">
(y, i, j, r) =
1
(i, j) y and r = +
or
(i, j) /
y and r =
0 otherwise
(7)
</equation>
<bodyText confidence="0.564109">
Then the score that word i and word j in the target
sentence y forms a projected dependency edge,
</bodyText>
<figureCaption confidence="0.8790134">
Figure 2: The word alignment matrix between a
Chinese sentence and its English translation. Note
that probabilities need not to be normalized across
rows or columns.
s+(i, j), can be defined as:
</figureCaption>
<equation confidence="0.963198833333333">
s+(i, j) =
X
i,j
Ai,i Aj,j (ye, i
, j
, +) (8)
</equation>
<bodyText confidence="0.9960015">
The score that they do not form a projected depen-
dency edge can be defined similarly:
</bodyText>
<equation confidence="0.975401166666667">
s(i, j) =
X
i,j
Ai,i Aj,j (ye, i
, j
, ) (9)
</equation>
<bodyText confidence="0.98137025">
Note that for simplicity, the condition factors ye
and A are omitted from these two formulas. We
finally define the probability of the supposed pro-
jected dependency edge as:
</bodyText>
<equation confidence="0.94924325">
Cp(i, j) =
exp(s+(i, j))
exp(s+(i, j)) + exp(s(i, j))
(10)
</equation>
<bodyText confidence="0.984032181818182">
The probability Cp(i, j) is a real value between
0 and 1. Obviously, Cp(i, j) = 0.5 indicates the
most ambiguous case, where we can not distin-
guish between positive and negative at all. On the
other hand, there are as many as 2|f|(|f|1) candi-
date projected dependency instances for the target
sentence f. Therefore, we need choose a threshold
b for Cp(i, j) to filter out the ambiguous instances:
the instances with Cp(i, j) &gt; b are selected as the
positive, and the instances with Cp(i, j) &lt; 1 b
are selected as the negative.
</bodyText>
<sectionHeader confidence="0.91393" genericHeader="method">
4 Boosting an MST Parser
</sectionHeader>
<bodyText confidence="0.995694833333333">
The classifier can be used to boost a existing parser
trained on human-annotated trees. We first estab-
lish a unified framework for the enhanced parser.
For a sentence to be parsed, x, the enhanced parser
selects the best parse y according to both the base-
line model B and the projected classifier C.
</bodyText>
<equation confidence="0.999772">
y = argmax
y
[sB(x, y) + sC(x, y)] (11)
</equation>
<page confidence="0.83632">
15
</page>
<bodyText confidence="0.998918884615385">
\x0cHere, sB and sC denote the evaluation functions
of the baseline model and the projected classi-
fier, respectively. The parameter is the relative
weight of the projected classifier against the base-
line model.
There are several strategies to integrate the two
evaluation functions. For example, they can be in-
tegrated deeply at each decoding step (Carreras et
al., 2008; Zhang and Clark, 2008; Huang, 2008),
or can be integrated shallowly in a reranking man-
ner (Collins, 2000; Charniak and Johnson, 2005).
As described previously, the score of a depen-
dency tree given by a word-pair classifier can be
factored into each candidate dependency edge in
this tree. Therefore, the projected classifier can
be integrated with a baseline model deeply at each
dependency edge, if the evaluation score given by
the baseline model can also be factored into de-
pendency edges.
We choose the 2nd-ordered MST model (Mc-
Donald and Pereira, 2006) as the baseline. Es-
pecially, the effect of the Collins distance in the
baseline model is also investigated. The relative
weight is adjusted to maximize the performance
on the development set, using an algorithm similar
to minimum error-rate training (Och, 2003).
</bodyText>
<sectionHeader confidence="0.9981" genericHeader="method">
5 Related Works
</sectionHeader>
<subsectionHeader confidence="0.951808">
5.1 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.99921315">
Both the graph-based (McDonald et al., 2005a;
McDonald and Pereira, 2006; Carreras et al.,
2006) and the transition-based (Yamada and Mat-
sumoto, 2003; Nivre et al., 2006) parsing algo-
rithms are related to our word-pair classification
model.
Similar to the graph-based method, our model
is factored on dependency edges, and its decod-
ing procedure also aims to find a maximum span-
ning tree in a fully connected directed graph. From
this point, our model can be classified into the
graph-based category. On the training method,
however, our model obviously differs from other
graph-based models, that we only need a set of
word-pair dependency instances rather than a reg-
ular dependency treebank. Therefore, our model is
more suitable for the partially bracketed or noisy
training corpus.
The most apparent similarity between our
model and the transition-based category is that
they all need a classifier to perform classification
conditioned on a certain configuration. However,
they differ from each other in the classification re-
sults. The classifier in our model predicates a de-
pendency probability for each pair of words, while
the classifier in a transition-based model gives a
possible next transition operation such as shift or
reduce. Another difference lies in the factoriza-
tion strategy. For our method, the evaluation score
of a candidate parse is factorized into each depen-
dency edge, while for the transition-based models,
the score is factorized into each transition opera-
tion.
Thanks to the reminding of the third reviewer
of our paper, we find that the pairwise classifica-
tion schema has also been used in Japanese de-
pendency parsing (Uchimoto et al., 1999; Kudo
and Matsumoto, 2000). However, our work shows
more advantage in feature engineering, model
training and decoding algorithm.
</bodyText>
<subsectionHeader confidence="0.998112">
5.2 Dependency Projection
</subsectionHeader>
<bodyText confidence="0.9973133125">
Many works try to learn parsing knowledge from
bilingual corpora. Lu et al. (2002) aims to
obtain Chinese bracketing knowledge via ITG
(Wu, 1997) alignment. Hwa et al. (2005) and
Ganchev et al. (2009) induce dependency gram-
mar via projection from aligned bilingual cor-
pora, and use some thresholds to filter out noise
and some hand-written rules to handle heterogene-
ity. Smith and Eisner (2009) perform depen-
dency projection and annotation adaptation with
Quasi-Synchronous Grammar features. Jiang and
Liu (2009) refer to alignment matrix and a dy-
namic programming search algorithm to obtain
better projected dependency trees.
All previous works for dependency projection
(Hwa et al., 2005; Ganchev et al., 2009; Smith and
Eisner, 2009; Jiang and Liu, 2009) need complete
projected trees to train the projected parsers. Be-
cause of the free translation, the word alignment
errors, and the heterogeneity between two lan-
guages, it is reluctant and less effective to project
the dependency tree completely to the target lan-
guage sentence. On the contrary, our dependency
projection strategy prefer to extract a set of depen-
dency instances, which coincides our models de-
mand for training corpus. An obvious advantage
of this strategy is that, we can select an appropriate
filtering threshold to obtain dependency instances
of good quality.
In addition, our word-pair classification model
can be integrated deeply into a state-of-the-art
MST dependency model. Since both of them are
</bodyText>
<page confidence="0.993298">
16
</page>
<table confidence="0.99219">
\x0cCorpus Train Dev Test
WSJ (section) 2-21 22 23
CTB 5.0 (chapter) others 301-325 271-300
</table>
<tableCaption confidence="0.9978">
Table 2: The corpus partition for WSJ and CTB
</tableCaption>
<bodyText confidence="0.929802625">
5.0.
factorized into dependency edges, the integration
can be conducted at each dependency edge, by
weightedly averaging their evaluation scores for
this dependency edge. This strategy makes better
use of the projected parser while with faster de-
coding, compared with the cascaded approach of
Jiang and Liu (2009).
</bodyText>
<sectionHeader confidence="0.998597" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999851416666667">
In this section, we first validate the word-pair
classification model by experimenting on human-
annotated treebanks. Then we investigate the ef-
fectiveness of the dependency projection by eval-
uating the projected classifiers trained on the pro-
jected classification instances. Finally, we re-
port the performance of the integrated dependency
parser which integrates the projected classifier and
the 2nd-ordered MST dependency parser. We
evaluate the parsing accuracy by the precision of
lexical heads, which is the percentage of the words
that have found their correct parents.
</bodyText>
<subsectionHeader confidence="0.997696">
6.1 Word-Pair Classification Model
</subsectionHeader>
<bodyText confidence="0.996178136363637">
We experiment on two popular treebanks, the Wall
Street Journal (WSJ) portion of the Penn English
Treebank (Marcus et al., 1993), and the Penn Chi-
nese Treebank (CTB) 5.0 (Xue et al., 2005). The
constituent trees in the two treebanks are trans-
formed to dependency trees according to the head-
finding rules of Yamada and Matsumoto (2003).
For English, we use the automatically-assigned
POS tags produced by an implementation of the
POS tagger of Collins (2002). While for Chinese,
we just use the gold-standard POS tags following
the tradition. Each treebank is splitted into three
partitions, for training, development and testing,
respectively, as shown in Table 2.
For a dependency tree with n words, only n
1 positive dependency instances can be extracted.
They account for only a small proportion of all the
dependency instances. As we know, it is important
to balance the proportions of the positive and the
negative instances for a batched-trained classifier.
We define a new parameter r to denote the ratio of
the negative instances relative to the positive ones.
</bodyText>
<page confidence="0.743742">
84
</page>
<figure confidence="0.983464923076923">
84.5
85
85.5
86
86.5
87
1 1.5 2 2.5 3
Dependency
Precision
(%)
Ratio r (#negative/#positive)
WSJ
CTB 5.0
</figure>
<figureCaption confidence="0.977923">
Figure 3: Performance curves of the word-pair
classification model on the development sets of
</figureCaption>
<table confidence="0.957704">
WSJ and CTB 5.0, with respect to a series of ratio
r.
Corpus System P %
WSJ Yamada and Matsumoto (2003) 90.3
Nivre and Scholz (2004) 87.3
1st-ordered MST 90.7
2nd-ordered MST 91.5
our model 86.8
CTB 5.0 1st-ordered MST 86.53
2nd-ordered MST 87.15
our model 82.06
</table>
<tableCaption confidence="0.999549">
Table 3: Performance of the word-pair classifica-
</tableCaption>
<bodyText confidence="0.99206552173913">
tion model on WSJ and CTB 5.0, compared with
the current state-of-the-art models.
For example, r = 2 means we reserve negative
instances two times as many as the positive ones.
The MaxEnt toolkit by Zhang 2 is adopted to
train the ME classifier on extracted instances. We
set the gaussian prior as 1.0 and the iteration limit
as 100, leaving other parameters as default values.
We first investigate the impact of the ratio r on
the performance of the classifier. Curves in Fig-
ure 3 show the performance of the English and
Chinese parsers, each of which is trained on an in-
stance set corresponding to a certain r. We find
that for both English and Chinese, maximum per-
formance is achieved at about r = 2.5. 3 The
English and Chinese classifiers trained on the in-
stance sets with r = 2.5 are used in the final eval-
uation phase. Table 3 shows the performances on
the test sets of WSJ and CTB 5.0.
We also compare them with previous works on
the same test sets. On both English and Chinese,
the word-pair classification model falls behind of
the state-of-the-art. We think that it is probably
</bodyText>
<page confidence="0.580147">
2
</page>
<footnote confidence="0.9692175">
http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
</footnote>
<page confidence="0.982697">
3
</page>
<bodyText confidence="0.997480333333333">
We did not investigate more fine-grained ratios, since the
performance curves show no dramatic fluctuation along with
the alteration of r.
</bodyText>
<page confidence="0.949239">
17
</page>
<figure confidence="0.9876552">
\x0c54
54.5
55
55.5
56
0.65 0.7 0.75 0.8 0.85 0.9 0.95
Dependency
Precision
(%)
Threshold b
</figure>
<figureCaption confidence="0.999952">
Figure 4: The performance curve of the word-
</figureCaption>
<bodyText confidence="0.996777384615385">
pair classification model on the development set
of CTB 5.0, with respect to a series of threshold b.
due to the local optimization of the training pro-
cedure. Given complete trees as training data, it
is easy for previous models to utilize structural,
global and linguistical information in order to ob-
tain more powerful parameters. The main advan-
tage of our model is that it doesnt need complete
trees to tune its parameters. Therefore, if trained
on instances extracted from human-annotated tree-
banks, the word-pair classification model would
not demonstrate its advantage over existed state-
of-the-art dependency parsing methods.
</bodyText>
<subsectionHeader confidence="0.997817">
6.2 Dependency Projection
</subsectionHeader>
<bodyText confidence="0.998855708333333">
In this work we focus on the dependency projec-
tion from English to Chinese. We use the FBIS
Chinese-English bitext as the bilingual corpus for
dependency projection. It contains 239K sen-
tence pairs with about 6.9M/8.9M words in Chi-
nese/English. Both English and Chinese sentences
are tagged by the implementations of the POS tag-
ger of Collins (2002), which trained on WSJ and
CTB 5.0 respectively. The English sentences are
then parsed by an implementation of 2nd-ordered
MST model of McDonald and Pereira (2006),
which is trained on dependency trees extracted
from WSJ. The alignment matrixes for sentence
pairs are generated according to (Liu et al., 2009).
Similar to the ratio r, the threshold b need also
be assigned an appropriate value to achieve a bet-
ter performance. Larger thresholds result in better
but less classification instances, the lower cover-
age of the instances would hurt the performance of
the classifier. On the other hand, smaller thresh-
olds lead to worse but more instances, and too
much noisy instances will bring down the classi-
fiers discriminating power.
We extract a series of classification instance sets
</bodyText>
<table confidence="0.972008">
Corpus System P %
CTB 2.0 Hwa et al. (2005) 53.9
our model 56.9
CTB 5.0 Jiang and Liu (2009) 53.28
our model 58.59
</table>
<tableCaption confidence="0.999724">
Table 4: The performance of the projected classi-
</tableCaption>
<bodyText confidence="0.771422">
fier on the test sets of CTB 2.0 and CTB 5.0, com-
pared with the performance of previous works on
the corresponding test sets.
</bodyText>
<table confidence="0.987451666666667">
Corpus Baseline P% Integrated P%
CTB 1.0 82.23 83.70
CTB 5.0 87.15 87.65
</table>
<tableCaption confidence="0.997783">
Table 5: Performance improvement brought by
</tableCaption>
<bodyText confidence="0.999395045454545">
the projected classifier to the baseline 2nd-ordered
MST parsers trained on CTB 1.0 and CTB 5.0, re-
spectively.
with different thresholds. Then, on each instance
set we train a classifier and test it on the develop-
ment set of CTB 5.0. Figure 4 presents the ex-
perimental results. The curve shows that the max-
imum performance is achieved at the threshold of
about 0.85. The classifier corresponding to this
threshold is evaluated on the test set of CTB 5.0,
and the test set of CTB 2.0 determined by Hwa et
al. (2005). Table 4 shows the performance of the
projected classifier, as well as the performance of
previous works on the corresponding test sets. The
projected classifier significantly outperforms pre-
vious works on both test sets, which demonstrates
that the word-pair classification model, although
falling behind of the state-of-the-art on human-
annotated treebanks, performs well in projected
dependency parsing. We give the credit to its good
collaboration with the word-pair classification in-
stance extraction for dependency projection.
</bodyText>
<subsectionHeader confidence="0.977212">
6.3 Integrated Dependency Parser
</subsectionHeader>
<bodyText confidence="0.999353307692308">
We integrate the word-pair classification model
into the state-of-the-art 2nd-ordered MST model.
First, we implement a chart-based dynamic pro-
gramming parser for the 2nd-ordered MST model,
and develop a training procedure based on the
perceptron algorithm with averaged parameters
(Collins, 2002). On the WSJ corpus, this parser
achieves the same performance as that of McDon-
ald and Pereira (2006). Then, at each derivation
step of this 2nd-ordered MST parser, we weight-
edly add the evaluation score given by the pro-
jected classifier to the original MST evaluation
score. Such a weighted summation of two eval-
</bodyText>
<page confidence="0.987641">
18
</page>
<bodyText confidence="0.999557166666667">
\x0cuation scores provides better evaluation for can-
didate parses. The weight parameter is tuned
by a minimum error-rate training algorithm (Och,
2003).
Given a 2nd-ordered MST parser trained on
CTB 5.0 as the baseline, the projected classi-
fier brings an accuracy improvement of about 0.5
points. For the baseline trained on the smaller
CTB 1.0, whose training set is chapters 1-270 of
CTB 5.0, the accuracy improvement is much sig-
nificant, about 1.5 points over the baseline. It
indicates that, the smaller the human-annotated
treebank we have, the more significant improve-
ment we can achieve by integrating the project-
ing classifier. This provides a promising strategy
for boosting the parsing performance of resource-
scarce languages. Table 5 summarizes the experi-
mental results.
</bodyText>
<sectionHeader confidence="0.984657" genericHeader="conclusions">
7 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999684227272727">
In this paper, we first describe an intuitionis-
tic method for dependency parsing, which re-
sorts to a classifier to determine whether a word
pair forms a dependency edge, and then propose
an effective strategy for dependency projection,
which produces a set of projected classification in-
stances rather than complete projected trees. Al-
though this parsing method falls behind of pre-
vious models, it can collaborate well with the
word-pair classification instance extraction strat-
egy for dependency projection, and achieves the
state-of-the-art in projected dependency parsing.
In addition, when integrated into a 2nd-ordered
MST parser, the projected parser brings signifi-
cant improvement to the baseline, especially for
the baseline trained on smaller treebanks. This
provides a new strategy for resource-scarce lan-
guages to train high-precision dependency parsers.
However, considering its lower performance on
human-annotated treebanks, the dependency pars-
ing method itself still need a lot of investigations,
especially on the training method of the classifier.
</bodyText>
<sectionHeader confidence="0.759894" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9965921">
This project was supported by National Natural
Science Foundation of China, Contract 60736014,
and 863 State Key Project No. 2006AA010108.
We are grateful to the anonymous reviewers for
their thorough reviewing and valuable sugges-
tions. We show special thanks to Dr. Rebecca
Hwa for generous help of sharing the experimen-
tal data. We also thank Dr. Yang Liu for sharing
the codes of alignment matrix generation, and Dr.
Liang Huang for helpful discussions.
</bodyText>
<sectionHeader confidence="0.984761" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992964022222222">
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics.
Xavier Carreras, Mihai Surdeanu, and Lluis Marquez.
2006. Projective dependency parsing with percep-
tron. In Proceedings of the CoNLL.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. Tag, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the CoNLL.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine-grained n-best parsing and discriminative
reranking. In Proceedings of the ACL.
Michael Collins. 1996. A new statistical parser based
on bigram lexical dependencies. In Proceedings of
ACL.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proceedings of the
ICML, pages 175182.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the EMNLP, pages 18, Philadelphia, USA.
Jason M. Eisner. 1996. Three new probabilistic mod-
els for dependency parsing: An exploration. In Pro-
ceedings of COLING, pages 340345.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In Proceedings of the
47th ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the IWPT, pages 5364.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the ACL.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
In Natural Language Engineering, volume 11, pages
311325.
</reference>
<page confidence="0.978432">
19
</page>
<reference confidence="0.995866947368421">
\x0cWenbin Jiang and Qun Liu. 2009. Automatic adapta-
tion of annotation standards for dependency parsing
using projected treebank as source corpus. In Pro-
ceedings of IWPT.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic adaptation of annotation standards: Chinese
word segmentation and pos tagginga case study. In
Proceedings of the 47th ACL.
Dan Klein and Christopher D. Manning. 2004. Cor-
pusbased induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
ACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of the ACL.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proceedings of the EMNLP.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the EMNLP.
Yajuan Lu, Sheng Li, Tiejun Zhao, and Muyun Yang.
2002. Learning chinese bracketing knowledge
based on a bilingual language model. In Proceed-
ings of the COLING.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. In Computa-
tional Linguistics.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL, pages 8188.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of ACL, pages 91
98.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP.
J. Nivre and M. Scholz. 2004. Deterministic depen-
dency parsing of english text. In Proceedings of the
COLING.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen
Eryigit, and Svetoslav Marinov. 2006. Labeled
pseudoprojective dependency parsing with support
vector machines. In Proceedings of CoNLL, pages
221225.
Franz J. Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
ACL.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the ACL, pages 160167.
David Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analysis
based on maximum entropy models. In Proceedings
of the EACL.
Vladimir N. Vapnik. 1998. Statistical learning theory.
In A Wiley-Interscience Publication.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The penn chinese treebank: Phrase
structure annotation of a large corpus. In Natural
Language Engineering.
H Yamada and Y Matsumoto. 2003. Statistical depen-
dency analysis using support vector machines. In
Proceedings of IWPT.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of the ACL.
</reference>
<figure confidence="0.4636905">
20
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608284">
<note confidence="0.934964666666667">b&apos;Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220, Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics</note>
<title confidence="0.964131">Dependency Parsing and Projection Based on Word-Pair Classification</title>
<author confidence="0.964007">Wenbin Jiang</author>
<author confidence="0.964007">Qun Liu</author>
<affiliation confidence="0.951413333333333">Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences</affiliation>
<address confidence="0.989125">P.O. Box 2704, Beijing 100190, China</address>
<email confidence="0.989853">jiangwenbin@ict.ac.cn</email>
<email confidence="0.989853">liuqun@ict.ac.cn</email>
<abstract confidence="0.9934167">In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing. Computational Linguistics.</title>
<date>1996</date>
<contexts>
<context position="6332" citStr="Berger et al., 1996" startWordPosition="996" endWordPosition="999">ependency edge from word xi to word xj, where xi is the parent of xj. The task of the word-pair classification model is to determine whether any candidate word pair, xi and xj s.t. 1 i, j |x |and i 6= j, forms a dependency edge. The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier (Vapnik, 1998). p = 1 indicates that the classifier supports the candidate edge (i, j), and p = 0 the contrary. C(i, j) can also be a realvalued probability: C(i, j) = p 0 p 1 (2) as produced by an maximum entropy (ME) classifier (Berger et al., 1996). p is a probability which indicates the degree the classifier support the candidate edge (i, j). Ideally, given the classification results for all candidate word pairs, the dependency parse tree can be composed of the candidate edges with higher score (1 for the boolean-valued classifier, and large p for the real-valued classifier). However, more robust strategies should be investigated since the ambiguity of the language syntax and the classification errors usually lead to illegal or incomplete parsing result, as shown in Figure 1. Follow the edge based factorization method (Eisner, 1996), w</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Mihai Surdeanu</author>
<author>Lluis Marquez</author>
</authors>
<title>Projective dependency parsing with perceptron.</title>
<date>2006</date>
<booktitle>In Proceedings of the CoNLL.</booktitle>
<contexts>
<context position="15555" citStr="Carreras et al., 2006" startWordPosition="2632" endWordPosition="2635">tegrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously differs from other graph-based models, that we only need a set of word-pair dependency instances rather than a regular dependency treebank. Th</context>
</contexts>
<marker>Carreras, Surdeanu, Marquez, 2006</marker>
<rawString>Xavier Carreras, Mihai Surdeanu, and Lluis Marquez. 2006. Projective dependency parsing with perceptron. In Proceedings of the CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the CoNLL.</booktitle>
<contexts>
<context position="14597" citStr="Carreras et al., 2008" startWordPosition="2478" endWordPosition="2481">irst establish a unified framework for the enhanced parser. For a sentence to be parsed, x, the enhanced parser selects the best parse y according to both the baseline model B and the projected classifier C. y = argmax y [sB(x, y) + sC(x, y)] (11) 15 \x0cHere, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect </context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. Tag, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine-grained n-best parsing and discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="14732" citStr="Charniak and Johnson, 2005" startWordPosition="2500" endWordPosition="2503">parse y according to both the baseline model B and the projected classifier C. y = argmax y [sB(x, y) + sC(x, y)] (11) 15 \x0cHere, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine-grained n-best parsing and discriminative reranking. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="9698" citStr="Collins, 1996" startWordPosition="1592" endWordPosition="1593">rounded word i and/or word j, as well as an optional distance representations between this two words. Table shows the feature templates we use. Previous graph-based dependency models usually use the index distance of word i and word j 1 We exclude the in between features of McDonald et al. (2005a) since preliminary experiments show that these features bring no improvement to the word-pair classification model. to enrich the features with word distance information. However, in order to utilize some syntax information between the pair of words, we adopt the syntactic distance representation of (Collins, 1996), named Collins distance for convenience. A Collins distance comprises the answers of 6 questions: Does word i precede or follow word j? Are word i and word j adjacent? Is there a verb between word i and word j? Are there 0, 1, 2 or more than 2 commas between word i and word j? Is there a comma immediately following the first of word i and word j? Is there a comma immediately preceding the second of word i and word j? Besides the original features generated according to the templates in Table 1, the enhanced features with Collins distance as postfixes are also used in training and decoding of </context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the ICML,</booktitle>
<pages>175182</pages>
<contexts>
<context position="14703" citStr="Collins, 2000" startWordPosition="2498" endWordPosition="2499">lects the best parse y according to both the baseline model B and the projected classifier C. y = argmax y [sB(x, y) + sC(x, y)] (11) 15 \x0cHere, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to max</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of the ICML, pages 175182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the EMNLP,</booktitle>
<pages>18</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="20299" citStr="Collins (2002)" startWordPosition="3377" endWordPosition="3378">g accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents. 6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993), and the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of Yamada and Matsumoto (2003). For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002). While for Chinese, we just use the gold-standard POS tags following the tradition. Each treebank is splitted into three partitions, for training, development and testing, respectively, as shown in Table 2. For a dependency tree with n words, only n 1 positive dependency instances can be extracted. They account for only a small proportion of all the dependency instances. As we know, it is important to balance the proportions of the positive and the negative instances for a batched-trained classifier. We define a new parameter r to denote the ratio of the negative instances relative to the pos</context>
<context position="23862" citStr="Collins (2002)" startWordPosition="3975" endWordPosition="3976">ete trees to tune its parameters. Therefore, if trained on instances extracted from human-annotated treebanks, the word-pair classification model would not demonstrate its advantage over existed stateof-the-art dependency parsing methods. 6.2 Dependency Projection In this work we focus on the dependency projection from English to Chinese. We use the FBIS Chinese-English bitext as the bilingual corpus for dependency projection. It contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. Both English and Chinese sentences are tagged by the implementations of the POS tagger of Collins (2002), which trained on WSJ and CTB 5.0 respectively. The English sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from WSJ. The alignment matrixes for sentence pairs are generated according to (Liu et al., 2009). Similar to the ratio r, the threshold b need also be assigned an appropriate value to achieve a better performance. Larger thresholds result in better but less classification instances, the lower coverage of the instances would hurt the performance of the classifier. On the other hand, sm</context>
<context position="26430" citStr="Collins, 2002" startWordPosition="4389" endWordPosition="4390">he word-pair classification model, although falling behind of the state-of-the-art on humanannotated treebanks, performs well in projected dependency parsing. We give the credit to its good collaboration with the word-pair classification instance extraction for dependency projection. 6.3 Integrated Dependency Parser We integrate the word-pair classification model into the state-of-the-art 2nd-ordered MST model. First, we implement a chart-based dynamic programming parser for the 2nd-ordered MST model, and develop a training procedure based on the perceptron algorithm with averaged parameters (Collins, 2002). On the WSJ corpus, this parser achieves the same performance as that of McDonald and Pereira (2006). Then, at each derivation step of this 2nd-ordered MST parser, we weightedly add the evaluation score given by the projected classifier to the original MST evaluation score. Such a weighted summation of two eval18 \x0cuation scores provides better evaluation for candidate parses. The weight parameter is tuned by a minimum error-rate training algorithm (Och, 2003). Given a 2nd-ordered MST parser trained on CTB 5.0 as the baseline, the projected classifier brings an accuracy improvement of about</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the EMNLP, pages 18, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>340345</pages>
<contexts>
<context position="6929" citStr="Eisner, 1996" startWordPosition="1093" endWordPosition="1094">er et al., 1996). p is a probability which indicates the degree the classifier support the candidate edge (i, j). Ideally, given the classification results for all candidate word pairs, the dependency parse tree can be composed of the candidate edges with higher score (1 for the boolean-valued classifier, and large p for the real-valued classifier). However, more robust strategies should be investigated since the ambiguity of the language syntax and the classification errors usually lead to illegal or incomplete parsing result, as shown in Figure 1. Follow the edge based factorization method (Eisner, 1996), we factorize the score of a dependency tree s(x, y) into its dependency edges, and design a dynamic programming algorithm to search for the candidate parse with maximum score. This strategy alleviate the classification errors to some degree and ensure a valid, complete dependency parsing tree. If a boolean-valued classifier is used, the search algorithm can be formalized as: y = argmax y s(x, y) = argmax y X (i,j)y C(i, j) (3) And if a probability-valued classifier is used instead, we replace the accumulation with cumula13 \x0cType Features Unigram wordi posi wordi posi wordj posj wordj posj</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING, pages 340345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL.</booktitle>
<contexts>
<context position="2123" citStr="Ganchev et al., 2009" startWordPosition="315" endWordPosition="318">utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduce noise, and some hand-designed rules to</context>
<context position="17486" citStr="Ganchev et al. (2009)" startWordPosition="2938" endWordPosition="2941">he transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features. Jiang and Liu (2009) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. All previous works for dependency projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang and Liu, 2009) need complete projected trees to train the</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In Proceedings of the 47th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the IWPT,</booktitle>
<pages>5364</pages>
<contexts>
<context position="11622" citStr="Huang and Chiang, 2005" startWordPosition="1937" endWordPosition="1941">erivations of buf 9: Output: the best derivation of V[1, |x|] 10: function DERIV(p, c) 11: d p c {(p root, c root)} new derivation 12: d evl EVAL(d) evaluation function 13: return d used in McDonald et al. (2005b), is also applicable here. In this work, however, we still adopt the more general, bottom-up dynamic programming algorithm Algorithm 1 in order to facilitate the possible expansions. Here, V[i, j] contains the candidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 3 Projected Classification Instance After the introduction of the word-pair classification model, we now describe the extraction of projected dependency instances. In order to alleviate the effect of word alignment errors, we base the projection on the alignment matrix, a compact representation of multiple GIZA++ (Och and Ney, 2000) results, rather than a single word alignment in previous dependency projection works. Figure 2 shows an example. Suppose a bilingual sentence pair, composed of a source sentence e </context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the IWPT, pages 5364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="14634" citStr="Huang, 2008" startWordPosition="2486" endWordPosition="2487">anced parser. For a sentence to be parsed, x, the enhanced parser selects the best parse y according to both the baseline model B and the projected classifier C. y = argmax y [sB(x, y) + sC(x, y)] (11) 15 \x0cHere, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseli</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Liang Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Okan Kolak</author>
</authors>
<title>Evaluating translational correspondence using annotation projection.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="2083" citStr="Hwa et al., 2002" startWordPosition="307" endWordPosition="310">works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduc</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Kolak, 2002</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, and Okan Kolak. 2002. Evaluating translational correspondence using annotation projection. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2005</date>
<journal>In Natural Language Engineering,</journal>
<volume>11</volume>
<pages>311325</pages>
<contexts>
<context position="2101" citStr="Hwa et al., 2005" startWordPosition="311" endWordPosition="314">en devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduce noise, and some </context>
<context position="17460" citStr="Hwa et al. (2005)" startWordPosition="2933" endWordPosition="2936">ency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features. Jiang and Liu (2009) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. All previous works for dependency projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang and Liu, 2009) need complete pr</context>
<context position="24691" citStr="Hwa et al. (2005)" startWordPosition="4110" endWordPosition="4113">from WSJ. The alignment matrixes for sentence pairs are generated according to (Liu et al., 2009). Similar to the ratio r, the threshold b need also be assigned an appropriate value to achieve a better performance. Larger thresholds result in better but less classification instances, the lower coverage of the instances would hurt the performance of the classifier. On the other hand, smaller thresholds lead to worse but more instances, and too much noisy instances will bring down the classifiers discriminating power. We extract a series of classification instance sets Corpus System P % CTB 2.0 Hwa et al. (2005) 53.9 our model 56.9 CTB 5.0 Jiang and Liu (2009) 53.28 our model 58.59 Table 4: The performance of the projected classifier on the test sets of CTB 2.0 and CTB 5.0, compared with the performance of previous works on the corresponding test sets. Corpus Baseline P% Integrated P% CTB 1.0 82.23 83.70 CTB 5.0 87.15 87.65 Table 5: Performance improvement brought by the projected classifier to the baseline 2nd-ordered MST parsers trained on CTB 1.0 and CTB 5.0, respectively. with different thresholds. Then, on each instance set we train a classifier and test it on the development set of CTB 5.0. Fig</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. In Natural Language Engineering, volume 11, pages 311325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cWenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards for dependency parsing using projected treebank as source corpus.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="2896" citStr="Jiang and Liu (2009)" startWordPosition="432" endWordPosition="435">across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduce noise, and some hand-designed rules to handle language heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. Jiang and Liu (2009) resort to a dynamic programming procedure to search for a completed projected tree. However, these strategies are all confined to the same category that dependency projection must produce completed projected trees. Because of the free translation, the syntactic isomerism between languages and word alignment errors, it would be strained to completely project the dependency structure from one language to another. We propose an effective method for dependency projection, which does not have to produce complete projected trees. Given a wordaligned bilingual corpus with source language sentences p</context>
<context position="17798" citStr="Jiang and Liu (2009)" startWordPosition="2984" endWordPosition="2987">ows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features. Jiang and Liu (2009) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. All previous works for dependency projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang and Liu, 2009) need complete projected trees to train the projected parsers. Because of the free translation, the word alignment errors, and the heterogeneity between two languages, it is reluctant and less effective to project the dependency tree completely to the target language sentence. On the contrary, our dependency projection strategy prefer to extract a set o</context>
<context position="19217" citStr="Jiang and Liu (2009)" startWordPosition="3208" endWordPosition="3211">tances of good quality. In addition, our word-pair classification model can be integrated deeply into a state-of-the-art MST dependency model. Since both of them are 16 \x0cCorpus Train Dev Test WSJ (section) 2-21 22 23 CTB 5.0 (chapter) others 301-325 271-300 Table 2: The corpus partition for WSJ and CTB 5.0. factorized into dependency edges, the integration can be conducted at each dependency edge, by weightedly averaging their evaluation scores for this dependency edge. This strategy makes better use of the projected parser while with faster decoding, compared with the cascaded approach of Jiang and Liu (2009). 6 Experiments In this section, we first validate the word-pair classification model by experimenting on humanannotated treebanks. Then we investigate the effectiveness of the dependency projection by evaluating the projected classifiers trained on the projected classification instances. Finally, we report the performance of the integrated dependency parser which integrates the projected classifier and the 2nd-ordered MST dependency parser. We evaluate the parsing accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents. 6.1 Word-P</context>
<context position="24740" citStr="Jiang and Liu (2009)" startWordPosition="4120" endWordPosition="4123">pairs are generated according to (Liu et al., 2009). Similar to the ratio r, the threshold b need also be assigned an appropriate value to achieve a better performance. Larger thresholds result in better but less classification instances, the lower coverage of the instances would hurt the performance of the classifier. On the other hand, smaller thresholds lead to worse but more instances, and too much noisy instances will bring down the classifiers discriminating power. We extract a series of classification instance sets Corpus System P % CTB 2.0 Hwa et al. (2005) 53.9 our model 56.9 CTB 5.0 Jiang and Liu (2009) 53.28 our model 58.59 Table 4: The performance of the projected classifier on the test sets of CTB 2.0 and CTB 5.0, compared with the performance of previous works on the corresponding test sets. Corpus Baseline P% Integrated P% CTB 1.0 82.23 83.70 CTB 5.0 87.15 87.65 Table 5: Performance improvement brought by the projected classifier to the baseline 2nd-ordered MST parsers trained on CTB 1.0 and CTB 5.0, respectively. with different thresholds. Then, on each instance set we train a classifier and test it on the development set of CTB 5.0. Figure 4 presents the experimental results. The curv</context>
</contexts>
<marker>Jiang, Liu, 2009</marker>
<rawString>\x0cWenbin Jiang and Qun Liu. 2009. Automatic adaptation of annotation standards for dependency parsing using projected treebank as source corpus. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagginga case study.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th ACL.</booktitle>
<contexts>
<context position="2168" citStr="Jiang et al., 2009" startWordPosition="323" endWordPosition="326">the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduce noise, and some hand-designed rules to handle language heterogeneity. Smith and Eis</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagginga case study. In Proceedings of the 47th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1610" citStr="Klein and Manning, 2004" startWordPosition="232" endWordPosition="235">on instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). For dependency projection, the relations</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D. Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1715" citStr="Koo et al., 2008" startWordPosition="249" endWordPosition="252">fier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in t</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency structure analysis based on support vector machines.</title>
<date>2000</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="17156" citStr="Kudo and Matsumoto, 2000" startWordPosition="2887" endWordPosition="2890">es a dependency probability for each pair of words, while the classifier in a transition-based model gives a possible next transition operation such as shift or reduce. Another difference lies in the factorization strategy. For our method, the evaluation score of a candidate parse is factorized into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchron</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2000. Japanese dependency structure analysis based on support vector machines. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Tian Xia</author>
<author>Xinyan Xiao</author>
<author>Qun Liu</author>
</authors>
<title>Weighted alignment matrices for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the EMNLP.</booktitle>
<contexts>
<context position="24171" citStr="Liu et al., 2009" startWordPosition="4022" endWordPosition="4025">rojection from English to Chinese. We use the FBIS Chinese-English bitext as the bilingual corpus for dependency projection. It contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. Both English and Chinese sentences are tagged by the implementations of the POS tagger of Collins (2002), which trained on WSJ and CTB 5.0 respectively. The English sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from WSJ. The alignment matrixes for sentence pairs are generated according to (Liu et al., 2009). Similar to the ratio r, the threshold b need also be assigned an appropriate value to achieve a better performance. Larger thresholds result in better but less classification instances, the lower coverage of the instances would hurt the performance of the classifier. On the other hand, smaller thresholds lead to worse but more instances, and too much noisy instances will bring down the classifiers discriminating power. We extract a series of classification instance sets Corpus System P % CTB 2.0 Hwa et al. (2005) 53.9 our model 56.9 CTB 5.0 Jiang and Liu (2009) 53.28 our model 58.59 Table 4:</context>
</contexts>
<marker>Liu, Xia, Xiao, Liu, 2009</marker>
<rawString>Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009. Weighted alignment matrices for statistical machine translation. In Proceedings of the EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Sheng Li</author>
<author>Tiejun Zhao</author>
<author>Muyun Yang</author>
</authors>
<title>Learning chinese bracketing knowledge based on a bilingual language model.</title>
<date>2002</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="17368" citStr="Lu et al. (2002)" startWordPosition="2918" endWordPosition="2921">y. For our method, the evaluation score of a candidate parse is factorized into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features. Jiang and Liu (2009) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. All previous works for dependency projection (Hwa et al</context>
</contexts>
<marker>Lu, Li, Zhao, Yang, 2002</marker>
<rawString>Yajuan Lu, Sheng Li, Tiejun Zhao, and Muyun Yang. 2002. Learning chinese bracketing knowledge based on a bilingual language model. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<booktitle>In Computational Linguistics.</booktitle>
<contexts>
<context position="19970" citStr="Marcus et al., 1993" startWordPosition="3320" endWordPosition="3323">hen we investigate the effectiveness of the dependency projection by evaluating the projected classifiers trained on the projected classification instances. Finally, we report the performance of the integrated dependency parser which integrates the projected classifier and the 2nd-ordered MST dependency parser. We evaluate the parsing accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents. 6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993), and the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of Yamada and Matsumoto (2003). For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002). While for Chinese, we just use the gold-standard POS tags following the tradition. Each treebank is splitted into three partitions, for training, development and testing, respectively, as shown in Table 2. For a dependency tree with n words, only n 1 positive dependenc</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. In Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>8188</pages>
<contexts>
<context position="1367" citStr="McDonald and Pereira, 2006" startWordPosition="193" endWordPosition="196">f the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the depe</context>
<context position="4687" citStr="McDonald and Pereira, 2006" startWordPosition="708" endWordPosition="711">ency parser without the need of complete projected trees. 12 \x0ci j j i Figure 1: Illegal (a) and incomplete (b) dependency tree produced by the simple-collection method. Experimental results show that, the classifier trained on the projected classification instances significantly outperforms the projected dependency parsers in previous works. The classifier trained on the Chinese projected classification instances achieves a precision of 58.59% on the CTB standard test set. More importantly, when this classifier is integrated into a 2nd-ordered maximum spanning tree (MST) dependency parser (McDonald and Pereira, 2006) in a weighted average manner, significant improvement is obtained over the MST baselines. For the 2nd-order MST parser trained on Penn Chinese Treebank (CTB) 5.0, the classifier give an precision increment of 0.5 points. Especially for the parser trained on the smaller CTB 1.0, more than 1 points precision increment is obtained. In the rest of this paper, we first describe the word-pair classification model for dependency parsing (section 2) and the generation method of projected classification instances (section 3). Then we describe an application of the projected parser: boosting a state-of</context>
<context position="15156" citStr="McDonald and Pereira, 2006" startWordPosition="2569" endWordPosition="2573">an be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored</context>
<context position="24025" citStr="McDonald and Pereira (2006)" startWordPosition="3999" endWordPosition="4002">d not demonstrate its advantage over existed stateof-the-art dependency parsing methods. 6.2 Dependency Projection In this work we focus on the dependency projection from English to Chinese. We use the FBIS Chinese-English bitext as the bilingual corpus for dependency projection. It contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English. Both English and Chinese sentences are tagged by the implementations of the POS tagger of Collins (2002), which trained on WSJ and CTB 5.0 respectively. The English sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from WSJ. The alignment matrixes for sentence pairs are generated according to (Liu et al., 2009). Similar to the ratio r, the threshold b need also be assigned an appropriate value to achieve a better performance. Larger thresholds result in better but less classification instances, the lower coverage of the instances would hurt the performance of the classifier. On the other hand, smaller thresholds lead to worse but more instances, and too much noisy instances will bring down the classifiers discriminating power. We extract a series of classi</context>
<context position="26531" citStr="McDonald and Pereira (2006)" startWordPosition="4404" endWordPosition="4408">manannotated treebanks, performs well in projected dependency parsing. We give the credit to its good collaboration with the word-pair classification instance extraction for dependency projection. 6.3 Integrated Dependency Parser We integrate the word-pair classification model into the state-of-the-art 2nd-ordered MST model. First, we implement a chart-based dynamic programming parser for the 2nd-ordered MST model, and develop a training procedure based on the perceptron algorithm with averaged parameters (Collins, 2002). On the WSJ corpus, this parser achieves the same performance as that of McDonald and Pereira (2006). Then, at each derivation step of this 2nd-ordered MST parser, we weightedly add the evaluation score given by the projected classifier to the original MST evaluation score. Such a weighted summation of two eval18 \x0cuation scores provides better evaluation for candidate parses. The weight parameter is tuned by a minimum error-rate training algorithm (Och, 2003). Given a 2nd-ordered MST parser trained on CTB 5.0 as the baseline, the projected classifier brings an accuracy improvement of about 0.5 points. For the baseline trained on the smaller CTB 1.0, whose training set is chapters 1-270 of</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL, pages 8188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1338" citStr="McDonald et al., 2005" startWordPosition="189" endWordPosition="192">pendency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising</context>
<context position="5540" citStr="McDonald et al., 2005" startWordPosition="836" endWordPosition="839"> for the parser trained on the smaller CTB 1.0, more than 1 points precision increment is obtained. In the rest of this paper, we first describe the word-pair classification model for dependency parsing (section 2) and the generation method of projected classification instances (section 3). Then we describe an application of the projected parser: boosting a state-of-the-art 2nd-ordered MST parser (section 4). After the comparisons with previous works on dependency parsing and projection, we finally five the experimental results. 2 Word-Pair Classification Model 2.1 Model Definition Following (McDonald et al., 2005a), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence. y denotes the dependency tree for sentence x, and (i, j) y represents a dependency edge from word xi to word xj, where xi is the parent of xj. The task of the word-pair classification model is to determine whether any candidate word pair, xi and xj s.t. 1 i, j |x |and i 6= j, forms a dependency edge. The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier (Vapnik, 1998). p = 1 indicates that the classifier support</context>
<context position="9023" citStr="McDonald et al., 2005" startWordPosition="1478" endWordPosition="1482"> the assumption that the word pair i and j has a dependency relationship r. The symbol r indicates the supposed classification result, where r = + means we suppose it as a dependency edge and r = means the contrary. A feature fk(i, j, r) f(i, j, r) equals 1 if it is activated by the assumption and equals 0 otherwise. The dependency probability can then be defined as: C(i, j) = exp(w f(i, j, +)) P r exp(w f(i, j, r)) = exp( P k wk fk(i, j, +)) P r exp( P k wk fk(i, j, r)) (5) 2.2 Features for Classification The feature templates for the classifier are similar to those of 1st-ordered MST model (McDonald et al., 2005a). 1 Each feature is composed of some words and POS tags surrounded word i and/or word j, as well as an optional distance representations between this two words. Table shows the feature templates we use. Previous graph-based dependency models usually use the index distance of word i and word j 1 We exclude the in between features of McDonald et al. (2005a) since preliminary experiments show that these features bring no improvement to the word-pair classification model. to enrich the features with word distance information. However, in order to utilize some syntax information between the pair </context>
<context position="11210" citStr="McDonald et al. (2005" startWordPosition="1870" endWordPosition="1873"> X (i,j)y log(C(i, j)) (6) Thus, the decoding algorithm for 1st-ordered MST model, such as the Chu-Liu-Edmonds algorithm 14 \x0cAlgorithm 1 Dependency Parsing Algorithm. 1: Input: sentence x to be parsed 2: for hi, ji h1, |x|i in topological order do 3: buf 4: for k i..j 1 do all partitions 5: for l V[i, k] and r V[k + 1, j] do 6: insert DERIV(l, r) into buf 7: insert DERIV(r, l) into buf 8: V[i, j] top K derivations of buf 9: Output: the best derivation of V[1, |x|] 10: function DERIV(p, c) 11: d p c {(p root, c root)} new derivation 12: d evl EVAL(d) evaluation function 13: return d used in McDonald et al. (2005b), is also applicable here. In this work, however, we still adopt the more general, bottom-up dynamic programming algorithm Algorithm 1 in order to facilitate the possible expansions. Here, V[i, j] contains the candidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 3 Projected Classification Instance After the introduction of the word-pair classification model, we no</context>
<context position="15502" citStr="McDonald et al., 2005" startWordPosition="2624" endWordPosition="2627"> tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously differs from other graph-based models, that we only need a set of word-pair dependency in</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 91 98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP.</booktitle>
<contexts>
<context position="1338" citStr="McDonald et al., 2005" startWordPosition="189" endWordPosition="192">pendency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising</context>
<context position="5540" citStr="McDonald et al., 2005" startWordPosition="836" endWordPosition="839"> for the parser trained on the smaller CTB 1.0, more than 1 points precision increment is obtained. In the rest of this paper, we first describe the word-pair classification model for dependency parsing (section 2) and the generation method of projected classification instances (section 3). Then we describe an application of the projected parser: boosting a state-of-the-art 2nd-ordered MST parser (section 4). After the comparisons with previous works on dependency parsing and projection, we finally five the experimental results. 2 Word-Pair Classification Model 2.1 Model Definition Following (McDonald et al., 2005a), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence. y denotes the dependency tree for sentence x, and (i, j) y represents a dependency edge from word xi to word xj, where xi is the parent of xj. The task of the word-pair classification model is to determine whether any candidate word pair, xi and xj s.t. 1 i, j |x |and i 6= j, forms a dependency edge. The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier (Vapnik, 1998). p = 1 indicates that the classifier support</context>
<context position="9023" citStr="McDonald et al., 2005" startWordPosition="1478" endWordPosition="1482"> the assumption that the word pair i and j has a dependency relationship r. The symbol r indicates the supposed classification result, where r = + means we suppose it as a dependency edge and r = means the contrary. A feature fk(i, j, r) f(i, j, r) equals 1 if it is activated by the assumption and equals 0 otherwise. The dependency probability can then be defined as: C(i, j) = exp(w f(i, j, +)) P r exp(w f(i, j, r)) = exp( P k wk fk(i, j, +)) P r exp( P k wk fk(i, j, r)) (5) 2.2 Features for Classification The feature templates for the classifier are similar to those of 1st-ordered MST model (McDonald et al., 2005a). 1 Each feature is composed of some words and POS tags surrounded word i and/or word j, as well as an optional distance representations between this two words. Table shows the feature templates we use. Previous graph-based dependency models usually use the index distance of word i and word j 1 We exclude the in between features of McDonald et al. (2005a) since preliminary experiments show that these features bring no improvement to the word-pair classification model. to enrich the features with word distance information. However, in order to utilize some syntax information between the pair </context>
<context position="11210" citStr="McDonald et al. (2005" startWordPosition="1870" endWordPosition="1873"> X (i,j)y log(C(i, j)) (6) Thus, the decoding algorithm for 1st-ordered MST model, such as the Chu-Liu-Edmonds algorithm 14 \x0cAlgorithm 1 Dependency Parsing Algorithm. 1: Input: sentence x to be parsed 2: for hi, ji h1, |x|i in topological order do 3: buf 4: for k i..j 1 do all partitions 5: for l V[i, k] and r V[k + 1, j] do 6: insert DERIV(l, r) into buf 7: insert DERIV(r, l) into buf 8: V[i, j] top K derivations of buf 9: Output: the best derivation of V[1, |x|] 10: function DERIV(p, c) 11: d p c {(p root, c root)} new derivation 12: d evl EVAL(d) evaluation function 13: return d used in McDonald et al. (2005b), is also applicable here. In this work, however, we still adopt the more general, bottom-up dynamic programming algorithm Algorithm 1 in order to facilitate the possible expansions. Here, V[i, j] contains the candidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 3 Projected Classification Instance After the introduction of the word-pair classification model, we no</context>
<context position="15502" citStr="McDonald et al., 2005" startWordPosition="2624" endWordPosition="2627"> tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously differs from other graph-based models, that we only need a set of word-pair dependency in</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic dependency parsing of english text.</title>
<date>2004</date>
<booktitle>In Proceedings of the COLING.</booktitle>
<contexts>
<context position="21245" citStr="Nivre and Scholz (2004)" startWordPosition="3535" endWordPosition="3538">a small proportion of all the dependency instances. As we know, it is important to balance the proportions of the positive and the negative instances for a batched-trained classifier. We define a new parameter r to denote the ratio of the negative instances relative to the positive ones. 84 84.5 85 85.5 86 86.5 87 1 1.5 2 2.5 3 Dependency Precision (%) Ratio r (#negative/#positive) WSJ CTB 5.0 Figure 3: Performance curves of the word-pair classification model on the development sets of WSJ and CTB 5.0, with respect to a series of ratio r. Corpus System P % WSJ Yamada and Matsumoto (2003) 90.3 Nivre and Scholz (2004) 87.3 1st-ordered MST 90.7 2nd-ordered MST 91.5 our model 86.8 CTB 5.0 1st-ordered MST 86.53 2nd-ordered MST 87.15 our model 82.06 Table 3: Performance of the word-pair classification model on WSJ and CTB 5.0, compared with the current state-of-the-art models. For example, r = 2 means we reserve negative instances two times as many as the positive ones. The MaxEnt toolkit by Zhang 2 is adopted to train the ME classifier on extracted instances. We set the gaussian prior as 1.0 and the iteration limit as 100, leaving other parameters as default values. We first investigate the impact of the rati</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>J. Nivre and M. Scholz. 2004. Deterministic dependency parsing of english text. In Proceedings of the COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Gulsen Eryigit</author>
<author>Svetoslav Marinov</author>
</authors>
<title>Labeled pseudoprojective dependency parsing with support vector machines.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL,</booktitle>
<pages>221225</pages>
<contexts>
<context position="1388" citStr="Nivre et al., 2006" startWordPosition="197" endWordPosition="200">ce language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this classifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline. 1 Introduction Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). Since it is costly and difficult to build humanannotated treebanks, a lot of works have also been devoted to the utilization of unannotated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures fro</context>
<context position="15629" citStr="Nivre et al., 2006" startWordPosition="2644" endWordPosition="2647">ion score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously differs from other graph-based models, that we only need a set of word-pair dependency instances rather than a regular dependency treebank. Therefore, our model is more suitable for the partially bracketed or noisy t</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Eryigit, Marinov, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit, and Svetoslav Marinov. 2006. Labeled pseudoprojective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="12041" citStr="Och and Ney, 2000" startWordPosition="2006" endWordPosition="2009">andidate parsing segments of the span [i, j], and the function EVAL(d) accumulates the scores of all the edges in dependency segment d. In practice, the cube-pruning strategy (Huang and Chiang, 2005) is used to speed up the enumeration of derivations (loops started by line 4 and 5). 3 Projected Classification Instance After the introduction of the word-pair classification model, we now describe the extraction of projected dependency instances. In order to alleviate the effect of word alignment errors, we base the projection on the alignment matrix, a compact representation of multiple GIZA++ (Och and Ney, 2000) results, rather than a single word alignment in previous dependency projection works. Figure 2 shows an example. Suppose a bilingual sentence pair, composed of a source sentence e and its target translation f. ye is the parse tree of the source sentence. A is the alignment matrix between them, and each element Ai,j denotes the degree of the alignment between word ei and word fj. We define a boolean-valued function (y, i, j, r) to investigate the dependency relationship of word i and word j in parse tree y: (y, i, j, r) = 1 (i, j) y and r = + or (i, j) / y and r = 0 otherwise (7) Then the scor</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Joseph Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL,</booktitle>
<pages>160167</pages>
<contexts>
<context position="15418" citStr="Och, 2003" startWordPosition="2613" endWordPosition="2614">ir classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously diffe</context>
<context position="26897" citStr="Och, 2003" startWordPosition="4466" endWordPosition="4467">rser for the 2nd-ordered MST model, and develop a training procedure based on the perceptron algorithm with averaged parameters (Collins, 2002). On the WSJ corpus, this parser achieves the same performance as that of McDonald and Pereira (2006). Then, at each derivation step of this 2nd-ordered MST parser, we weightedly add the evaluation score given by the projected classifier to the original MST evaluation score. Such a weighted summation of two eval18 \x0cuation scores provides better evaluation for candidate parses. The weight parameter is tuned by a minimum error-rate training algorithm (Och, 2003). Given a 2nd-ordered MST parser trained on CTB 5.0 as the baseline, the projected classifier brings an accuracy improvement of about 0.5 points. For the baseline trained on the smaller CTB 1.0, whose training set is chapters 1-270 of CTB 5.0, the accuracy improvement is much significant, about 1.5 points over the baseline. It indicates that, the smaller the human-annotated treebank we have, the more significant improvement we can achieve by integrating the projecting classifier. This provides a promising strategy for boosting the parsing performance of resourcescarce languages. Table 5 summar</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Joseph Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the ACL, pages 160167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2147" citStr="Smith and Eisner, 2009" startWordPosition="319" endWordPosition="322">ated text. For example, the unsupervised dependency parsing (Klein and Manning, 2004) which is totally based on unannotated data, and the semisupervised dependency parsing (Koo et al., 2008) which is based on both annotated and unannotated data. Considering the higher complexity and lower performance in unsupervised parsing, and the need of reliable priori knowledge in semisupervised parsing, it is a promising strategy to project the dependency structures from a resource-rich language to a resource-scarce one across a bilingual corpus (Hwa et al., 2002; Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang et al., 2009). For dependency projection, the relationship between words in the parsed sentences can be simply projected across the word alignment to words in the unparsed sentences, according to the DCA assumption (Hwa et al., 2005). Such a projection procedure suffers much from the word alignment errors and syntactic isomerism between languages, which usually lead to relationship projection conflict and incomplete projected dependency structures. To tackle this problem, Hwa et al. (2005) use some filtering rules to reduce noise, and some hand-designed rules to handle language heterog</context>
<context position="17680" citStr="Smith and Eisner (2009)" startWordPosition="2969" endWordPosition="2972">has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features. Jiang and Liu (2009) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. All previous works for dependency projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang and Liu, 2009) need complete projected trees to train the projected parsers. Because of the free translation, the word alignment errors, and the heterogeneity between two languages, it is reluctant and less effective to project the dependency tree com</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Japanese dependency structure analysis based on maximum entropy models.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL.</booktitle>
<contexts>
<context position="17129" citStr="Uchimoto et al., 1999" startWordPosition="2883" endWordPosition="2886">r in our model predicates a dependency probability for each pair of words, while the classifier in a transition-based model gives a possible next transition operation such as shift or reduce. Another difference lies in the factorization strategy. For our method, the evaluation score of a candidate parse is factorized into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation ada</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 1999</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 1999. Japanese dependency structure analysis based on maximum entropy models. In Proceedings of the EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical learning theory. In A Wiley-Interscience Publication.</title>
<date>1998</date>
<contexts>
<context position="6095" citStr="Vapnik, 1998" startWordPosition="949" endWordPosition="950">odel 2.1 Model Definition Following (McDonald et al., 2005a), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence. y denotes the dependency tree for sentence x, and (i, j) y represents a dependency edge from word xi to word xj, where xi is the parent of xj. The task of the word-pair classification model is to determine whether any candidate word pair, xi and xj s.t. 1 i, j |x |and i 6= j, forms a dependency edge. The classification result C(i, j) can be a boolean value: C(i, j) = p p {0, 1} (1) as produced by a support vector machine (SVM) classifier (Vapnik, 1998). p = 1 indicates that the classifier supports the candidate edge (i, j), and p = 0 the contrary. C(i, j) can also be a realvalued probability: C(i, j) = p 0 p 1 (2) as produced by an maximum entropy (ME) classifier (Berger et al., 1996). p is a probability which indicates the degree the classifier support the candidate edge (i, j). Ideally, given the classification results for all candidate word pairs, the dependency parse tree can be composed of the candidate edges with higher score (1 for the boolean-valued classifier, and large p for the real-valued classifier). However, more robust strate</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical learning theory. In A Wiley-Interscience Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics.</journal>
<contexts>
<context position="17431" citStr="Wu, 1997" startWordPosition="2930" endWordPosition="2931">ized into each dependency edge, while for the transition-based models, the score is factorized into each transition operation. Thanks to the reminding of the third reviewer of our paper, we find that the pairwise classification schema has also been used in Japanese dependency parsing (Uchimoto et al., 1999; Kudo and Matsumoto, 2000). However, our work shows more advantage in feature engineering, model training and decoding algorithm. 5.2 Dependency Projection Many works try to learn parsing knowledge from bilingual corpora. Lu et al. (2002) aims to obtain Chinese bracketing knowledge via ITG (Wu, 1997) alignment. Hwa et al. (2005) and Ganchev et al. (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. Smith and Eisner (2009) perform dependency projection and annotation adaptation with Quasi-Synchronous Grammar features. Jiang and Liu (2009) refer to alignment matrix and a dynamic programming search algorithm to obtain better projected dependency trees. All previous works for dependency projection (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Jiang an</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>In Natural Language Engineering.</journal>
<contexts>
<context position="20030" citStr="Xue et al., 2005" startWordPosition="3332" endWordPosition="3335">on by evaluating the projected classifiers trained on the projected classification instances. Finally, we report the performance of the integrated dependency parser which integrates the projected classifier and the 2nd-ordered MST dependency parser. We evaluate the parsing accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents. 6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993), and the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of Yamada and Matsumoto (2003). For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002). While for Chinese, we just use the gold-standard POS tags following the tradition. Each treebank is splitted into three partitions, for training, development and testing, respectively, as shown in Table 2. For a dependency tree with n words, only n 1 positive dependency instances can be extracted. They account for only a small </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. In Natural Language Engineering.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="15608" citStr="Yamada and Matsumoto, 2003" startWordPosition="2639" endWordPosition="2643">endency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance in the baseline model is also investigated. The relative weight is adjusted to maximize the performance on the development set, using an algorithm similar to minimum error-rate training (Och, 2003). 5 Related Works 5.1 Dependency Parsing Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. Similar to the graph-based method, our model is factored on dependency edges, and its decoding procedure also aims to find a maximum spanning tree in a fully connected directed graph. From this point, our model can be classified into the graph-based category. On the training method, however, our model obviously differs from other graph-based models, that we only need a set of word-pair dependency instances rather than a regular dependency treebank. Therefore, our model is more suitable for the partially</context>
<context position="20176" citStr="Yamada and Matsumoto (2003)" startWordPosition="3356" endWordPosition="3359">integrated dependency parser which integrates the projected classifier and the 2nd-ordered MST dependency parser. We evaluate the parsing accuracy by the precision of lexical heads, which is the percentage of the words that have found their correct parents. 6.1 Word-Pair Classification Model We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993), and the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). The constituent trees in the two treebanks are transformed to dependency trees according to the headfinding rules of Yamada and Matsumoto (2003). For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002). While for Chinese, we just use the gold-standard POS tags following the tradition. Each treebank is splitted into three partitions, for training, development and testing, respectively, as shown in Table 2. For a dependency tree with n words, only n 1 positive dependency instances can be extracted. They account for only a small proportion of all the dependency instances. As we know, it is important to balance the proportions of the positive and the negative instances for </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H Yamada and Y Matsumoto. 2003. Statistical dependency analysis using support vector machines. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Joint word segmentation and pos tagging using a single perceptron.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="14620" citStr="Zhang and Clark, 2008" startWordPosition="2482" endWordPosition="2485">d framework for the enhanced parser. For a sentence to be parsed, x, the enhanced parser selects the best parse y according to both the baseline model B and the projected classifier C. y = argmax y [sB(x, y) + sC(x, y)] (11) 15 \x0cHere, sB and sC denote the evaluation functions of the baseline model and the projected classifier, respectively. The parameter is the relative weight of the projected classifier against the baseline model. There are several strategies to integrate the two evaluation functions. For example, they can be integrated deeply at each decoding step (Carreras et al., 2008; Zhang and Clark, 2008; Huang, 2008), or can be integrated shallowly in a reranking manner (Collins, 2000; Charniak and Johnson, 2005). As described previously, the score of a dependency tree given by a word-pair classifier can be factored into each candidate dependency edge in this tree. Therefore, the projected classifier can be integrated with a baseline model deeply at each dependency edge, if the evaluation score given by the baseline model can also be factored into dependency edges. We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. Especially, the effect of the Collins distance</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. Joint word segmentation and pos tagging using a single perceptron. In Proceedings of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>