<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<affiliation confidence="0.105975">
b&amp;apos;2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 7079,
</affiliation>
<address confidence="0.315195">
Montreal, Canada, June 3-8, 2012. c
</address>
<title confidence="0.6365705">
2012 Association for Computational Linguistics
Multi Event Extraction Guided by Global Constraints
</title>
<author confidence="0.936024">
Roi Reichart Regina Barzilay
</author>
<affiliation confidence="0.9932865">
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
</affiliation>
<email confidence="0.975449">
{roiri, regina}@csail.mit.edu
</email>
<sectionHeader confidence="0.990662" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99920465">
This paper addresses the extraction of event
records from documents that describe multi-
ple events. Specifically, we aim to identify
the fields of information contained in a docu-
ment and aggregate together those fields that
describe the same event. To exploit the in-
herent connections between field extraction
and event identification, we propose to model
them jointly. Our model is novel in that it
integrates information from separate sequen-
tial models, using global potentials that en-
courage the extracted event records to have
desired properties. While the model con-
tains high-order potentials, efficient approxi-
mate inference can be performed with dual-
decomposition. We experiment with two data
sets that consist of newspaper articles de-
scribing multiple terrorism events, and show
that our model substantially outperforms tra-
ditional pipeline models.
</bodyText>
<sectionHeader confidence="0.998316" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999748">
Today, most efforts in information extraction have
focused on the field extraction task, commonly for-
mulated as a sequence tagging problem. When a
document describes a single event, the list of ex-
tracted fields provides a useful abstraction of the in-
put document. In practice, however, a typical news-
paper document describes multiple events, and a flat
list of field values may not contain the sufficient
structure required for many NLP applications. Our
goal is therefore to extract event templates which ag-
gregate field values for individual events.
Consider, for instance, the New York Times arti-
cle excerpt in Figure 1 that describes three related
terrorist events. As this example illustrates, in order
to populate the corresponding event templates, the
model needs to identify segments that describe indi-
vidual events. Such segmentation is challenging, as
event boundaries are not explicitly demarcated in the
text. Moreover, descriptions of different events are
often intermingled, as in the above example, further
complicating boundary recovery.
In this paper, we consider a model that jointly
performs event segmentation and field extraction.
This model capitalizes on the inherent connection
between the two tasks in order to reduce the ambi-
guity of template-based extraction. For example, the
distribution of field values in the text provides strong
clues about event segmentation, such as the presence
of multiple new fields strongly signaling a segment
boundary. Likewise, knowledge of the boundaries
enables the model to rule out mutually inconsistent
predictions, such as extracting two distinct locations
for the same event.
We formulate our approach as a joint model that
marks each word with field and event labels si-
multaneously. At the sentence level, segmentation
and field extraction taggers are implemented using
separate sequence models operating over local fea-
tures. At the document level, the model encourages
global consistency via potentials that link the ex-
tracted event records and their fields. Some of these
potentials are limited to fields of an individual event
such as the single city per event constraint. Others
encode discourse-level properties of the whole doc-
ument and thus involve records of multiple events,
</bodyText>
<page confidence="0.993476">
70
</page>
<bodyText confidence="0.9957802">
\x0cA powerful car bomb exploded today in Baghdad inside the holiest Shiite shrine . As many as 95 people were killed
in the event, according to sources in Washington. The blast came only two days after another car bomb exploded in a crowded
street in Mosul in the northern part of Iraq, killing 13 pedestrians, in an attack carried out by Al Qaeda. Together with the
previous attack by Al Qaeda, the shooting in Najaf three weeks ago that killed 15 American soldiers, violence seemed
to spike to its highest level. The bombing today, happened around 9am, when the roads are crowded with people. ...
</bodyText>
<figure confidence="0.34190375">
Organization Tactic Target Weapon Fatalities City Country
Event 1 bombing Shiite shrine car bomb 95 people Baghdad
Event 2 Al Qaeda bombing car bomb 13 pedestrians Mosul Iraq
Event 3 Al Qaeda shooting - 15 American Soldiers Najaf
</figure>
<figureCaption confidence="0.998643">
Figure 1: A New York times article describing three terrorist events and a table demonstrating the corresponding event records.
</figureCaption>
<bodyText confidence="0.998712571428571">
such as the tendency in newspaper reporting to fea-
ture the main event at the beginning and repeatedly
throughout the document.
While these high-order potentials encode impor-
tant linguistic properties of valid assignments, they
greatly complicate learning and inference. There-
fore, our method estimates the parameters of the lo-
cal sequence models and the global potentials sep-
arately. Then, at inference time, it finds variable
assignments that are most consistent with both the
local models and the global potentials. Inference
is implemented via dual-decomposition, an efficient
algorithm shown to be effective for complex joint
inference problems.
We evaluate our approach for event extraction on
two data sets, one is a new collection of long news-
paper articles and the other is a subset of the MUC-
4 documents. Both data sets consist of articles that
describe multiple terrorist events (40.3 and 12.4 sen-
tences and 4.4 and 3.1 events per article for each data
set on average). We demonstrate the benefits of the
joint model for event extraction; it outperforms a tra-
ditional pipeline model by a significant margin. For
instance, it yields an absolute gain of 8.5% for our
new corpus when measured using document-level F-
score. Our results show the effectiveness of global
constraints in the context of template extraction and
motivate their exploration in other IE tasks.
</bodyText>
<sectionHeader confidence="0.987548" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.974002413043479">
Event-Template Extraction Event template extrac-
tion has been previously explored in the MUC-4
scenario template task. Work on this task has fo-
cused on pipeline models which decouple the task
into the sub-tasks of field extraction and event-based
text segmentation. For example, rule-based meth-
ods (Rau et al., 1992; Chinchor et al., 1993) identify
generalizations both for single field fillers and for re-
lations between fields and use them to fill event tem-
plates. Likewise, classifier-based algorithms (Chieu
et al., 2003; Xiao et al., 2004; Maslennikov and
Chua, 2007; Patwardhan and Riloff, 2009) gener-
ally train individual classifiers for each type of field
and aggregate candidate fillers based on a senten-
tial event classifier. Finally, unsupervised techniques
(Chambers and Jurafsky, 2011) have combined clus-
tering, semantic roles, and syntactic relations in or-
der to both construct and fill event templates.
In our work, we also address the sub-tasks of
field extraction and event segmentation individu-
ally; however, we link them through soft global con-
straints and encourage consistency through joint in-
ference. To facilitate the joint inference, we use a
linear-chain CRF for each sub-task.
Global Constraints Previous work demonstrated
the benefits of applying declarative constraints in in-
formation extraction (Finkel et al., 2005; Roth and
tau Yih, 2004; Chang et al., 2007; Druck and Mc-
Callum, 2010). Constraints have been explored both
at sentence and document level. For example, Finkel
et al. (2005) employ document-level constraints to
encourage global consistency of named entity as-
signments. Likewise, Chang et al. (2007) use con-
straints at multiple levels, such as sentence-level
constraints to specify field boundaries and global
constraints to ensure relation-level consistency. In
our work we focus on document-level constraints.
We utilize both discourse and record-coherence con-
straints to encourage consistency between local se-
quence models.
There has also been unsupervised work that
demonstrates the benefit of domain-specific con-
straints (Chen et al., 2011). In our work we show
that domain-specific constraints based on the com-
mon structure of newspaper articles are also useful
to guide a supervised model.
</bodyText>
<page confidence="0.99769">
71
</page>
<sectionHeader confidence="0.498113" genericHeader="method">
\x0c3 Model
</sectionHeader>
<bodyText confidence="0.973086930232558">
Problem Formulation Given a document, our goal
is to extract field values and aggregate them into
event records. The training data consists of event an-
notations where each word in the document is tagged
with a field and with an event id. If a word is not a
filler for a field, it is annotated with a default NULL
field value. At test time, the number of events is not
given and has to be inferred from the data.
Model Structure Our model is built around the
connection between local extraction decisions and
global constraints on event structure. Based on
local cues, the model can identify candidate field
fillers. However, connecting them to events requires
a broader document context. To effectively capture
this context, the model needs to group together por-
tions of the document that describe the same event.
Global constraints are instrumental in this process,
as they drive the aggregation of contiguous segments
computed by a local segmentation model. In ad-
dition, global constraints coordinate local decisions
and thereby enable us to express important discourse
dependencies between various assignments.
To implement these ideas in a computational
framework, we define an undirected graphical model
with a vertex set V = X Y Z. X is a set of ob-
served nodes; xi represents the ithe word in a docu-
ment. Y and Z are sets of unobserved nodes corre-
sponding to the field and event assignments respec-
tively of the ith word. The number of input words in
a document is denoted by n.
We define three types of potentials:
Field-labeling Potentials associate words in a
document with field labels based on their local
sentential context.
Event-labeling Potentials associate words in a
document with event boundaries based on the
local surroundings of a candidate boundary.
Global Consistency Potentials link the ex-
tracted event records and their fields to encour-
age global consistency. These potentials are de-
fined over the entire set of variables related to a
document.
The resulting maximum aposteriori problem is:
</bodyText>
<equation confidence="0.9991815">
MAP() =
X
fF
f (rf )
</equation>
<bodyText confidence="0.992037">
where f are the potential functions and {rf |f
{1, . . . , n}, f F} is the set of their variables.
</bodyText>
<subsectionHeader confidence="0.769122">
3.1 Modeling Local Dependencies
Field Labeling The first step of the model is tagging
</subsectionHeader>
<bodyText confidence="0.985033055555555">
the words in the input document with fields. Fol-
lowing traditional approaches, we employ a linear-
chain CRF (Lafferty et al., 2001) that operates
over standard lexical, POS-based and syntactic fea-
tures (Finkel et al., 2005; Finkel and Manning, 2009;
Bellare and McCallum, 2009; Yao et al., 2010).
Event Segmentation At the local level, event analy-
sis involves identification of event boundaries which
we model as linear segmentation. To this end, we
employ a binary CRF that predicts whether a given
word starts a description of a new event or continues
the description of the current event, based on lex-
ical and POS-based features. In addition, we add
features obtained from the output of the field extrac-
tion CRF. These features capture the intuition that
boundary sentences often contain multiple fields.
The potential functions of these components are
given by the likelihoods of the corresponding CRFs.
</bodyText>
<subsectionHeader confidence="0.999922">
3.2 Modeling Global Dependencies
</subsectionHeader>
<bodyText confidence="0.999132454545455">
The main function of the global constraints is to
link extracted fields to the corresponding events.
In addition, the model can use global constraints
to resolve potentially inconsistent decisions of the
local models by encouraging them to agree with
global, document-level properties. We consider two
types of global consistency potentials: discourse po-
tentials that involve interactions between multiple
records, and record coherence potentials that cap-
ture patterns at the level of individual records.
The general form of a global potential p is:
</bodyText>
<equation confidence="0.98289">
f (xfp, yfp, zfp) =
\x1a
p if potential-property holds
0 otherwise
</equation>
<bodyText confidence="0.976269333333333">
Where f p is the index set of variables over
which the potential is defined. Table 1 gives a formal
description of all the potentials. Below we describe
the linguistic intuition behind these potentials.
Discourse Potentials To populate event records
with extracted information, the model needs to
</bodyText>
<page confidence="0.981133">
72
</page>
<construct confidence="0.8493001875">
\x0cDiscourse
MAIN EVENT Two consecutive sentences without fields indicate a transition
to the main event:
(Si, Si+1 s.t. (k Si, yk = NULL) (k Si+1, yk = NULL))
(l i s.t. (u, u i, u &lt; l, 1fME(Su)=1), p Sl, zp = CENT RAL)
SEGMENT BOUNDARY Event changes should take place in multi-field sentences:
i, j I, ((i = j + 1) (zi! = zj))
(i1 . . . it I s.t. 1[fsSB(i,i1,...it)=1] 1[ffSB(i1,...it)=1])
EVENT REDUNDANCY Events should not significantly overlap:
i, j {1, . . . , |Z|}, k, l I s.t.
((yk = yl) (yk! = NULL) (zk = i) (zl = j) (xk! = xl))
Record Coherence
FIELD SPARSITY Some fields take a single unique value per record:
K, L I, C , ((YK = C) (YL = C) (ZK = ZL)) (XK = XL)
RECORD DENSITY Words associated with a field should fill the field if it is otherwise empty:
i , C , (k I s.t. (1[Cind(xk)=1]) (zk = i)) (l I s.t. (yl = C) (zl = i))
</construct>
<tableCaption confidence="0.905021">
Table 1: Logical formulations of the properties encouraged by the global potentials. Si is the set of indexes corre-
</tableCaption>
<bodyText confidence="0.99346874074074">
sponding the the ith sentence. fME(Su) = 1 iff there is no event change in sentence Su. fsSB(i1, . . . , it) = 1 iff the
corresponding words appear in the same sentence. ffSB(i1, . . . , it) = 1 iff the corresponding words have different,
non-NULL, field values. Cind(xk) = 1 iff xk is assigned to C in a training event record. CENT RAL is the central
event of the document, defined to be its first event. I = {1, . . ., n}, = {1, . . . |Y |}, = {1, . . ., |Z|}.
group together sentences that describe the same
event. The local boundary model can only predict
contiguous blocks of event descriptions, but it can-
not link together blocks that appear in different parts
of the document. Our approach towards this task
is informed by regularity in the discourse organiza-
tion of news articles. A typical news story is de-
voted to a single event, mixed with short descrip-
tions of other events. Therefore, we prefer event as-
signments where long segments with no field values
e.g., background descriptions are associated with
the main event. This intuition is formalized in the
Main Event Potential shown in Table 1.
The second discourse constraint concerns detec-
tion of event boundaries. We prefer assignments in
which the boundary sentence contains a large num-
ber of fields. This preference is expressed in the Seg-
ment Boundary Potential shown in Table 1.
The final discourse constraint favors assignments
that reduce redundancy in generated records. It is
unlikely that a document describes several events
with significant factual overlap. This constraint
is implemented in the Event Redundancy Potential
shown in Table 1.
Record Coherence Potentials These potentials
capture properties of valid field assignments in the
context of a given event record. The first potential
in this group Field Sparsity Potential is ap-
plied to fields, such as City, that tend to take a single
unique value per event record.1 This potential dis-
courages assignments that link this field with multi-
ple values within the same event. Similar constraints
have been effectively used in information extraction
in the past (Finkel et al., 2005). In our work, we ap-
ply this constraint at the event level, rather than at
the document level, thereby enabling multiple vari-
able values for multi-event documents.
The second record coherence potential Record
Density Potential aims to reduce empty fields in
the event record. This potential turns on when a lo-
cal extractor fails to identify a filler for a field when
processing a given event segment. If this segment
contains words that are labeled as potential fillers in
the context of other events in the training data, we
prefer assignments that associate them with the field
that otherwise would have been empty. This poten-
tial is inspired by the one sense per discourse con-
straint (Gale et al., 1992) that associates all the oc-
currences of the word in a document with the same
semantic meaning.
</bodyText>
<page confidence="0.860301">
1
</page>
<bodyText confidence="0.913326">
The potential is defined for the following fields: Terrorist
Organization, Weapon, City, and Country.
</bodyText>
<page confidence="0.998487">
73
</page>
<sectionHeader confidence="0.905535" genericHeader="method">
\x0c4 Inference
</sectionHeader>
<bodyText confidence="0.990055807692308">
Dual Decomposition The global potentials encode
important document level information that links to-
gether the extracted event records and their fields.
Introducing these potentials, however, greatly com-
plicates inference. Consider the MAP equation of
Section 3. If the intersection between each pair of
subsets, fi, fj F, had been empty, we could have
found the MAP assignment by solving each poten-
tial separately. However, since many subset pairs do
overlap, we must enforce agreement among the as-
signments which results in an NP-hard problem.
In order to avoid this computational bottleneck we
turn to dual-decomposition (Rush et al., 2010; Koo
et al., 2010), an inference technique that enables ef-
ficient computation of a tight upper bound on the
MAP objective, while preserving the original depen-
dencies of the model. Dual decomposition has been
recently applied to a joint model for biomedical en-
tity and event extraction by Riedel and McCallum
(2011). In their work, however, events are defined in
the sentence level. Here we show how this technique
can be applied to a model which involves document-
level potentials.
We first re-write the MAP equation, such that it
contains a local potential for each of the unobserved
variables, as required by the inference algorithm:
</bodyText>
<equation confidence="0.999289625">
MAP() = max
y,z
X
jJ
j(rj) +
X
fF
f (rf )
</equation>
<bodyText confidence="0.997806666666667">
where we denote the set of indexes of all unob-
served variables with J and refer to each of them
with rj. We then define the dual problem:
</bodyText>
<equation confidence="0.985391166666667">
min
L(), L() =
X
jJ
max
rj
[j (rj) +
X
f:jf
fj(rj)]+
X
fF
max
rf
[f (rf )
X
jf
fj(rj)]
</equation>
<bodyText confidence="0.965931266666667">
where for every f F and j f, fj is a vector of
Lagrange multipliers with an entry for each possi-
ble assignment of rj. We add the notation f for the
matrix of Lagrange multipliers for all the variables
in f, and for an assignment M of the variables in f
we define f (M) to be the corresponding vector of
Lagrange multipliers. The multipliers can be viewed
as messages transferred between the potentials to en-
courage agreement between their assignments.
The dual objective, L(), forms an upper bound
on the MAP objective. Our inference algorithm
Set g0
fj 0 for all j J, f F
for k = 1 to K do
for j J do
</bodyText>
<equation confidence="0.959454625">
rlk
j = arg max
rj
[j(rj ) +
X
f:jf
fj(rj )]
end TRUE
for f F do
rpk
f = arg max
rf
[f (rf )
X
jf
fj(rj )]
for j f do
if rlk
j 6= rpk
fj then
gk
fj(rlk
j ) + = 1
gk
fj(rpk
fj) = 1
end F ALSE
k+1
fj = k
fj k gk
fj
if end then
return Rk
k 1/k
return (RK )
(a)
rlk
j : Sort [j(rj ) +
X
f:jf
fj(rj )]. Return the minimizing rj .
rpk
f :
MMAk
f : Minimum-Message assignment
P RAk
f : Property-Respecting assignment
if (p sum(f (P RA)) &gt; (1) sum(f (MMA)) then
rpk
f = P RAk
f
else
rpk
f = MMAk
f
(b)
</equation>
<figureCaption confidence="0.997114">
Figure 2: The inference algorithm. (a): The dual-
</figureCaption>
<bodyText confidence="0.944687952380952">
decomposition algorithm. (b): Algorithms for the
arg max operations of the dual-decomposition algorithm.
therefore searches for its minimum, i.e. the tightest
upper bound of the original MAP objective. L() is
convex and non-differentiable and can therefore be
minimized by the subgradient descent algorithm in
Figure 2 (a).
Individual Potentials Maximization The inference
algorithm requires efficient solvers for its arg max
problems. For the field labeling and event segmen-
tation potentials, the messages are encoded into the
feature space of the CRF, and exact maximization is
achieved through standard CRF decoding. For the
local potentials, (rlk
j ), the maximizing assignments
are computed by sorting the messages for each un-
observed variable (Figure 2 (b)).
The global potentials are more challenging. Ide-
ally, we could find the optimal assignment, rp
f , that
agrees with the assignments of the other potentials
</bodyText>
<equation confidence="0.998037666666667">
( rp
f = argmin
P
</equation>
<bodyText confidence="0.9594875">
jf fj(rpj)) and at the same
time respects the property encouraged by its own po-
</bodyText>
<page confidence="0.979214">
74
</page>
<bodyText confidence="0.989975490909091">
\x0ctential (p(rp
f ) &gt; 0). In practice, however, there
may be no such assignment, in which case the as-
signment conflict needs to be resolved.
We first compute the minimum-message assign-
ment (MMA), the assignment that minimizes the
message sum. If this assignment respects the poten-
tial property then it is the optimal assignment. Oth-
erwise, we compute the property-respecting assign-
ment (PRA), the assignment with the (approximate)
lowest message sum under the condition that the po-
tential property holds. From these two assignments
we select the one with the higher score.
Finding the MMA is simple, as it is the minimum-
message assignment of each unobserved variable
separately. However, finding the global optimal
PRA is computationally demanding, as it requires
searching over a very large assignment space. We
therefore trade accuracy for efficiency and restrict
each potential to modify the MMA assignment for
only one type of variables: Y (fields) or Z (events).
The discourse potentials and the FIELD SPARSITY
potential are restricted to changes of the event vari-
ables, while the RECORD DENSITY potential is re-
stricted to changes of the field variables.
For the MAIN EVENT potential, consecutive sen-
tences with no fields trigger a return to the main
event. For the SEGMENT BOUNDARY potential,
event changes that take place in sentences with a
small number of fields are removed. For our work,
this threshold is set to three. For the EVENT RE-
DUNDANCY potential, redundant events are inte-
grated with the largest event in which they are con-
tained. For the RECORD DENSITY potential, words
seen in both training records and event text are used
to fill empty fields. For each empty field in each
event, words labeled with event are scanned for can-
didate fillers, and those with the minimal impact on
the message sum are assigned to that field.
Finally, for the FIELD SPARSITY potential, if a
field contains more than one word or phrase per
event, the event assignments of these words or
phrases are recomputed. This computation is imple-
mented as a minimum matching problem in a bipar-
tite graph. One side of the graph consists of a vertex
for every word or phrase assigned to the addressed
field, and the other side consists of one vertex for
each event in the document. If the number of phrases
assigned to the field is larger than the number of
events in the document, some of the event vertices
will be assigned to new events. The edge weights
are the sum of message changes corresponding to
relabeling the word or phrase with the new event.
We solve this problem efficiently (O(n3)) using the
Kuhn-Munkres algorithm (Kuhn, 1955).
</bodyText>
<sectionHeader confidence="0.998449" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.997573076923077">
Data This work focuses on multi-event extraction.
While some of the articles in the MUC test corpus
do have multiple events, the majority contain only
one (77.5%) or two (12%). We therefore created two
corpora for our experiments. The first is a new cor-
pus of 70 articles from New York Times (NYT) LDC
corpus, each describing one or more terrorist events
from various parts of the world. The second, also of
70 articles, consists of a subset of the MUC articles
that describe more than one event. We stripped this
corpus from the MUC annotation and annotated it
according to our scheme.
Annotations were provided by two annotators
with graduate school educations. Every word was
tagged with a field and an event id. The 8 fields
we use are: Terrorist Organization, Target, Tactic,
Weapon, Fatalities, Injuries, Country and City.
We compared the agreement between annotators
on 10 articles by computing the percentage of words
for which the annotators gave the same labeling.
The inter-annotator agreement was 90.9% (kappa =
0.9) when fields and events are evaluated together
(i.e., the annotators are considered to agree only
when they assign the same field and event id to the
word), 97.8% (kappa = 0.97) for events only, and
92% (kappa = 0.91) for fields only.
The two corpora differ from each other with re-
spect to several important properties. The New-York
Times articles are longer (40.3 compared to 12.4
sentences per article) and describe a larger number
of events (4.4 compared to 3.1 events per article on
average). In addition, while our hypothesis about
the predominance of the main (first) event cover-
age holds for both corpora, it better characterizes the
New-York Times corpus, as is demonstrated by the
following two statistics.
First, in the NYT corpus the average number of
sentences containing field fillers for the main event
is 14.7, while for any other event the average number
</bodyText>
<page confidence="0.989418">
75
</page>
<bodyText confidence="0.996618766666667">
\x0cis 3.2. In the MUC corpus the corresponding num-
bers are 5.3 and 2.0. Second, in the NYT corpus
the number of times an article goes back to a pre-
viously described event is 182 (average of 2.6 times
per article), of which 154 (84.6%) are transitions to
the main event. In the MUC corpus the number of
times an article goes back to a previously described
event is only 38 (average of 0.54 times per article),
but, similarly to the NYT, in as much as 32 (84.2%)
of these cases the transitions are to the main event.
Experimental Setup For both corpora, we used 30
articles for training (1218 sentences in NYT, 423 in
MUC), 7 articles for development (358 sentences in
NYT, 79 in MUC) and 33 articles for test (1244 sen-
tences in NYT, 367 in MUC). The sentences were
POS tagged with the MXPOST tagger (Ratnaparkhi,
1996) and parsed with the Charniak parser (Char-
niak and Johnson, 2005).
We trained our model with a two steps procedure.
First, the local CRFs were separately trained on the
training articles. Then, we trained the parameters of
the global potentials using the structured perceptron
algorithm (Collins, 2002) on the development data.
We perform joint inference over the local CRFs
as well as the global potentials with dual decompo-
sition. This algorithm is guaranteed to give the MAP
assignment if it converges to a solution in which all
the potentials agree on the label assignment for the
variables in their scope. To deal with disagreements,
we ran the algorithm for 200 iterations past the point
of fluctuations around the dual minimum. The final
label assignment is determined by a majority vote
between the potentials in the 10 iterations with the
highest total inter potential agreement (Sontag et al.,
2010).
Baselines We compare our algorithm to two base-
line models. The first baseline is related to previous
techniques that decompose the task into field extrac-
tion and event segmentation sub-tasks (Jean-Louis
et al., 2011; Patwardhan and Riloff, 2007; Patward-
han and Riloff, 2009). For this PIPELINE baseline,
we run the CRF models described in Section 3.1,
first the field CRF and then the event CRF. The field-
based features of the event CRF are extracted from
the output of the field CRF.
Our model incorporates global dependencies into
a document level model. An alternative approach is
to encode this information as local features that re-
flect global dependencies (Liang et al., 2008). We
therefore constructed a second baseline, the bidirec-
tional pipeline model (BI-PIPELINE), that considers
global features which encode similar properties to
those encouraged by our global potentials. We im-
plement this by incorporating event-based features
into the feature set of the field labeling CRF, while
kipping the event segmentation CRF fixed. 2 As in
the pipeline model, each CRF is trained separately
on the training data. The BI-PIPELINE model, how-
ever, emulates our joint inference procedure by it-
eratively running a field labeling and an event seg-
mentation CRFs. The number of iterations for this
model was estimated on development data.
Evaluation Measures We follow the MUC-4
scoring guidelines (Chinchor, 1992). To compare
between a learned and a gold standard event, we
compute the word-level F-score between each of
their fields and average the results. If a field is empty
in both event records, it is not counted in the mutual
event score, while if it is empty in only one of the
event records, its F-score is 0.
Ideally, the measure should be able to capture
paraphrases. For example, if the Tactic field in
a gold event record contains the words bombing
and blast, the measure is expected to give a per-
fect score to a learned record that contains one of
these words. Therefore, as in the MUC-4 guidelines,
we count pre-specified synonyms and morphologi-
cal derivations of the same word only once.
For every document, we then map the learned
events to the gold events in a greedy 1-1 manner
using the Kuhn-Munkres algorithm (Kuhn, 1955).
Once we have an event mapping, we can report
an average recall, precision and F-score across the
test set for all fields, events and documents (where
the document F-score is the average F-score of its
events). We use the sign test to measure the statis-
tical significance for our results. Since the number
of events described in a document is not given to the
models as input, we also report the average ratio be-
tween the number of induced and gold events.
</bodyText>
<page confidence="0.965297">
2
</page>
<bodyText confidence="0.997647333333333">
Example additional features are: (1) whether a word with
the same most frequent field (MFF) as the encoded word previ-
ously appeared in its event; (2) whether a new event is started
in the sentence of the encoded word; and (3) whether the event
of the encoded word contains at least one word annotated with
the MFF of the encoded word.
</bodyText>
<page confidence="0.985088">
76
</page>
<table confidence="0.9973391">
\x0cNYT Documents Events Fields Event Number
R P F R P F R P F Ratio
Joint Model 38.7 42.4 38.5 36.2 40.8 36.4 43.6 49.1 43.8 0.95
Bi-pipeline Model 33.3 30.8 30.2 31.9 30.1 29.4 38.8 36.6 35.7 1.14
Pipeline Model 28.3 27.0 26.2 27.1 26.8 25.5 35.4 34.8 33.2 1.5
MUC Documents Events Fields Event Number
R P F R P F R P F Ratio
Joint Model 49.8 43.2 43.5 48.7 43.0 42.7 53.6 45.9 46.2 0.88
Bi-pipeline Model 38.1 38.6 36.3 34.3 33.9 32.2 41.5 40.5 38.6 0.92
Pipeline Model 30.8 32.8 29.7 29.9 32.0 28.9 37.9 40.1 36.6 0.89
</table>
<tableCaption confidence="0.787233">
Table 2: Performance of the joint model and the pipeline models on the event record extraction task. Top table is for
the New-York Times data. Bottom table is for the MUC data. All results are statistically significant with p &lt; 0.05.
</tableCaption>
<table confidence="0.9977145">
NYT TO TAR TAC WEAP INJ FAT CO CITY
Joint Model 21.9 23.4 49.0 39.6 40.8 49.1 43.1 46.6
Bi-pipeline Model 8.4 19.7 47.5 20.9 25.9 18.3 38.8 38.1
Pipeline Model 7.1 18.1 41.9 36.9 19.1 16.5 38.0 46.1
MUC TO TAR TAC WEAP INJ FAT CO CITY
Joint Model 49.0 25.2 63.6 62.0 43.3 21.1 19.7 38.3
Bi-pipeline Model 28.0 24.7 38.2 55.8 42.7 25.6 37.5 37.2
Pipeline Model 34.9 23.4 50.3 56.5 10.4 12.4 30.0 32.0
</table>
<tableCaption confidence="0.7649905">
Table 3: Comparison between the joint model and the pipeline models for the different fields. When the joint model is
superior results are statistically significance with p &lt; 0.05.
</tableCaption>
<table confidence="0.97197075">
(a)
NYT Fields Events
R P F GF LF
Joint model 47.3 51.3 49.2 54.8 61.3
Bi-Pipeline 31.0 43.8 36.3 48.8 56.2
Pipeline Model 39.2 55.4 45.9 51.3 52.9
(b)
MUC Fields Events
R P F GF LF
Joint model 47.3 51.3 49.2 62.8 70.0
Bi-Pipeline 49.5 36.1 41.8 62.2 62.0
Pipeline Model 31.0 43.8 36.3 65.5 70.3
</table>
<tableCaption confidence="0.999291">
Table 4: Performance of the joint and the pipeline models on the labeling tasks of assigning words to fields (left) and
</tableCaption>
<bodyText confidence="0.98988">
to events (right). Field values are computed for words tagged with the non-NULL field. Events values are computed
for words that are assigned to a non-NULL field by the gold standard (GF) or by the model (LF). When the joint model
is superior, results for fields are statistically significant with p &lt; 0.01 and for events with p &lt; 0.05.
</bodyText>
<sectionHeader confidence="0.9680355" genericHeader="method">
6 Results
Event-Records Results for event record extraction,
</sectionHeader>
<bodyText confidence="0.964130884615385">
the main task addressed in this paper, are presented
in Table 2. For all measures, the model outperforms
the pipeline baselines, with an F-score difference of
up to 13.8%.
The rightmost column of the table demonstrates
the tendency of our model to under-segment. For
both corpora our model extracts a smaller number
of events than the gold standard on average (5% for
NYT, 12% for MUC). The pipeline baselines extract
more events than our model on average. For NYT
they over-segment (14% for bi-pipeline, 53% for the
pipeline) while for MUC they under-segment (8%
and 11% respectively). These differences are ex-
pected as the baselines cannot combine different text
segments that describe the same event.
Table 3 presents per-field F-score performance.
The joint model outperforms the pipeline baselines
for 7 out of the 8 fields in the NYT experiments, and
for 6 out of 8 fields in the MUC experiments.
Model Components Table 6 presents the perfor-
mance of variants of the joint model created by ex-
cluding each potential type. The results demonstrate
the significance of both discourse and record co-
herence potentials for the performance of the full
model.
Sub-tasks Performance A model for our task
</bodyText>
<page confidence="0.455448">
77
</page>
<table confidence="0.90947995">
\x0c(a)
Gold Fields Gold Events
NYT Doc. Events Fields Ratio Doc. Events Fields
Joint
Model
69.1 62.5 64.4 1.05 45.7 46.5 50.0
Bi-
Pipeline
41.7 40.8 46.1
Pipeline 47.9 43.9 51.3 1.56 40.8 40.4 43.9
(b)
Gold Fields Gold Events
MUC Doc. Events Fields Ratio Doc. Events Fields
Joint
model
78.5 75.0 74.5 0.76 50.8 47.9 51.4
Bi-
Pipeline
- 37.0 34.3 39.9
Pipeline 76.1 71.1 72.0 0.78 32.6 31.2 36.0
</table>
<tableCaption confidence="0.967081">
Table 5: Performance of the joint model and the pipeline models when the gold standard for one of the labeling tasks
</tableCaption>
<table confidence="0.960517555555556">
is given at test time. Results are statistically significant with p &lt; 0.05.
NYT
Excluded Component Documents Events Fields Event
Rat.
Record Coherence 32.1 31.0 37.7 1.04
Discourse 26.7 26.3 34.3 1.5
MUC
Record Coherence 37.4 33.6 39.6 0.88
Discourse 37.7 36.6 42.7 0.89
</table>
<tableCaption confidence="0.999493">
Table 6: The effect of the record coherence potentials and
</tableCaption>
<bodyText confidence="0.997925914893617">
of the discourse potentials on the performance of the joint
model. Results are presented for F-scores, each line is for
the full model when potentials of one type are excluded.
should determine both when a word is a good field
filler and to which event the field belongs. Since
our main evaluation collapses the effect of these de-
cisions together, we performed two additional sets
of experiments to analyze the models accuracy on
each sub-task separately.
Figure 4 presents the performance of the different
models on the labeling tasks of assigning words to
fields and to events. The number of words associated
with a field differs between the gold standard and
the models output. For fields, we therefore report
word level recall, precision and F-score between the
set of words assigned a non-NULL field by a model
and the corresponding gold standard set. For events,
we compute the fraction of words assigned the cor-
rect event among the words assigned to a non-NULL
field in either the gold standard or the output of the
model.
Figure 5 presents the document F-score when the
gold-standard fields (left) or events (right) of the test
set are known at test time. Note that when the gold
standard fields are known, the BI-PIPELINE model
is not applicable anymore since it is designed to
improve field assignment using event-informed fea-
tures. The results demonstrate that encoding field
information to the models is more valuable than en-
coding information about events. This provides us
with an important direction for future improvement
of our model.
Accuracy and Efficiency When we ran our algo-
rithm on the joint task of the NYT data-set it con-
verged after 89 iterations. For the MUC joint task
and the ablation analysis experiments we ran the al-
gorithm for 200 iterations past the point of fluctua-
tions around the dual minimum.
On a 2GHz CPU, 2GB RAM machine, it took
our dual-decomposition algorithm 15 minutes and
10 seconds to complete its run on the entire NYT test
set. For the MUC joint task experiment, in the 10
iterations considered for the majority vote, there is
full agreement between the potentials for 97.77% of
the unobserved variables. That is, the voting scheme
affects the assignment of only 2.23% of the unob-
served variables.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.993114571428571">
In this paper we presented a joint model for identify-
ing fields of information and aggregating them into
event records. We experimented with two data sets
of newspaper articles containing multiple event de-
scriptions. Our results demonstrate the importance
and effectiveness of global constraints for event
record extraction.
</bodyText>
<sectionHeader confidence="0.96283" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.994639333333333">
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C0172. Any opin-
ions, findings and conclusions expressed in the ma-
terial are those of the author(s) and do not neces-
sarily reflect the views of DARPA, AFRL or the US
government. Thanks also to the members of the MIT
NLP group and to Amir Globerson for their sugges-
tions and comments.
</bodyText>
<page confidence="0.988156">
78
</page>
<reference confidence="0.998225515789474">
\x0cReferences
Kedar Bellare and Andrew McCallum. 2009. General-
ized expectation criteria for bootstrapping extractors
using record-text alignment. In EMNLP.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
ACL.
Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007.
Guiding semi-supervision with constraint driven learn-
ing. In ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In ACL.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In ACL.
Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee.
2003. Closing the gap: Learning-based information
extraction rivaling knowledge-engineering methods.
In ACL.
Nancy Chinchor, David Lewis, and Lynette Hirschman.
1993. Evaluating message understanding systems: an
analysis of the third message understanding confer-
ence. Computational Linguistics, 19(3):409449.
Nancy Chinchor. 1992. Muc-4 evaluation metrics. In
Fourth Message Understanding Conference (MUC-4).
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In EMNLP.
Gregory Druck and Andrew McCallum. 2010. High-
performance semi-supervised learning using discrimi-
natively constrained generative models. In ICML.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In
NAACL.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In ACL.
William Gale, Kenneth Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings of the
4th DARPA Speech and Natural Language Workshop.
Ludovic Jean-Louis, Romaric Besancon, and Olivier Fer-
ret. 2011. Text segmentation and graph-based meth-
ods for template filling in information extraction. In
IJCNLP.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
EMNLP.
Harold W. Kuhn. 1955. The hungarian method for the
assignment problem. Naval Research Logistics Quar-
terly, 2:8397.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In ICML.
Percy Liang, Hal Daume, and Dan Klein. 2008. Struc-
ture compilation: trading structure for features. In
ICML.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extraction
from free text. In ACL.
Siddharth Patwardhan and Ellen Riloff. 2007. Effective
ie with semantic affinity patterns and relevant regions.
In EMNLP.
Siddharth Patwardhan and Ellen Riloff. 2009. A unified
model of phrasal and sentential evidence for informa-
tion extraction. In EMNLP.
Adwait Ratnaparkhi. 1996. A maximum entropy part-
of-speech tagger. In WVLC.
Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and
Lois Childs. 1992. Muc-4 test results and analysis.
In Fourth Message Understanding Conference (MUC-
4).
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
EMNLP.
Dan Roth and Wen tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In CoNLL.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In EMNLP.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2010. Introduction to dual decomposition for infer-
ence. In Optimization for Machine Learning, editors
S. Sra, S. Nowozin, and S. J. Wright: MIT Press.
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules for
weakly supervised information extraction. In COL-
ING.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction-
without labelled data. In EMNLP.
</reference>
<page confidence="0.959952">
79
</page>
<figure confidence="0.265163">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.729026">
<note confidence="0.872472">b&amp;apos;2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 7079, Montreal, Canada, June 3-8, 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.992257">Multi Event Extraction Guided by Global Constraints</title>
<author confidence="0.999875">Roi Reichart Regina Barzilay</author>
<affiliation confidence="0.999972">Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology</affiliation>
<email confidence="0.998965">roiri@csail.mit.edu</email>
<email confidence="0.998965">regina@csail.mit.edu</email>
<abstract confidence="0.998965">This paper addresses the extraction of event records from documents that describe multiple events. Specifically, we aim to identify the fields of information contained in a document and aggregate together those fields that describe the same event. To exploit the inherent connections between field extraction and event identification, we propose to model them jointly. Our model is novel in that it integrates information from separate sequential models, using global potentials that encourage the extracted event records to have desired properties. While the model contains high-order potentials, efficient approximate inference can be performed with dualdecomposition. We experiment with two data sets that consist of newspaper articles describing multiple terrorism events, and show that our model substantially outperforms traditional pipeline models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Kedar Bellare</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for bootstrapping extractors using record-text alignment.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="10658" citStr="Bellare and McCallum, 2009" startWordPosition="1676" endWordPosition="1679"> consistency. These potentials are defined over the entire set of variables related to a document. The resulting maximum aposteriori problem is: MAP() = X fF f (rf ) where f are the potential functions and {rf |f {1, . . . , n}, f F} is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these components are given by the likelih</context>
</contexts>
<marker>Bellare, McCallum, 2009</marker>
<rawString>\x0cReferences Kedar Bellare and Andrew McCallum. 2009. Generalized expectation criteria for bootstrapping extractors using record-text alignment. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6704" citStr="Chambers and Jurafsky, 2011" startWordPosition="1033" endWordPosition="1036">hich decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for relations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task. Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck an</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming-Wei Chang</author>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Guiding semi-supervision with constraint driven learning.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7294" citStr="Chang et al., 2007" startWordPosition="1125" endWordPosition="1128">Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task. Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck and McCallum, 2010). Constraints have been explored both at sentence and document level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. In our work we focus on document-level constraints. We utilize both discourse and record-coherence constraints to encourage consistency between local sequence models. There has also be</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2007</marker>
<rawString>Ming-Wei Chang, Lev Ratinov, and Dan Roth. 2007. Guiding semi-supervision with constraint driven learning. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="25372" citStr="Charniak and Johnson, 2005" startWordPosition="4247" endWordPosition="4251">he main event. In the MUC corpus the number of times an article goes back to a previously described event is only 38 (average of 0.54 times per article), but, similarly to the NYT, in as much as 32 (84.2%) of these cases the transitions are to the main event. Experimental Setup For both corpora, we used 30 articles for training (1218 sentences in NYT, 423 in MUC), 7 articles for development (358 sentences in NYT, 79 in MUC) and 33 articles for test (1244 sentences in NYT, 367 in MUC). The sentences were POS tagged with the MXPOST tagger (Ratnaparkhi, 1996) and parsed with the Charniak parser (Charniak and Johnson, 2005). We trained our model with a two steps procedure. First, the local CRFs were separately trained on the training articles. Then, we trained the parameters of the global potentials using the structured perceptron algorithm (Collins, 2002) on the development data. We perform joint inference over the local CRFs as well as the global potentials with dual decomposition. This algorithm is guaranteed to give the MAP assignment if it converges to a solution in which all the potentials agree on the label assignment for the variables in their scope. To deal with disagreements, we ran the algorithm for 2</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harr Chen</author>
<author>Edward Benson</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>In-domain relation discovery with meta-constraints via posterior regularization.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="7995" citStr="Chen et al., 2011" startWordPosition="1226" endWordPosition="1229">cument level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. In our work we focus on document-level constraints. We utilize both discourse and record-coherence constraints to encourage consistency between local sequence models. There has also been unsupervised work that demonstrates the benefit of domain-specific constraints (Chen et al., 2011). In our work we show that domain-specific constraints based on the common structure of newspaper articles are also useful to guide a supervised model. 71 \x0c3 Model Problem Formulation Given a document, our goal is to extract field values and aggregate them into event records. The training data consists of event annotations where each word in the document is tagged with a field and with an event id. If a word is not a filler for a field, it is annotated with a default NULL field value. At test time, the number of events is not given and has to be inferred from the data. Model Structure Our m</context>
</contexts>
<marker>Chen, Benson, Naseem, Barzilay, 2011</marker>
<rawString>Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay. 2011. In-domain relation discovery with meta-constraints via posterior regularization. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
<author>Yoong Keok Lee</author>
</authors>
<title>Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6430" citStr="Chieu et al., 2003" startWordPosition="993" endWordPosition="996">ontext of template extraction and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for relations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. T</context>
</contexts>
<marker>Chieu, Ng, Lee, 2003</marker>
<rawString>Hai Leong Chieu, Hwee Tou Ng, and Yoong Keok Lee. 2003. Closing the gap: Learning-based information extraction rivaling knowledge-engineering methods. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
<author>David Lewis</author>
<author>Lynette Hirschman</author>
</authors>
<title>Evaluating message understanding systems: an analysis of the third message understanding conference.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="6246" citStr="Chinchor et al., 1993" startWordPosition="965" endWordPosition="968">ant margin. For instance, it yields an absolute gain of 8.5% for our new corpus when measured using document-level Fscore. Our results show the effectiveness of global constraints in the context of template extraction and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for relations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also a</context>
</contexts>
<marker>Chinchor, Lewis, Hirschman, 1993</marker>
<rawString>Nancy Chinchor, David Lewis, and Lynette Hirschman. 1993. Evaluating message understanding systems: an analysis of the third message understanding conference. Computational Linguistics, 19(3):409449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Muc-4 evaluation metrics.</title>
<date>1992</date>
<booktitle>In Fourth Message Understanding Conference (MUC-4).</booktitle>
<contexts>
<context position="27629" citStr="Chinchor, 1992" startWordPosition="4610" endWordPosition="4611">l features which encode similar properties to those encouraged by our global potentials. We implement this by incorporating event-based features into the feature set of the field labeling CRF, while kipping the event segmentation CRF fixed. 2 As in the pipeline model, each CRF is trained separately on the training data. The BI-PIPELINE model, however, emulates our joint inference procedure by iteratively running a field labeling and an event segmentation CRFs. The number of iterations for this model was estimated on development data. Evaluation Measures We follow the MUC-4 scoring guidelines (Chinchor, 1992). To compare between a learned and a gold standard event, we compute the word-level F-score between each of their fields and average the results. If a field is empty in both event records, it is not counted in the mutual event score, while if it is empty in only one of the event records, its F-score is 0. Ideally, the measure should be able to capture paraphrases. For example, if the Tactic field in a gold event record contains the words bombing and blast, the measure is expected to give a perfect score to a learned record that contains one of these words. Therefore, as in the MUC-4 guidelines</context>
</contexts>
<marker>Chinchor, 1992</marker>
<rawString>Nancy Chinchor. 1992. Muc-4 evaluation metrics. In Fourth Message Understanding Conference (MUC-4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="25609" citStr="Collins, 2002" startWordPosition="4286" endWordPosition="4287">in event. Experimental Setup For both corpora, we used 30 articles for training (1218 sentences in NYT, 423 in MUC), 7 articles for development (358 sentences in NYT, 79 in MUC) and 33 articles for test (1244 sentences in NYT, 367 in MUC). The sentences were POS tagged with the MXPOST tagger (Ratnaparkhi, 1996) and parsed with the Charniak parser (Charniak and Johnson, 2005). We trained our model with a two steps procedure. First, the local CRFs were separately trained on the training articles. Then, we trained the parameters of the global potentials using the structured perceptron algorithm (Collins, 2002) on the development data. We perform joint inference over the local CRFs as well as the global potentials with dual decomposition. This algorithm is guaranteed to give the MAP assignment if it converges to a solution in which all the potentials agree on the label assignment for the variables in their scope. To deal with disagreements, we ran the algorithm for 200 iterations past the point of fluctuations around the dual minimum. The final label assignment is determined by a majority vote between the potentials in the 10 iterations with the highest total inter potential agreement (Sontag et al.</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Druck</author>
<author>Andrew McCallum</author>
</authors>
<title>Highperformance semi-supervised learning using discriminatively constrained generative models.</title>
<date>2010</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7321" citStr="Druck and McCallum, 2010" startWordPosition="1129" endWordPosition="1133">y, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task. Global Constraints Previous work demonstrated the benefits of applying declarative constraints in information extraction (Finkel et al., 2005; Roth and tau Yih, 2004; Chang et al., 2007; Druck and McCallum, 2010). Constraints have been explored both at sentence and document level. For example, Finkel et al. (2005) employ document-level constraints to encourage global consistency of named entity assignments. Likewise, Chang et al. (2007) use constraints at multiple levels, such as sentence-level constraints to specify field boundaries and global constraints to ensure relation-level consistency. In our work we focus on document-level constraints. We utilize both discourse and record-coherence constraints to encourage consistency between local sequence models. There has also been unsupervised work that d</context>
</contexts>
<marker>Druck, McCallum, 2010</marker>
<rawString>Gregory Druck and Andrew McCallum. 2010. Highperformance semi-supervised learning using discriminatively constrained generative models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint parsing and named entity recognition.</title>
<date>2009</date>
<booktitle>In NAACL. Jenny Rose Finkel, Trond Grenager, and</booktitle>
<contexts>
<context position="10630" citStr="Finkel and Manning, 2009" startWordPosition="1672" endWordPosition="1675">fields to encourage global consistency. These potentials are defined over the entire set of variables related to a document. The resulting maximum aposteriori problem is: MAP() = X fF f (rf ) where f are the potential functions and {rf |f {1, . . . , n}, f F} is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these compone</context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny Rose Finkel and Christopher D. Manning. 2009. Joint parsing and named entity recognition. In NAACL. Jenny Rose Finkel, Trond Grenager, and Christopher D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<marker>Manning, 2005</marker>
<rawString>Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the 4th DARPA Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="16013" citStr="Gale et al., 1992" startWordPosition="2600" endWordPosition="2603">level, thereby enabling multiple variable values for multi-event documents. The second record coherence potential Record Density Potential aims to reduce empty fields in the event record. This potential turns on when a local extractor fails to identify a filler for a field when processing a given event segment. If this segment contains words that are labeled as potential fillers in the context of other events in the training data, we prefer assignments that associate them with the field that otherwise would have been empty. This potential is inspired by the one sense per discourse constraint (Gale et al., 1992) that associates all the occurrences of the word in a document with the same semantic meaning. 1 The potential is defined for the following fields: Terrorist Organization, Weapon, City, and Country. 73 \x0c4 Inference Dual Decomposition The global potentials encode important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi, fj F, had been empty, we could have found the MAP assignment by sol</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the 4th DARPA Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Jean-Louis</author>
<author>Romaric Besancon</author>
<author>Olivier Ferret</author>
</authors>
<title>Text segmentation and graph-based methods for template filling in information extraction.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<contexts>
<context position="26433" citStr="Jean-Louis et al., 2011" startWordPosition="4419" endWordPosition="4422">s to a solution in which all the potentials agree on the label assignment for the variables in their scope. To deal with disagreements, we ran the algorithm for 200 iterations past the point of fluctuations around the dual minimum. The final label assignment is determined by a majority vote between the potentials in the 10 iterations with the highest total inter potential agreement (Sontag et al., 2010). Baselines We compare our algorithm to two baseline models. The first baseline is related to previous techniques that decompose the task into field extraction and event segmentation sub-tasks (Jean-Louis et al., 2011; Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009). For this PIPELINE baseline, we run the CRF models described in Section 3.1, first the field CRF and then the event CRF. The fieldbased features of the event CRF are extracted from the output of the field CRF. Our model incorporates global dependencies into a document level model. An alternative approach is to encode this information as local features that reflect global dependencies (Liang et al., 2008). We therefore constructed a second baseline, the bidirectional pipeline model (BI-PIPELINE), that considers global features which en</context>
</contexts>
<marker>Jean-Louis, Besancon, Ferret, 2011</marker>
<rawString>Ludovic Jean-Louis, Romaric Besancon, and Olivier Ferret. 2011. Text segmentation and graph-based methods for template filling in information extraction. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16890" citStr="Koo et al., 2010" startWordPosition="2738" endWordPosition="2741">ncode important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi, fj F, had been empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as required by the infe</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold W Kuhn</author>
</authors>
<title>The hungarian method for the assignment problem.</title>
<date>1955</date>
<journal>Naval Research Logistics Quarterly,</journal>
<pages>2--8397</pages>
<contexts>
<context position="22571" citStr="Kuhn, 1955" startWordPosition="3768" endWordPosition="3769">utation is implemented as a minimum matching problem in a bipartite graph. One side of the graph consists of a vertex for every word or phrase assigned to the addressed field, and the other side consists of one vertex for each event in the document. If the number of phrases assigned to the field is larger than the number of events in the document, some of the event vertices will be assigned to new events. The edge weights are the sum of message changes corresponding to relabeling the word or phrase with the new event. We solve this problem efficiently (O(n3)) using the Kuhn-Munkres algorithm (Kuhn, 1955). 5 Experiments Data This work focuses on multi-event extraction. While some of the articles in the MUC test corpus do have multiple events, the majority contain only one (77.5%) or two (12%). We therefore created two corpora for our experiments. The first is a new corpus of 70 articles from New York Times (NYT) LDC corpus, each describing one or more terrorist events from various parts of the world. The second, also of 70 articles, consists of a subset of the MUC articles that describe more than one event. We stripped this corpus from the MUC annotation and annotated it according to our schem</context>
<context position="28459" citStr="Kuhn, 1955" startWordPosition="4758" endWordPosition="4759">l event score, while if it is empty in only one of the event records, its F-score is 0. Ideally, the measure should be able to capture paraphrases. For example, if the Tactic field in a gold event record contains the words bombing and blast, the measure is expected to give a perfect score to a learned record that contains one of these words. Therefore, as in the MUC-4 guidelines, we count pre-specified synonyms and morphological derivations of the same word only once. For every document, we then map the learned events to the gold events in a greedy 1-1 manner using the Kuhn-Munkres algorithm (Kuhn, 1955). Once we have an event mapping, we can report an average recall, precision and F-score across the test set for all fields, events and documents (where the document F-score is the average F-score of its events). We use the sign test to measure the statistical significance for our results. Since the number of events described in a document is not given to the models as input, we also report the average ratio between the number of induced and gold events. 2 Example additional features are: (1) whether a word with the same most frequent field (MFF) as the encoded word previously appeared in its e</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Harold W. Kuhn. 1955. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2:8397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="10513" citStr="Lafferty et al., 2001" startWordPosition="1654" endWordPosition="1657">cal surroundings of a candidate boundary. Global Consistency Potentials link the extracted event records and their fields to encourage global consistency. These potentials are defined over the entire set of variables related to a document. The resulting maximum aposteriori problem is: MAP() = X fF f (rf ) where f are the potential functions and {rf |f {1, . . . , n}, f F} is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Hal Daume</author>
<author>Dan Klein</author>
</authors>
<title>Structure compilation: trading structure for features.</title>
<date>2008</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="26900" citStr="Liang et al., 2008" startWordPosition="4497" endWordPosition="4500">first baseline is related to previous techniques that decompose the task into field extraction and event segmentation sub-tasks (Jean-Louis et al., 2011; Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009). For this PIPELINE baseline, we run the CRF models described in Section 3.1, first the field CRF and then the event CRF. The fieldbased features of the event CRF are extracted from the output of the field CRF. Our model incorporates global dependencies into a document level model. An alternative approach is to encode this information as local features that reflect global dependencies (Liang et al., 2008). We therefore constructed a second baseline, the bidirectional pipeline model (BI-PIPELINE), that considers global features which encode similar properties to those encouraged by our global potentials. We implement this by incorporating event-based features into the feature set of the field labeling CRF, while kipping the event segmentation CRF fixed. 2 As in the pipeline model, each CRF is trained separately on the training data. The BI-PIPELINE model, however, emulates our joint inference procedure by iteratively running a field labeling and an event segmentation CRFs. The number of iterati</context>
</contexts>
<marker>Liang, Daume, Klein, 2008</marker>
<rawString>Percy Liang, Hal Daume, and Dan Klein. 2008. Structure compilation: trading structure for features. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mstislav Maslennikov</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A multi-resolution framework for information extraction from free text.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6477" citStr="Maslennikov and Chua, 2007" startWordPosition="1001" endWordPosition="1004">ate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for relations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a line</context>
</contexts>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>Mstislav Maslennikov and Tat-Seng Chua. 2007. A multi-resolution framework for information extraction from free text. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>Effective ie with semantic affinity patterns and relevant regions.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="26462" citStr="Patwardhan and Riloff, 2007" startWordPosition="4423" endWordPosition="4426">all the potentials agree on the label assignment for the variables in their scope. To deal with disagreements, we ran the algorithm for 200 iterations past the point of fluctuations around the dual minimum. The final label assignment is determined by a majority vote between the potentials in the 10 iterations with the highest total inter potential agreement (Sontag et al., 2010). Baselines We compare our algorithm to two baseline models. The first baseline is related to previous techniques that decompose the task into field extraction and event segmentation sub-tasks (Jean-Louis et al., 2011; Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009). For this PIPELINE baseline, we run the CRF models described in Section 3.1, first the field CRF and then the event CRF. The fieldbased features of the event CRF are extracted from the output of the field CRF. Our model incorporates global dependencies into a document level model. An alternative approach is to encode this information as local features that reflect global dependencies (Liang et al., 2008). We therefore constructed a second baseline, the bidirectional pipeline model (BI-PIPELINE), that considers global features which encode similar properties to th</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2007. Effective ie with semantic affinity patterns and relevant regions. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ellen Riloff</author>
</authors>
<title>A unified model of phrasal and sentential evidence for information extraction.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6507" citStr="Patwardhan and Riloff, 2009" startWordPosition="1005" endWordPosition="1008">er IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for relations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the joint inference, we use a linear-chain CRF for each sub-task</context>
<context position="26492" citStr="Patwardhan and Riloff, 2009" startWordPosition="4427" endWordPosition="4431">he label assignment for the variables in their scope. To deal with disagreements, we ran the algorithm for 200 iterations past the point of fluctuations around the dual minimum. The final label assignment is determined by a majority vote between the potentials in the 10 iterations with the highest total inter potential agreement (Sontag et al., 2010). Baselines We compare our algorithm to two baseline models. The first baseline is related to previous techniques that decompose the task into field extraction and event segmentation sub-tasks (Jean-Louis et al., 2011; Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009). For this PIPELINE baseline, we run the CRF models described in Section 3.1, first the field CRF and then the event CRF. The fieldbased features of the event CRF are extracted from the output of the field CRF. Our model incorporates global dependencies into a document level model. An alternative approach is to encode this information as local features that reflect global dependencies (Liang et al., 2008). We therefore constructed a second baseline, the bidirectional pipeline model (BI-PIPELINE), that considers global features which encode similar properties to those encouraged by our global p</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>Siddharth Patwardhan and Ellen Riloff. 2009. A unified model of phrasal and sentential evidence for information extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy partof-speech tagger.</title>
<date>1996</date>
<booktitle>In WVLC.</booktitle>
<contexts>
<context position="25307" citStr="Ratnaparkhi, 1996" startWordPosition="4239" endWordPosition="4240"> per article), of which 154 (84.6%) are transitions to the main event. In the MUC corpus the number of times an article goes back to a previously described event is only 38 (average of 0.54 times per article), but, similarly to the NYT, in as much as 32 (84.2%) of these cases the transitions are to the main event. Experimental Setup For both corpora, we used 30 articles for training (1218 sentences in NYT, 423 in MUC), 7 articles for development (358 sentences in NYT, 79 in MUC) and 33 articles for test (1244 sentences in NYT, 367 in MUC). The sentences were POS tagged with the MXPOST tagger (Ratnaparkhi, 1996) and parsed with the Charniak parser (Charniak and Johnson, 2005). We trained our model with a two steps procedure. First, the local CRFs were separately trained on the training articles. Then, we trained the parameters of the global potentials using the structured perceptron algorithm (Collins, 2002) on the development data. We perform joint inference over the local CRFs as well as the global potentials with dual decomposition. This algorithm is guaranteed to give the MAP assignment if it converges to a solution in which all the potentials agree on the label assignment for the variables in th</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy partof-speech tagger. In WVLC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Rau</author>
<author>George Krupka</author>
<author>Paul Jacobs</author>
<author>Ira Sider</author>
<author>Lois Childs</author>
</authors>
<title>Muc-4 test results and analysis.</title>
<date>1992</date>
<booktitle>In Fourth Message Understanding Conference (MUC4).</booktitle>
<contexts>
<context position="6222" citStr="Rau et al., 1992" startWordPosition="961" endWordPosition="964">odel by a significant margin. For instance, it yields an absolute gain of 8.5% for our new corpus when measured using document-level Fscore. Our results show the effectiveness of global constraints in the context of template extraction and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for relations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates</context>
</contexts>
<marker>Rau, Krupka, Jacobs, Sider, Childs, 1992</marker>
<rawString>Lisa Rau, George Krupka, Paul Jacobs, Ira Sider, and Lois Childs. 1992. Muc-4 test results and analysis. In Fourth Message Understanding Conference (MUC4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Fast and robust joint models for biomedical event extraction.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="17186" citStr="Riedel and McCallum (2011)" startWordPosition="2786" endWordPosition="2789"> empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as required by the inference algorithm: MAP() = max y,z X jJ j(rj) + X fF f (rf ) where we denote the set of indexes of all unobserved variables with J and refer to each of them with rj. We then define the dual problem: min L(), L() = X jJ max rj [j (rj) + X f:jf fj(rj)]+ X fF max rf [f (rf ) X jf fj(rj)] where for ev</context>
</contexts>
<marker>Riedel, McCallum, 2011</marker>
<rawString>Sebastian Riedel and Andrew McCallum. 2011. Fast and robust joint models for biomedical event extraction. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In CoNLL.</booktitle>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="16871" citStr="Rush et al., 2010" startWordPosition="2734" endWordPosition="2737">global potentials encode important document level information that links together the extracted event records and their fields. Introducing these potentials, however, greatly complicates inference. Consider the MAP equation of Section 3. If the intersection between each pair of subsets, fi, fj F, had been empty, we could have found the MAP assignment by solving each potential separately. However, since many subset pairs do overlap, we must enforce agreement among the assignments which results in an NP-hard problem. In order to avoid this computational bottleneck we turn to dual-decomposition (Rush et al., 2010; Koo et al., 2010), an inference technique that enables efficient computation of a tight upper bound on the MAP objective, while preserving the original dependencies of the model. Dual decomposition has been recently applied to a joint model for biomedical entity and event extraction by Riedel and McCallum (2011). In their work, however, events are defined in the sentence level. Here we show how this technique can be applied to a model which involves documentlevel potentials. We first re-write the MAP equation, such that it contains a local potential for each of the unobserved variables, as r</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M. Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
<author>Amir Globerson</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Introduction to dual decomposition for inference.</title>
<date>2010</date>
<booktitle>In Optimization for Machine Learning,</booktitle>
<editor>editors S. Sra, S. Nowozin, and S. J. Wright:</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="26216" citStr="Sontag et al., 2010" startWordPosition="4385" endWordPosition="4388">ollins, 2002) on the development data. We perform joint inference over the local CRFs as well as the global potentials with dual decomposition. This algorithm is guaranteed to give the MAP assignment if it converges to a solution in which all the potentials agree on the label assignment for the variables in their scope. To deal with disagreements, we ran the algorithm for 200 iterations past the point of fluctuations around the dual minimum. The final label assignment is determined by a majority vote between the potentials in the 10 iterations with the highest total inter potential agreement (Sontag et al., 2010). Baselines We compare our algorithm to two baseline models. The first baseline is related to previous techniques that decompose the task into field extraction and event segmentation sub-tasks (Jean-Louis et al., 2011; Patwardhan and Riloff, 2007; Patwardhan and Riloff, 2009). For this PIPELINE baseline, we run the CRF models described in Section 3.1, first the field CRF and then the event CRF. The fieldbased features of the event CRF are extracted from the output of the field CRF. Our model incorporates global dependencies into a document level model. An alternative approach is to encode this</context>
</contexts>
<marker>Sontag, Globerson, Jaakkola, 2010</marker>
<rawString>David Sontag, Amir Globerson, and Tommi Jaakkola. 2010. Introduction to dual decomposition for inference. In Optimization for Machine Learning, editors S. Sra, S. Nowozin, and S. J. Wright: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Xiao</author>
<author>Tat-Seng Chua</author>
<author>Hang Cui</author>
</authors>
<title>Cascading use of soft and hard matching pattern rules for weakly supervised information extraction.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="6449" citStr="Xiao et al., 2004" startWordPosition="997" endWordPosition="1000">xtraction and motivate their exploration in other IE tasks. 2 Previous Work Event-Template Extraction Event template extraction has been previously explored in the MUC-4 scenario template task. Work on this task has focused on pipeline models which decouple the task into the sub-tasks of field extraction and event-based text segmentation. For example, rule-based methods (Rau et al., 1992; Chinchor et al., 1993) identify generalizations both for single field fillers and for relations between fields and use them to fill event templates. Likewise, classifier-based algorithms (Chieu et al., 2003; Xiao et al., 2004; Maslennikov and Chua, 2007; Patwardhan and Riloff, 2009) generally train individual classifiers for each type of field and aggregate candidate fillers based on a sentential event classifier. Finally, unsupervised techniques (Chambers and Jurafsky, 2011) have combined clustering, semantic roles, and syntactic relations in order to both construct and fill event templates. In our work, we also address the sub-tasks of field extraction and event segmentation individually; however, we link them through soft global constraints and encourage consistency through joint inference. To facilitate the jo</context>
</contexts>
<marker>Xiao, Chua, Cui, 2004</marker>
<rawString>Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cascading use of soft and hard matching pattern rules for weakly supervised information extraction. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extractionwithout labelled data.</title>
<date>2010</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="10677" citStr="Yao et al., 2010" startWordPosition="1680" endWordPosition="1683">ls are defined over the entire set of variables related to a document. The resulting maximum aposteriori problem is: MAP() = X fF f (rf ) where f are the potential functions and {rf |f {1, . . . , n}, f F} is the set of their variables. 3.1 Modeling Local Dependencies Field Labeling The first step of the model is tagging the words in the input document with fields. Following traditional approaches, we employ a linearchain CRF (Lafferty et al., 2001) that operates over standard lexical, POS-based and syntactic features (Finkel et al., 2005; Finkel and Manning, 2009; Bellare and McCallum, 2009; Yao et al., 2010). Event Segmentation At the local level, event analysis involves identification of event boundaries which we model as linear segmentation. To this end, we employ a binary CRF that predicts whether a given word starts a description of a new event or continues the description of the current event, based on lexical and POS-based features. In addition, we add features obtained from the output of the field extraction CRF. These features capture the intuition that boundary sentences often contain multiple fields. The potential functions of these components are given by the likelihoods of the corresp</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extractionwithout labelled data. In EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>