<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.611071">
b&amp;apos;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 11371147,
</bodyText>
<address confidence="0.403754">
Portland, Oregon, June 19-24, 2011. c
</address>
<title confidence="0.414939">
2011 Association for Computational Linguistics
Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts
</title>
<author confidence="0.787984">
Ruihong Huang and Ellen Riloff
</author>
<affiliation confidence="0.959734">
School of Computing
University of Utah
</affiliation>
<address confidence="0.571339">
Salt Lake City, UT 84112
</address>
<email confidence="0.998489">
{huangrh,riloff}@cs.utah.edu
</email>
<sectionHeader confidence="0.990845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999798133333333">
The goal of our research is to improve
event extraction by learning to identify sec-
ondary role filler contexts in the absence
of event keywords. We propose a multi-
layered event extraction architecture that pro-
gressively zooms in on relevant informa-
tion. Our extraction model includes a docu-
ment genre classifier to recognize event nar-
ratives, two types of sentence classifiers, and
noun phrase classifiers to extract role fillers.
These modules are organized as a pipeline to
gradually zero in on event-related information.
We present results on the MUC-4 event ex-
traction data set and show that this model per-
forms better than previous systems.
</bodyText>
<sectionHeader confidence="0.99726" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99773908">
Event extraction is an information extraction (IE)
task that involves identifying the role fillers for
events in a particular domain. For example, the
Message Understanding Conferences (MUCs) chal-
lenged NLP researchers to create event extraction
systems for domains such as terrorism (e.g., to iden-
tify the perpetrators, victims, and targets of terrorism
events) and management succession (e.g., to iden-
tify the people and companies involved in corporate
management changes).
Most event extraction systems use either a
learning-based classifier to label words as role
fillers, or lexico-syntactic patterns to extract role
fillers from pattern contexts. Both approaches, how-
ever, generally tackle event recognition and role
filler extraction at the same time. In other words,
most event extraction systems primarily recognize
contexts that explicitly refer to a relevant event. For
example, a system that extracts information about
murders will recognize expressions associated with
murder (e.g., killed, assassinated, or shot to
death) and extract role fillers from the surround-
ing context. But many role fillers occur in contexts
that do not explicitly mention the event, and those
fillers are often overlooked. For example, the per-
petrator of a murder may be mentioned in the con-
text of an arrest, an eyewitness report, or specula-
tion about possible suspects. Victims may be named
in sentences that discuss the aftermath of the event,
such as the identification of bodies, transportation
of the injured to a hospital, or conclusions drawn
from an investigation. We will refer to these types of
sentences as secondary contexts because they are
generally not part of the main event description. Dis-
course analysis is one option to explicitly link these
secondary contexts to the event, but discourse mod-
elling is itself a difficult problem.
The goal of our research is to improve event ex-
traction by learning to identify secondary role filler
contexts in the absence of event keywords. We cre-
ate a set of classifiers to recognize role-specific con-
texts that suggest the presence of a likely role filler
regardless of whether a relevant event is mentioned
or not. For example, our model should recognize
that a sentence describing an arrest probably in-
cludes a reference to a perpetrator, even though the
crime itself is reported elsewhere.
Extracting information from these secondary con-
texts can be risky, however, unless we know that
the larger context is discussing a relevant event. To
</bodyText>
<page confidence="0.964254">
1137
</page>
<bodyText confidence="0.997518395833333">
\x0caddress this, we adopt a two-pronged strategy for
event extraction that handles event narrative docu-
ments differently from other documents. We define
an event narrative as an article whose main purpose
is to report the details of an event. We apply the role-
specific sentence classifiers only to event narratives
to aggressively search for role fillers in these sto-
ries. However, other types of documents can men-
tion relevant events too. The MUC-4 corpus, for ex-
ample, includes interviews, speeches, and terrorist
propaganda that contain information about terrorist
events. We will refer to these documents as fleet-
ing reference texts because they mention a relevant
event somewhere in the document, albeit briefly. To
ensure that relevant information is extracted from all
documents, we also apply a conservative extraction
process to every document to extract facts from ex-
plicit event sentences.
Our complete event extraction model, called
TIER, incorporates both document genre and role-
specific context recognition into 3 layers of analy-
sis: document analysis, sentence analysis, and noun
phrase (NP) analysis. At the top level, we train a
text genre classifier to identify event narrative doc-
uments. At the middle level, we create two types
of sentence classifiers. Event sentence classifiers
identify sentences that are associated with relevant
events, and role-specific context classifiers identify
sentences that contain possible role fillers irrespec-
tive of whether an event is mentioned. At the low-
est level, we use role filler extractors to label indi-
vidual noun phrases as role fillers. As documents
pass through the pipeline, they are analyzed at dif-
ferent levels of granularity. All documents pass
through the event sentence classifier, and event sen-
tences are given to the role filler extractors. Docu-
ments identified as event narratives additionally pass
through role-specific sentence classifiers, and the
role-specific sentences are also given to the role filler
extractors. This multi-layered approach creates an
event extraction system that can discover role fillers
in a variety of different contexts, while maintaining
good precision.
In the following sections, we position our research
with respect to related work, present the details of
our multi-layered event extraction model, and show
experimental results for five event roles using the
MUC-4 data set.
</bodyText>
<sectionHeader confidence="0.999523" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997803717391305">
Some event extraction data sets only include doc-
uments that describe relevant events (e.g., well-
known data sets for the domains of corporate ac-
quisitions (Freitag, 1998b; Freitag and McCallum,
2000; Finn and Kushmerick, 2004), job postings
(Califf and Mooney, 2003; Freitag and McCallum,
2000), and seminar announcements (Freitag, 1998b;
Ciravegna, 2001; Chieu and Ng, 2002; Finn and
Kushmerick, 2004; Gu and Cercone, 2006). But
many IE data sets present a more realistic task where
the IE system must determine whether a relevant
event is present in the document, and if so, extract
its role fillers. Most of the Message Understand-
ing Conference data sets represent this type of event
extraction task, containing (roughly) a 50/50 mix
of relevant and irrelevant documents (e.g., MUC-3,
MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)).
Our research focuses on this setting where the event
extraction system is not assured of getting only rele-
vant documents to process.
Most event extraction models can be character-
ized as either pattern-based or classifier-based ap-
proaches. Early event extraction systems used hand-
crafted patterns (e.g., (Appelt et al., 1993; Lehn-
ert et al., 1991)), but more recent systems gener-
ate patterns or rules automatically using supervised
learning (e.g., (Kim and Moldovan, 1993; Riloff,
1993; Soderland et al., 1995; Huffman, 1996; Fre-
itag, 1998b; Ciravegna, 2001; Califf and Mooney,
2003)), weakly supervised learning (e.g., (Riloff,
1996; Riloff and Jones, 1999; Yangarber et al.,
2000; Sudo et al., 2003; Stevenson and Greenwood,
2005)), or unsupervised learning (e.g., (Shinyama
and Sekine, 2006; Sekine, 2006)). In addition, many
classifiers have been created to sequentially label
event role fillers in a sentence (e.g., (Freitag, 1998a;
Chieu and Ng, 2002; Finn and Kushmerick, 2004;
Li et al., 2005; Yu et al., 2005)). Research has
also been done on relation extraction (e.g., (Roth
and Yih, 2001; Zelenko et al., 2003; Bunescu and
Mooney, 2007)), but that task is different from event
extraction because it focuses on isolated relations
rather than template-based event analysis.
Most event extraction systems scan a text and
search small context windows using patterns or a
classifier. However, recent work has begun to ex-
</bodyText>
<page confidence="0.979053">
1138
</page>
<bodyText confidence="0.991790514285715">
\x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction
plore more global approaches. (Maslennikov and
Chua, 2007) use discourse trees and local syntactic
dependencies in a pattern-based framework to incor-
porate wider context. Ji and Grishman (2008) en-
force event role consistency across different docu-
ments. (Liao and Grishman, 2010) use cross-event
inference to help with the extraction of role fillers
shared across events. And there have been several
recent IE models that explore the idea of identify-
ing relevant sentences to gain a wider contextual
view and then extracting role fillers. (Gu and Cer-
cone, 2006) created HMMs to first identify relevant
sentences, but their research focused on eliminating
redundant extractions and worked with seminar an-
nouncements, where the system was only given rel-
evant documents. (Patwardhan and Riloff, 2007) de-
veloped a system that learns to recognize event sen-
tences and uses patterns that have a semantic affinity
for an event role to extract role fillers. GLACIER
(Patwardhan and Riloff, 2009) jointly considers sen-
tential evidence and phrasal evidence in a unified
probabilistic framework. Our research follows in
the same spirit as these approaches by performing
multiple levels of text analysis. But our event ex-
traction model includes two novel contributions: (1)
we develop a set of role-specific sentence classifiers
to learn to recognize secondary contexts associated
with each type of event role , and (2) we exploit text
genre to incorporate a third level of analysis that en-
ables the system to aggressively hunt for role fillers
in documents that are event narratives. In Section 5,
we compare the performance of our model with both
the GLACIER system and Patwardhan &amp; Riloffs
semantic affinity model.
</bodyText>
<sectionHeader confidence="0.7110075" genericHeader="method">
3 A Multi-Layered Approach to Event
Extraction
</sectionHeader>
<bodyText confidence="0.999090966666667">
The main idea behind our approach is to analyze
documents at multiple levels of granularity in order
to identify role fillers that occur in different types of
contexts. Our event extraction model progressively
zooms in on relevant information by first identi-
fying the document type, then identifying sentences
that are likely to contain relevant information, and
finally analyzing individual noun phrases to identify
role fillers. The key advantage of this architecture is
that it allows us to search for information using two
different principles: (1) we look for contexts that di-
rectly refer to the event, as per most traditional event
extraction systems, and (2) we look for secondary
contexts that are often associated with a specific type
of role filler. Identifying these role-specific contexts
can root out important facts would have been oth-
erwise missed. Figure 1 shows the multi-layered
pipeline of our event extraction system.
An important aspect of our model is that two dif-
ferent strategies are employed to handle documents
of different types. The event extraction task is to
find any description of a relevant event, even if the
event is not the topic of the article.1 Consequently,
all documents are given to the event sentence recog-
nizers and their mission is to identify any sentence
that mentions a relevant event. This path through the
pipeline is conservative because information is ex-
tracted only from event sentences, but all documents
are processed, including stories that contain only a
fleeting reference to a relevant event.
</bodyText>
<page confidence="0.755347">
1
</page>
<bodyText confidence="0.7357735">
Per the MUC-4 task definition (MUC-4 Proceedings,
1992).
</bodyText>
<page confidence="0.965096">
1139
</page>
<bodyText confidence="0.9988348">
\x0cThe second path through the pipeline performs
additional processing for documents that belong to
the event narrative text genre. For event narratives,
we assume that most of the document discusses a
relevant event so we can more aggressively hunt for
event-related information in secondary contexts.
In this section, we explain how we create the two
types of sentence classifiers and the role filler extrac-
tors. We will return to the issue of document genre
and the event narrative classifier in Section 4.
</bodyText>
<subsectionHeader confidence="0.999536">
3.1 Sentence Classification
</subsectionHeader>
<bodyText confidence="0.999665292307692">
We have argued that event role fillers commonly oc-
cur in two types of contexts: event contexts and
role-specific secondary contexts. For the purposes
of this research, we use sentences as our definition
of a context, although there are obviously many
other possible definitions. An event context is a sen-
tence that describes the actual event. A secondary
context is a sentence that provides information re-
lated to an event but in the context of other activities
that precede or follow the event.
For both types of classifiers, we use exactly the
same feature set, but we train them in different ways.
The MUC-4 corpus used in our experiments in-
cludes a training set consisting of documents and an-
swer keys. Each document that describes a relevant
event has answer key templates with the role fillers
(answer key strings) for each event. To train the
event sentence recognizer, we consider a sentence
to be a positive training instance if it contains one or
more answer key strings from any of the event roles.
This produced 3,092 positive training sentences. All
remaining sentences that do not contain any answer
key strings are used as negative instances. This pro-
duced 19,313 negative training sentences, yielding a
roughly 6:1 ratio of negative to positive instances.
There is no guarantee that a classifier trained in
this way will identify event sentences, but our hy-
pothesis was that training across all of the event
roles together would produce a classifier that learns
to recognize general event contexts. This approach
was also used to train GLACIERs sentential event
recognizer (Patwardhan and Riloff, 2009), and they
demonstrated that this approach worked reasonably
well when compared to training with event sentences
labelled by human judges.
The main contribution of our work is introducing
additional role-specific sentence classifiers to seek
out role fillers that appear in less obvious secondary
contexts. We train a set of role-specific sentence
classifiers, one for each type of event role. Every
sentence that contains a role filler of the appropri-
ate type is used as a positive training instance. Sen-
tences that do not contain any answer key strings are
negative instances.2 In this way, we force each clas-
sifier to focus on the contexts specific to its particu-
lar event role. We expect the role-specific sentence
classifiers to find some secondary contexts that the
event sentence classifier will miss, although some
sentences may be classified as both.
Using all possible negative instances would pro-
duce an extremely skewed ratio of negative to pos-
itive instances. To control the skew and keep the
training set-up consistent with the event sentence
classifier, we randomly choose from the negative in-
stances to produce a 6:1 ratio of negative to positive
instances.
Both types of classifiers use an SVM model cre-
ated with SVMlin (Keerthi and DeCoste, 2005), and
exactly the same features. The feature set consists
of the unigrams and bigrams that appear in the train-
ing texts, the semantic class of each noun phrase3,
plus a few additional features to represent the tense
of the main verb phrase in the sentence and whether
the document is long (&amp;gt; 35 words) or short (&amp;lt; 5
words). All of the feature values are binary.
</bodyText>
<subsectionHeader confidence="0.998771">
3.2 Role Filler Extractors
</subsectionHeader>
<bodyText confidence="0.9993328">
Our extraction model also includes a set of role filler
extractors, one per event role. Each extractor re-
ceives a sentence as input and determines which
noun phrases (NPs) in the sentence are fillers for the
event role. To train an SVM classifier, noun phrases
corresponding to answer key strings for the event
role are positive instances. We randomly choose
among all noun phrases that are not in the answer
keys to create a 10:1 ratio of negative to positive in-
stances.
</bodyText>
<page confidence="0.972707">
2
</page>
<bodyText confidence="0.9998892">
We intentionally do not use sentences that contain fillers
for competing event roles as negative instances because sen-
tences often contain multiple role fillers of different types (e.g.,
a weapon may be found near a body). Sentences without any
role fillers are certain to be irrelevant contexts.
</bodyText>
<page confidence="0.989369">
3
</page>
<bodyText confidence="0.9970475">
We used the Sundance parser (Riloff and Phillips, 2004) to
identify noun phrases and assign semantic class labels.
</bodyText>
<page confidence="0.953569">
1140
</page>
<bodyText confidence="0.999399826086957">
\x0cThe feature set for the role filler extractors is
much richer than that of the sentence classifiers be-
cause they must carefully consider the local context
surrounding a noun phrase. We will refer to the noun
phrase being labelled as the targeted NP. The role
filler extractors use three types of features:
Lexical features: we represent four words to the
left and four words to the right of the targeted NP, as
well as the head noun and modifiers (adjectives and
noun modifiers) of the targeted NP itself.
Lexico-syntactic patterns: we use the AutoSlog
pattern generator (Riloff, 1993) to automatically
create lexico-syntactic patterns around each noun
phrase in the sentence. These patterns are similar
to dependency relations in that they typically repre-
sent the syntactic role of the NP with respect to other
constituents (e.g., subject-of, object-of, and noun ar-
guments).
Semantic features: we use the Stanford NER tag-
ger (Finkel et al., 2005) to determine if the targeted
NP is a named entity, and we use the Sundance
parser (Riloff and Phillips, 2004) to assign seman-
tic class labels to each NPs head noun.
</bodyText>
<sectionHeader confidence="0.97335" genericHeader="method">
4 Event Narrative Document Classification
</sectionHeader>
<bodyText confidence="0.998540428571428">
One of our goals was to explore the use of document
genre to permit more aggressive strategies for ex-
tracting role fillers. In this section, we first present
an analysis of the MUC-4 data set which reveals the
distribution of event narratives in the corpus, and
then explain how we train a classifier to automati-
cally identify event narrative stories.
</bodyText>
<subsectionHeader confidence="0.992331">
4.1 Manual Analysis
</subsectionHeader>
<bodyText confidence="0.995463608695652">
We define an event narrative as an article whose
main focus is on reporting the details of an event.
For the purposes of this research, we are only con-
cerned with events that are relevant to the event ex-
traction task (i.e., terrorism). An irrelevant docu-
ment is an article that does not mention any rele-
vant events. In between these extremes is another
category of documents that briefly mention a rele-
vant event, but the event is not the focus of the ar-
ticle. We will refer to these documents as fleeting
reference documents. Many of the fleeting reference
documents in the MUC-4 corpus are transcripts of
interviews, speeches, or terrorist propaganda com-
muniques that refer to a terrorist event and mention
at least one role filler, but within a discussion about
a different topic (e.g., the political ramifications of a
terrorist incident).
To gain a better understanding of how we might
create a system to automatically distinguish event
narrative documents from fleeting reference docu-
ments, we manually labelled the 116 relevant docu-
ments in our tuning set. This was an informal study
solely to help us understand the nature of these texts.
</bodyText>
<table confidence="0.99718275">
# of Event # of Fleeting
Narratives Ref. Docs Acc
Gold Standard 54 62
Heuristics 40 55 .82
</table>
<tableCaption confidence="0.992666">
Table 1: Manual Analysis of Document Types
</tableCaption>
<bodyText confidence="0.976053714285714">
The first row of Table 1 shows the distribution of
event narratives and fleeting references based on our
gold standard manual annotations. We see that
more than half of the relevant documents (62/116)
are not focused on reporting a terrorist event, even
though they contain information about a terrorist
event somewhere in the document.
</bodyText>
<subsectionHeader confidence="0.811573">
4.2 Heuristics for Event Narrative
Identification
</subsectionHeader>
<bodyText confidence="0.999763571428572">
Our goal is to train a document classifier to automat-
ically identify event narratives. The MUC-4 answer
keys reveal which documents are relevant and irrel-
evant with respect to the terrorism domain, but they
do not tell us which relevant documents are event
narratives and which are fleeting reference stories.
Based on our manual analysis of the tuning set, we
developed several heuristics to help separate them.
We observed two types of clues: the location of
the relevant information, and the density of rele-
vant information. First, we noticed that event nar-
ratives tend to mention relevant information within
the first several sentences, whereas fleeting refer-
ence texts usually mention relevant information only
in the middle or end of the document. Therefore our
first heuristic requires that an event narrative men-
tion a role filler within the first 7 sentences.
Second, event narratives generally have a higher
density of relevant information. We use several cri-
teria to estimate information density because a sin-
gle criterion was inadequate to cover different sce-
</bodyText>
<page confidence="0.811251">
1141
</page>
<bodyText confidence="0.944611">
\x0cnarios. For example, some documents mention role
fillers throughout the document. Other documents
contain a high concentration of role fillers in some
parts of the document but no role fillers in other
parts. We developed three density heuristics to ac-
count for different situations. All of these heuristics
count distinct role fillers. The first density heuristic
requires that more than 50% of the sentences contain
at least one role filler (|RelSents|
|AllSents |&amp;gt; 0.5) . Figure 2
shows histograms for different values of this ratio in
the event narrative (a) vs. the fleeting reference doc-
uments (b). The histograms clearly show that docu-
ments with a high (&amp;gt; 50%) ratio are almost always
event narratives.
</bodyText>
<figure confidence="0.99439815">
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
0
5
10
15
Ratio of Relevant Sentences
#
of
Documents
(a)
0 .1 .2 .3 .4 .5 .6 .7 .8 .9 1
0
5
10
15
Ratio of Relevant Sentences
#
of
Documents
(b)
</figure>
<figureCaption confidence="0.821561">
Figure 2: Histograms of Density Heuristic #1 in Event
Narratives (a) vs. Fleeting References (b).
</figureCaption>
<bodyText confidence="0.9605815">
A second density heuristic requires that the ratio
of different types of roles filled to sentences be &amp;gt;
</bodyText>
<equation confidence="0.565388">
50% ( |Roles|
|AllSents |&amp;gt; 0.5). A third density heuristic
</equation>
<bodyText confidence="0.9811455">
requires that the ratio of distinct role fillers to sen-
tences be &amp;gt; 70% (|RoleF illers|
|AllSents |&amp;gt; 0.7). If any of
these three criteria are satisfied, then the document
is considered to have a high density of relevant in-
formation.4
We use these heuristics to label a document as an
event narrative if: (1) it has a high density of relevant
information, and (2) it mentions a role filler within
the first 7 sentences.
The second row of Table 1 shows the performance
of these heuristics on the tuning set. The heuristics
</bodyText>
<figure confidence="0.431030333333333">
correctly identify 40
54 event narratives and 55
62 fleeting
</figure>
<bodyText confidence="0.9944782">
reference stories, to achieve an overall accuracy of
82%. These results are undoubtedly optimistic be-
cause the heuristics were derived from analysis of
the tuning set. But we felt confident enough to move
forward with using these heuristics to generate train-
</bodyText>
<page confidence="0.979233">
4
</page>
<bodyText confidence="0.8929515">
Heuristic #1 covers most of the event narratives.
ing data for an event narrative classifier.
</bodyText>
<subsectionHeader confidence="0.998803">
4.3 Event Narrative Classifier
</subsectionHeader>
<bodyText confidence="0.999691636363636">
The heuristics above use the answer keys to help de-
termine whether a story belongs to the event narra-
tive genre, but our goal is to create a classifier that
can identify event narrative documents without the
benefit of answer keys. So we used the heuristics
to automatically create training data for a classifier
by labelling each relevant document in the training
set as an event narrative or a fleeting reference doc-
ument. Of the 700 relevant documents, 292 were
labeled as event narratives. We then trained a doc-
ument classifier using the 292 event narrative docu-
ments as positive instances and all irrelevent training
documents as negative instances. The 308 relevant
documents that were not identified as event narra-
tives were discarded to minimize noise (i.e., we es-
timate that our heuristics fail to identify 25% of the
event narratives). We then trained an SVM classifier
using bag-of-words (unigram) features.
Table 2 shows the performance of the event nar-
rative classifier on the manually labeled tuning set.
The classifier identified 69% of the event narratives
with 63% precision. Overall accuracy was 81%.
</bodyText>
<table confidence="0.4107775">
Recall Precision Accuracy
.69 .63 .81
</table>
<tableCaption confidence="0.934728">
Table 2: Event Narrative Classifier Results
</tableCaption>
<bodyText confidence="0.991087055555555">
At first glance, the performance of this classifier
is mediocre. However, these results should be inter-
preted loosely because there is not always a clear di-
viding line between event narratives and other doc-
uments. For example, some documents begin with
a specific event description in the first few para-
graphs but then digress to discuss other topics. For-
tunately, it is not essential for TIER to have a per-
fect event narrative classifier since all documents
will be processed by the event sentence recognizer
anyway. The recall of the event narrative classifier
means that nearly 70% of the event narratives will
get additional scrutiny, which should help to find ad-
ditional role fillers. Its precision of 63% means that
some documents that are not event narratives will
also get additional scrutiny, but information will be
extracted only if both the role-specific sentence rec-
ognizer and NP extractors believe they have found
</bodyText>
<page confidence="0.97433">
1142
</page>
<table confidence="0.998482714285714">
\x0cMethod PerpInd PerpOrg Target Victim Weapon Average
Baselines
AutoSlog-TS 33/49/40 52/33/41 54/59/56 49/54/51 38/44/41 45/48/46
Semantic Affinity 48/39/43 36/58/45 56/46/50 46/44/45 53/46/50 48/47/47
GLACIER 51/58/54 34/45/38 43/72/53 55/58/56 57/53/55 48/57/52
New Results without document classification
AllSent 25/67/36 26/78/39 34/83/49 32/72/45 30/75/43 30/75/42
EventSent 52/54/53 50/44/47 52/67/59 55/51/53 56/57/56 53/54/54
RoleSent 37/54/44 37/58/45 49/75/59 52/60/55 38/66/48 43/63/51
EventSent+RoleSent 38/60/46 36/63/46 47/78/59 52/64/57 36/66/47 42/66/51
New Results with document classification
DomDoc/EventSent+DomDoc/RoleSent 45/54/49 42/51/46 51/68/58 54/56/55 46/63/53 48/58/52
EventSent+DomDoc/RoleSent 43/59/50 45/61/52 51/77/61 52/61/56 44/66/53 47/65/54
EventSent+ENarrDoc/RoleSent 48/57/52 46/53/50 51/73/60 56/60/58 53/64/58 51/62/56
</table>
<tableCaption confidence="0.8918245">
Table 3: Experimental results, reported as Precision/Recall/F-score
something relevant.
</tableCaption>
<subsectionHeader confidence="0.997297">
4.4 Domain-relevant Document Classifier
</subsectionHeader>
<bodyText confidence="0.998588272727273">
For comparisons sake, we also created a docu-
ment classifier to identify domain-relevant docu-
ments. That is, we trained a classifier to determine
whether a document is relevant to the domain of
terrorism, irrespective of the style of the document.
We trained an SVM classifier with the same bag-of-
words feature set, using all relevant documents in the
training set as positive instances and all irrelevant
documents as negative instances. We use this clas-
sifier for several experiments described in the next
section.
</bodyText>
<sectionHeader confidence="0.974139" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.992745">
5.1 Data Set and Metrics
</subsectionHeader>
<bodyText confidence="0.959153379310345">
We evaluated our approach on a standard benchmark
collection for event extraction systems, the MUC-4
data set (MUC-4 Proceedings, 1992). The MUC-4
corpus consists of 1700 documents with associated
answer key templates. To be consistent with previ-
ously reported results on this data set, we use the
1300 DEV documents for training, 200 documents
(TST1+TST2) as a tuning set and 200 documents
(TST3+TST4) as the test set. Roughly half of the
documents are relevant (i.e., they mention at least 1
terrorist event) and the rest are irrelevant.
We evaluate our system on the five MUC-4
string-fill event roles: perpetrator individuals,
perpetrator organizations, physical targets, victims
and weapons. The complete IE task involves tem-
plate generation, which is complex because many
documents have multiple templates (i.e., they dis-
cuss multiple events). Our work focuses on extract-
ing individual facts and not on template generation
per se (e.g., we do not perform coreference resolu-
tion or event tracking). Consequently, our evalua-
tion follows that of other recent work and evaluates
the accuracy of the extractions themselves by match-
ing the head nouns of extracted NPs with the head
nouns of answer key strings (e.g., armed guerril-
las is considered to match guerrillas)5. Our re-
sults are reported as Precision/Recall/F(1)-score for
each event role separately. We also show an overall
average for all event roles combined.6
</bodyText>
<subsectionHeader confidence="0.998637">
5.2 Baselines
</subsectionHeader>
<bodyText confidence="0.994263272727273">
As baselines, we compare the performance of our
IE system with three other event extraction sys-
tems. The first baseline is AutoSlog-TS (Riloff,
1996), which uses domain-specific extraction pat-
terns. AutoSlog-TS applies its patterns to every sen-
tence in every document, so does not attempt to
explicitly identify relevant sentences or documents.
The next two baselines are more recent systems:
the (Patwardhan and Riloff, 2007) semantic affin-
ity model and the (Patwardhan and Riloff, 2009)
GLACIER system. The semantic affinity approach
</bodyText>
<page confidence="0.96482">
5
</page>
<bodyText confidence="0.991870666666667">
Pronouns were discarded since we do not perform corefer-
ence resolution. Duplicate extractions with the same head noun
were counted as one hit or one miss.
</bodyText>
<page confidence="0.991239">
6
</page>
<bodyText confidence="0.9982845">
We generated the Average scores ourselves by macro-
averaging over the scores reported for the individual event roles.
</bodyText>
<page confidence="0.842658">
1143
</page>
<bodyText confidence="0.99816875">
\x0cexplicitly identifies event sentences and uses pat-
terns that have a semantic affinity for an event role
to extract role fillers. GLACIER is a probabilistic
model that incorporates both phrasal and sentential
evidence jointly to label role fillers.
The first 3 rows in Table 3 show the results for
each of these systems on the MUC-4 data set. They
all used the same evaluation criteria as our results.
</bodyText>
<subsectionHeader confidence="0.991139">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.998737307692307">
The lower portion of Table 3 shows the results of
a variety of event extraction models that we cre-
ated using different components of our system. The
AllSent row shows the performance of our Role
Filler Extractors when applied to every sentence in
every document. This system produced high recall,
but precision was consistently low.
The EventSent row shows the performance of
our Role Filler Extractors applied only to the event
sentences identified by our event sentence classi-
fier. This boosts precision across all event roles, but
with a sharp reduction in recall. We see a roughly
20 point swing from recall to precision. These re-
sults are similar to GLACIERs results on most event
roles, which isnt surprising because GLACIER also
incorporates event sentence identification.
The RoleSent row shows the results of our Role
Filler Extractors applied only to the role-specific
sentences identified by our classifiers. We see a 12-
13 point swing from recall to precision compared
to the AllSent row. This result is consistent with
our hypothesis that many role fillers exist in role-
specific contexts that are not event sentences. As ex-
pected, extracting facts from role-specific contexts
that do not necessarily refer to an event is less reli-
able. The EventSent+RoleSent row shows the re-
sults when information is extracted from both types
of sentences. We see slightly higher recall, which
confirms that one set of extractions is not a strict
subset of the other, but precision is still relatively
low.
The next set of experiments incorporates docu-
ment classification as the third layer of text analy-
sis. The DomDoc/EventSent+DomDoc/RoleSent
row shows the results of applying both types of
sentence classifiers only to documents identified as
domain-relevant by the Domain-relevant Document
(DomDoc) Classifier described in Section 4.4. Ex-
tracting information only from domain-relevant doc-
uments improves precision by +6, but also sacrifices
8 points of recall.
The EventSent row reveals that information
found in event sentences has the highest precision,
even without relying on document classification. We
concluded that evidence of an event sentence is
probably sufficient to warrant role filler extraction
irrespective of the style of the document. As we dis-
cussed in Section 4, many documents contain only
a fleeting reference to an event, so it is important
to be able to extract information from those isolated
event descriptions as well. Consequently, we cre-
ated a system, EventSent+DomDoc/RoleSent, that
extracts information from event sentences in all doc-
uments, but extracts information from role-specific
sentences only if they appear in a domain-relevant
document. This architecture captured the best of
both worlds: recall improved from 58% to 65% with
only a one point drop in precision.
Finally, we evaluated the idea of using document
genre as a filter instead of domain relevance. The
last row, EventSent+ENarrDoc/RoleSent, shows
the results of our final architecture which extracts
information from event sentences in all documents,
but extracts information from role-specific sentences
only in Event Narrative documents. This architec-
ture produced the best F1 score of 56. This model in-
creases precision by an additional 4 points and pro-
duces the best balance of recall and precision.
Overall, TIERs multi-layered extraction architec-
ture produced higher F1 scores than previous sys-
tems on four of the five event roles. The improved
recall is due to the additional extractions from sec-
ondary contexts. The improved precision comes
from our two-pronged strategy of treating event nar-
ratives differently from other documents. TIER ag-
gressively searches for extractions in event narrative
stories but is conservative and extracts information
only from event sentences in all other documents.
</bodyText>
<subsectionHeader confidence="0.994327">
5.4 Analysis
</subsectionHeader>
<bodyText confidence="0.999884333333333">
We looked through some examples of TIERs output
to try to gain insight about its strengths and limita-
tions. TIERs role-specific sentence classifiers did
correctly identify some sentences containing role
fillers that were not classified as event sentences.
Several examples are shown below, with the role
</bodyText>
<page confidence="0.973433">
1144
</page>
<listItem confidence="0.949818">
\x0cfillers in italics:
(1) The victims were identified as David Lecky, director
of the Columbus school, and James Arthur Donnelly.
(2) There were seven children, including four of the
Vice Presidents children, in the home at the time.
(3) The woman fled and sought refuge inside the
</listItem>
<bodyText confidence="0.9589076">
facilities of the Salvadoran Alberto Masferrer University,
where she took a group of students as hostages, threaten-
ing them with hand grenades.
(4) The FMLN stated that several homes were damaged
and that animals were killed in the surrounding hamlets
and villages.
The first two sentences identify victims, but the
terrorist event itself was mentioned earlier in the
document. The third sentence contains a perpetrator
(the woman), victims (students), and weapons (hand
grenades) in the context of a hostage situation after
the main event (a bus attack), when the perpetrator
escaped. The fourth sentence describes incidental
damage to civilian homes following clashes between
government forces and guerrillas.
However there is substantial room for improve-
ment in each of TIERs subcomponents, and many
role fillers are still overlooked. One reason is that it
can be difficult to recognize acts of terrorism. Many
sentences refer to a potentially relevant subevent
(e.g., injury or physical damage) but recognizing
that the event is part of a terrorist incident depends
on the larger discourse. For example, consider the
examples below that TIER did not recognize as
relevant sentences:
</bodyText>
<listItem confidence="0.833013857142857">
(5) Later, two individuals in a Chevrolet Opala automo-
bile pointed AK rifles at the students, fired some shots,
and quickly drove away.
(6) Meanwhile, national police members who were
dressed in civilian clothes seized university students
Hugo Martinez and Raul Ramirez, who are still missing.
(7) All labor union offices in San Salvador were looted.
</listItem>
<bodyText confidence="0.999444384615385">
In the first sentence, the event is described as
someone pointing rifles at people and the perpetra-
tors are referred to simply as individuals. There are
no strong keywords in this sentence that reveal this
is a terrorist attack. In the second sentence, police
are being accused of state-sponsored terrorism when
they seize civilians. The verb seize is common
in this corpus, but usually refers to the seizing of
weapons or drug stashes, not people. The third sen-
tence describes a looting subevent. Acts of looting
and vandalism are not usually considered to be ter-
rorism, but in this article it is in the context of accu-
sations of terrorist acts by government officials.
</bodyText>
<sectionHeader confidence="0.99883" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.997636652173913">
We have presented a new approach to event extrac-
tion that uses three levels of analysis: document
genre classification to identify event narrative sto-
ries, two types of sentence classifiers, and noun
phrase classifiers. A key contribution of our work is
the creation of role-specific sentence classifiers that
can detect role fillers in secondary contexts that do
not directly refer to the event. Another important as-
pect of our approach is a two-pronged strategy that
handles event narratives differently from other doc-
uments. TIER aggressively hunts for role fillers in
event narratives, but is conservative about extract-
ing information from other documents. This strategy
produced improvements in both recall and precision
over previous state-of-the-art systems.
This work just scratches the surface of using doc-
ument genre identification to improve information
extraction accuracy. In future work, we hope to
identify additional types of document genre styles
and incorporate genre directly into the extraction
model. Coreference resolution and discourse anal-
ysis will also be important to further improve event
extraction performance.
</bodyText>
<sectionHeader confidence="0.998735" genericHeader="references">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9927889">
We gratefully acknowledge the support of the Na-
tional Science Foundation under grant IIS-1018314
and the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0172. Any opinions, find-
ings, and conclusion or recommendations expressed
in this material are those of the authors and do not
necessarily reflect the view of the DARPA, AFRL,
or the U.S. government.
</bodyText>
<page confidence="0.807293">
1145
</page>
<reference confidence="0.971964327102804">
\x0cReferences
D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson.
1993. FASTUS: a finite-state processor for informa-
tion extraction from real-world text. In Proceedings of
the Thirteenth International Joint Conference on Arti-
ficial Intelligence.
R. Bunescu and R. Mooney. 2007. Learning to Extract
Relations from the Web using Minimal Supervision.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics.
M.E. Califf and R. Mooney. 2003. Bottom-up Relational
Learning of Pattern Matching rules for Information
Extraction. Journal of Machine Learning Research,
4:177210.
H.L. Chieu and H.T. Ng. 2002. A Maximum En-
tropy Approach to Information Extraction from Semi-
Structured and Free Text. In Proceedings of the 18th
National Conference on Artificial Intelligence.
F. Ciravegna. 2001. Adaptive Information Extraction
from Text by Rule Induction and Generalisation. In
Proceedings of the 17th International Joint Confer-
ence on Artificial Intelligence.
J. Finkel, T. Grenager, and C. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 363370, Ann Ar-
bor, MI, June.
A. Finn and N. Kushmerick. 2004. Multi-level Boundary
Classification for Information Extraction. In In Pro-
ceedings of the 15th European Conference on Machine
Learning, pages 111122, Pisa, Italy, September.
D. Freitag and A. McCallum. 2000. Information Ex-
traction with HMM Structures Learned by Stochas-
tic Optimization. In Proceedings of the Seventeenth
National Conference on Artificial Intelligence, pages
584589, Austin, TX, August.
Dayne Freitag. 1998a. Multistrategy Learning for In-
formation Extraction. In Proceedings of the Fifteenth
International Conference on Machine Learning. Mor-
gan Kaufmann Publishers.
Dayne Freitag. 1998b. Toward General-Purpose Learn-
ing for Information Extraction. In Proceedings of the
36th Annual Meeting of the Association for Computa-
tional Linguistics.
Z. Gu and N. Cercone. 2006. Segment-Based Hidden
Markov Models for Information Extraction. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
481488, Sydney, Australia, July.
L. Hirschman. 1998. The Evolution of Evaluation:
Lessons from the Message Understanding Confer-
ences. Computer Speech and Language, 12.
S. Huffman. 1996. Learning Information Extraction Pat-
terns from Examples. In Stefan Wermter, Ellen Riloff,
and Gabriele Scheler, editors, Connectionist, Statisti-
cal, and Symbolic Approaches to Learning for Nat-
ural Language Processing, pages 246260. Springer-
Verlag, Berlin.
H. Ji and R. Grishman. 2008. Refining Event Extraction
through Cross-Document Inference. In Proceedings of
ACL-08: HLT, pages 254262, Columbus, OH, June.
S. Keerthi and D. DeCoste. 2005. A Modified Finite
Newton Method for Fast Solution of Large Scale Lin-
ear SVMs. Journal of Machine Learning Research.
J. Kim and D. Moldovan. 1993. Acquisition of Semantic
Patterns for Information Extraction from Corpora. In
Proceedings of the Ninth IEEE Conference on Artifi-
cial Intelligence for Applications, pages 171176, Los
Alamitos, CA. IEEE Computer Society Press.
W. Lehnert, C. Cardie, D. Fisher, E. Riloff, and
R. Williams. 1991. University of Massachusetts: De-
scription of the CIRCUS System as Used for MUC-
3. In Proceedings of the Third Message Understand-
ing Conference (MUC-3), pages 223233, San Mateo,
CA. Morgan Kaufmann.
Y. Li, K. Bontcheva, and H. Cunningham. 2005. Us-
ing Uneven Margins SVM and Perceptron for Infor-
mation Extraction. In Proceedings of Ninth Confer-
ence on Computational Natural Language Learning,
pages 7279, Ann Arbor, MI, June.
Shasha Liao and Ralph Grishman. 2010. Using docu-
ment level cross-event inference to improve event ex-
traction. In Proceedings of the 48st Annual Meeting on
Association for Computational Linguistics (ACL-10).
M. Maslennikov and T. Chua. 2007. A Multi-Resolution
Framework for Information Extraction from Free Text.
In Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics.
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
S. Patwardhan and E. Riloff. 2007. Effective Information
Extraction with Semantic Affinity Patterns and Rele-
vant Regions. In Proceedings of 2007 the Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2007).
S. Patwardhan and E. Riloff. 2009. A Unified Model of
Phrasal and Sentential Evidence for Information Ex-
traction. In Proceedings of 2009 the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2009).
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
</reference>
<page confidence="0.557047">
1146
</page>
<reference confidence="0.999826690909091">
\x0cE. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff. 1993. Automatically Constructing a Dictio-
nary for Information Extraction Tasks. In Proceedings
of the 11th National Conference on Artificial Intelli-
gence.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 10441049. The AAAI Press/MIT Press.
D. Roth and W. Yih. 2001. Relational Learning via
Propositional Algorithms: An Information Extraction
Case Study. In Proceedings of the Seventeenth In-
ternational Joint Conference on Artificial Intelligence,
pages 12571263, Seattle, WA, August.
Satoshi Sekine. 2006. On-demand information ex-
traction. In Proceedings of Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguistics
(COLING/ACL-06.
Y. Shinyama and S. Sekine. 2006. Preemptive Informa-
tion Extraction using Unrestricted Relation Discovery.
In Proceedings of the Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 304
311, New York City, NY, June.
S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert.
1995. CRYSTAL: Inducing a conceptual dictionary.
In Proc. of the Fourteenth International Joint Confer-
ence on Artificial Intelligence, pages 13141319.
M. Stevenson and M. Greenwood. 2005. A Seman-
tic Approach to IE Pattern Induction. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics, pages 379386, Ann Ar-
bor, MI, June.
K. Sudo, S. Sekine, and R. Grishman. 2003. An Im-
proved Extraction Pattern Representation Model for
Automatic IE Pattern Acquisition. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL-03).
R. Yangarber, R. Grishman, P. Tapanainen, and S. Hut-
tunen. 2000. Automatic Acquisition of Domain
Knowledge for Information Extraction. In Proceed-
ings of the Eighteenth International Conference on
Computational Linguistics (COLING 2000).
K. Yu, G. Guan, and M. Zhou. 2005. Resume Infor-
mation Extraction with Cascaded Hybrid Model. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 499506,
Ann Arbor, MI, June.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel Methods for Relation
Extraction. Journal of Machine Learning Research, 3.
</reference>
<page confidence="0.775813">
1147
</page>
<figure confidence="0.288629">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.881612">
<note confidence="0.983086">b&amp;apos;Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 11371147, Portland, Oregon, June 19-24, 2011. c 2011 Association for Computational Linguistics</note>
<title confidence="0.945826">Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</title>
<author confidence="0.999383">Ruihong Huang</author>
<author confidence="0.999383">Ellen Riloff</author>
<affiliation confidence="0.999764">School of Computing University of Utah</affiliation>
<address confidence="0.991226">Salt Lake City, UT 84112</address>
<email confidence="0.999703">huangrh@cs.utah.edu</email>
<email confidence="0.999703">riloff@cs.utah.edu</email>
<abstract confidence="0.9994743125">The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords. We propose a multilayered event extraction architecture that progressively zooms in on relevant information. Our extraction model includes a document genre classifier to recognize event narratives, two types of sentence classifiers, and noun phrase classifiers to extract role fillers. These modules are organized as a pipeline to gradually zero in on event-related information. We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences D Appelt</author>
<author>J Hobbs</author>
<author>J Bear</author>
<author>D Israel</author>
<author>M Tyson</author>
</authors>
<title>FASTUS: a finite-state processor for information extraction from real-world text.</title>
<date>1993</date>
<booktitle>In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7110" citStr="Appelt et al., 1993" startWordPosition="1095" endWordPosition="1098">t is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Fr</context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Tyson, 1993</marker>
<rawString>\x0cReferences D. Appelt, J. Hobbs, J. Bear, D. Israel, and M. Tyson. 1993. FASTUS: a finite-state processor for information extraction from real-world text. In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>Learning to Extract Relations from the Web using Minimal Supervision.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7933" citStr="Bunescu and Mooney, 2007" startWordPosition="1223" endWordPosition="1226">96; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different docu</context>
</contexts>
<marker>Bunescu, Mooney, 2007</marker>
<rawString>R. Bunescu and R. Mooney. 2007. Learning to Extract Relations from the Web using Minimal Supervision. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Califf</author>
<author>R Mooney</author>
</authors>
<title>Bottom-up Relational Learning of Pattern Matching rules for Information Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>4--177210</pages>
<contexts>
<context position="6220" citStr="Califf and Mooney, 2003" startWordPosition="956" endWordPosition="959"> extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this sett</context>
</contexts>
<marker>Califf, Mooney, 2003</marker>
<rawString>M.E. Califf and R. Mooney. 2003. Bottom-up Relational Learning of Pattern Matching rules for Information Extraction. Journal of Machine Learning Research, 4:177210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H L Chieu</author>
<author>H T Ng</author>
</authors>
<title>A Maximum Entropy Approach to Information Extraction from SemiStructured and Free Text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 18th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6329" citStr="Chieu and Ng, 2002" startWordPosition="971" endWordPosition="974">sion. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most even</context>
<context position="7742" citStr="Chieu and Ng, 2002" startWordPosition="1190" endWordPosition="1193">l., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 20</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Approach to Information Extraction from SemiStructured and Free Text. In Proceedings of the 18th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ciravegna</author>
</authors>
<title>Adaptive Information Extraction from Text by Rule Induction and Generalisation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6309" citStr="Ciravegna, 2001" startWordPosition="969" endWordPosition="970">aining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents t</context>
</contexts>
<marker>Ciravegna, 2001</marker>
<rawString>F. Ciravegna. 2001. Adaptive Information Extraction from Text by Rule Induction and Generalisation. In Proceedings of the 17th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>363370</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="17307" citStr="Finkel et al., 2005" startWordPosition="2740" endWordPosition="2743">represent four words to the left and four words to the right of the targeted NP, as well as the head noun and modifiers (adjectives and noun modifiers) of the targeted NP itself. Lexico-syntactic patterns: we use the AutoSlog pattern generator (Riloff, 1993) to automatically create lexico-syntactic patterns around each noun phrase in the sentence. These patterns are similar to dependency relations in that they typically represent the syntactic role of the NP with respect to other constituents (e.g., subject-of, object-of, and noun arguments). Semantic features: we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NPs head noun. 4 Event Narrative Document Classification One of our goals was to explore the use of document genre to permit more aggressive strategies for extracting role fillers. In this section, we first present an analysis of the MUC-4 data set which reveals the distribution of event narratives in the corpus, and then explain how we train a classifier to automatically identify event narrative stories. 4.1 Manual Analysis We define an event n</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 363370, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finn</author>
<author>N Kushmerick</author>
</authors>
<title>Multi-level Boundary Classification for Information Extraction. In</title>
<date>2004</date>
<booktitle>In Proceedings of the 15th European Conference on Machine Learning,</booktitle>
<pages>111122</pages>
<location>Pisa, Italy,</location>
<contexts>
<context position="6181" citStr="Finn and Kushmerick, 2004" startWordPosition="950" endWordPosition="953">is multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 19</context>
<context position="7769" citStr="Finn and Kushmerick, 2004" startWordPosition="1194" endWordPosition="1197"> recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and</context>
</contexts>
<marker>Finn, Kushmerick, 2004</marker>
<rawString>A. Finn and N. Kushmerick. 2004. Multi-level Boundary Classification for Information Extraction. In In Proceedings of the 15th European Conference on Machine Learning, pages 111122, Pisa, Italy, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
<author>A McCallum</author>
</authors>
<title>Information Extraction with HMM Structures Learned by Stochastic Optimization.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence,</booktitle>
<pages>584589</pages>
<location>Austin, TX,</location>
<contexts>
<context position="6153" citStr="Freitag and McCallum, 2000" startWordPosition="946" endWordPosition="949">e role filler extractors. This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC</context>
</contexts>
<marker>Freitag, McCallum, 2000</marker>
<rawString>D. Freitag and A. McCallum. 2000. Information Extraction with HMM Structures Learned by Stochastic Optimization. In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pages 584589, Austin, TX, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Multistrategy Learning for Information Extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="6124" citStr="Freitag, 1998" startWordPosition="944" endWordPosition="945">also given to the role filler extractors. This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant docum</context>
<context position="7721" citStr="Freitag, 1998" startWordPosition="1188" endWordPosition="1189">93; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Mas</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998a. Multistrategy Learning for Information Extraction. In Proceedings of the Fifteenth International Conference on Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward General-Purpose Learning for Information Extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6124" citStr="Freitag, 1998" startWordPosition="944" endWordPosition="945">also given to the role filler extractors. This multi-layered approach creates an event extraction system that can discover role fillers in a variety of different contexts, while maintaining good precision. In the following sections, we position our research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant docum</context>
<context position="7721" citStr="Freitag, 1998" startWordPosition="1188" endWordPosition="1189">93; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Mas</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998b. Toward General-Purpose Learning for Information Extraction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Gu</author>
<author>N Cercone</author>
</authors>
<title>Segment-Based Hidden Markov Models for Information Extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>481488</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6379" citStr="Gu and Cercone, 2006" startWordPosition="979" endWordPosition="982">r research with respect to related work, present the details of our multi-layered event extraction model, and show experimental results for five event roles using the MUC-4 data set. 2 Related Work Some event extraction data sets only include documents that describe relevant events (e.g., wellknown data sets for the domains of corporate acquisitions (Freitag, 1998b; Freitag and McCallum, 2000; Finn and Kushmerick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either</context>
<context position="8847" citStr="Gu and Cercone, 2006" startWordPosition="1363" endWordPosition="1367">e 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing mul</context>
</contexts>
<marker>Gu, Cercone, 2006</marker>
<rawString>Z. Gu and N. Cercone. 2006. Segment-Based Hidden Markov Models for Information Extraction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 481488, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<title>The Evolution of Evaluation: Lessons from the Message Understanding Conferences.</title>
<date>1998</date>
<journal>Computer Speech and Language,</journal>
<volume>12</volume>
<contexts>
<context position="6784" citStr="Hirschman, 1998" startWordPosition="1047" endWordPosition="1048">merick, 2004), job postings (Califf and Mooney, 2003; Freitag and McCallum, 2000), and seminar announcements (Freitag, 1998b; Ciravegna, 2001; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Gu and Cercone, 2006). But many IE data sets present a more realistic task where the IE system must determine whether a relevant event is present in the document, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly super</context>
</contexts>
<marker>Hirschman, 1998</marker>
<rawString>L. Hirschman. 1998. The Evolution of Evaluation: Lessons from the Message Understanding Conferences. Computer Speech and Language, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Huffman</author>
</authors>
<title>Learning Information Extraction Patterns from Examples.</title>
<date>1996</date>
<booktitle>Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing,</booktitle>
<pages>246260</pages>
<editor>In Stefan Wermter, Ellen Riloff, and Gabriele Scheler, editors,</editor>
<publisher>SpringerVerlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="7310" citStr="Huffman, 1996" startWordPosition="1128" endWordPosition="1129">levant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bu</context>
</contexts>
<marker>Huffman, 1996</marker>
<rawString>S. Huffman. 1996. Learning Information Extraction Patterns from Examples. In Stefan Wermter, Ellen Riloff, and Gabriele Scheler, editors, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, pages 246260. SpringerVerlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ji</author>
<author>R Grishman</author>
</authors>
<title>Refining Event Extraction through Cross-Document Inference.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>254262</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="8480" citStr="Ji and Grishman (2008)" startWordPosition="1304" endWordPosition="1307">(e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 20</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>H. Ji and R. Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In Proceedings of ACL-08: HLT, pages 254262, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Keerthi</author>
<author>D DeCoste</author>
</authors>
<title>A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="15074" citStr="Keerthi and DeCoste, 2005" startWordPosition="2366" endWordPosition="2369">exts specific to its particular event role. We expect the role-specific sentence classifiers to find some secondary contexts that the event sentence classifier will miss, although some sentences may be classified as both. Using all possible negative instances would produce an extremely skewed ratio of negative to positive instances. To control the skew and keep the training set-up consistent with the event sentence classifier, we randomly choose from the negative instances to produce a 6:1 ratio of negative to positive instances. Both types of classifiers use an SVM model created with SVMlin (Keerthi and DeCoste, 2005), and exactly the same features. The feature set consists of the unigrams and bigrams that appear in the training texts, the semantic class of each noun phrase3, plus a few additional features to represent the tense of the main verb phrase in the sentence and whether the document is long (&amp;gt; 35 words) or short (&amp;lt; 5 words). All of the feature values are binary. 3.2 Role Filler Extractors Our extraction model also includes a set of role filler extractors, one per event role. Each extractor receives a sentence as input and determines which noun phrases (NPs) in the sentence are fillers for the eve</context>
</contexts>
<marker>Keerthi, DeCoste, 2005</marker>
<rawString>S. Keerthi and D. DeCoste. 2005. A Modified Finite Newton Method for Fast Solution of Large Scale Linear SVMs. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>D Moldovan</author>
</authors>
<title>Acquisition of Semantic Patterns for Information Extraction from Corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications,</booktitle>
<pages>171176</pages>
<publisher>IEEE Computer Society Press.</publisher>
<location>Los Alamitos, CA.</location>
<contexts>
<context position="7257" citStr="Kim and Moldovan, 1993" startWordPosition="1118" endWordPosition="1121"> event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction</context>
</contexts>
<marker>Kim, Moldovan, 1993</marker>
<rawString>J. Kim and D. Moldovan. 1993. Acquisition of Semantic Patterns for Information Extraction from Corpora. In Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications, pages 171176, Los Alamitos, CA. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>C Cardie</author>
<author>D Fisher</author>
<author>E Riloff</author>
<author>R Williams</author>
</authors>
<title>University of Massachusetts: Description of the CIRCUS System as Used for MUC3.</title>
<date>1991</date>
<booktitle>In Proceedings of the Third Message Understanding Conference (MUC-3),</booktitle>
<pages>223233</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="7133" citStr="Lehnert et al., 1991" startWordPosition="1099" endWordPosition="1103">ocument, and if so, extract its role fillers. Most of the Message Understanding Conference data sets represent this type of event extraction task, containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and</context>
</contexts>
<marker>Lehnert, Cardie, Fisher, Riloff, Williams, 1991</marker>
<rawString>W. Lehnert, C. Cardie, D. Fisher, E. Riloff, and R. Williams. 1991. University of Massachusetts: Description of the CIRCUS System as Used for MUC3. In Proceedings of the Third Message Understanding Conference (MUC-3), pages 223233, San Mateo, CA. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
<author>K Bontcheva</author>
<author>H Cunningham</author>
</authors>
<title>Using Uneven Margins SVM and Perceptron for Information Extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of Ninth Conference on Computational Natural Language Learning,</booktitle>
<pages>7279</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="7786" citStr="Li et al., 2005" startWordPosition="1198" endWordPosition="1201">tterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic </context>
</contexts>
<marker>Li, Bontcheva, Cunningham, 2005</marker>
<rawString>Y. Li, K. Bontcheva, and H. Cunningham. 2005. Using Uneven Margins SVM and Perceptron for Information Extraction. In Proceedings of Ninth Conference on Computational Natural Language Learning, pages 7279, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Liao</author>
<author>Ralph Grishman</author>
</authors>
<title>Using document level cross-event inference to improve event extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48st Annual Meeting on Association for Computational Linguistics (ACL-10).</booktitle>
<contexts>
<context position="8565" citStr="Liao and Grishman, 2010" startWordPosition="1317" endWordPosition="1320">hat task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns tha</context>
</contexts>
<marker>Liao, Grishman, 2010</marker>
<rawString>Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction. In Proceedings of the 48st Annual Meeting on Association for Computational Linguistics (ACL-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maslennikov</author>
<author>T Chua</author>
</authors>
<title>A Multi-Resolution Framework for Information Extraction from Free Text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8345" citStr="Maslennikov and Chua, 2007" startWordPosition="1284" endWordPosition="1287">998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redu</context>
</contexts>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>M. Maslennikov and T. Chua. 2007. A Multi-Resolution Framework for Information Extraction from Free Text. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-4 Proceedings</author>
</authors>
<date>1992</date>
<booktitle>Proceedings of the Fourth Message Understanding Conference (MUC-4).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="11646" citStr="Proceedings, 1992" startWordPosition="1811" endWordPosition="1812">nt strategies are employed to handle documents of different types. The event extraction task is to find any description of a relevant event, even if the event is not the topic of the article.1 Consequently, all documents are given to the event sentence recognizers and their mission is to identify any sentence that mentions a relevant event. This path through the pipeline is conservative because information is extracted only from event sentences, but all documents are processed, including stories that contain only a fleeting reference to a relevant event. 1 Per the MUC-4 task definition (MUC-4 Proceedings, 1992). 1139 \x0cThe second path through the pipeline performs additional processing for documents that belong to the event narrative text genre. For event narratives, we assume that most of the document discusses a relevant event so we can more aggressively hunt for event-related information in secondary contexts. In this section, we explain how we create the two types of sentence classifiers and the role filler extractors. We will return to the issue of document genre and the event narrative classifier in Section 4. 3.1 Sentence Classification We have argued that event role fillers commonly occur </context>
<context position="26562" citStr="Proceedings, 1992" startWordPosition="4220" endWordPosition="4221"> domain-relevant documents. That is, we trained a classifier to determine whether a document is relevant to the domain of terrorism, irrespective of the style of the document. We trained an SVM classifier with the same bag-ofwords feature set, using all relevant documents in the training set as positive instances and all irrelevant documents as negative instances. We use this classifier for several experiments described in the next section. 5 Evaluation 5.1 Data Set and Metrics We evaluated our approach on a standard benchmark collection for event extraction systems, the MUC-4 data set (MUC-4 Proceedings, 1992). The MUC-4 corpus consists of 1700 documents with associated answer key templates. To be consistent with previously reported results on this data set, we use the 1300 DEV documents for training, 200 documents (TST1+TST2) as a tuning set and 200 documents (TST3+TST4) as the test set. Roughly half of the documents are relevant (i.e., they mention at least 1 terrorist event) and the rest are irrelevant. We evaluate our system on the five MUC-4 string-fill event roles: perpetrator individuals, perpetrator organizations, physical targets, victims and weapons. The complete IE task involves template</context>
</contexts>
<marker>Proceedings, 1992</marker>
<rawString>MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions.</title>
<date>2007</date>
<booktitle>In Proceedings of 2007 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2007).</booktitle>
<contexts>
<context position="9083" citStr="Patwardhan and Riloff, 2007" startWordPosition="1398" endWordPosition="1401">t. Ji and Grishman (2008) enforce event role consistency across different documents. (Liao and Grishman, 2010) use cross-event inference to help with the extraction of role fillers shared across events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event extraction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role </context>
<context position="28289" citStr="Patwardhan and Riloff, 2007" startWordPosition="4488" endWordPosition="4491">is considered to match guerrillas)5. Our results are reported as Precision/Recall/F(1)-score for each event role separately. We also show an overall average for all event roles combined.6 5.2 Baselines As baselines, we compare the performance of our IE system with three other event extraction systems. The first baseline is AutoSlog-TS (Riloff, 1996), which uses domain-specific extraction patterns. AutoSlog-TS applies its patterns to every sentence in every document, so does not attempt to explicitly identify relevant sentences or documents. The next two baselines are more recent systems: the (Patwardhan and Riloff, 2007) semantic affinity model and the (Patwardhan and Riloff, 2009) GLACIER system. The semantic affinity approach 5 Pronouns were discarded since we do not perform coreference resolution. Duplicate extractions with the same head noun were counted as one hit or one miss. 6 We generated the Average scores ourselves by macroaveraging over the scores reported for the individual event roles. 1143 \x0cexplicitly identifies event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER is a probabilistic model that incorporates both phrasal and sententi</context>
</contexts>
<marker>Patwardhan, Riloff, 2007</marker>
<rawString>S. Patwardhan and E. Riloff. 2007. Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions. In Proceedings of 2007 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>E Riloff</author>
</authors>
<title>A Unified Model of Phrasal and Sentential Evidence for Information Extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of 2009 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2009).</booktitle>
<contexts>
<context position="9272" citStr="Patwardhan and Riloff, 2009" startWordPosition="1430" endWordPosition="1433">cross events. And there have been several recent IE models that explore the idea of identifying relevant sentences to gain a wider contextual view and then extracting role fillers. (Gu and Cercone, 2006) created HMMs to first identify relevant sentences, but their research focused on eliminating redundant extractions and worked with seminar announcements, where the system was only given relevant documents. (Patwardhan and Riloff, 2007) developed a system that learns to recognize event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER (Patwardhan and Riloff, 2009) jointly considers sentential evidence and phrasal evidence in a unified probabilistic framework. Our research follows in the same spirit as these approaches by performing multiple levels of text analysis. But our event extraction model includes two novel contributions: (1) we develop a set of role-specific sentence classifiers to learn to recognize secondary contexts associated with each type of event role , and (2) we exploit text genre to incorporate a third level of analysis that enables the system to aggressively hunt for role fillers in documents that are event narratives. In Section 5, </context>
<context position="13810" citStr="Patwardhan and Riloff, 2009" startWordPosition="2162" endWordPosition="2165">of the event roles. This produced 3,092 positive training sentences. All remaining sentences that do not contain any answer key strings are used as negative instances. This produced 19,313 negative training sentences, yielding a roughly 6:1 ratio of negative to positive instances. There is no guarantee that a classifier trained in this way will identify event sentences, but our hypothesis was that training across all of the event roles together would produce a classifier that learns to recognize general event contexts. This approach was also used to train GLACIERs sentential event recognizer (Patwardhan and Riloff, 2009), and they demonstrated that this approach worked reasonably well when compared to training with event sentences labelled by human judges. The main contribution of our work is introducing additional role-specific sentence classifiers to seek out role fillers that appear in less obvious secondary contexts. We train a set of role-specific sentence classifiers, one for each type of event role. Every sentence that contains a role filler of the appropriate type is used as a positive training instance. Sentences that do not contain any answer key strings are negative instances.2 In this way, we forc</context>
<context position="28351" citStr="Patwardhan and Riloff, 2009" startWordPosition="4498" endWordPosition="4501">as Precision/Recall/F(1)-score for each event role separately. We also show an overall average for all event roles combined.6 5.2 Baselines As baselines, we compare the performance of our IE system with three other event extraction systems. The first baseline is AutoSlog-TS (Riloff, 1996), which uses domain-specific extraction patterns. AutoSlog-TS applies its patterns to every sentence in every document, so does not attempt to explicitly identify relevant sentences or documents. The next two baselines are more recent systems: the (Patwardhan and Riloff, 2007) semantic affinity model and the (Patwardhan and Riloff, 2009) GLACIER system. The semantic affinity approach 5 Pronouns were discarded since we do not perform coreference resolution. Duplicate extractions with the same head noun were counted as one hit or one miss. 6 We generated the Average scores ourselves by macroaveraging over the scores reported for the individual event roles. 1143 \x0cexplicitly identifies event sentences and uses patterns that have a semantic affinity for an event role to extract role fillers. GLACIER is a probabilistic model that incorporates both phrasal and sentential evidence jointly to label role fillers. The first 3 rows in</context>
</contexts>
<marker>Patwardhan, Riloff, 2009</marker>
<rawString>S. Patwardhan and E. Riloff. 2009. A Unified Model of Phrasal and Sentential Evidence for Information Extraction. In Proceedings of 2009 the Conference on Empirical Methods in Natural Language Processing (EMNLP-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>R Jones</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7443" citStr="Riloff and Jones, 1999" startWordPosition="1145" endWordPosition="1148">ng where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than templ</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Riloff</author>
<author>W Phillips</author>
</authors>
<title>An Introduction to the Sundance and AutoSlog Systems.</title>
<date>2004</date>
<tech>Technical Report UUCS-04-015,</tech>
<institution>School of Computing, University of Utah.</institution>
<contexts>
<context position="16291" citStr="Riloff and Phillips, 2004" startWordPosition="2576" endWordPosition="2579">r the event role. To train an SVM classifier, noun phrases corresponding to answer key strings for the event role are positive instances. We randomly choose among all noun phrases that are not in the answer keys to create a 10:1 ratio of negative to positive instances. 2 We intentionally do not use sentences that contain fillers for competing event roles as negative instances because sentences often contain multiple role fillers of different types (e.g., a weapon may be found near a body). Sentences without any role fillers are certain to be irrelevant contexts. 3 We used the Sundance parser (Riloff and Phillips, 2004) to identify noun phrases and assign semantic class labels. 1140 \x0cThe feature set for the role filler extractors is much richer than that of the sentence classifiers because they must carefully consider the local context surrounding a noun phrase. We will refer to the noun phrase being labelled as the targeted NP. The role filler extractors use three types of features: Lexical features: we represent four words to the left and four words to the right of the targeted NP, as well as the head noun and modifiers (adjectives and noun modifiers) of the targeted NP itself. Lexico-syntactic patterns</context>
</contexts>
<marker>Riloff, Phillips, 2004</marker>
<rawString>\x0cE. Riloff and W. Phillips. 2004. An Introduction to the Sundance and AutoSlog Systems. Technical Report UUCS-04-015, School of Computing, University of Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Constructing a Dictionary for Information Extraction Tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of the 11th National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="7271" citStr="Riloff, 1993" startWordPosition="1122" endWordPosition="1123">containing (roughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth </context>
<context position="16945" citStr="Riloff, 1993" startWordPosition="2686" endWordPosition="2687">ntic class labels. 1140 \x0cThe feature set for the role filler extractors is much richer than that of the sentence classifiers because they must carefully consider the local context surrounding a noun phrase. We will refer to the noun phrase being labelled as the targeted NP. The role filler extractors use three types of features: Lexical features: we represent four words to the left and four words to the right of the targeted NP, as well as the head noun and modifiers (adjectives and noun modifiers) of the targeted NP itself. Lexico-syntactic patterns: we use the AutoSlog pattern generator (Riloff, 1993) to automatically create lexico-syntactic patterns around each noun phrase in the sentence. These patterns are similar to dependency relations in that they typically represent the syntactic role of the NP with respect to other constituents (e.g., subject-of, object-of, and noun arguments). Semantic features: we use the Stanford NER tagger (Finkel et al., 2005) to determine if the targeted NP is a named entity, and we use the Sundance parser (Riloff and Phillips, 2004) to assign semantic class labels to each NPs head noun. 4 Event Narrative Document Classification One of our goals was to explor</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>E. Riloff. 1993. Automatically Constructing a Dictionary for Information Extraction Tasks. In Proceedings of the 11th National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>10441049</pages>
<publisher>The AAAI Press/MIT Press.</publisher>
<contexts>
<context position="7419" citStr="Riloff, 1996" startWordPosition="1143" endWordPosition="1144"> on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated rel</context>
<context position="28012" citStr="Riloff, 1996" startWordPosition="4449" endWordPosition="4450"> resolution or event tracking). Consequently, our evaluation follows that of other recent work and evaluates the accuracy of the extractions themselves by matching the head nouns of extracted NPs with the head nouns of answer key strings (e.g., armed guerrillas is considered to match guerrillas)5. Our results are reported as Precision/Recall/F(1)-score for each event role separately. We also show an overall average for all event roles combined.6 5.2 Baselines As baselines, we compare the performance of our IE system with three other event extraction systems. The first baseline is AutoSlog-TS (Riloff, 1996), which uses domain-specific extraction patterns. AutoSlog-TS applies its patterns to every sentence in every document, so does not attempt to explicitly identify relevant sentences or documents. The next two baselines are more recent systems: the (Patwardhan and Riloff, 2007) semantic affinity model and the (Patwardhan and Riloff, 2009) GLACIER system. The semantic affinity approach 5 Pronouns were discarded since we do not perform coreference resolution. Duplicate extractions with the same head noun were counted as one hit or one miss. 6 We generated the Average scores ourselves by macroaver</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 10441049. The AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Relational Learning via Propositional Algorithms: An Information Extraction Case Study.</title>
<date>2001</date>
<booktitle>In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>12571263</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="7884" citStr="Roth and Yih, 2001" startWordPosition="1215" endWordPosition="1218"> 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enf</context>
</contexts>
<marker>Roth, Yih, 2001</marker>
<rawString>D. Roth and W. Yih. 2001. Relational Learning via Propositional Algorithms: An Information Extraction Case Study. In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, pages 12571263, Seattle, WA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>On-demand information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06.</booktitle>
<contexts>
<context position="7579" citStr="Sekine, 2006" startWordPosition="1167" endWordPosition="1168">ed as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. How</context>
</contexts>
<marker>Sekine, 2006</marker>
<rawString>Satoshi Sekine. 2006. On-demand information extraction. In Proceedings of Joint Conference of the International Committee on Computational Linguistics and the Association for Computational Linguistics (COLING/ACL-06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Shinyama</author>
<author>S Sekine</author>
</authors>
<title>Preemptive Information Extraction using Unrestricted Relation Discovery.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>304--311</pages>
<location>New York City, NY,</location>
<contexts>
<context position="7579" citStr="Shinyama and Sekine, 2006" startWordPosition="1165" endWordPosition="1168">e characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. How</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Y. Shinyama and S. Sekine. 2006. Preemptive Information Extraction using Unrestricted Relation Discovery. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 304 311, New York City, NY, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
<author>D Fisher</author>
<author>J Aseltine</author>
<author>W Lehnert</author>
</authors>
<title>CRYSTAL: Inducing a conceptual dictionary.</title>
<date>1995</date>
<booktitle>In Proc. of the Fourteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>13141319</pages>
<contexts>
<context position="7295" citStr="Soderland et al., 1995" startWordPosition="1124" endWordPosition="1127">ughly) a 50/50 mix of relevant and irrelevant documents (e.g., MUC-3, MUC-4, MUC-6, and MUC-7 (Hirschman, 1998)). Our research focuses on this setting where the event extraction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko e</context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>S. Soderland, D. Fisher, J. Aseltine, and W. Lehnert. 1995. CRYSTAL: Inducing a conceptual dictionary. In Proc. of the Fourteenth International Joint Conference on Artificial Intelligence, pages 13141319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stevenson</author>
<author>M Greenwood</author>
</authors>
<title>A Semantic Approach to IE Pattern Induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>379386</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="7518" citStr="Stevenson and Greenwood, 2005" startWordPosition="1157" endWordPosition="1160"> relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and sea</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>M. Stevenson and M. Greenwood. 2005. A Semantic Approach to IE Pattern Induction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 379386, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sudo</author>
<author>S Sekine</author>
<author>R Grishman</author>
</authors>
<title>An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03).</booktitle>
<contexts>
<context position="7486" citStr="Sudo et al., 2003" startWordPosition="1153" endWordPosition="1156">red of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extrac</context>
</contexts>
<marker>Sudo, Sekine, Grishman, 2003</marker>
<rawString>K. Sudo, S. Sekine, and R. Grishman. 2003. An Improved Extraction Pattern Representation Model for Automatic IE Pattern Acquisition. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>R Grishman</author>
<author>P Tapanainen</author>
<author>S Huttunen</author>
</authors>
<title>Automatic Acquisition of Domain Knowledge for Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="7467" citStr="Yangarber et al., 2000" startWordPosition="1149" endWordPosition="1152">ction system is not assured of getting only relevant documents to process. Most event extraction models can be characterized as either pattern-based or classifier-based approaches. Early event extraction systems used handcrafted patterns (e.g., (Appelt et al., 1993; Lehnert et al., 1991)), but more recent systems generate patterns or rules automatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, Huttunen, 2000</marker>
<rawString>R. Yangarber, R. Grishman, P. Tapanainen, and S. Huttunen. 2000. Automatic Acquisition of Domain Knowledge for Information Extraction. In Proceedings of the Eighteenth International Conference on Computational Linguistics (COLING 2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yu</author>
<author>G Guan</author>
<author>M Zhou</author>
</authors>
<title>Resume Information Extraction with Cascaded Hybrid Model.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>499506</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="7804" citStr="Yu et al., 2005" startWordPosition="1202" endWordPosition="1205">utomatically using supervised learning (e.g., (Kim and Moldovan, 1993; Riloff, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a </context>
</contexts>
<marker>Yu, Guan, Zhou, 2005</marker>
<rawString>K. Yu, G. Guan, and M. Zhou. 2005. Resume Information Extraction with Cascaded Hybrid Model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 499506, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel Methods for Relation Extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="7906" citStr="Zelenko et al., 2003" startWordPosition="1219" endWordPosition="1222">al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna, 2001; Califf and Mooney, 2003)), weakly supervised learning (e.g., (Riloff, 1996; Riloff and Jones, 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005)), or unsupervised learning (e.g., (Shinyama and Sekine, 2006; Sekine, 2006)). In addition, many classifiers have been created to sequentially label event role fillers in a sentence (e.g., (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005)). Research has also been done on relation extraction (e.g., (Roth and Yih, 2001; Zelenko et al., 2003; Bunescu and Mooney, 2007)), but that task is different from event extraction because it focuses on isolated relations rather than template-based event analysis. Most event extraction systems scan a text and search small context windows using patterns or a classifier. However, recent work has begun to ex1138 \x0cFigure 1: TIER: A Multi-Layered Architecture for Event Extraction plore more global approaches. (Maslennikov and Chua, 2007) use discourse trees and local syntactic dependencies in a pattern-based framework to incorporate wider context. Ji and Grishman (2008) enforce event role consis</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel Methods for Relation Extraction. Journal of Machine Learning Research, 3.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>