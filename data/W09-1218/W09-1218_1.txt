b'Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 114119,
Boulder, Colorado, June 2009. c

2009 Association for Computational Linguistics
Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage
Approximate Max-Margin Linear Models
Yotaro Watanabe, Masayuki Asahara and Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, Japan, 630-0192
{yotaro-w, masayu-a, matsu}@is.naist.jp
Abstract
This paper describes a system for syntactic-
semantic dependency parsing for multiple lan-
guages. The system consists of three parts: a
state-of-the-art higher-order projective depen-
dency parser for syntactic dependency pars-
ing, a predicate classifier, and an argument
classifier for semantic dependency parsing.
For semantic dependency parsing, we ex-
plore use of global features. All components
are trained with an approximate max-margin
learning algorithm.
In the closed challenge of the CoNLL-2009
Shared Task (Hajic et al., 2009), our system
achieved the 3rd best performances for En-
glish and Czech, and the 4th best performance
for Japanese.
1 Introduction
In recent years, joint inference of syntactic and se-
mantic dependencies has attracted attention in NLP
communities. Ideally, we would like to choose the
most plausible syntactic-semantic structure among
all possible structures in that syntactic dependencies
and semantic dependencies are correlated. How-
ever, solving this problem is too difficult because
the search space of the problem is extremely large.
Therefore we focus on improving performance for
each subproblem: dependency parsing and semantic
role labeling.
In the past few years, research investigating
higher-order dependency parsing algorithms has
found its superiority to first-order parsing algo-
rithms. To reap the benefits of these advances, we
use a higher-order projective dependency parsing al-
gorithm (Carreras, 2007) which is an extension of
the span-based parsing algorithm (Eisner, 1996), for
syntactic dependency parsing.
In terms of semantic role labeling, we would
like to capture global information about predicate-
argument structures in order to accurately predict the
correct predicate-argument structure. Previous re-
search dealt with such information using re-ranking
(Toutanova et al., 2005; Johansson and Nugues,
2008). We explore a different approach to deal
with such information using global features. Use
of global features for structured prediction problem
has been explored by several NLP applications such
as sequential labeling (Finkel et al., 2005; Krishnan
and Manning, 2006; Kazama and Torisawa, 2007)
and dependency parsing (Nakagawa, 2007) with a
great deal of success. We attempt to use global fea-
tures for argument classification in which the most
plausible semantic role assignment is selected using
both local and global information. We present an
approximate max-margin learning algorithm for ar-
gument classifiers with global features.
2 Dependency Parsing
As in previous work, we use a linear model for de-
pendency parsing. The score function used in our
dependency parser is defined as follows.
s(y) =

(h,m)y
F(h, m, x) (1)
where h and m denote the head and the dependent
of the dependency edge in y, and F(h, m, x) is a
Factor that specifies dependency edge scores.
114
\x0cWe used a second-order factorization as in (Car-
reras, 2007). The second-order factor F is defined
as follows.
F(h, m, x) = w(h, m, x)+w(h, m, ch, x)
+ w  (h, m, cmi, x) + w  (h, m, cmo, x) (2)
where w is a parameter vector,  is a feature vector,
ch is the child of h in the span [h...m] that is closest
to m, cmi is the child of m in the span [h...m] that is
farthest from m and cmo is the child of m outside the
span [h...m] that is farthest from m. For more details
of the second-order parsing algorithm, see (Carreras,
2007).
For parser training, we use the Passive Aggres-
sive Algorithm (Crammer et al., 2006), which is an
approximate max-margin variant of the perceptron
algorithm. Also, we apply an efficient parameter av-
eraging technique (Daume III, 2006). The resulting
learning algorithm is shown in Algorithm 1.
Algorithm 1 A Passive Aggressive Algorithm with
parameter averaging
input Training set T = {xt, yt}T
t=1, Number of iterations
N and Parameter C
w  0, v  0, c  1
for i  0 to N do
for (xt, yt)  T do
y = arg maxy w  (xt, y) + (yt, y)
t = min

C, w(xt,y)w(xt,yt)+(yt,y)
||(xt,yt)(xt,y)||2

w  w + t((xt, yt)  (xt, y))
v  v + ct((xt, yt)  (xt, y))
c  c + 1
end for
end for
return w  v/c
We set (yt, y) as the number of incorrect head
predictions in the y, and C as 1.0.
Among the 7 languages of the task, 4 languages
(Czech, English, German and Japanese) contain
non-projective edges (13.94 %, 3.74 %, 25.79 %
and 0.91 % respectively), therefore we need to deal
with non-projectivity. In order to avoid losing the
benefits of higher-order parsing, we considered ap-
plying pseudo-projective transformation (Nivre and
Nilsson, 2005). However, growth of the number of
dependency labels by pseudo-projective transforma-
tion increases the dependency parser training time,
so we did not adopt transformations. Therefore, the
parser ignores the presence of non-projective edges
in the training and the testing phases.
The features used for our dependency parser are
based on those listed in (Johansson, 2008). In addi-
tion, distance features are used. We use shorthand
notations in order to simplify the feature represen-
tations: h, d, c, l, p, 1 and +1 cor-
respond to head, dependent, heads or dependents
child, lemma , POS, left position and right position
respectively.
First-order Features
Token features: hl, hp, hl+hp, dl, dp and dl+dp.
Head-Dependent features: hp+dp, hl+dl, hl+dl,
hl+hp+dl, hl+hp+dp, hl+dl+dp, hp+dl+dp and
hl+hp+dl+dp.
Context features: hp+hp+1+dp1+dp,
hp1+hp+dp1+dp, hp+hp+1+dp+dp+1 and
hp1+hp+dp+dp+1.
Distance features: The number of tokens between the
head and the dependent.
Second-order Features
Head-Dependent-Heads or Dependents Child:
hl+cl, hl+cl+cp, hp+cl, hp+cp, hp+dp+cp, dp+cp,
dp+cl+cp, dl+cp, dl+cp+cl
3 Semantic Role Labeling
Our SRL module consists of two parts: a predicate
classifier and an argument classifier. First, our sys-
tem determines the word sense for each predicate
with the predicate classifier, and then it detects the
highest scored argument assignment using the argu-
ment classifier with global features.
3.1 Predicate Classification
The first phase of SRL in our system is to detect
the word sense for each predicate. WSD can be for-
malized as a multi-class classification problem given
lemmas. We created a linear model for each lemma
and used the Passive Aggressive Algorithm with pa-
rameter averaging to train the models.
3.1.1 Features for Predicate Classification
Word features: Predicted lemma and the predicted POS
of the predicate, predicates head, and its conjunc-
tions.
Dependency label: The dependency label between the
predicate and the predicates head.
115
\x0cDependency label sequence: The concatenation of the
dependency labels of the predicate dependents.
Since effective features for predicate classifica-
tion are different for each language, we performed
greedy forward feature selection.
3.2 Argument Classification
In order to capture global clues of predicate-
argument structures, we consider introducing global
features for linear models. Let A(p) be a joint
assignment of role labels for argument candidates
given the predicate p. Then we define a score func-
tion s(A(p)) for argument label assignments A(p).
s(A(p)) =

k
Fk(x, A(p)) (3)
We introduce two factors: Local Factor FL and
Global Factor FG defined as follows.
FL(x, a(p)) = w  L(x, a(p)) (4)
FG(x, A(p)) = w  G(x, A(p)) (5)
where L, G denote feature vectors for the local
factor and the global factor respectively. FL scores a
particular role assignment for each argument candi-
date individually, and FG treats global features that
capture what structure the assignment A has. Re-
sulting scoring function for the assignment A(p) is
as follows.
s(A(p)) =

a(p)A(p)
wL(x, a(p))+wG(x, A(p))
(6)
Use of global features is problematic, because it
becomes difficult to find the highest assignment ef-
ficiently. In order to deal with the problem, we use
a simple approach, n-best relaxation as in (Kazama
and Torisawa, 2007). At first we generate n-best as-
signments using only the local factor, and then add
the global factor score for each n-best assignment, fi-
nally select the best scoring assignment from them.
In order to generate n-best assignments, we used a
beam-search algorithm.
3.2.1 Learning the Model
As in dependency parser and predicate classifier,
we train the model using the PA algorithm with pa-
rameter averaging. The learning algorithm is shown
in Algorithm 2. In this algorithm, the weights cor-
respond to local factor features L and global factor
features G are updated simultaneously.
Algorithm 2 Learning with Global Features for Ar-
gument Classification
input Training set T = {xt, At}T
t=1, Number of iterations
N and Parameter C
w  0, v  0, c  1
for i  0 to N do
for (xt, At)  T do
let (xt, A) =
P
aA L(xt, a) + G(xt, A)
generate n-best assignments {An
} using FL
A = arg maxA{An} w  (xt, A) + (At, A)
t = min

C, w(xt,A)w(xt,At)+(At,A)
||(xt,At)(xt,A)||2

w  w + t((xt, At)  (xt, A))
v  v + ct((xt, At)  (xt, A))
c  c + 1
end for
end for
return w  v/c
We set the margin value (A, A) as the number
of incorrect assignments plus (A, A), and C as 1.0.
The delta function returns 1 if at least one assign-
ment is different from the correct assignment and 0
otherwise.
The model is similar to re-ranking (Toutanova et
al., 2005; Johansson and Nugues, 2008). However
in contrast to re-ranking, we only have to prepare
one model. The re-ranking approach requires other
training datasets that are different from the data used
in local model training.
3.2.2 Features for Argument Classification
The local features used in our system are the same
as our previous work (Watanabe et al., 2008) except
for language dependent features. The global features
that used in our system are based on (Johansson and
Nugues, 2008) that used for re-ranking.
Local Features
Word features: Predicted lemma and predicted POS of
the predicate, predicates head, argument candidate,
argument candidates head, leftmost/rightmost de-
pendent and leftmost/rightmost sibling.
Dependency label: The dependency label of predicate,
argument candidate and argument candidates de-
pendent.
Family: The position of the argument candicate with re-
spect to the predicate position in the dependency
tree (e.g. child, sibling).
116
\x0cAverage Catalan Chinese Czech English German Japanese Spanish
Macro F1 Score 78.43 75.91 73.43 81.43 86.40 69.84 84.86 77.12
(78.00*) (74.83*) (73.43*) (81.38*) (86.40*) (68.39*) (84.84*) (76.74*)
Semantic Labeled F1 75.65 72.35 74.17 84.69 84.26 63.66 77.93 72.50
(75.17*) (71.05*) (74.17*) (84.66*) (84.26*) (61.94*) (77.91*) (72.25*)
Labeled Syntactic Accuracy 81.16 79.48 72.66 78.17 88.54 75.85 91.69 81.74
(80.77*) (78.62*) (72.66*) (78.10*) (88.54*) (74.60*) (91.66*) (81.23*)
Macro F1 Score 84.30 84.79 81.63 83.08 87.93 83.25 85.54 83.94
Semantic Labeled F1 81.58 80.99 79.99 86.67 85.09 79.46 79.03 79.85
Labeled Syntactic Accuracy 87.02 88.59 83.27 79.48 90.77 87.03 91.96 88.04
Table 1: Scores of our system.
Position: The position of the head of the dependency re-
lation with respect to the predicate position in the
sentence.
Pattern: The left-to-right chain of the predicted
POS/dependency labels of the predicates children.
Path features: Predicted lemma, predicted POS and de-
pendency label paths between the predicate and the
argument candidate.
Distance: The number of dependency edges between the
predicate and the argument candidate.
Global Features
Predicate-argument label sequence: The sequence of
the predicate sense and argument labels in the
predicate-argument strucuture.
Presence of labels defined in frame files: Whether the
semantic roles defined in the frame present in the
predicate-argument structure (e.g. MISSING:A1 or
CONTAINS:A1.)
3.2.3 Argument Pruning
We observe that most arguments tend to be not far
from its predicate, so we can prune argument candi-
dates to reduce search space. Since the characteris-
tics of the languages are slightly different, we apply
two types of pruning algorithms.
Pruning Algorithm 1: Let S be an argument candi-
date set. Initially set S   and start at predicate node.
Add dependents of the node to S, and move current node
to its parent. Repeat until current node reaches to ROOT.
Pruning Algorithm 2: Same as the Algorithm 1 ex-
cept that added nodes are its grandchildren as well as its
dependents.
The pruning results are shown in Table 2. Since
we could not prune arguments in Japanese accu-
rately using the two algorithms, we pruned argument
candidates simply by POS.
algorithm coverage (%) reduction (%)
Catalan 1 100 69.1
Chinese 1 98.9 69.1
Czech 2 98.5 49.1
English 1 97.3 63.1
German 1 98.3 64.3
Japanese POS 99.9 41.0
Spanish 1 100 69.7
Table 2: Pruning results.
4 Results
The submitted results on the test data are shown in
the upper part of Table 1. Due to a bug, we mistak-
enly used the gold lemmas in the dependency parser.
Corrected results are shown in the part marked with
*. The lower part shows the post evaluation results
with the gold lemmas and POSs.
For some of the 7 languages, since the global
model described in Section 3.2 degraded perfor-
mance compare to a model trained with only FL,
we did NOT use the model for all languages. We
used the global model for only three languages: Chi-
nese, English and Japanese. The remaining lan-
guages (Catalan, Czech, German and Spanish) used
a model trained with only FL.
4.1 Dependency Parsing Results
The parser achieved relatively high accuracies for
Czech, English and Japanese, and for each language,
the difference between the performance with correct
POS and predicted POS is not so large. However, in
Catalan, Chinese German and Spanish, the parsing
accuracies was seriously degraded by replacing cor-
rect POSs with predicted POSs (6.3 - 11.2 %). This
is likely because these languages have relatively low
predicted POS accuracies (92.3 - 95.5 %) ; Chinese
117
\x0cFL FL+FG (P, R)
Catalan 85.80 85.68 (+0.01, -0.26)
Chinese 86.58 87.39 (+0.24, +1.36)
Czech 89.63 89.05 (-0.87, -0.28)
English 85.66 85.74 (-0.87, +0.98)
German 80.82 77.30 (-7.27, +0.40)
Japanese 79.87 81.01 (+0.17, +1.88)
Spanish 84.38 83.89 (-0.42, -0.57)
Table 3: Effect of global features (semantic labeled F1).
P and R denote the differentials of labeled precision
and labeled recall between FL and FL+FG respectively.
has especially low accuracy (92.3%). The POS ac-
curacy may affect the parsing performances.
4.2 SRL Results
In order to highlight the effect of the global fea-
tures, we compared two models. The first model
is trained with only the local factor FL. The sec-
ond model is trained with both the local factor FL
and the global factor FG. The results are shown in
Table 3. In the experiments, we used the develop-
ment data with gold parse trees. For Chinese and
Japanese, significant improvements are obtained us-
ing the global features (over +1.0% in labeled re-
call and the slightly better labeled precision). How-
ever, for Catalan, Czech, German and Spanish, the
global features degraded the performance in labeled
F1. Especially, in German, the precision is substan-
tially degraded (-7.27% in labeled F1). These results
indicate that it is necessary to introduce language de-
pendent features.
4.3 Training, Evaluation Time and Memory
Requirements
Table 4 and 5 shows the training/evaluation times
and the memory consumption of the second-order
dependency parsers and the global argument classi-
fiers respectively. The training times of the predi-
cate classifier were less than one day, and the testing
times were mere seconds.
As reported in (Carreras, 2007; Johansson and
Nugues, 2008), training and inference of the second-
order parser are very expensive. For Chinese, we
could only complete 2 iterations.
In terms of the argument classifier, since N-best
generation time account for a substantial proportion
of the training time (in this work N = 100), chang-
iter hrs./iter sent./min. mem.
Catalan 9 14.6 9.0 9.6 GB
Chinese 2 56.5 3.7 16.2 GB
Czech 8 14.6 20.5 12.6 GB
English 7 22.0 13.4 15.1 GB
German 4 12.3 59.1 13.1 GB
Japanese 7 11.2 21.8 13.0 GB
Spanish 7 19.5 7.3 17.9 GB
Table 4: Training, evaluation time and memory require-
ments of the second-order dependency parsers. The iter
column denote the number of iterations of the model
used for the evaluations. Catalan, Czech and English
are trained on Xeon 3.0GHz, Chinese and Japanese are
trained on Xeon 2.66GHz, German and Spanish are
trained on Opteron 2.3GHz machines.
train (hrs.) sent./min. mem.
Chinese 6.5 453.7 2.0 GB
English 13.5 449.8 3.2 GB
Japanese 3.5 137.6 1.1 GB
Table 5: Training, evaluation time and memory require-
ments of the global argument classifiers. The classifiers
are all trained on Opteron 2.3GHz machines.
ing N affects the training and evaluation times sig-
nificantly.
All modules of our system are implemented in
Java. The required memory spaces shown in Table
4 and 5 are calculated by subtracting free memory
size from the total memory size of the Java VM.
Note that we observed that the value fluctuated dras-
tically while measuring memory usage, so the value
may not indicate precise memory requirements of
our system.
5 Conclusion
In this paper, we have described our system for syn-
tactic and semantic dependency analysis in multilin-
gual. Although our system is not a joint approach
but a pipeline approach, the system is comparable to
the top system for some of the 7 languages.
A further research direction we are investigating
is the application of various types of global features.
We believe that there is still room for improvements
since we used only two types of global features for
the argument classifier.
Another research direction is investigating joint
approaches. To the best of our knowledge, three
118
\x0ctypes of joint approaches have been proposed:
N-best based approach (Johansson and Nugues,
2008), synchronous joint approach (Henderson et
al., 2008), and a joint approach where parsing
and SRL are performed simultaneously (Llus and
Marquez, 2008). We attempted to perform N-
best based joint approach, however, the expen-
sive computational cost of the 2nd-order projective
parser discouraged it. We would like to investigate
syntactic-semantic joint approaches with reasonable
time complexities.
Acknowledgments
We would like to thank Richard Johansson for his
advice on parser implementation, and the CoNLL-
2009 organizers (Hajic et al., 2009; Taule et al.,
2008; Palmer and Xue, 2009; Hajic et al., 2006; Sur-
deanu et al., 2008; Burchardt et al., 2006; Kawahara
et al., 2002; Taule et al., 2008).
References
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pado, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proc. of LREC-2006, Genoa,
Italy.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proc. of EMNLP-
CoNLL 2007.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7:551585.
Hal Daume III. 2006. Practical Structured Learning
Techniques for Natural Language Processing. Ph.D.
thesis, University of Southern California, Los Ange-
les, CA, August.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing. In Proc. of ICCL 1996.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proc. of ACL 2005.
Jan Hajic, Jarmila Panevova, Eva Hajicova, Petr
Sgall, Petr Pajas, Jan Stepanek, Jir Havelka, Marie
Mikulova, and Zdenek Zabokrtsky. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajic, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antonia Mart, Llus
Marquez, Adam Meyers, Joakim Nivre, Sebastian
Pado, Jan Stepanek, Pavel Stranak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proc. of CoNLL-2009,
Boulder, Colorado, USA.
James Henderson, Paola Merlo, Gabriele Musillo, and
Ivan Titov. 2008. A latent variable model of syn-
chronous parsing for syntactic and semantic dependen-
cies. In Proc. of CoNLL 2008.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proc. of CoNLL
2008.
Richard Johansson. 2008. Dependency-based Semantic
Analysis of Natural-language Text. Ph.D. thesis, Lund
University.
Daisuke Kawahara, Sadao Kurohashi, and Koiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proc. of LREC-2002, pages 20082013,
Las Palmas, Canary Islands.
JunIchi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proc. of EMNLP-CoNLL 2007.
Vijay Krishnan and Christopher D. Manning. 2006. An
effective two-stage model for exploiting non-local de-
pendencies in named entity recognition. In Proc. of
ACL-COLING 2006.
Xavier Llus and Llus Marquez. 2008. A joint model for
parsing syntactic and semantic dependencies. In Proc.
of CoNLL 2008.
Tetsuji Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. of ACL 2005.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llus Marquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proc. of CoNLL-2008.
Mariona Taule, Maria Antonia Mart, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proc. of LREC-2008, Mar-
rakesh, Morroco.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proc. of ACL 2005.
Yotaro Watanabe, Masakazu Iwatate, Masayuki Asahara,
and Yuji Matsumoto. 2008. A pipeline approach for
syntactic and semantic dependency parsing. In Proc.
of CoNLL 2008.
119
\x0c'