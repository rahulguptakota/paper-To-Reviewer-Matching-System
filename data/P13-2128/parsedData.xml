<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<note confidence="0.301614">
b&apos;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731735,
</note>
<address confidence="0.488505">
Sofia, Bulgaria, August 4-9 2013. c
</address>
<figure confidence="0.29436125">
2013 Association for Computational Linguistics
Derivational Smoothing for Syntactic Distributional Semantics
Sebastian Pado
Jan Snajder
</figure>
<author confidence="0.636671">
Britta Zeller
</author>
<affiliation confidence="0.736984">
Heidelberg University, Institut fur Computerlinguistik
</affiliation>
<address confidence="0.635521">
69120 Heidelberg, Germany
</address>
<affiliation confidence="0.983558">
University of Zagreb, Faculty of Electrical Engineering and Computing
</affiliation>
<address confidence="0.918606">
Unska 3, 10000 Zagreb, Croatia
</address>
<email confidence="0.966791">
{pado, zeller}@cl.uni-heidelberg.de jan.snajder@fer.hr
</email>
<sectionHeader confidence="0.990075" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999318333333333">
Syntax-based vector spaces are used widely
in lexical semantics and are more versatile
than word-based spaces (Baroni and Lenci,
2010). However, they are also sparse, with
resulting reliability and coverage problems.
We address this problem by derivational
smoothing, which uses knowledge about
derivationally related words (oldish old)
to improve semantic similarity estimates.
We develop a set of derivational smoothing
methods and evaluate them on two lexical
semantics tasks in German. Even for mod-
els built from very large corpora, simple
derivational smoothing can improve cover-
age considerably.
</bodyText>
<sectionHeader confidence="0.99822" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999358142857143">
Distributional semantics (Turney and Pantel, 2010)
builds on the assumption that the semantic similar-
ity of words is strongly correlated to the overlap
between their linguistic contexts. This hypothesis
can be used to construct context vectors for words
directly from large text corpora in an unsupervised
manner. Such vector spaces have been applied suc-
cessfully to many problems in NLP (see Turney and
Pantel (2010) or Erk (2012) for current overviews).
Most distributional models in computational lex-
ical semantics are either (a) bag-of-words models,
where the context features are words within a sur-
face window around the target word, or (b) syn-
tactic models, where context features are typically
pairs of dependency relations and context words.
The advantage of syntactic models is that they
incorporate a richer, structured notion of context.
This makes them more versatile; the Distributional
Memory framework by Baroni and Lenci (2010) is
applicable to a wide range of tasks. It is also able
at least in principle to capture more fine-grained
types of semantic similarity such as predicate-
argument plausibility (Erk et al., 2010). At the
same time, syntactic spaces are much more prone
to sparsity problems, as their contexts are sparser.
This leads to reliability and coverage problems.
In this paper, we propose a novel strategy
for combating sparsity in syntactic vector spaces,
derivational smoothing. It follows the intuition that
derivationally related words (feed feeder, blocked
blockage) are, as a rule, semantically highly simi-
lar. Consequently, knowledge about derivationally
related words can be used as a back off for sparse
vectors in syntactic spaces. For example, the pair
oldish ancient should receive a high semantic
similarity, but in practice, the vector for oldish will
be very sparse, which makes this result uncertain.
Knowing that oldish is derivationally related to old
allows us to use the much less sparse vector for old
as a proxy for oldish.
We present a set of general methods for smooth-
ing vector similarity computations given a resource
that groups words into derivational families (equiv-
alence classes) and evaluate these methods on Ger-
man for two distributional tasks (similarity predic-
tion and synonym choice). We find that even for
syntactic models built from very large corpora, a
simple derivational resource that groups words on
morphological grounds can improve the results.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998743090909091">
Smoothing techniques either statistical, distribu-
tional, or knowledge-based are widely applied in
all areas of NLP. Many of the methods were first
applied in Language Modeling to deal with unseen
n-grams (Chen and Goodman, 1999; Dagan et al.,
1999). Query expansion methods in Information
Retrieval are also prominent cases of smoothing
that addresses the lexical mismatch between query
and document (Voorhees, 1994; Gonzalo et al.,
1998; Navigli and Velardi, 2003). In lexical se-
mantics, smoothing is often achieved by backing
</bodyText>
<page confidence="0.993695">
731
</page>
<bodyText confidence="0.999046842105263">
\x0coff from words to semantic classes, either adopted
from a resource such as WordNet (Resnik, 1996) or
induced from data (Pantel and Lin, 2002; Wang et
al., 2005; Erk et al., 2010). Similarly, distributional
features support generalization in Named Entity
Recognition (Finkel et al., 2005).
Although distributional information is often used
for smoothing, to our knowledge there is little
work on smoothing distributional models them-
selves. We see two main precursor studies for our
work. Bergsma et al. (2008) build models of se-
lectional preferences that include morphological
features such as capitalization and the presence of
digits. However, their approach is task-specific and
requires a (semi-)supervised setting. Allan and Ku-
maran (2003) make use of morphology by building
language models for stemming-based equivalence
classes. Our approach also uses morphological
processing, albeit more precise than stemming.
</bodyText>
<sectionHeader confidence="0.884404" genericHeader="method">
3 A Resource for German Derivation
</sectionHeader>
<bodyText confidence="0.99334712">
Derivational morphology describes the process of
building new (derived) words from other (basis)
words. Derived words can, but do not have to, share
the part-of-speech (POS) with their basis (oldA
oldishA vs. warmA warmV , warmthN ). Words
can be grouped into derivational families by form-
ing the transitive closure over individual derivation
relations. The words in these families are typically
semantically similar, although the exact degree de-
pends on the type of relation and idiosyncratic fac-
tors (bookN bookishA, Lieber (2009)).
For German, there are several resources with
derivational information. We use version 1.3
of DERIVBASE (Zeller et al., 2013),1 a freely
available resource that groups over 280,000 verbs,
nouns, and adjectives into more than 17,000 non-
singleton derivational families. It has a precision of
84% and a recall of 71%. Its higher coverage com-
pared to CELEX (Baayen et al., 1996) and IMSLEX
(Fitschen, 2004) makes it particularly suitable for
the use in smoothing, where the resource should
include low-frequency lemmas.
The following example illustrates a family that
covers three POSes as well as a word with a pre-
dominant metaphorical reading (to kneel to beg):
</bodyText>
<equation confidence="0.7130358">
knieenV (to kneelV ), beknieenV (to
begV ), KniendeN (kneeling personN ),
kniendA (kneelingA), KnieNn (kneeN )
1
Downloadable from: http://goo.gl/7KG2U
</equation>
<bodyText confidence="0.997790190476191">
Using derivational knowledge for smoothing raises
the question of how semantically similar the lem-
mas within a family really are. Fortunately, DE-
RIVBASE provides information that can be used in
this manner. It was constructed with hand-written
derivation rules, employing string transformation
functions that map basis lemmas onto derived lem-
mas. For example, a suffixation rule using the affix
heit generates the derivation dunkel Dunkel-
heit (darkA darknessN ). Since derivational fam-
ilies are defined as transitive closures, each pair
of words in a family is connected by a derivation
path. Because the rules do not have a perfect pre-
cision, our confidence in pairs of words decreases
the longer the derivation path between them. To re-
flect this, we assign each pair a confidence of 1/n,
where n is the length of the shortest path between
the lemmas. For example, bekleiden (enrobeV ) is
connected to Verkleidung (disguiseN ) through three
steps via the lemmas kleiden (dressV ) and verklei-
den (disguiseV ) and is assigned the confidence 1/3.
</bodyText>
<sectionHeader confidence="0.997358" genericHeader="method">
4 Models for Derivational Smoothing
</sectionHeader>
<bodyText confidence="0.991012740740741">
Derivational smoothing exploits the fact that deriva-
tionally related words are also semantically related,
by combining and/or comparing distributional rep-
resentations of derivationally related words. The
definition of a derivational smoothing algorithm
consists of two parts: a trigger and a scheme.
Notation. Given a word w, we use w to denote
its distributional vector and D(w) to denote the set
of vectors for the derivational family of w. We
assume that w D(w). For words that have no
derivations in DERIVBASE, D(w) is a singleton
set, D(w) = {w}. Let (w, w0) denote the confi-
dence of the pair (w, w0), as explained in Section 3.
Smoothing trigger. As discussed above, there is
no guarantee for high semantic similarity within a
derivational family. For this reason, smoothing may
also drown out information. In this paper, we report
on two triggers: smooth always always performs
smoothing; smooth if sim=0 smooths only when
the unsmoothed similarity sim(w1, w2) is zero or
unknown (due to w1 or w2 not being in the model).
Smoothing scheme. We present three smoothing
schemes, all of which apply to the level of complete
families. The first two schemes are exemplar-based
schemes, which define the smoothed similarity for
a word pair as a function of the pairwise similarities
between all words of the two derivational families.
</bodyText>
<page confidence="0.990103">
732
</page>
<bodyText confidence="0.7847555">
\x0cThe first one, maxSim, checks for particularly simi-
lar words in the families:
</bodyText>
<equation confidence="0.9411885">
maxSim(w1, w2) = max
w0
1D(w1)
w0
2D(w2)
sim(w0
1, w0
2)
</equation>
<bodyText confidence="0.9378545">
The second one, avgSim, computes the average
pairwise similarity (N is the number of pairs):
</bodyText>
<equation confidence="0.943365636363637">
avgSim(w1, w2) =
1
N
X
w0
1D(w1)
w0
2D(w2)
sim(w0
1, w0
2)
</equation>
<bodyText confidence="0.98357075">
The third scheme, centSim, is prototype-based. It
computes a centroid vector for each derivational
family, which can be thought of as a representation
for the concept(s) that it expresses:
</bodyText>
<equation confidence="0.9932596">
centSim(w1, w2) = sim c(D(w1)), c(D(w2))
\x01
where c(D(w)) =
P
w0D(w) (w, w0) w0 is the
</equation>
<bodyText confidence="0.998968818181818">
confidence-weighted centroid vector. centSim is
similar to avgSim. It is more efficient to calculate
and effectively introduces a kind or regularization,
where outliers in either family have less impact on
the overall result.
These models only represents a sample of possi-
ble derivational smoothing methods. We performed
a number of additional experiments (POS-restricted
smoothing, word-based, and pair-based smoothing
triggers), but they did not yield any improvements
over the simpler models we present here.
</bodyText>
<sectionHeader confidence="0.995057" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.991654166666667">
Syntactic Distributional Model. The syntactic
distributional model that we use represents target
words by pairs of dependency relations and context
words. More specifically, we use the W LW
matricization of DM.DE, the German version (Pado
and Utt, 2012) of Distributional Memory (Baroni
and Lenci, 2010). DM.DE was created on the basis
of the 884M-token SDEWAC web corpus (Faa et
al., 2010), lemmatized, tagged, and parsed with the
German MATE toolkit (Bohnet, 2010).
Experiments. We evaluate the impact of smooth-
ing on two standard tasks from lexical semantics.
The first task is predicting semantic similarity. We
lemmatized and POS-tagged the German GUR350
dataset (Zesch et al., 2007), a set of 350 word pairs
with human similarity judgments, created analo-
gously to the well-known Rubenstein and Good-
enough (1965) dataset for English.2 We predict
</bodyText>
<page confidence="0.875041">
2
</page>
<bodyText confidence="0.983754411764706">
Downloadable from: http://goo.gl/bFokI
semantic similarity as cosine similarity. We make
a prediction for a word pair if both words are repre-
sented in the semantic space and their vectors have
a non-zero similarity.
The second task is synonym choice on the Ger-
man version of the Readers Digest WordPower
dataset (Wallace and Wallace, 2005).2 This dataset,
which we also lemmatized and POS-tagged, con-
sists of 984 target words with four synonym can-
didates each (including phrases), one of which is
correct. Again, we compute semantic similarity as
the cosine between target and a candidate vector
and pick the highest-similarity candidate as syn-
onym. For phrases, we compute the maximum
pairwise word similarity. We make a prediction for
an item if the target as well as at least one candi-
date are represented in the semantic space and their
vectors have a non-zero similarity.
We expect differences between the two tasks
with regard to derivational smoothing, since the
words within derivational families are generally re-
lated but often not synonymous (cf. the example
in Section 3). Thus, semantic similarity judgments
should profit more easily from derivational smooth-
ing than synonym choice.
Baseline. Our baseline is a standard bag-of-
words vector space (BOW), which represents target
words by the words occurring in their context. We
use standard parameters (10 word window, 8.000
most frequent verb, noun, and adjective lemmas).
The model was created from the same corpus as
DM.DE. We also applied derivational smoothing
to this model, but did not obtain improvements.
Evaluation. To analyze the impact of smoothing,
we evaluate the coverage of models and the quality
of their predictions separately. In both tasks, cover-
age is the percentage of items for which we make
a prediction. We measure quality of the semantic
similarity task as the Pearson correlation between
the model predictions and the human judgments
for covered items (Zesch et al., 2007). For syn-
onym choice, we follow the method established by
Mohammad et al. (2007), measuring accuracy over
covered items, with partial credit for ties.
Results for Semantic Similarity. Table 1 shows
the results for the first task. The unsmoothed
DM.DE model attains a correlation of r = 0.44
and a coverage of 58.9%. Smoothing increases the
coverage substantially to 88%. Additionally, con-
servative, prototype-based smoothing (if sim = 0)
</bodyText>
<page confidence="0.983719">
733
</page>
<figure confidence="0.753457928571428">
\x0cSmoothing
trigger
Smoothing
scheme
r Cov
%
DM.DE, unsmoothed .44 58.9
DM.DE,
smooth always
avgSim .30 88.0
maxSim .43 88.0
centSim .44 88.0
DM.DE,
smooth if sim = 0
</figure>
<table confidence="0.87033525">
avgSim .43 88.0
maxSim .42 88.0
centSim .47 88.0
BOW baseline .36 94.9
</table>
<tableCaption confidence="0.998485">
Table 1: Results on the semantic similarity task
</tableCaption>
<bodyText confidence="0.988864051282051">
(r: Pearson correlation, Cov: Coverage)
increases correlation somewhat to r = 0.47. The
difference to the unsmoothed model is not signif-
icant at p = 0.05 according to Fishers (1925)
method of comparing correlation coefficients.
The bag-of-words baseline (BOW) has a greater
coverage than DM.DE models, but at the cost
of lower correlation across the board. The only
DM.DE model that performs worse than the BOW
baseline is the non-conservative avgSim (average
similarity) scheme. We attribute this weak per-
formance to the presence of many pairwise zero
similarities in the data, which makes the avgSim
predictions unreliable.
To our knowledge, there are no previous pub-
lished papers on distributional approaches to mod-
eling this dataset. The best previous result is a
GermaNet/Wikipedia-based model by Zesch et al.
(2007). It reports a higher correlation (r = 0.59)
but a very low coverage at 33.1%.
Results for Synonym Choice. The results for
the second task are shown in Table 2. The un-
smoothed model achieves an accuracy of 53.7%
and a coverage of 80.8%, as reported by Pado
and Utt (2012). Smoothing increases the cover-
age by almost 6% to 86.6% (for example, a ques-
tion item for inferior becomes covered after back-
ing off from the target to Inferioritat (inferiority)).
All smoothed models show a loss in accuracy, al-
beit small. The best model is again a conservative
smoothing model (sim = 0) with a loss of 1.1% ac-
curacy. Using bootstrap resampling (Efron and Tib-
shirani, 1993), we established that the difference
to the unsmoothed DM.DE model is not signifi-
cant at p &lt; 0.05. This time, the avgSim (average
similarity) smoothing scheme performs best, with
the prototype-based scheme in second place. Thus,
the results for synonym choice are less clear-cut:
derivational smoothing can trade accuracy against
</bodyText>
<figure confidence="0.884032454545455">
Smoothing
trigger
Smoothing
scheme
Acc
%
Cov
%
DM.DE, unsmoothed (Pado &amp; Utt 2012) 53.7 80.8
DM.DE,
smooth always
</figure>
<table confidence="0.977772777777778">
avgSim 46.0 86.6
maxSim 50.3 86.6
centSim 49.1 86.6
DM.DE,
smooth if sim = 0
avgSim 52.6 86.6
maxSim 51.2 86.6
centSim 51.3 86.6
BOW baseline 56.9 98.5
</table>
<tableCaption confidence="0.984754">
Table 2: Results on the synonym choice task
</tableCaption>
<bodyText confidence="0.863671666666667">
(Acc: Accuracy, Cov: Coverage)
coverage but does not lead to a clear improvement.
What is more, the BOW baseline significantly
outperforms all syntactic models, smoothed and
unsmoothed, with an almost perfect coverage com-
bined with a higher accuracy.
</bodyText>
<sectionHeader confidence="0.972922" genericHeader="conclusions">
6 Conclusions and Outlook
</sectionHeader>
<bodyText confidence="0.999437566666666">
In this paper, we have introduced derivational
smoothing, a novel strategy to combating sparsity
in syntactic vector spaces by comparing and com-
bining the vectors of morphologically related lem-
mas. The only information strictly necessary for
the methods we propose is a grouping of lemmas
into derivationally related classes. We have demon-
strated that derivational smoothing improves two
tasks, increasing coverage substantially and also
leading to a numerically higher correlation in the
semantic similarity task, even for vectors created
from a very large corpus. We obtained the best re-
sults for a conservative approach, smoothing only
zero similarities. This also explains our failure
to improve less sparse word-based models, where
very few pairs are assigned a similarity of zero.
A comparison of prototype- and exemplar-based
schemes did not yield a clear winner. The estima-
tion of generic semantic similarity can profit more
from derivational smoothing than the induction of
specific lexical relations.
In future work, we plan to work on other eval-
uation tasks, application to other languages, and
more sophisticated smoothing schemes.
Acknowledgments. Authors 1 and 3 were sup-
ported by the EC project EXCITEMENT (FP7 ICT-
287923). Author 2 was supported by the Croatian
Science Foundation (project 02.03/162: Deriva-
tional Semantic Models for Information Retrieval).
We thank Jason Utt for his support and expertise.
</bodyText>
<page confidence="0.996189">
734
</page>
<reference confidence="0.993225915887851">
\x0cReferences
James Allan and Giridhar Kumaran. 2003. Stemming
in the Language Modeling Framework. In Proceed-
ings of SIGIR, pages 455456.
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX Lexical Database. Re-
lease 2. LDC96L14. Linguistic Data Consortium,
University of Pennsylvania, Philadelphia, Pennsyl-
vania.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A General Framework for
Corpus-based Semantics. Computational Linguis-
tics, 36(4):673721.
Shane Bergsma, Dekang Lin, and Randy Goebel. 2008.
Discriminative Learning of Selectional Preference
from Unlabeled Text. In Proceedings of EMNLP,
pages 5968, Honolulu, Hawaii.
Bernd Bohnet. 2010. Top Accuracy and Fast Depen-
dency Parsing is not a Contradiction. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, pages 8997, Beijing, China.
Stanley F. Chen and Joshua Goodman. 1999. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Computer Speech and Language,
13(4):359394.
Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.
1999. Similarity-Based Models of Word Cooccur-
rence Probabilities. Machine Learning, 34(13):43
69.
Bradley Efron and Robert J. Tibshirani. 1993. An
Introduction to the Bootstrap. Chapman and Hall,
New York.
Katrin Erk, Sebastian Pado, and Ulrike Pado. 2010.
A Flexible, Corpus-driven Model of Regular and In-
verse Selectional Preferences. Computational Lin-
guistics, 36(4):723763.
Katrin Erk. 2012. Vector Space Models of Word Mean-
ing and Phrase Meaning: A Survey. Language and
Linguistics Compass, 6(10):635653.
Gertrud Faa, Ulrich Heid, and Helmut Schmid. 2010.
Design and Application of a Gold Standard for Mor-
phological Analysis: SMOR in Validation. In Pro-
ceedings of LREC-2010, pages 803810.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 363370.
Ronald Aylmer Fisher. 1925. Statistical methods for
research workers. Oliver and Boyd, Edinburgh.
Arne Fitschen. 2004. Ein computerlinguistisches
Lexikon als komplexes System. Ph.D. thesis, IMS,
Universitat Stuttgart.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and
Juan M. Cigarran. 1998. Indexing with WordNet
Synsets Can Improve Text Retrieval. In Proceed-
ings of the COLING/ACL Workshop on Usage of
WordNet in Natural Language Processing Systems,
Montreal, Canada.
Rochelle Lieber. 2009. Morphology and Lexical Se-
mantics. Cambridge University Press.
Saif Mohammad, Iryna Gurevych, Graeme Hirst, and
Torsten Zesch. 2007. Cross-Lingual Distributional
Profiles of Concepts for Measuring Semantic Dis-
tance. In Proceedings of the 2007 Joint Conference
on EMNLP and CoNLL, pages 571580, Prague,
Czech Republic.
Roberto Navigli and Paola Velardi. 2003. An Analysis
of Ontology-based Query Expansion Strategies. In
Workshop on Adaptive Text Extraction and Mining,
Dubrovnik, Croatia.
Sebastian Pado and Jason Utt. 2012. A Distributional
Memory for German. In Proceedings of KONVENS
2012 workshop on lexical-semantic resources and
applications, pages 462470, Vienna, Austria.
Patrick Pantel and Dekang Lin. 2002. Discovering
Word Senses from Text. In In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining, pages 613619.
Philip Resnik. 1996. Selectional Constraints: An
Information-theoretic Model and its Computational
Realization. Cognition, 61(1-2):127159.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual Correlates of Synonymy. Communica-
tions of the ACM, 8(10):627633.
Peter D. Turney and Patrick Pantel. 2010. From Fre-
quency to Meaning: Vector Space Models of Se-
mantics. Journal of Artificial Intelligence Research,
37(1):141188.
Ellen M. Voorhees. 1994. Query Expansion Using
Lexical-semantic Relations. In Proceedings of SI-
GIR, pages 6169.
DeWitt Wallace and Lila Acheson Wallace. 2005.
Readers Digest, das Beste fur Deutschland. Verlag
Das Beste, Stuttgart.
Qin Iris Wang, Dale Schuurmans, and Dekang Lin.
2005. Strictly Lexical Dependency Parsing. In Pro-
ceedings of IWPT, pages 152159.
Britta Zeller, Jan Snajder, and Sebastian Pado. 2013.
DErivBase: Inducing and Evaluating a Derivational
Morphology Resource for German. In Proceedings
of ACL, Sofia, Bulgaria.
Torsten Zesch, Iryna Gurevych, and Max Muhlhauser.
2007. Comparing Wikipedia and German Word-
net by Evaluating Semantic Relatedness on Multi-
ple Datasets. In Proceedings of NAACL/HLT, pages
205208, Rochester, NY.
</reference>
<page confidence="0.978066">
735
</page>
<figure confidence="0.249692">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347694">
<title>Derivational Smoothing for Syntactic Distributional Semantics</title>
<note confidence="0.82801375">b&apos;Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 731735, Sofia, Bulgaria, August 4-9 2013. c 2013 Association for Computational Linguistics Derivational Smoothing for Syntactic Distributional Semantics</note>
<author confidence="0.976360666666667">Sebastian Pado Jan Snajder Britta Zeller</author>
<affiliation confidence="0.994145">Heidelberg University, Institut fur Computerlinguistik</affiliation>
<address confidence="0.999805">69120 Heidelberg, Germany</address>
<affiliation confidence="0.994682">University of Zagreb, Faculty of Electrical Engineering and Computing</affiliation>
<address confidence="0.994535">Unska 3, 10000 Zagreb, Croatia</address>
<email confidence="0.938809">pado@cl.uni-heidelberg.dejan.snajder@fer.hr</email>
<email confidence="0.938809">zeller@cl.uni-heidelberg.dejan.snajder@fer.hr</email>
<abstract confidence="0.9951729375">Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010). However, they are also sparse, with resulting reliability and coverage problems. We address this problem by derivational smoothing, which uses knowledge about derivationally related words (oldish old) to improve semantic similarity estimates. We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German. Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences James Allan</author>
<author>Giridhar Kumaran</author>
</authors>
<title>Stemming in the Language Modeling Framework.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>455456</pages>
<contexts>
<context position="4871" citStr="Allan and Kumaran (2003)" startWordPosition="722" endWordPosition="726">(Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building language models for stemming-based equivalence classes. Our approach also uses morphological processing, albeit more precise than stemming. 3 A Resource for German Derivation Derivational morphology describes the process of building new (derived) words from other (basis) words. Derived words can, but do not have to, share the part-of-speech (POS) with their basis (oldA oldishA vs. warmA warmV , warmthN ). Words can be grouped into derivational families by forming the transitive closure over individual derivation relations. The words in these families are typ</context>
</contexts>
<marker>Allan, Kumaran, 2003</marker>
<rawString>\x0cReferences James Allan and Giridhar Kumaran. 2003. Stemming in the Language Modeling Framework. In Proceedings of SIGIR, pages 455456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald R Baayen</author>
<author>Richard Piepenbrock</author>
<author>Leon Gulikers</author>
</authors>
<date>1996</date>
<booktitle>The CELEX Lexical Database. Release 2. LDC96L14. Linguistic Data</booktitle>
<institution>Consortium, University of Pennsylvania,</institution>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="5990" citStr="Baayen et al., 1996" startWordPosition="895" endWordPosition="898">ming the transitive closure over individual derivation relations. The words in these families are typically semantically similar, although the exact degree depends on the type of relation and idiosyncratic factors (bookN bookishA, Lieber (2009)). For German, there are several resources with derivational information. We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families. It has a precision of 84% and a recall of 71%. Its higher coverage compared to CELEX (Baayen et al., 1996) and IMSLEX (Fitschen, 2004) makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas. The following example illustrates a family that covers three POSes as well as a word with a predominant metaphorical reading (to kneel to beg): knieenV (to kneelV ), beknieenV (to begV ), KniendeN (kneeling personN ), kniendA (kneelingA), KnieNn (kneeN ) 1 Downloadable from: http://goo.gl/7KG2U Using derivational knowledge for smoothing raises the question of how semantically similar the lemmas within a family really are. Fortunately, DERIVBASE provides </context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1996</marker>
<rawString>Harald R. Baayen, Richard Piepenbrock, and Leon Gulikers. 1996. The CELEX Lexical Database. Release 2. LDC96L14. Linguistic Data Consortium, University of Pennsylvania, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional Memory: A General Framework for Corpus-based Semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="671" citStr="Baroni and Lenci, 2010" startWordPosition="79" endWordPosition="82">Association for Computational Linguistics, pages 731735, Sofia, Bulgaria, August 4-9 2013. c 2013 Association for Computational Linguistics Derivational Smoothing for Syntactic Distributional Semantics Sebastian Pado Jan Snajder Britta Zeller Heidelberg University, Institut fur Computerlinguistik 69120 Heidelberg, Germany University of Zagreb, Faculty of Electrical Engineering and Computing Unska 3, 10000 Zagreb, Croatia {pado, zeller}@cl.uni-heidelberg.de jan.snajder@fer.hr Abstract Syntax-based vector spaces are used widely in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010). However, they are also sparse, with resulting reliability and coverage problems. We address this problem by derivational smoothing, which uses knowledge about derivationally related words (oldish old) to improve semantic similarity estimates. We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German. Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably. 1 Introduction Distributional semantics (Turney and Pantel, 2010) builds on the assumption that the semantic similarity of words is </context>
<context position="2097" citStr="Baroni and Lenci (2010)" startWordPosition="293" endWordPosition="296">ector spaces have been applied successfully to many problems in NLP (see Turney and Pantel (2010) or Erk (2012) for current overviews). Most distributional models in computational lexical semantics are either (a) bag-of-words models, where the context features are words within a surface window around the target word, or (b) syntactic models, where context features are typically pairs of dependency relations and context words. The advantage of syntactic models is that they incorporate a richer, structured notion of context. This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks. It is also able at least in principle to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010). At the same time, syntactic spaces are much more prone to sparsity problems, as their contexts are sparser. This leads to reliability and coverage problems. In this paper, we propose a novel strategy for combating sparsity in syntactic vector spaces, derivational smoothing. It follows the intuition that derivationally related words (feed feeder, blocked blockage) are, as a rule, semantically highly si</context>
<context position="10258" citStr="Baroni and Lenci, 2010" startWordPosition="1576" endWordPosition="1579">result. These models only represents a sample of possible derivational smoothing methods. We performed a number of additional experiments (POS-restricted smoothing, word-based, and pair-based smoothing triggers), but they did not yield any improvements over the simpler models we present here. 5 Experimental Evaluation Syntactic Distributional Model. The syntactic distributional model that we use represents target words by pairs of dependency relations and context words. More specifically, we use the W LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic si</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional Memory: A General Framework for Corpus-based Semantics. Computational Linguistics, 36(4):673721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Discriminative Learning of Selectional Preference from Unlabeled Text.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>5968</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="4636" citStr="Bergsma et al. (2008)" startWordPosition="690" endWordPosition="693">o et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building language models for stemming-based equivalence classes. Our approach also uses morphological processing, albeit more precise than stemming. 3 A Resource for German Derivation Derivational morphology describes the process of building new (derived) words from other (basis) words. Derived words can, but do not have to, share the p</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2008</marker>
<rawString>Shane Bergsma, Dekang Lin, and Randy Goebel. 2008. Discriminative Learning of Selectional Preference from Unlabeled Text. In Proceedings of EMNLP, pages 5968, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top Accuracy and Fast Dependency Parsing is not a Contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>8997</pages>
<location>Beijing, China.</location>
<contexts>
<context position="10421" citStr="Bohnet, 2010" startWordPosition="1605" endWordPosition="1606">ed, and pair-based smoothing triggers), but they did not yield any improvements over the simpler models we present here. 5 Experimental Evaluation Syntactic Distributional Model. The syntactic distributional model that we use represents target words by pairs of dependency relations and context words. More specifically, we use the W LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic similarity as cosine similarity. We make a prediction for a word pair if both words are represented in the semantic space and their vectors have a non-zero similarit</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top Accuracy and Fast Dependency Parsing is not a Contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 8997, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling.</title>
<date>1999</date>
<journal>Computer Speech and Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="3819" citStr="Chen and Goodman, 1999" startWordPosition="565" endWordPosition="568">ity computations given a resource that groups words into derivational families (equivalence classes) and evaluate these methods on German for two distributional tasks (similarity prediction and synonym choice). We find that even for syntactic models built from very large corpora, a simple derivational resource that groups words on morphological grounds can improve the results. 2 Related Work Smoothing techniques either statistical, distributional, or knowledge-based are widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Al</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An Empirical Study of Smoothing Techniques for Language Modeling. Computer Speech and Language, 13(4):359394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Similarity-Based Models of Word Cooccurrence Probabilities.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>34</volume>
<issue>13</issue>
<pages>69</pages>
<contexts>
<context position="3840" citStr="Dagan et al., 1999" startWordPosition="569" endWordPosition="572"> resource that groups words into derivational families (equivalence classes) and evaluate these methods on German for two distributional tasks (similarity prediction and synonym choice). We find that even for syntactic models built from very large corpora, a simple derivational resource that groups words on morphological grounds can improve the results. 2 Related Work Smoothing techniques either statistical, distributional, or knowledge-based are widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1999</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando C. N. Pereira. 1999. Similarity-Based Models of Word Cooccurrence Probabilities. Machine Learning, 34(13):43 69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap. Chapman and Hall,</title>
<date>1993</date>
<location>New York.</location>
<contexts>
<context position="14981" citStr="Efron and Tibshirani, 1993" startWordPosition="2343" endWordPosition="2347">r = 0.59) but a very low coverage at 33.1%. Results for Synonym Choice. The results for the second task are shown in Table 2. The unsmoothed model achieves an accuracy of 53.7% and a coverage of 80.8%, as reported by Pado and Utt (2012). Smoothing increases the coverage by almost 6% to 86.6% (for example, a question item for inferior becomes covered after backing off from the target to Inferioritat (inferiority)). All smoothed models show a loss in accuracy, albeit small. The best model is again a conservative smoothing model (sim = 0) with a loss of 1.1% accuracy. Using bootstrap resampling (Efron and Tibshirani, 1993), we established that the difference to the unsmoothed DM.DE model is not significant at p &lt; 0.05. This time, the avgSim (average similarity) smoothing scheme performs best, with the prototype-based scheme in second place. Thus, the results for synonym choice are less clear-cut: derivational smoothing can trade accuracy against Smoothing trigger Smoothing scheme Acc % Cov % DM.DE, unsmoothed (Pado &amp; Utt 2012) 53.7 80.8 DM.DE, smooth always avgSim 46.0 86.6 maxSim 50.3 86.6 centSim 49.1 86.6 DM.DE, smooth if sim = 0 avgSim 52.6 86.6 maxSim 51.2 86.6 centSim 51.3 86.6 BOW baseline 56.9 98.5 Tabl</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman and Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
<author>Ulrike Pado</author>
</authors>
<title>A Flexible,</title>
<date>2010</date>
<booktitle>Corpus-driven Model of Regular and Inverse Selectional Preferences. Computational Linguistics,</booktitle>
<pages>36--4</pages>
<contexts>
<context position="2291" citStr="Erk et al., 2010" startWordPosition="326" endWordPosition="329">e either (a) bag-of-words models, where the context features are words within a surface window around the target word, or (b) syntactic models, where context features are typically pairs of dependency relations and context words. The advantage of syntactic models is that they incorporate a richer, structured notion of context. This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks. It is also able at least in principle to capture more fine-grained types of semantic similarity such as predicateargument plausibility (Erk et al., 2010). At the same time, syntactic spaces are much more prone to sparsity problems, as their contexts are sparser. This leads to reliability and coverage problems. In this paper, we propose a novel strategy for combating sparsity in syntactic vector spaces, derivational smoothing. It follows the intuition that derivationally related words (feed feeder, blocked blockage) are, as a rule, semantically highly similar. Consequently, knowledge about derivationally related words can be used as a back off for sparse vectors in syntactic spaces. For example, the pair oldish ancient should receive a high sem</context>
<context position="4306" citStr="Erk et al., 2010" startWordPosition="643" endWordPosition="646"> all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building</context>
</contexts>
<marker>Erk, Pado, Pado, 2010</marker>
<rawString>Katrin Erk, Sebastian Pado, and Ulrike Pado. 2010. A Flexible, Corpus-driven Model of Regular and Inverse Selectional Preferences. Computational Linguistics, 36(4):723763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--10</pages>
<contexts>
<context position="1585" citStr="Erk (2012)" startWordPosition="218" endWordPosition="219">uate them on two lexical semantics tasks in German. Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably. 1 Introduction Distributional semantics (Turney and Pantel, 2010) builds on the assumption that the semantic similarity of words is strongly correlated to the overlap between their linguistic contexts. This hypothesis can be used to construct context vectors for words directly from large text corpora in an unsupervised manner. Such vector spaces have been applied successfully to many problems in NLP (see Turney and Pantel (2010) or Erk (2012) for current overviews). Most distributional models in computational lexical semantics are either (a) bag-of-words models, where the context features are words within a surface window around the target word, or (b) syntactic models, where context features are typically pairs of dependency relations and context words. The advantage of syntactic models is that they incorporate a richer, structured notion of context. This makes them more versatile; the Distributional Memory framework by Baroni and Lenci (2010) is applicable to a wide range of tasks. It is also able at least in principle to captur</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector Space Models of Word Meaning and Phrase Meaning: A Survey. Language and Linguistics Compass, 6(10):635653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertrud Faa</author>
<author>Ulrich Heid</author>
<author>Helmut Schmid</author>
</authors>
<date>2010</date>
<contexts>
<context position="10345" citStr="Faa et al., 2010" startWordPosition="1592" endWordPosition="1595">performed a number of additional experiments (POS-restricted smoothing, word-based, and pair-based smoothing triggers), but they did not yield any improvements over the simpler models we present here. 5 Experimental Evaluation Syntactic Distributional Model. The syntactic distributional model that we use represents target words by pairs of dependency relations and context words. More specifically, we use the W LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic similarity as cosine similarity. We make a prediction for a word pair if both words are r</context>
</contexts>
<marker>Faa, Heid, Schmid, 2010</marker>
<rawString>Gertrud Faa, Ulrich Heid, and Helmut Schmid. 2010.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Design</author>
</authors>
<title>Application of a Gold Standard for Morphological Analysis: SMOR in Validation.</title>
<booktitle>In Proceedings of LREC-2010,</booktitle>
<pages>803810</pages>
<marker>Design, </marker>
<rawString>Design and Application of a Gold Standard for Morphological Analysis: SMOR in Validation. In Proceedings of LREC-2010, pages 803810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL,</booktitle>
<pages>363370</pages>
<contexts>
<context position="4415" citStr="Finkel et al., 2005" startWordPosition="656" endWordPosition="659"> (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of morphology by building language models for stemming-based equivalence classes. Our approach also uses morphological processing, alb</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43rd Annual Meeting of the ACL, pages 363370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Aylmer Fisher</author>
</authors>
<title>Statistical methods for research workers. Oliver and Boyd,</title>
<date>1925</date>
<location>Edinburgh.</location>
<marker>Fisher, 1925</marker>
<rawString>Ronald Aylmer Fisher. 1925. Statistical methods for research workers. Oliver and Boyd, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Fitschen</author>
</authors>
<title>Ein computerlinguistisches Lexikon als komplexes System.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>IMS, Universitat Stuttgart.</institution>
<contexts>
<context position="6018" citStr="Fitschen, 2004" startWordPosition="901" endWordPosition="902">individual derivation relations. The words in these families are typically semantically similar, although the exact degree depends on the type of relation and idiosyncratic factors (bookN bookishA, Lieber (2009)). For German, there are several resources with derivational information. We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families. It has a precision of 84% and a recall of 71%. Its higher coverage compared to CELEX (Baayen et al., 1996) and IMSLEX (Fitschen, 2004) makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas. The following example illustrates a family that covers three POSes as well as a word with a predominant metaphorical reading (to kneel to beg): knieenV (to kneelV ), beknieenV (to begV ), KniendeN (kneeling personN ), kniendA (kneelingA), KnieNn (kneeN ) 1 Downloadable from: http://goo.gl/7KG2U Using derivational knowledge for smoothing raises the question of how semantically similar the lemmas within a family really are. Fortunately, DERIVBASE provides information that can be used</context>
</contexts>
<marker>Fitschen, 2004</marker>
<rawString>Arne Fitschen. 2004. Ein computerlinguistisches Lexikon als komplexes System. Ph.D. thesis, IMS, Universitat Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
<author>Irina Chugur</author>
<author>Juan M Cigarran</author>
</authors>
<title>Indexing with WordNet Synsets Can Improve Text Retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING/ACL Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="4029" citStr="Gonzalo et al., 1998" startWordPosition="596" endWordPosition="599">e find that even for syntactic models built from very large corpora, a simple derivational resource that groups words on morphological grounds can improve the results. 2 Related Work Smoothing techniques either statistical, distributional, or knowledge-based are widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al.</context>
</contexts>
<marker>Gonzalo, Verdejo, Chugur, Cigarran, 1998</marker>
<rawString>Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan M. Cigarran. 1998. Indexing with WordNet Synsets Can Improve Text Retrieval. In Proceedings of the COLING/ACL Workshop on Usage of WordNet in Natural Language Processing Systems, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rochelle Lieber</author>
</authors>
<title>Morphology and Lexical Semantics.</title>
<date>2009</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5614" citStr="Lieber (2009)" startWordPosition="836" endWordPosition="837">rocessing, albeit more precise than stemming. 3 A Resource for German Derivation Derivational morphology describes the process of building new (derived) words from other (basis) words. Derived words can, but do not have to, share the part-of-speech (POS) with their basis (oldA oldishA vs. warmA warmV , warmthN ). Words can be grouped into derivational families by forming the transitive closure over individual derivation relations. The words in these families are typically semantically similar, although the exact degree depends on the type of relation and idiosyncratic factors (bookN bookishA, Lieber (2009)). For German, there are several resources with derivational information. We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families. It has a precision of 84% and a recall of 71%. Its higher coverage compared to CELEX (Baayen et al., 1996) and IMSLEX (Fitschen, 2004) makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas. The following example illustrates a family that covers three POSes as well as a wo</context>
</contexts>
<marker>Lieber, 2009</marker>
<rawString>Rochelle Lieber. 2009. Morphology and Lexical Semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Iryna Gurevych</author>
<author>Graeme Hirst</author>
<author>Torsten Zesch</author>
</authors>
<title>Cross-Lingual Distributional Profiles of Concepts for Measuring Semantic Distance.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on EMNLP and CoNLL,</booktitle>
<pages>571580</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="12847" citStr="Mohammad et al. (2007)" startWordPosition="1991" endWordPosition="1994">jective lemmas). The model was created from the same corpus as DM.DE. We also applied derivational smoothing to this model, but did not obtain improvements. Evaluation. To analyze the impact of smoothing, we evaluate the coverage of models and the quality of their predictions separately. In both tasks, coverage is the percentage of items for which we make a prediction. We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items (Zesch et al., 2007). For synonym choice, we follow the method established by Mohammad et al. (2007), measuring accuracy over covered items, with partial credit for ties. Results for Semantic Similarity. Table 1 shows the results for the first task. The unsmoothed DM.DE model attains a correlation of r = 0.44 and a coverage of 58.9%. Smoothing increases the coverage substantially to 88%. Additionally, conservative, prototype-based smoothing (if sim = 0) 733 \x0cSmoothing trigger Smoothing scheme r Cov % DM.DE, unsmoothed .44 58.9 DM.DE, smooth always avgSim .30 88.0 maxSim .43 88.0 centSim .44 88.0 DM.DE, smooth if sim = 0 avgSim .43 88.0 maxSim .42 88.0 centSim .47 88.0 BOW baseline .36 94.</context>
</contexts>
<marker>Mohammad, Gurevych, Hirst, Zesch, 2007</marker>
<rawString>Saif Mohammad, Iryna Gurevych, Graeme Hirst, and Torsten Zesch. 2007. Cross-Lingual Distributional Profiles of Concepts for Measuring Semantic Distance. In Proceedings of the 2007 Joint Conference on EMNLP and CoNLL, pages 571580, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>An Analysis of Ontology-based Query Expansion Strategies.</title>
<date>2003</date>
<booktitle>In Workshop on Adaptive Text Extraction and Mining,</booktitle>
<location>Dubrovnik, Croatia.</location>
<contexts>
<context position="4057" citStr="Navigli and Velardi, 2003" startWordPosition="600" endWordPosition="603">yntactic models built from very large corpora, a simple derivational resource that groups words on morphological grounds can improve the results. 2 Related Work Smoothing techniques either statistical, distributional, or knowledge-based are widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of sele</context>
</contexts>
<marker>Navigli, Velardi, 2003</marker>
<rawString>Roberto Navigli and Paola Velardi. 2003. An Analysis of Ontology-based Query Expansion Strategies. In Workshop on Adaptive Text Extraction and Mining, Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Jason Utt</author>
</authors>
<title>A Distributional Memory for German.</title>
<date>2012</date>
<booktitle>In Proceedings of KONVENS 2012 workshop on lexical-semantic resources and applications,</booktitle>
<pages>462470</pages>
<location>Vienna, Austria.</location>
<contexts>
<context position="10208" citStr="Pado and Utt, 2012" startWordPosition="1569" endWordPosition="1572">either family have less impact on the overall result. These models only represents a sample of possible derivational smoothing methods. We performed a number of additional experiments (POS-restricted smoothing, word-based, and pair-based smoothing triggers), but they did not yield any improvements over the simpler models we present here. 5 Experimental Evaluation Syntactic Distributional Model. The syntactic distributional model that we use represents target words by pairs of dependency relations and context words. More specifically, we use the W LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 </context>
<context position="14590" citStr="Pado and Utt (2012)" startWordPosition="2276" endWordPosition="2279">cheme. We attribute this weak performance to the presence of many pairwise zero similarities in the data, which makes the avgSim predictions unreliable. To our knowledge, there are no previous published papers on distributional approaches to modeling this dataset. The best previous result is a GermaNet/Wikipedia-based model by Zesch et al. (2007). It reports a higher correlation (r = 0.59) but a very low coverage at 33.1%. Results for Synonym Choice. The results for the second task are shown in Table 2. The unsmoothed model achieves an accuracy of 53.7% and a coverage of 80.8%, as reported by Pado and Utt (2012). Smoothing increases the coverage by almost 6% to 86.6% (for example, a question item for inferior becomes covered after backing off from the target to Inferioritat (inferiority)). All smoothed models show a loss in accuracy, albeit small. The best model is again a conservative smoothing model (sim = 0) with a loss of 1.1% accuracy. Using bootstrap resampling (Efron and Tibshirani, 1993), we established that the difference to the unsmoothed DM.DE model is not significant at p &lt; 0.05. This time, the avgSim (average similarity) smoothing scheme performs best, with the prototype-based scheme in </context>
<context position="15393" citStr="Pado &amp; Utt 2012" startWordPosition="2408" endWordPosition="2411">moothed models show a loss in accuracy, albeit small. The best model is again a conservative smoothing model (sim = 0) with a loss of 1.1% accuracy. Using bootstrap resampling (Efron and Tibshirani, 1993), we established that the difference to the unsmoothed DM.DE model is not significant at p &lt; 0.05. This time, the avgSim (average similarity) smoothing scheme performs best, with the prototype-based scheme in second place. Thus, the results for synonym choice are less clear-cut: derivational smoothing can trade accuracy against Smoothing trigger Smoothing scheme Acc % Cov % DM.DE, unsmoothed (Pado &amp; Utt 2012) 53.7 80.8 DM.DE, smooth always avgSim 46.0 86.6 maxSim 50.3 86.6 centSim 49.1 86.6 DM.DE, smooth if sim = 0 avgSim 52.6 86.6 maxSim 51.2 86.6 centSim 51.3 86.6 BOW baseline 56.9 98.5 Table 2: Results on the synonym choice task (Acc: Accuracy, Cov: Coverage) coverage but does not lead to a clear improvement. What is more, the BOW baseline significantly outperforms all syntactic models, smoothed and unsmoothed, with an almost perfect coverage combined with a higher accuracy. 6 Conclusions and Outlook In this paper, we have introduced derivational smoothing, a novel strategy to combating sparsit</context>
</contexts>
<marker>Pado, Utt, 2012</marker>
<rawString>Sebastian Pado and Jason Utt. 2012. A Distributional Memory for German. In Proceedings of KONVENS 2012 workshop on lexical-semantic resources and applications, pages 462470, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering Word Senses from Text. In</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613619</pages>
<contexts>
<context position="4268" citStr="Pantel and Lin, 2002" startWordPosition="635" endWordPosition="638"> or knowledge-based are widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (20</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering Word Senses from Text. In In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 613619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selectional Constraints: An Information-theoretic Model and its Computational Realization.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="4225" citStr="Resnik, 1996" startWordPosition="629" endWordPosition="630"> either statistical, distributional, or knowledge-based are widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi</context>
</contexts>
<marker>Resnik, 1996</marker>
<rawString>Philip Resnik. 1996. Selectional Constraints: An Information-theoretic Model and its Computational Realization. Cognition, 61(1-2):127159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual Correlates of Synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="10772" citStr="Rubenstein and Goodenough (1965)" startWordPosition="1657" endWordPosition="1661">W LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic similarity as cosine similarity. We make a prediction for a word pair if both words are represented in the semantic space and their vectors have a non-zero similarity. The second task is synonym choice on the German version of the Readers Digest WordPower dataset (Wallace and Wallace, 2005).2 This dataset, which we also lemmatized and POS-tagged, consists of 984 target words with four synonym candidates each (including phrases), one of which is correct. Again, we compute semantic similarity as the cosine betwee</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the ACM, 8(10):627633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From Frequency to Meaning: Vector Space Models of Semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1204" citStr="Turney and Pantel, 2010" startWordPosition="154" endWordPosition="157">in lexical semantics and are more versatile than word-based spaces (Baroni and Lenci, 2010). However, they are also sparse, with resulting reliability and coverage problems. We address this problem by derivational smoothing, which uses knowledge about derivationally related words (oldish old) to improve semantic similarity estimates. We develop a set of derivational smoothing methods and evaluate them on two lexical semantics tasks in German. Even for models built from very large corpora, simple derivational smoothing can improve coverage considerably. 1 Introduction Distributional semantics (Turney and Pantel, 2010) builds on the assumption that the semantic similarity of words is strongly correlated to the overlap between their linguistic contexts. This hypothesis can be used to construct context vectors for words directly from large text corpora in an unsupervised manner. Such vector spaces have been applied successfully to many problems in NLP (see Turney and Pantel (2010) or Erk (2012) for current overviews). Most distributional models in computational lexical semantics are either (a) bag-of-words models, where the context features are words within a surface window around the target word, or (b) synt</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From Frequency to Meaning: Vector Space Models of Semantics. Journal of Artificial Intelligence Research, 37(1):141188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Query Expansion Using Lexical-semantic Relations.</title>
<date>1994</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>6169</pages>
<contexts>
<context position="4007" citStr="Voorhees, 1994" startWordPosition="594" endWordPosition="595">nonym choice). We find that even for syntactic models built from very large corpora, a simple derivational resource that groups words on morphological grounds can improve the results. 2 Related Work Smoothing techniques either statistical, distributional, or knowledge-based are widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for ou</context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Ellen M. Voorhees. 1994. Query Expansion Using Lexical-semantic Relations. In Proceedings of SIGIR, pages 6169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DeWitt Wallace</author>
<author>Lila Acheson Wallace</author>
</authors>
<date>2005</date>
<booktitle>Readers Digest, das Beste fur Deutschland. Verlag Das Beste,</booktitle>
<location>Stuttgart.</location>
<contexts>
<context position="11147" citStr="Wallace and Wallace, 2005" startWordPosition="1718" endWordPosition="1721"> first task is predicting semantic similarity. We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic similarity as cosine similarity. We make a prediction for a word pair if both words are represented in the semantic space and their vectors have a non-zero similarity. The second task is synonym choice on the German version of the Readers Digest WordPower dataset (Wallace and Wallace, 2005).2 This dataset, which we also lemmatized and POS-tagged, consists of 984 target words with four synonym candidates each (including phrases), one of which is correct. Again, we compute semantic similarity as the cosine between target and a candidate vector and pick the highest-similarity candidate as synonym. For phrases, we compute the maximum pairwise word similarity. We make a prediction for an item if the target as well as at least one candidate are represented in the semantic space and their vectors have a non-zero similarity. We expect differences between the two tasks with regard to der</context>
</contexts>
<marker>Wallace, Wallace, 2005</marker>
<rawString>DeWitt Wallace and Lila Acheson Wallace. 2005. Readers Digest, das Beste fur Deutschland. Verlag Das Beste, Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Iris Wang</author>
<author>Dale Schuurmans</author>
<author>Dekang Lin</author>
</authors>
<title>Strictly Lexical Dependency Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT,</booktitle>
<pages>152159</pages>
<contexts>
<context position="4287" citStr="Wang et al., 2005" startWordPosition="639" endWordPosition="642">e widely applied in all areas of NLP. Many of the methods were first applied in Language Modeling to deal with unseen n-grams (Chen and Goodman, 1999; Dagan et al., 1999). Query expansion methods in Information Retrieval are also prominent cases of smoothing that addresses the lexical mismatch between query and document (Voorhees, 1994; Gonzalo et al., 1998; Navigli and Velardi, 2003). In lexical semantics, smoothing is often achieved by backing 731 \x0coff from words to semantic classes, either adopted from a resource such as WordNet (Resnik, 1996) or induced from data (Pantel and Lin, 2002; Wang et al., 2005; Erk et al., 2010). Similarly, distributional features support generalization in Named Entity Recognition (Finkel et al., 2005). Although distributional information is often used for smoothing, to our knowledge there is little work on smoothing distributional models themselves. We see two main precursor studies for our work. Bergsma et al. (2008) build models of selectional preferences that include morphological features such as capitalization and the presence of digits. However, their approach is task-specific and requires a (semi-)supervised setting. Allan and Kumaran (2003) make use of mor</context>
</contexts>
<marker>Wang, Schuurmans, Lin, 2005</marker>
<rawString>Qin Iris Wang, Dale Schuurmans, and Dekang Lin. 2005. Strictly Lexical Dependency Parsing. In Proceedings of IWPT, pages 152159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Britta Zeller</author>
<author>Jan Snajder</author>
<author>Sebastian Pado</author>
</authors>
<title>DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="5741" citStr="Zeller et al., 2013" startWordPosition="853" endWordPosition="856">rocess of building new (derived) words from other (basis) words. Derived words can, but do not have to, share the part-of-speech (POS) with their basis (oldA oldishA vs. warmA warmV , warmthN ). Words can be grouped into derivational families by forming the transitive closure over individual derivation relations. The words in these families are typically semantically similar, although the exact degree depends on the type of relation and idiosyncratic factors (bookN bookishA, Lieber (2009)). For German, there are several resources with derivational information. We use version 1.3 of DERIVBASE (Zeller et al., 2013),1 a freely available resource that groups over 280,000 verbs, nouns, and adjectives into more than 17,000 nonsingleton derivational families. It has a precision of 84% and a recall of 71%. Its higher coverage compared to CELEX (Baayen et al., 1996) and IMSLEX (Fitschen, 2004) makes it particularly suitable for the use in smoothing, where the resource should include low-frequency lemmas. The following example illustrates a family that covers three POSes as well as a word with a predominant metaphorical reading (to kneel to beg): knieenV (to kneelV ), beknieenV (to begV ), KniendeN (kneeling pe</context>
</contexts>
<marker>Zeller, Snajder, Pado, 2013</marker>
<rawString>Britta Zeller, Jan Snajder, and Sebastian Pado. 2013. DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German. In Proceedings of ACL, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
<author>Max Muhlhauser</author>
</authors>
<title>Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple Datasets.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL/HLT,</booktitle>
<pages>205208</pages>
<location>Rochester, NY.</location>
<contexts>
<context position="10643" citStr="Zesch et al., 2007" startWordPosition="1637" endWordPosition="1640">at we use represents target words by pairs of dependency relations and context words. More specifically, we use the W LW matricization of DM.DE, the German version (Pado and Utt, 2012) of Distributional Memory (Baroni and Lenci, 2010). DM.DE was created on the basis of the 884M-token SDEWAC web corpus (Faa et al., 2010), lemmatized, tagged, and parsed with the German MATE toolkit (Bohnet, 2010). Experiments. We evaluate the impact of smoothing on two standard tasks from lexical semantics. The first task is predicting semantic similarity. We lemmatized and POS-tagged the German GUR350 dataset (Zesch et al., 2007), a set of 350 word pairs with human similarity judgments, created analogously to the well-known Rubenstein and Goodenough (1965) dataset for English.2 We predict 2 Downloadable from: http://goo.gl/bFokI semantic similarity as cosine similarity. We make a prediction for a word pair if both words are represented in the semantic space and their vectors have a non-zero similarity. The second task is synonym choice on the German version of the Readers Digest WordPower dataset (Wallace and Wallace, 2005).2 This dataset, which we also lemmatized and POS-tagged, consists of 984 target words with four</context>
<context position="12767" citStr="Zesch et al., 2007" startWordPosition="1977" endWordPosition="1980">e standard parameters (10 word window, 8.000 most frequent verb, noun, and adjective lemmas). The model was created from the same corpus as DM.DE. We also applied derivational smoothing to this model, but did not obtain improvements. Evaluation. To analyze the impact of smoothing, we evaluate the coverage of models and the quality of their predictions separately. In both tasks, coverage is the percentage of items for which we make a prediction. We measure quality of the semantic similarity task as the Pearson correlation between the model predictions and the human judgments for covered items (Zesch et al., 2007). For synonym choice, we follow the method established by Mohammad et al. (2007), measuring accuracy over covered items, with partial credit for ties. Results for Semantic Similarity. Table 1 shows the results for the first task. The unsmoothed DM.DE model attains a correlation of r = 0.44 and a coverage of 58.9%. Smoothing increases the coverage substantially to 88%. Additionally, conservative, prototype-based smoothing (if sim = 0) 733 \x0cSmoothing trigger Smoothing scheme r Cov % DM.DE, unsmoothed .44 58.9 DM.DE, smooth always avgSim .30 88.0 maxSim .43 88.0 centSim .44 88.0 DM.DE, smooth </context>
<context position="14319" citStr="Zesch et al. (2007)" startWordPosition="2225" endWordPosition="2228">correlation coefficients. The bag-of-words baseline (BOW) has a greater coverage than DM.DE models, but at the cost of lower correlation across the board. The only DM.DE model that performs worse than the BOW baseline is the non-conservative avgSim (average similarity) scheme. We attribute this weak performance to the presence of many pairwise zero similarities in the data, which makes the avgSim predictions unreliable. To our knowledge, there are no previous published papers on distributional approaches to modeling this dataset. The best previous result is a GermaNet/Wikipedia-based model by Zesch et al. (2007). It reports a higher correlation (r = 0.59) but a very low coverage at 33.1%. Results for Synonym Choice. The results for the second task are shown in Table 2. The unsmoothed model achieves an accuracy of 53.7% and a coverage of 80.8%, as reported by Pado and Utt (2012). Smoothing increases the coverage by almost 6% to 86.6% (for example, a question item for inferior becomes covered after backing off from the target to Inferioritat (inferiority)). All smoothed models show a loss in accuracy, albeit small. The best model is again a conservative smoothing model (sim = 0) with a loss of 1.1% acc</context>
</contexts>
<marker>Zesch, Gurevych, Muhlhauser, 2007</marker>
<rawString>Torsten Zesch, Iryna Gurevych, and Max Muhlhauser. 2007. Comparing Wikipedia and German Wordnet by Evaluating Semantic Relatedness on Multiple Datasets. In Proceedings of NAACL/HLT, pages 205208, Rochester, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>