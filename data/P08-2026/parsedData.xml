<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000676">
<bodyText confidence="0.615211">
b&amp;apos;Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 101104,
Columbus, Ohio, USA, June 2008. c
</bodyText>
<sectionHeader confidence="0.495962" genericHeader="abstract">
2008 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.81338">
Self-Training for Biomedical Parsing
</title>
<author confidence="0.907809">
David McClosky and Eugene Charniak
</author>
<affiliation confidence="0.9016645">
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
</affiliation>
<address confidence="0.974578">
Providence, RI 02912
</address>
<email confidence="0.998536">
{dmcc|ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.990505" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.9975468">
Parser self-training is the technique of
taking an existing parser, parsing extra
data and then creating a second parser
by treating the extra data as further
training data. Here we apply this tech-
nique to parser adaptation. In partic-
ular, we self-train the standard Char-
niak/Johnson Penn-Treebank parser us-
ing unlabeled biomedical abstracts. This
achieves an f-score of 84.3% on a stan-
dard test set of biomedical abstracts from
the Genia corpus. This is a 20% error re-
duction over the best previous result on
biomedical data (80.2% on the same test
set).
</bodyText>
<sectionHeader confidence="0.997491" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9966105">
Parser self-training is the technique of taking an
existing parser, parsing extra data and then cre-
ating a second parser by treating the extra data
as further training data. While for many years it
was thought not to help state-of-the art parsers,
more recent work has shown otherwise. In this
paper we apply this technique to parser adap-
tation. In particular we self-train the standard
Charniak/Johnson Penn-Treebank (C/J) parser
using unannotated biomedical data. As is well
known, biomedical data is hard on parsers be-
cause it is so far from more standard English.
To our knowledge this is the first application of
self-training where the gap between the training
and self-training data is so large.
In section two, we look at previous work. In
particular we note that there is, in fact, very
little data on self-training when the corpora for
self-training is so different from the original la-
beled data. Section three describes our main
experiment on standard test data (Clegg and
Shepherd, 2005). Section four looks at some
preliminary results we obtained on development
data that show in slightly more detail how self-
training improved the parser. We conclude in
section five.
</bodyText>
<sectionHeader confidence="0.994591" genericHeader="method">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.99937424">
While self-training has worked in several do-
mains, the early results on self-training for pars-
ing were negative (Steedman et al., 2003; Char-
niak, 1997). However more recent results have
shown that it can indeed improve parser perfor-
mance (Bacchiani et al., 2006; McClosky et al.,
2006a; McClosky et al., 2006b).
One possible use for this technique is for
parser adaptation initially training the parser
on one type of data for which hand-labeled trees
are available (e.g., Wall Street Journal (M. Mar-
cus et al., 1993)) and then self-training on a sec-
ond type of data in order to adapt the parser
to the second domain. Interestingly, there is lit-
tle to no data showing that this actually works.
Two previous papers would seem to address this
issue: the work by Bacchiani et al. (2006) and
McClosky et al. (2006b). However, in both cases
the evidence is equivocal.
Bacchiani and Roark train the Roark parser
(Roark, 2001) on trees from the Brown treebank
and then self-train and test on data from Wall
Street Journal. While they show some improve-
ment (from 75.7% to 80.5% f-score) there are
several aspects of this work which leave its re-
</bodyText>
<page confidence="0.998956">
101
</page>
<bodyText confidence="0.993435139534884">
\x0csults less than convincing as to the utility of self-
training for adaptation. The first is the pars-
ing results are quite poor by modern standards.1
Steedman et al. (2003) generally found that self-
training does not work, but found that it does
help if the baseline results were sufficiently bad.
Secondly, the difference between the Brown
corpus treebank and the Wall Street Journal
corpus is not that great. One way to see this
is to look at out-of-vocabulary statistics. The
Brown corpus has an out-of-vocabulary rate of
approximately 6% when given WSJ training as
the lexicon. In contrast, the out-of-vocabulary
rate of biomedical abstracts given the same lex-
icon is significantly higher at about 25% (Lease
and Charniak, 2005). Thus the bridge the self-
trained parser is asked to build is quite short.
This second point is emphasized by the sec-
ond paper on self-training for adaptation (Mc-
Closky et al., 2006b). This paper is based on the
C/J parser and thus its results are much more
in line with modern expectations. In particu-
lar, it was able to achieve an f-score of 87% on
Brown treebank test data when trained and self-
trained on WSJ-like data. Note this last point.
It was not the case that it used the self-training
to bridge the corpora difference. It self-trained
on NANC, not Brown. NANC is a news corpus,
quite like WSJ data. Thus the point of that
paper was that self-training a WSJ parser on
similar data makes the parser more flexible, not
better adapted to the target domain in particu-
lar. It said nothing about the task we address
here. Thus our claim is that previous results are
quite ambiguous on the issue of bridging corpora
for parser adaptation.
Turning briefly to previous results on Medline
data, the best comparative study of parsers is
that of Clegg and Shepherd (2005), which eval-
uates several statistical parsers. Their best re-
sult was an f-score of 80.2%. This was on the
Lease/Charniak (L/C) parser (Lease and Char-
niak, 2005).2 A close second (1% behind) was
</bodyText>
<page confidence="0.872666">
1
</page>
<bodyText confidence="0.984477">
This is not a criticism of the work. The results are
completely in line with what one would expect given the
base parser and the relatively small size of the Brown
treebank.
</bodyText>
<page confidence="0.968041">
2
</page>
<bodyText confidence="0.999737555555556">
This is the standard Charniak parser (without
the parser of Bikel (2004). The other parsers
were not close. However, several very good cur-
rent parsers were not available when this paper
was written (e.g., the Berkeley Parser (Petrov
et al., 2006)). However, since the newer parsers
do not perform quite as well as the C/J parser
on WSJ data, it is probably the case that they
would not significantly alter the landscape.
</bodyText>
<sectionHeader confidence="0.950641" genericHeader="method">
3 Central Experimental Result
</sectionHeader>
<bodyText confidence="0.997970527777778">
We used as the base parser the standardly avail-
able C/J parser. We then self-trained the parser
on approximately 270,000 sentences a ran-
dom selection of abstracts from Medline.3 Med-
line is a large database of abstracts and citations
from a wide variety of biomedical literature. As
we note in the next section, the number 270,000
was selected by observing performance on a de-
velopment set.
We weighted the original WSJ hand anno-
tated sentences equally with self-trained Med-
line data. So, for example, McClosky et al.
(2006a) found that the data from the hand-
annotated WSJ data should be considered at
least five times more important than NANC
data on an event by event level. We did no tun-
ing to find out if there is some better weighting
for our domain than one-to-one.
The resulting parser was tested on a test cor-
pus of hand-parsed sentences from the Genia
Treebank (Tateisi et al., 2005). These are ex-
actly the same sentences as used in the com-
parisons of the last section. Genia is a corpus
of abstracts from the Medline database selected
from a search with the keywords Human, Blood
Cells, and Transcription Factors. Thus the Ge-
nia treebank data are all from a small domain
within Biology. As already noted, the Medline
abstracts used for self-training were chosen ran-
domly and thus span a large number of biomed-
ical sub-domains.
The results, the central results of this paper,
are shown in Figure 1. Clegg and Shepherd
(2005) do not provide separate precision and
recall numbers. However we can see that the
reranker) modified to use an in-domain tagger.
</bodyText>
<page confidence="0.743396">
3
</page>
<footnote confidence="0.311104">
http://www.ncbi.nlm.nih.gov/PubMed/
</footnote>
<page confidence="0.960183">
102
</page>
<table confidence="0.980791666666667">
\x0cSystem Precision Recall f-score
L/C 80.2%
Self-trained 86.3% 82.4% 84.3%
</table>
<figureCaption confidence="0.997544">
Figure 1: Comparison of the Medline self-trained
</figureCaption>
<bodyText confidence="0.9781734">
parser against the previous best
Medline self-trained parser achieves an f-score
of 84.3%, which is an absolute reduction in er-
ror of 4.1%. This corresponds to an error rate
reduction of 20% over the L/C baseline.
</bodyText>
<sectionHeader confidence="0.996995" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999732018518519">
Prior to the above experiment on the test data,
we did several preliminary experiments on devel-
opment data from the Genia Treebank. These
results are summarized in Figure 2. Here we
show the f-score for four versions of the parser
as a function of number of self-training sen-
tences. The dashed line on the bottom is the
raw C/J parser with no self-training. At 80.4, it
is clearly the worst of the lot. On the other hand,
it is already better than the 80.2% best previous
result for biomedical data. This is solely due to
the introduction of the 50-best reranker which
distinguishes the C/J parser from the preceding
Charniak parser.
The almost flat line above it is the C/J parser
with NANC self-training data. As mentioned
previously, NANC is a news corpus, quite like
the original WSJ data. At 81.4% it gives us a
one percent improvement over the original WSJ
parser.
The topmost line, is the C/J parser trained
on Medline data. As can be seen, even just a
thousand lines of Medline is already enough to
drive our results to a new level and it contin-
ues to improve until about 150,000 sentences at
which point performance is nearly flat. How-
ever, as 270,000 sentences is fractionally better
than 150,000 sentences that is the number of
self-training sentences we used for our results
on the test set.
Lastly, the middle jagged line is for an inter-
esting idea that failed to work. We mention it
in the hope that others might be able to succeed
where we have failed.
We reasoned that textbooks would be a par-
ticularly good bridging corpus. After all, they
are written to introduce someone ignorant of
a field to the ideas and terminology within it.
Thus one might expect that the English of a Bi-
ology textbook would be intermediate between
the more typical English of a news article and
the specialized English native to the domain.
To test this we created a corpus of seven texts
(BioBooks) on various areas of biology that
were available on the web. We observe in Fig-
ure 2 that for all quantities of self-training data
one does better with Medline than BioBooks.
For example, at 37,000 sentences the BioBook
corpus is only able to achieve and an f-measure
of 82.8% while the Medline corpus is at 83.4%.
Furthermore, BioBooks levels off in performance
while Medline has significant improvement left
in it. Thus, while the hypothesis seems reason-
able, we were unable to make it work.
</bodyText>
<sectionHeader confidence="0.994467" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99886425">
We self-trained the standard C/J parser on
270,000 sentences of Medline abstracts. By do-
ing so we achieved a 20% error reduction over
the best previous result for biomedical parsing.
In terms of the gap between the supervised data
and the self-trained data, this is the largest that
has been attempted.
Furthermore, the resulting parser is of interest
in its own right, being as it is the most accurate
biomedical parser yet developed. This parser is
available on the web.4
Finally, there is no reason to believe that
84.3% is an upper bound on what can be
achieved with current techniques. Lease and
Charniak (2005) achieve their results using small
amounts of hand-annotated biomedical part-of-
speech-tagged data and also explore other pos-
sible sources or information. It is reasonable to
assume that its use would result in further im-
provement.
</bodyText>
<sectionHeader confidence="0.917288" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989314333333333">
This work was supported by DARPA GALE con-
tract HR0011-06-2-0001. We would like to thank the
BLLIP team for their comments.
</bodyText>
<figure confidence="0.942710285714286">
4
http://bllip.cs.brown.edu/biomedical/
103
\x0c0 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000
Number of sentences added
80.0
80.2
80.4
80.6
80.8
81.0
81.2
81.4
81.6
81.8
82.0
82.2
82.4
82.6
82.8
83.0
83.2
83.4
83.6
83.8
84.0
84.2
84.4
Reranking
parser
f-score
WSJ+Medline
WSJ+BioBooks
WSJ+NANC
WSJ (baseline)
</figure>
<figureCaption confidence="0.981405">
Figure 2: Labeled Precision-Recall results on development data for four versions of the parser as a function
of number of self-training sentences
</figureCaption>
<sectionHeader confidence="0.897319" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998662959183674">
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of
stochastic grammars. Computer Speech and Lan-
guage, 20(1):4168.
Daniel M. Bikel. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4).
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proc. AAAI, pages 598603.
Andrew B. Clegg and Adrian Shepherd. 2005.
Evaluating and integrating treebank parsers on
a biomedical corpus. In Proceedings of the ACL
Workshop on Software.
Matthew Lease and Eugene Charniak. 2005. Pars-
ing biomedical literature. In Second International
Joint Conference on Natural Language Processing
(IJCNLP05).
M. Marcus et al. 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp.
Linguistics, 19(2):313330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Main Conference,
pages 152159.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for
parser adaptation. In Proceedings of COLING-
ACL 2006, pages 337344, Sydney, Australia,
July. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceed-
ings of COLING-ACL 2006, pages 433440, Syd-
ney, Australia, July. Association for Computa-
tional Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249276.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proc. of European ACL (EACL),
pages 331338.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii.
2005. Syntax Annotation for the GENIA corpus.
Proc. IJCNLP 2005, Companion volume, pages
222227.
</reference>
<page confidence="0.996765">
104
</page>
<figure confidence="0.244766">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.759213">
<note confidence="0.911187">b&amp;apos;Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 101104, Columbus, Ohio, USA, June 2008. c 2008 Association for Computational Linguistics</note>
<title confidence="0.998346">Self-Training for Biomedical Parsing</title>
<author confidence="0.99914">David McClosky</author>
<author confidence="0.99914">Eugene Charniak</author>
<affiliation confidence="0.985863">Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University</affiliation>
<address confidence="0.999058">Providence, RI 02912</address>
<email confidence="0.999781">{dmcc|ec}@cs.brown.edu</email>
<abstract confidence="0.9961231875">Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. Here we apply this technique to parser adaptation. In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts. This achieves an f-score of 84.3% on a standard test set of biomedical abstracts from the Genia corpus. This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michiel Bacchiani</author>
<author>Michael Riley</author>
<author>Brian Roark</author>
<author>Richard Sproat</author>
</authors>
<title>MAP adaptation of stochastic grammars.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="2393" citStr="Bacchiani et al., 2006" startWordPosition="375" endWordPosition="378">pora for self-training is so different from the original labeled data. Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005). Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser. We conclude in section five. 2 Previous Work While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997). However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b). One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain. Interestingly, there is little to no data showing that this actually works. Two previous papers would seem to address this issue: the work by Bacchiani et al. (2006) and McClosky et al. (2006b). However, in both cases the evidence is equivocal</context>
</contexts>
<marker>Bacchiani, Riley, Roark, Sproat, 2006</marker>
<rawString>Michiel Bacchiani, Michael Riley, Brian Roark, and Richard Sproat. 2006. MAP adaptation of stochastic grammars. Computer Speech and Language, 20(1):4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Intricacies of collins parsing model.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="5517" citStr="Bikel (2004)" startWordPosition="921" endWordPosition="922"> bridging corpora for parser adaptation. Turning briefly to previous results on Medline data, the best comparative study of parsers is that of Clegg and Shepherd (2005), which evaluates several statistical parsers. Their best result was an f-score of 80.2%. This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 1 This is not a criticism of the work. The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank. 2 This is the standard Charniak parser (without the parser of Bikel (2004). The other parsers were not close. However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al., 2006)). However, since the newer parsers do not perform quite as well as the C/J parser on WSJ data, it is probably the case that they would not significantly alter the landscape. 3 Central Experimental Result We used as the base parser the standardly available C/J parser. We then self-trained the parser on approximately 270,000 sentences a random selection of abstracts from Medline.3 Medline is a large database of abstracts an</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of collins parsing model. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. AAAI,</booktitle>
<pages>598603</pages>
<contexts>
<context position="2283" citStr="Charniak, 1997" startWordPosition="358" endWordPosition="360">ious work. In particular we note that there is, in fact, very little data on self-training when the corpora for self-training is so different from the original labeled data. Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005). Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser. We conclude in section five. 2 Previous Work While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997). However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b). One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain. Interestingly, there is little to no data showing that this actually works. Two previous papers would seem to address this issue: the</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. AAAI, pages 598603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew B Clegg</author>
<author>Adrian Shepherd</author>
</authors>
<title>Evaluating and integrating treebank parsers on a biomedical corpus.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Software.</booktitle>
<contexts>
<context position="1934" citStr="Clegg and Shepherd, 2005" startWordPosition="300" endWordPosition="303">he standard Charniak/Johnson Penn-Treebank (C/J) parser using unannotated biomedical data. As is well known, biomedical data is hard on parsers because it is so far from more standard English. To our knowledge this is the first application of self-training where the gap between the training and self-training data is so large. In section two, we look at previous work. In particular we note that there is, in fact, very little data on self-training when the corpora for self-training is so different from the original labeled data. Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005). Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser. We conclude in section five. 2 Previous Work While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997). However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b). One possible use for this technique is for parser adaptation initially training the parser</context>
<context position="5073" citStr="Clegg and Shepherd (2005)" startWordPosition="840" endWordPosition="843"> It was not the case that it used the self-training to bridge the corpora difference. It self-trained on NANC, not Brown. NANC is a news corpus, quite like WSJ data. Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular. It said nothing about the task we address here. Thus our claim is that previous results are quite ambiguous on the issue of bridging corpora for parser adaptation. Turning briefly to previous results on Medline data, the best comparative study of parsers is that of Clegg and Shepherd (2005), which evaluates several statistical parsers. Their best result was an f-score of 80.2%. This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 1 This is not a criticism of the work. The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank. 2 This is the standard Charniak parser (without the parser of Bikel (2004). The other parsers were not close. However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Pe</context>
<context position="7328" citStr="Clegg and Shepherd (2005)" startWordPosition="1233" endWordPosition="1236"> test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors. Thus the Genia treebank data are all from a small domain within Biology. As already noted, the Medline abstracts used for self-training were chosen randomly and thus span a large number of biomedical sub-domains. The results, the central results of this paper, are shown in Figure 1. Clegg and Shepherd (2005) do not provide separate precision and recall numbers. However we can see that the reranker) modified to use an in-domain tagger. 3 http://www.ncbi.nlm.nih.gov/PubMed/ 102 \x0cSystem Precision Recall f-score L/C 80.2% Self-trained 86.3% 82.4% 84.3% Figure 1: Comparison of the Medline self-trained parser against the previous best Medline self-trained parser achieves an f-score of 84.3%, which is an absolute reduction in error of 4.1%. This corresponds to an error rate reduction of 20% over the L/C baseline. 4 Discussion Prior to the above experiment on the test data, we did several preliminary </context>
</contexts>
<marker>Clegg, Shepherd, 2005</marker>
<rawString>Andrew B. Clegg and Adrian Shepherd. 2005. Evaluating and integrating treebank parsers on a biomedical corpus. In Proceedings of the ACL Workshop on Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
</authors>
<title>Parsing biomedical literature.</title>
<date>2005</date>
<booktitle>In Second International Joint Conference on Natural Language Processing (IJCNLP05).</booktitle>
<contexts>
<context position="4004" citStr="Lease and Charniak, 2005" startWordPosition="649" endWordPosition="652">esults are quite poor by modern standards.1 Steedman et al. (2003) generally found that selftraining does not work, but found that it does help if the baseline results were sufficiently bad. Secondly, the difference between the Brown corpus treebank and the Wall Street Journal corpus is not that great. One way to see this is to look at out-of-vocabulary statistics. The Brown corpus has an out-of-vocabulary rate of approximately 6% when given WSJ training as the lexicon. In contrast, the out-of-vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25% (Lease and Charniak, 2005). Thus the bridge the selftrained parser is asked to build is quite short. This second point is emphasized by the second paper on self-training for adaptation (McClosky et al., 2006b). This paper is based on the C/J parser and thus its results are much more in line with modern expectations. In particular, it was able to achieve an f-score of 87% on Brown treebank test data when trained and selftrained on WSJ-like data. Note this last point. It was not the case that it used the self-training to bridge the corpora difference. It self-trained on NANC, not Brown. NANC is a news corpus, quite like </context>
<context position="5233" citStr="Lease and Charniak, 2005" startWordPosition="866" endWordPosition="870">data. Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular. It said nothing about the task we address here. Thus our claim is that previous results are quite ambiguous on the issue of bridging corpora for parser adaptation. Turning briefly to previous results on Medline data, the best comparative study of parsers is that of Clegg and Shepherd (2005), which evaluates several statistical parsers. Their best result was an f-score of 80.2%. This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 1 This is not a criticism of the work. The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank. 2 This is the standard Charniak parser (without the parser of Bikel (2004). The other parsers were not close. However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al., 2006)). However, since the newer parsers do not perform quite as well as the C/J parser on WSJ data, it is probably the case that they would not si</context>
<context position="10867" citStr="Lease and Charniak (2005)" startWordPosition="1839" endWordPosition="1842">nclusion We self-trained the standard C/J parser on 270,000 sentences of Medline abstracts. By doing so we achieved a 20% error reduction over the best previous result for biomedical parsing. In terms of the gap between the supervised data and the self-trained data, this is the largest that has been attempted. Furthermore, the resulting parser is of interest in its own right, being as it is the most accurate biomedical parser yet developed. This parser is available on the web.4 Finally, there is no reason to believe that 84.3% is an upper bound on what can be achieved with current techniques. Lease and Charniak (2005) achieve their results using small amounts of hand-annotated biomedical part-ofspeech-tagged data and also explore other possible sources or information. It is reasonable to assume that its use would result in further improvement. Acknowledgments This work was supported by DARPA GALE contract HR0011-06-2-0001. We would like to thank the BLLIP team for their comments. 4 http://bllip.cs.brown.edu/biomedical/ 103 \x0c0 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000 Number of sentences added 80.0 80.2 80.4 80.6 80.8 81.0 81.2 81.4 81.6 81.8 82.0 82.2 82.4 82.6 82.8 83.0 </context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>Matthew Lease and Eugene Charniak. 2005. Parsing biomedical literature. In Second International Joint Conference on Natural Language Processing (IJCNLP05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Comp. Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<marker>Marcus, 1993</marker>
<rawString>M. Marcus et al. 1993. Building a large annotated corpus of English: The Penn Treebank. Comp. Linguistics, 19(2):313330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>152159</pages>
<contexts>
<context position="2416" citStr="McClosky et al., 2006" startWordPosition="379" endWordPosition="382">s so different from the original labeled data. Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005). Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser. We conclude in section five. 2 Previous Work While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997). However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b). One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain. Interestingly, there is little to no data showing that this actually works. Two previous papers would seem to address this issue: the work by Bacchiani et al. (2006) and McClosky et al. (2006b). However, in both cases the evidence is equivocal. Bacchiani and Roark t</context>
<context position="4185" citStr="McClosky et al., 2006" startWordPosition="681" endWordPosition="685"> bad. Secondly, the difference between the Brown corpus treebank and the Wall Street Journal corpus is not that great. One way to see this is to look at out-of-vocabulary statistics. The Brown corpus has an out-of-vocabulary rate of approximately 6% when given WSJ training as the lexicon. In contrast, the out-of-vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25% (Lease and Charniak, 2005). Thus the bridge the selftrained parser is asked to build is quite short. This second point is emphasized by the second paper on self-training for adaptation (McClosky et al., 2006b). This paper is based on the C/J parser and thus its results are much more in line with modern expectations. In particular, it was able to achieve an f-score of 87% on Brown treebank test data when trained and selftrained on WSJ-like data. Note this last point. It was not the case that it used the self-training to bridge the corpora difference. It self-trained on NANC, not Brown. NANC is a news corpus, quite like WSJ data. Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular. It </context>
<context position="6418" citStr="McClosky et al. (2006" startWordPosition="1071" endWordPosition="1074">ably the case that they would not significantly alter the landscape. 3 Central Experimental Result We used as the base parser the standardly available C/J parser. We then self-trained the parser on approximately 270,000 sentences a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We weighted the original WSJ hand annotated sentences equally with self-trained Medline data. So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level. We did no tuning to find out if there is some better weighting for our domain than one-to-one. The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors.</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006a. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 152159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL</booktitle>
<pages>337344</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="2416" citStr="McClosky et al., 2006" startWordPosition="379" endWordPosition="382">s so different from the original labeled data. Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005). Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser. We conclude in section five. 2 Previous Work While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997). However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b). One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain. Interestingly, there is little to no data showing that this actually works. Two previous papers would seem to address this issue: the work by Bacchiani et al. (2006) and McClosky et al. (2006b). However, in both cases the evidence is equivocal. Bacchiani and Roark t</context>
<context position="4185" citStr="McClosky et al., 2006" startWordPosition="681" endWordPosition="685"> bad. Secondly, the difference between the Brown corpus treebank and the Wall Street Journal corpus is not that great. One way to see this is to look at out-of-vocabulary statistics. The Brown corpus has an out-of-vocabulary rate of approximately 6% when given WSJ training as the lexicon. In contrast, the out-of-vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25% (Lease and Charniak, 2005). Thus the bridge the selftrained parser is asked to build is quite short. This second point is emphasized by the second paper on self-training for adaptation (McClosky et al., 2006b). This paper is based on the C/J parser and thus its results are much more in line with modern expectations. In particular, it was able to achieve an f-score of 87% on Brown treebank test data when trained and selftrained on WSJ-like data. Note this last point. It was not the case that it used the self-training to bridge the corpora difference. It self-trained on NANC, not Brown. NANC is a news corpus, quite like WSJ data. Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular. It </context>
<context position="6418" citStr="McClosky et al. (2006" startWordPosition="1071" endWordPosition="1074">ably the case that they would not significantly alter the landscape. 3 Central Experimental Result We used as the base parser the standardly available C/J parser. We then self-trained the parser on approximately 270,000 sentences a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We weighted the original WSJ hand annotated sentences equally with self-trained Medline data. So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level. We did no tuning to find out if there is some better weighting for our domain than one-to-one. The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors.</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings of COLINGACL 2006, pages 337344, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<pages>433440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="5691" citStr="Petrov et al., 2006" startWordPosition="948" endWordPosition="951">5), which evaluates several statistical parsers. Their best result was an f-score of 80.2%. This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 1 This is not a criticism of the work. The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank. 2 This is the standard Charniak parser (without the parser of Bikel (2004). The other parsers were not close. However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al., 2006)). However, since the newer parsers do not perform quite as well as the C/J parser on WSJ data, it is probably the case that they would not significantly alter the landscape. 3 Central Experimental Result We used as the base parser the standardly available C/J parser. We then self-trained the parser on approximately 270,000 sentences a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We we</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of COLING-ACL 2006, pages 433440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="3051" citStr="Roark, 2001" startWordPosition="491" endWordPosition="492">6b). One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain. Interestingly, there is little to no data showing that this actually works. Two previous papers would seem to address this issue: the work by Bacchiani et al. (2006) and McClosky et al. (2006b). However, in both cases the evidence is equivocal. Bacchiani and Roark train the Roark parser (Roark, 2001) on trees from the Brown treebank and then self-train and test on data from Wall Street Journal. While they show some improvement (from 75.7% to 80.5% f-score) there are several aspects of this work which leave its re101 \x0csults less than convincing as to the utility of selftraining for adaptation. The first is the parsing results are quite poor by modern standards.1 Steedman et al. (2003) generally found that selftraining does not work, but found that it does help if the baseline results were sufficiently bad. Secondly, the difference between the Brown corpus treebank and the Wall Street Jo</context>
</contexts>
<marker>Roark, 2001</marker>
<rawString>Brian Roark. 2001. Probabilistic top-down parsing and language modeling. Computational Linguistics, 27(2):249276.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Stephen Clark</author>
<author>Rebecca Hwa</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In Proc. of European ACL (EACL),</booktitle>
<pages>331338</pages>
<contexts>
<context position="2266" citStr="Steedman et al., 2003" startWordPosition="354" endWordPosition="357">on two, we look at previous work. In particular we note that there is, in fact, very little data on self-training when the corpora for self-training is so different from the original labeled data. Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005). Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser. We conclude in section five. 2 Previous Work While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997). However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b). One possible use for this technique is for parser adaptation initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain. Interestingly, there is little to no data showing that this actually works. Two previous papers would seem to addres</context>
</contexts>
<marker>Steedman, Osborne, Sarkar, Clark, Hwa, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In Proc. of European ACL (EACL), pages 331338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tateisi</author>
<author>A Yakushiji</author>
<author>T Ohta</author>
<author>J Tsujii</author>
</authors>
<title>Syntax Annotation for the GENIA corpus.</title>
<date>2005</date>
<contexts>
<context position="6787" citStr="Tateisi et al., 2005" startWordPosition="1139" endWordPosition="1142">erature. As we note in the next section, the number 270,000 was selected by observing performance on a development set. We weighted the original WSJ hand annotated sentences equally with self-trained Medline data. So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level. We did no tuning to find out if there is some better weighting for our domain than one-to-one. The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005). These are exactly the same sentences as used in the comparisons of the last section. Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors. Thus the Genia treebank data are all from a small domain within Biology. As already noted, the Medline abstracts used for self-training were chosen randomly and thus span a large number of biomedical sub-domains. The results, the central results of this paper, are shown in Figure 1. Clegg and Shepherd (2005) do not provide separate precision and recall numbers. Howe</context>
</contexts>
<marker>Tateisi, Yakushiji, Ohta, Tsujii, 2005</marker>
<rawString>Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii. 2005. Syntax Annotation for the GENIA corpus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>IJCNLP</author>
</authors>
<date>2005</date>
<journal>Companion</journal>
<volume>volume,</volume>
<pages>222227</pages>
<marker>IJCNLP, 2005</marker>
<rawString>Proc. IJCNLP 2005, Companion volume, pages 222227.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>