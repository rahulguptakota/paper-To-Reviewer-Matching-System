<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<figure confidence="0.359321875">
b&apos;Proceedings of ACL-08: HLT, pages 10391047,
Columbus, Ohio, USA, June 2008. c
2008 Association for Computational Linguistics
Finding Contradictions in Text
Marie-Catherine de Marneffe,
Linguistics Department
Stanford University
Stanford, CA 94305
</figure>
<email confidence="0.947316">
mcdm@stanford.edu
</email>
<author confidence="0.834294">
Anna N. Rafferty and Christopher D. Manning
</author>
<affiliation confidence="0.9779005">
Computer Science Department
Stanford University
</affiliation>
<address confidence="0.966263">
Stanford, CA 94305
</address>
<email confidence="0.998312">
{rafferty,manning}@stanford.edu
</email>
<sectionHeader confidence="0.9907" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999404555555556">
Detecting conflicting statements is a foun-
dational text understanding task with appli-
cations in information analysis. We pro-
pose an appropriate definition of contradiction
for NLP tasks and develop available corpora,
from which we construct a typology of con-
tradictions. We demonstrate that a system for
contradiction needs to make more fine-grained
distinctions than the common systems for en-
tailment. In particular, we argue for the cen-
trality of event coreference and therefore in-
corporate such a component based on topical-
ity. We present the first detailed breakdown
of performance on this task. Detecting some
types of contradiction requires deeper inferen-
tial paths than our system is capable of, but
we achieve good performance on types arising
from negation and antonymy.
</bodyText>
<sectionHeader confidence="0.9983" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998055724137931">
In this paper, we seek to understand the ways con-
tradictions occur across texts and describe a system
for automatically detecting such constructions. As a
foundational task in text understanding (Condoravdi
et al., 2003), contradiction detection has many possi-
ble applications. Consider applying a contradiction
detection system to political candidate debates: by
drawing attention to topics in which candidates have
conflicting positions, the system could enable voters
to make more informed choices between candidates
and sift through the amount of available informa-
tion. Contradiction detection could also be applied
to intelligence reports, demonstrating which infor-
mation may need further verification. In bioinfor-
matics where protein-protein interaction is widely
studied, automatically finding conflicting facts about
such interactions would be beneficial.
Here, we shed light on the complex picture of con-
tradiction in text. We provide a definition of contra-
diction suitable for NLP tasks, as well as a collec-
tion of contradiction corpora. Analyzing these data,
we find contradiction is a rare phenomenon that may
be created in different ways; we propose a typol-
ogy of contradiction classes and tabulate their fre-
quencies. Contradictions arise from relatively obvi-
ous features such as antonymy, negation, or numeric
mismatches. They also arise from complex differ-
ences in the structure of assertions, discrepancies
based on world-knowledge, and lexical contrasts.
</bodyText>
<listItem confidence="0.541718333333333">
(1) Police specializing in explosives defused the rock-
ets. Some 100 people were working inside the plant.
(2) 100 people were injured.
</listItem>
<bodyText confidence="0.99888125">
This pair is contradictory: defused rockets cannot go
off, and thus cannot injure anyone. Detecting con-
tradictions appears to be a harder task than detecting
entailments. Here, it is relatively easy to identify the
lack of entailment: the first sentence involves no in-
juries, so the second is unlikely to be entailed. Most
entailment systems function as weak proof theory
(Hickl et al., 2006; MacCartney et al., 2006; Zan-
zotto et al., 2007), but contradictions require deeper
inferences and model building. While mismatch-
ing information between sentences is often a good
cue of non-entailment (Vanderwende et al., 2006),
it is not sufficient for contradiction detection which
requires more precise comprehension of the conse-
quences of sentences. Assessing event coreference
is also essential: for texts to contradict, they must
</bodyText>
<page confidence="0.979694">
1039
</page>
<bodyText confidence="0.9964871">
\x0crefer to the same event. The importance of event
coreference was recognized in the MUC information
extraction tasks in which it was key to identify sce-
narios related to the same event (Humphreys et al.,
1997). Recent work in text understanding has not
focused on this issue, but it must be tackled in a suc-
cessful contradiction system. Our system includes
event coreference, and we present the first detailed
examination of contradiction detection performance,
on the basis of our typology.
</bodyText>
<sectionHeader confidence="0.99585" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.993219928571429">
Little work has been done on contradiction detec-
tion. The PASCAL Recognizing Textual Entailment
(RTE) Challenges (Dagan et al., 2006; Bar-Haim
et al., 2006; Giampiccolo et al., 2007) focused on
textual inference in any domain. Condoravdi et al.
(2003) first recognized the importance of handling
entailment and contradiction for text understanding,
but they rely on a strict logical definition of these
phenomena and do not report empirical results. To
our knowledge, Harabagiu et al. (2006) provide the
first empirical results for contradiction detection, but
they focus on specific kinds of contradiction: those
featuring negation and those formed by paraphrases.
They constructed two corpora for evaluating their
system. One was created by overtly negating each
entailment in the RTE2 data, producing a bal-
anced dataset (LCC negation). To avoid overtrain-
ing, negative markers were also added to each non-
entailment, ensuring that they did not create con-
tradictions. The other was produced by paraphras-
ing the hypothesis sentences from LCC negation, re-
moving the negation (LCC paraphrase): A hunger
strike was not attempted A hunger strike was
called off. They achieved very good performance:
accuracies of 75.63% on LCC negation and 62.55%
on LCC paraphrase. Yet, contradictions are not lim-
ited to these constructions; to be practically useful,
any system must provide broader coverage.
</bodyText>
<sectionHeader confidence="0.967603" genericHeader="method">
3 Contradictions
</sectionHeader>
<subsectionHeader confidence="0.996235">
3.1 What is a contradiction?
</subsectionHeader>
<bodyText confidence="0.998255653846154">
One standard is to adopt a strict logical definition of
contradiction: sentences A and B are contradictory
if there is no possible world in which A and B are
both true. However, for contradiction detection to be
useful, a looser definition that more closely matches
human intuitions is necessary; contradiction occurs
when two sentences are extremely unlikely to be true
simultaneously. Pairs such as Sally sold a boat to
John and John sold a boat to Sally are tagged as con-
tradictory even though it could be that each sold a
boat to the other. This definition captures intuitions
of incompatiblity, and perfectly fits applications that
seek to highlight discrepancies in descriptions of the
same event. Examples of contradiction are given in
table 1. For texts to be contradictory, they must in-
volve the same event. Two phenomena must be con-
sidered in this determination: implied coreference
and embedded texts. Given limited context, whether
two entities are coreferent may be probable rather
than certain. To match human intuitions, compatible
noun phrases between sentences are assumed to be
coreferent in the absence of clear countervailing ev-
idence. In the following example, it is not necessary
that the woman in the first and second sentences is
the same, but one would likely assume it is if the two
sentences appeared together:
</bodyText>
<listItem confidence="0.80553">
(1) Passions surrounding Germanys final match turned
</listItem>
<bodyText confidence="0.79498775">
violent when a woman stabbed her partner because
she didnt want to watch the game.
(2) A woman passionately wanted to watch the game.
We also mark as contradictions pairs reporting con-
tradictory statements. The following sentences refer
to the same event (de Menezes in a subway station),
and display incompatible views of this event:
(1) Eyewitnesses said de Menezes had jumped over the
turnstile at Stockwell subway station.
(2) The documents leaked to ITV News suggest that
Menezes walked casually into the subway station.
This example contains an embedded contradic-
tion. Contrary to Zaenen et al. (2005), we argue
that recognizing embedded contradictions is impor-
tant for the application of a contradiction detection
system: if John thinks that he is incompetent, and his
boss believes that John is not being given a chance,
one would like to detect that the targeted information
in the two sentences is contradictory, even though
the two sentences can be true simultaneously.
</bodyText>
<subsectionHeader confidence="0.998915">
3.2 Typology of contradictions
</subsectionHeader>
<bodyText confidence="0.92664">
Contradictions may arise from a number of different
constructions, some overt and others that are com-
</bodyText>
<page confidence="0.936093">
1040
</page>
<listItem confidence="0.35343975">
\x0cID Type Text Hypothesis
1 Antonym Capital punishment is a catalyst for more crime. Capital punishment is a deterrent to
crime.
2 Negation A closely divided Supreme Court said that juries and
</listItem>
<bodyText confidence="0.962391666666667">
not judges must impose a death sentence.
The Supreme Court decided that only
judges can impose the death sentence.
</bodyText>
<sectionHeader confidence="0.94584" genericHeader="method">
3 Numeric The tragedy of the explosion in Qana that killed more
</sectionHeader>
<bodyText confidence="0.998876333333333">
than 50 civilians has presented Israel with a dilemma.
An investigation into the strike in Qana
found 28 confirmed dead thus far.
</bodyText>
<sectionHeader confidence="0.980271" genericHeader="method">
4 Factive Prime Minister John Howard says he will not be
</sectionHeader>
<bodyText confidence="0.989577333333333">
swayed by a warning that Australia faces more terror-
ism attacks unless it withdraws its troops from Iraq.
Australia withdraws from Iraq.
</bodyText>
<sectionHeader confidence="0.982947" genericHeader="method">
5 Factive The bombers had not managed to enter the embassy. The bombers entered the embassy.
6 Structure Jacques Santer succeeded Jacques Delors as president
</sectionHeader>
<bodyText confidence="0.982348">
of the European Commission in 1995.
Delors succeeded Santer in the presi-
dency of the European Commission.
</bodyText>
<sectionHeader confidence="0.962266" genericHeader="method">
7 Structure The Channel Tunnel stretches from England to
</sectionHeader>
<bodyText confidence="0.9738185">
France. It is the second-longest rail tunnel in the
world, the longest being a tunnel in Japan.
The Channel Tunnel connects France
and Japan.
</bodyText>
<sectionHeader confidence="0.846827" genericHeader="method">
8 Lexical The Canadian parliaments Ethics Commission said
</sectionHeader>
<bodyText confidence="0.8880272">
former immigration minister, Judy Sgro, did nothing
wrong and her staff had put her into a conflict of in-
terest.
The Canadian parliaments Ethics
Commission accuses Judy Sgro.
</bodyText>
<listItem confidence="0.808061857142857">
9 Lexical In the election, Bush called for U.S. troops to be with-
drawn from the peacekeeping mission in the Balkans.
He cites such missions as an example of
how America must stay the course.
10 WK Microsoft Israel, one of the first Microsoft branches
outside the USA, was founded in 1989.
Microsoft was established in 1989.
</listItem>
<tableCaption confidence="0.974306">
Table 1: Examples of contradiction types.
</tableCaption>
<bodyText confidence="0.996033279069768">
plex even for humans to detect. Analyzing contra-
diction corpora (see section 3.3), we find two pri-
mary categories of contradiction: (1) those occur-
ring via antonymy, negation, and date/number mis-
match, which are relatively simple to detect, and
(2) contradictions arising from the use of factive or
modal words, structural and subtle lexical contrasts,
as well as world knowledge (WK).
We consider contradictions in category (1) easy
because they can often be automatically detected
without full sentence comprehension. For exam-
ple, if words in the two passages are antonyms and
the sentences are reasonably similar, especially in
polarity, a contradiction occurs. Additionally, little
external information is needed to gain broad cover-
age of antonymy, negation, and numeric mismatch
contradictions; each involves only a closed set of
words or data that can be obtained using existing
resources and techniques (e.g., WordNet (Fellbaum,
1998), VerbOcean (Chklovski and Pantel, 2004)).
However, contradictions in category (2) are more
difficult to detect automatically because they require
precise models of sentence meaning. For instance,
to find the contradiction in example 8 (table 1),
it is necessary to learn that X said Y did nothing
wrong and X accuses Y are incompatible. Presently,
there exist methods for learning oppositional terms
(Marcu and Echihabi, 2002) and paraphrase learn-
ing has been thoroughly studied, but successfully
extending these techniques to learn incompatible
phrases poses difficulties because of the data dis-
tribution. Example 9 provides an even more dif-
ficult instance of contradiction created by a lexical
discrepancy. Structural issues also create contradic-
tions (examples 6 and 7). Lexical complexities and
variations in the function of arguments across verbs
can make recognizing these contradictions compli-
cated. Even when similar verbs are used and ar-
gument differences exist, structural differences may
indicate non-entailment or contradiction, and distin-
guishing the two automatically is problematic. Con-
sider contradiction 7 in table 1 and the following
non-contradiction:
</bodyText>
<listItem confidence="0.995702666666667">
(1) The CFAP purchases food stamps from the govern-
ment and distributes them to eligible recipients.
(2) A government purchases food.
</listItem>
<page confidence="0.976178">
1041
</page>
<table confidence="0.997279142857143">
\x0cData # contradictions # total pairs
RTE1 dev1 48 287
RTE1 dev2 55 280
RTE1 test 149 800
RTE2 dev 111 800
RTE3 dev 80 800
RTE3 test 72 800
</table>
<tableCaption confidence="0.996083">
Table 2: Number of contradictions in the RTE datasets.
</tableCaption>
<bodyText confidence="0.998162833333333">
In both cases, the first sentence discusses one en-
tity (CFAP, The Channel Tunnel) with a relationship
(purchase, stretch) to other entities. The second sen-
tence posits a similar relationship that includes one
of the entities involved in the original relationship
as well as an entity that was not involved. However,
different outcomes result because a tunnel connects
only two unique locations whereas more than one
entity may purchase food. These frequent interac-
tions between world-knowledge and structure make
it hard to ensure that any particular instance of struc-
tural mismatch is a contradiction.
</bodyText>
<subsectionHeader confidence="0.999747">
3.3 Contradiction corpora
</subsectionHeader>
<bodyText confidence="0.994326142857143">
Following the guidelines above, we annotated the
RTE datasets for contradiction. These datasets con-
tain pairs consisting of a short text and a one-
sentence hypothesis. Table 2 gives the number of
contradictions in each dataset. The RTE datasets are
balanced between entailments and non-entailments,
and even in these datasets targeting inference, there
are few contradictions. Using our guidelines,
RTE3 test was annotated by NIST as part of the
RTE3 Pilot task in which systems made a 3-way de-
cision as to whether pairs of sentences were entailed,
contradictory, or neither (Voorhees, 2008).1
Our annotations and those of NIST were per-
formed on the original RTE datasets, contrary to
Harabagiu et al. (2006). Because their corpora are
constructed using negation and paraphrase, they are
unlikely to cover all types of contradictions in sec-
tion 3.2. We might hypothesize that rewriting ex-
plicit negations commonly occurs via the substitu-
tion of antonyms. Imagine, e.g.:
H: Bill has finished his math.
</bodyText>
<footnote confidence="0.489887">
1
Information about this task as well as data can be found at
http://nlp.stanford.edu/RTE3-pilot/.
</footnote>
<table confidence="0.9680905">
Type RTE sets Real corpus
1 Antonym 15.0 9.2
Negation 8.8 17.6
Numeric 8.8 29.0
2 Factive/Modal 5.0 6.9
Structure 16.3 3.1
Lexical 18.8 21.4
WK 27.5 13.0
</table>
<tableCaption confidence="0.993704">
Table 3: Percentages of contradiction types in the
</tableCaption>
<bodyText confidence="0.983732117647059">
RTE3 dev dataset and the real contradiction corpus.
Neg-H: Bill hasnt finished his math.
Para-Neg-H: Bill is still working on his math.
The rewriting in both the negated and the para-
phrased corpora is likely to leave one in the space of
easy contradictions and addresses fewer than 30%
of contradictions (table 3). We contacted the LCC
authors to obtain their datasets, but they were unable
to make them available to us. Thus, we simulated the
LCC negation corpus, adding negative markers to
the RTE2 test data (Neg test), and to a development
set (Neg dev) constructed by randomly sampling 50
pairs of entailments and 50 pairs of non-entailments
from the RTE2 development set.
Since the RTE datasets were constructed for tex-
tual inference, these corpora do not reflect real-life
contradictions. We therefore collected contradic-
tions in the wild. The resulting corpus contains
131 contradictory pairs: 19 from newswire, mainly
looking at related articles in Google News, 51 from
Wikipedia, 10 from the Lexis Nexis database, and
51 from the data prepared by LDC for the distillation
task of the DARPA GALE program. Despite the ran-
domness of the collection, we argue that this corpus
best reflects naturally occurring contradictions.2
Table 3 gives the distribution of contradiction
types for RTE3 dev and the real contradiction cor-
pus. Globally, we see that contradictions in category
(2) occur frequently and dominate the RTE develop-
ment set. In the real contradiction corpus, there is a
much higher rate of the negation, numeric and lex-
ical contradictions. This supports the intuition that
in the real world, contradictions primarily occur for
two reasons: information is updated as knowledge
</bodyText>
<page confidence="0.927909">
2
</page>
<bodyText confidence="0.928308666666667">
Our corporathe simulation of the LLC negation corpus,
the RTE datasets and the real contradictionsare available at
http://nlp.stanford.edu/projects/contradiction.
</bodyText>
<page confidence="0.938281">
1042
</page>
<bodyText confidence="0.952304666666667">
\x0cof an event is acquired over time (e.g., a rising death
toll) or various parties have divergent views of an
event (e.g., example 9 in table 1).
</bodyText>
<sectionHeader confidence="0.879643" genericHeader="method">
4 System overview
</sectionHeader>
<bodyText confidence="0.943530333333333">
Our system is based on the stage architecture of the
Stanford RTE system (MacCartney et al., 2006), but
adds a stage for event coreference decision.
</bodyText>
<subsectionHeader confidence="0.993902">
4.1 Linguistic analysis
</subsectionHeader>
<bodyText confidence="0.998367">
The first stage computes linguistic representations
containing information about the semantic content
of the passages. The text and hypothesis are con-
verted to typed dependency graphs produced by
the Stanford parser (Klein and Manning, 2003; de
Marneffe et al., 2006). To improve the dependency
graph as a pseudo-semantic representation, colloca-
tions in WordNet and named entities are collapsed,
causing entities and multiword relations to become
single nodes.
</bodyText>
<subsectionHeader confidence="0.991494">
4.2 Alignment between graphs
</subsectionHeader>
<bodyText confidence="0.99936125">
The second stage provides an alignment between
text and hypothesis graphs, consisting of a mapping
from each node in the hypothesis to a unique node
in the text or to null. The scoring measure uses
node similarity (irrespective of polarity) and struc-
tural information based on the dependency graphs.
Similarity measures and structural information are
combined via weights learned using the passive-
aggressive online learning algorithm MIRA (Cram-
mer and Singer, 2001). Alignment weights were
learned using manually annotated RTE development
sets (see Chambers et al., 2007).
</bodyText>
<subsectionHeader confidence="0.985646">
4.3 Filtering non-coreferent events
</subsectionHeader>
<bodyText confidence="0.998213685185185">
Contradiction features are extracted based on mis-
matches between the text and hypothesis. Therefore,
we must first remove pairs of sentences which do not
describe the same event, and thus cannot be contra-
dictory to one another. In the following example, it
is necessary to recognize that Plutos moon is not the
same as the moon Titan; otherwise conflicting diam-
eters result in labeling the pair a contradiction.
T: Plutos moon, which is only about 25 miles in di-
ameter, was photographed 13 years ago.
H: The moon Titan has a diameter of 5100 kms.
This issue does not arise for textual entailment: el-
ements in the hypothesis not supported by the text
lead to non-entailment, regardless of whether the
same event is described. For contradiction, however,
it is critical to filter unrelated sentences to avoid
finding false evidence of contradiction when there
is contrasting information about different events.
Given the structure of RTE data, in which the
hypotheses are shorter and simpler than the texts,
one straightforward strategy for detecting coreferent
events is to check whether the root of the hypothesis
graph is aligned in the text graph. However, some
RTE hypotheses are testing systems abilities to de-
tect relations between entities (e.g., John of IBM ...
John works for IBM). Thus, we do not filter verb
roots that are indicative of such relations. As shown
in table 4, this strategy improves results on RTE
data. For real world data, however, the assumption
of directionality made in this strategy is unfounded,
and we cannot assume that one sentence will be
short and the other more complex. Assuming two
sentences of comparable complexity, we hypothe-
size that modeling topicality could be used to assess
whether the sentences describe the same event.
There is a continuum of topicality from the start to
the end of a sentence (Firbas, 1971). We thus orig-
inally defined the topicality of an NP by nw where
n is the nth NP in the sentence. Additionally, we
accounted for multiple clauses by weighting each
clause equally; in example 4 in table 1, Australia
receives the same weight as Prime Minister because
each begins a clause. However, this weighting was
not supported empirically, and we thus use a sim-
pler, unweighted model. The topicality score of a
sentence is calculated as a normalized score across
all aligned NPs.3 The text and hypothesis are topi-
cally related if either sentence score is above a tuned
threshold. Modeling topicality provides an addi-
tional improvement in precision (table 4).
While filtering provides improvements in perfor-
mance, some examples of non-coreferent events are
still not filtered, such as:
T: Also Friday, five Iraqi soldiers were killed and nine
</bodyText>
<page confidence="0.987461">
3
</page>
<bodyText confidence="0.997348">
Since dates can often be viewed as scene setting rather than
what the sentence is about, we ignore these in the model. How-
ever, ignoring or including dates in the model creates no signif-
icant differences in performance on RTE data.
</bodyText>
<page confidence="0.966862">
1043
</page>
<table confidence="0.997402">
\x0cStrategy Precision Recall
No filter 55.10 32.93
Root 61.36 32.93
Root + topic 61.90 31.71
</table>
<tableCaption confidence="0.999116">
Table 4: Precision and recall for contradiction detection
</tableCaption>
<bodyText confidence="0.989782866666667">
on RTE3 dev using different filtering strategies.
wounded in a bombing, targeting their convoy near
Beiji, 150 miles north of Baghdad.
H: Three Iraqi soldiers also died Saturday when their
convoy was attacked by gunmen near Adhaim.
It seems that the real world frequency of events
needs to be taken into account. In this case, attacks
in Iraq are unfortunately frequent enough to assert
that it is unlikely that the two sentences present mis-
matching information (i.e., different location) about
the same event. But compare the following example:
T: President Kennedy was assassinated in Texas.
H: Kennedys murder occurred in Washington.
The two sentences refer to one unique event, and the
location mismatch renders them contradictory.
</bodyText>
<subsectionHeader confidence="0.999801">
4.4 Extraction of contradiction features
</subsectionHeader>
<bodyText confidence="0.99957175">
In the final stage, we extract contradiction features
on which we apply logistic regression to classify the
pair as contradictory or not. The feature weights are
hand-set, guided by linguistic intuition.
</bodyText>
<sectionHeader confidence="0.987394" genericHeader="method">
5 Features for contradiction detection
</sectionHeader>
<bodyText confidence="0.99945693442623">
In this section, we define each of the feature sets
used to capture salient patterns of contradiction.
Polarity features. Polarity difference between the
text and hypothesis is often a good indicator of con-
tradiction, provided there is a good alignment (see
example 2 in table 1). The polarity features cap-
ture the presence (or absence) of linguistic mark-
ers of negative polarity contexts. These markers are
scoped such that words are considered negated if
they have a negation dependency in the graph or are
an explicit linguistic marker of negation (e.g., sim-
ple negation (not), downward-monotone quantifiers
(no, few), or restricting prepositions). If one word is
negated and the other is not, we may have a polarity
difference. This difference is confirmed by checking
that the words are not antonyms and that they lack
unaligned prepositions or other context that suggests
they do not refer to the same thing. In some cases,
negations are propagated onto the governor, which
allows one to see that no bullet penetrated and a bul-
let did not penetrate have the same polarity.
Number, date and time features. Numeric mis-
matches can indicate contradiction (example 3
in table 1). The numeric features recognize
(mis-)matches between numbers, dates, and times.
We normalize date and time expressions, and rep-
resent numbers as ranges. This includes expression
matching (e.g., over 100 and 200 is not a mismatch).
Aligned numbers are marked as mismatches when
they are incompatible and surrounding words match
well, indicating the numbers refer to the same entity.
Antonymy features. Aligned antonyms are a very
good cue for contradiction. Our list of antonyms
and contrasting words comes from WordNet, from
which we extract words with direct antonymy links
and expand the list by adding words from the same
synset as the antonyms. We also use oppositional
verbs from VerbOcean. We check whether an
aligned pair of words appears in the list, as well as
checking for common antonym prefixes (e.g., anti,
un). The polarity of the context is used to determine
if the antonyms create a contradiction.
Structural features. These features aim to deter-
mine whether the syntactic structures of the text and
hypothesis create contradictory statements. For ex-
ample, we compare the subjects and objects for each
aligned verb. If the subject in the text overlaps with
the object in the hypothesis, we find evidence for a
contradiction. Consider example 6 in table 1. In the
text, the subject of succeed is Jacques Santer while
in the hypothesis, Santer is the object of succeed,
suggesting that the two sentences are incompatible.
Factivity features. The context in which a verb
phrase is embedded may give rise to contradiction,
as in example 5 (table 1). Negation influences some
factivity patterns: Bill forgot to take his wallet con-
tradicts Bill took his wallet while Bill did not forget
to take his wallet does not contradict Bill took his
wallet. For each text/hypothesis pair, we check the
(grand)parent of the text word aligned to the hypoth-
esis verb, and generate a feature based on its factiv-
</bodyText>
<page confidence="0.936337">
1044
</page>
<bodyText confidence="0.998017625">
\x0city class. Factivity classes are formed by clustering
our expansion of the PARC lists of factive, implica-
tive and non-factive verbs (Nairn et al., 2006) ac-
cording to how they create contradiction.
Modality features. Simple patterns of modal rea-
soning are captured by mapping the text and hy-
pothesis to one of six modalities ((not )possible,
(not )actual, (not )necessary), according to the
presence of predefined modality markers such as
can or maybe. A feature is produced if the
text/hypothesis modality pair gives rise to a con-
tradiction. For instance, the following pair will
be mapped to the contradiction judgment (possible,
not possible):
T: The trial court may allow the prevailing party rea-
sonable attorney fees as part of costs.
H: The prevailing party may not recover attorney fees.
Relational features. A large proportion of the
RTE data is derived from information extraction
tasks where the hypothesis captures a relation be-
tween elements in the text. Using Semgrex, a pat-
tern matching language for dependency graphs, we
find such relations and ensure that the arguments be-
tween the text and the hypothesis match. In the fol-
lowing example, we detect that Fernandez works for
FEMA, and that because of the negation, a contra-
diction arises.
T: Fernandez, of FEMA, was on scene when Martin
arrived at a FEMA base camp.
H: Fernandez doesnt work for FEMA.
Relational features provide accurate information but
are difficult to extend for broad coverage.
</bodyText>
<sectionHeader confidence="0.999917" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999506666666667">
Our contradiction detection system was developed
on all datasets listed in the first part of table 5. As
test sets, we used RTE1 test, the independently an-
notated RTE3 test, and Neg test. We focused on at-
taining high precision. In a real world setting, it is
likely that the contradiction rate is extremely low;
rather than overwhelming true positives with false
positives, rendering the system impractical, we mark
contradictions conservatively. We found reasonable
inter-annotator agreement between NIST and our
post-hoc annotation of RTE3 test ( = 0.81), show-
ing that, even with limited context, humans tend to
</bodyText>
<table confidence="0.994724909090909">
Precision Recall Accuracy
RTE1 dev1 70.37 40.43
RTE1 dev2 72.41 38.18
RTE2 dev 64.00 28.83
RTE3 dev 61.90 31.71
Neg dev 74.07 78.43 75.49
Neg test 62.97 62.50 62.74
LCC negation 75.63
RTE1 test 42.22 26.21
RTE3 test 22.95 19.44
Avg. RTE3 test 10.72 11.69
</table>
<tableCaption confidence="0.7380455">
Table 5: Precision and recall figures for contradiction de-
tection. Accuracy is given for balanced datasets only.
</tableCaption>
<bodyText confidence="0.995072">
LCC negation refers to performance of Harabagiu et al.
(2006); Avg. RTE3 test refers to mean performance of
the 12 submissions to the RTE3 Pilot.
agree on contradictions.4 The results on the test sets
show that performance drops on new data, highlight-
ing the difficulty in generalizing from a small corpus
of positive contradiction examples, as well as under-
lining the complexity of building a broad coverage
system. This drop in accuracy on the test sets is
greater than that of many RTE systems, suggesting
that generalizing for contradiction is more difficult
than for entailment. Particularly when addressing
contradictions that require lexical and world knowl-
edge, we are only able to add coverage in a piece-
meal fashion, resulting in improved performance on
the development sets but only small gains for the
test sets. Thus, as shown in table 6, we achieve
13.3% recall on lexical contradictions in RTE3 dev
but are unable to identify any such contradictions in
RTE3 test. Additionally, we found that the preci-
sion of category (2) features was less than that of
category (1) features. Structural features, for exam-
ple, caused us to tag 36 non-contradictions as con-
tradictions in RTE3 test, over 75% of the precision
errors. Despite these issues, we achieve much higher
precision and recall than the average submission to
the RTE3 Pilot task on detecting contradictions, as
shown in the last two lines of table 5.
</bodyText>
<page confidence="0.93124">
4
</page>
<bodyText confidence="0.992797">
This stands in contrast with the low inter-annotator agree-
ment reported by Sanchez-Graillet and Poesio (2007) for con-
tradictions in protein-protein interactions. The only hypothesis
we have to explain this contrast is the difficulty of scientific ma-
terial.
</bodyText>
<page confidence="0.932507">
1045
</page>
<table confidence="0.99917775">
\x0cType RTE3 dev RTE3 test
1 Antonym 25.0 (3/12) 42.9 (3/7)
Negation 71.4 (5/7) 60.0 (3/5)
Numeric 71.4 (5/7) 28.6 (2/7)
2 Factive/Modal 25.0 (1/4) 10.0 (1/10)
Structure 46.2 (6/13) 21.1 (4/19)
Lexical 13.3 (2/15) 0.0 (0/12)
WK 18.2 (4/22) 8.3 (1/12)
</table>
<tableCaption confidence="0.993594">
Table 6: Recall by contradiction type.
</tableCaption>
<sectionHeader confidence="0.976212" genericHeader="conclusions">
7 Error analysis and discussion
</sectionHeader>
<bodyText confidence="0.991027808219178">
One significant issue in contradiction detection is
lack of feature generalization. This problem is es-
pecially apparent for items in category (2) requiring
lexical and world knowledge, which proved to be
the most difficult contradictions to detect on a broad
scale. While we are able to find certain specific re-
lationships in the development sets, these features
attained only limited coverage. Many contradictions
in this category require multiple inferences and re-
main beyond our capabilities:
T: The Auburn High School Athletic Hall of Fame re-
cently introduced its Class of 2005 which includes
10 members.
H: The Auburn High School Athletic Hall of Fame has
ten members.
Of the types of contradictions in category (2), we are
best at addressing those formed via structural differ-
ences and factive/modal constructions as shown in
table 6. For instance, we detect examples 5 and 6 in
table 1. However, creating features with sufficient
precision is an issue for these types of contradic-
tions. Intuitively, two sentences that have aligned
verbs with the same subject and different objects (or
vice versa) are contradictory. This indeed indicates
a contradiction 55% of the time on our development
sets, but this is not high enough precision given the
rarity of contradictions.
Another type of contradiction where precision fal-
ters is numeric mismatch. We obtain high recall for
this type (table 6), as it is relatively simple to deter-
mine if two numbers are compatible, but high preci-
sion is difficult to achieve due to differences in what
numbers may mean. Consider:
T: Nike Inc. said that its profit grew 32 percent, as the
company posted broad gains in sales and orders.
H: Nike said orders for footwear totaled $4.9 billion,
including a 12 percent increase in U.S. orders.
Our system detects a mismatch between 32 percent
and 12 percent, ignoring the fact that one refers to
profit and the other to orders. Accounting for con-
text requires extensive text comprehension; it is not
enough to simply look at whether the two numbers
are headed by similar words (grew and increase).
This emphasizes the fact that mismatching informa-
tion is not sufficient to indicate contradiction.
As demonstrated by our 63% accuracy on
Neg test, we are reasonably good at detecting nega-
tion and correctly ascertaining whether it is a symp-
tom of contradiction. Similarly, we handle single
word antonymy with high precision (78.9%). Never-
theless, Harabagiu et al.s performance demonstrates
that further improvement on these types is possible;
indeed, they use more sophisticated techniques to
extract oppositional terms and detect polarity differ-
ences. Thus, detecting category (1) contradictions is
feasible with current systems.
While these contradictions are only a third of
those in the RTE datasets, detecting such contra-
dictions accurately would solve half of the prob-
lems found in the real corpus. This suggests that
we may be able to gain sufficient traction on contra-
diction detection for real world applications. Even
so, category (2) contradictions must be targeted to
detect many of the most interesting examples and to
solve the entire problem of contradiction detection.
Some types of these contradictions, such as lexi-
cal and world knowledge, are currently beyond our
grasp, but we have demonstrated that progress may
be made on the structure and factive/modal types.
Despite being rare, contradiction is foundational
in text comprehension. Our detailed investigation
demonstrates which aspects of it can be resolved and
where further research must be directed.
</bodyText>
<sectionHeader confidence="0.961333" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.687749666666667">
This paper is based on work funded in part by
the Defense Advanced Research Projects Agency
through IBM and by the Disruptive Technology
</bodyText>
<figure confidence="0.243928666666667">
Office (DTO) Phase III Program for Advanced
Question Answering for Intelligence (AQUAINT)
through Broad Agency Announcement (BAA)
N61339-06-R-0034.
1046
\x0cReferences
</figure>
<reference confidence="0.998610770833333">
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL recognising textual en-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
Christopher D. Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP-04.
Cleo Condoravdi, Dick Crouch, Valeria de Pavia, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. Workshop on
Text Meaning (2003 May 31).
Koby Crammer and Yoram Singer. 2001. Ultraconser-
vative online algorithms for multiclass problems. In
Proceedings of COLT-2001.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Quinonero-Candela et al., editor, MLCW
2005, LNAI Volume 3944, pages 177190. Springer-
Verlag.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC-06).
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
Jan Firbas. 1971. On the concept of communicative dy-
namism in the theory of functional sentence perspec-
tive. Brno Studies in English, 7:2347.
Danilo Giampiccolo, Ido Dagan, Bernardo Magnini, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast, and contradiction in text
processing. In Proceedings of the Twenty-First Na-
tional Conference on Artificial Intelligence (AAAI-06).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCCs GROUNDHOG
system. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop on Operational
Factors in Pratical, Robust Anaphora Resolution for
Unrestricted Texts, 35th ACL meeting.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06).
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Olivia Sanchez-Graillet and Massimo Poesio. 2007. Dis-
covering contradiction protein-protein interactions in
text. In Proceedings of BioNLP 2007: Biological,
translational, and clinical language processing.
Lucy Vanderwende, Arul Menezes, and Rion Snow.
2006. Microsoft research at rte-2: Syntactic contri-
butions in the entailment task: an implementation. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment.
Ellen Voorhees. 2008. Contradictions and justifications:
Extensions to the textual entailment task. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics.
Annie Zaenen, Lauri Karttunen, and Richard S. Crouch.
2005. Local textual inference: can it be defined or
circumscribed? In ACL 2005 Workshop on Empirical
Modeling of Semantic Equivalence and Entailment.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2007. Shallow semantics in
fast textual entailment rule learners. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
</reference>
<page confidence="0.753544">
1047
</page>
<figure confidence="0.25447">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.396127">
<note confidence="0.917861666666667">b&apos;Proceedings of ACL-08: HLT, pages 10391047, Columbus, Ohio, USA, June 2008. c 2008 Association for Computational Linguistics</note>
<title confidence="0.7432">Finding Contradictions in Text</title>
<author confidence="0.492094">Marie-Catherine de_Marneffe</author>
<affiliation confidence="0.9732905">Linguistics Department Stanford University</affiliation>
<address confidence="0.999137">Stanford, CA 94305</address>
<email confidence="0.999306">mcdm@stanford.edu</email>
<author confidence="0.998767">Anna N Rafferty</author>
<author confidence="0.998767">Christopher D Manning</author>
<affiliation confidence="0.9998685">Computer Science Department Stanford University</affiliation>
<address confidence="0.999744">Stanford, CA 94305</address>
<email confidence="0.999676">rafferty@stanford.edu</email>
<email confidence="0.999676">manning@stanford.edu</email>
<abstract confidence="0.998915736842105">Detecting conflicting statements is a foundational text understanding task with applications in information analysis. We propose an appropriate definition of contradiction for NLP tasks and develop available corpora, from which we construct a typology of contradictions. We demonstrate that a system for contradiction needs to make more fine-grained distinctions than the common systems for entailment. In particular, we argue for the centrality of event coreference and therefore incorporate such a component based on topicality. We present the first detailed breakdown of performance on this task. Detecting some types of contradiction requires deeper inferential paths than our system is capable of, but we achieve good performance on types arising from negation and antonymy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="4319" citStr="Bar-Haim et al., 2006" startWordPosition="641" endWordPosition="644">of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., 1997). Recent work in text understanding has not focused on this issue, but it must be tackled in a successful contradiction system. Our system includes event coreference, and we present the first detailed examination of contradiction detection performance, on the basis of our typology. 2 Related work Little work has been done on contradiction detection. The PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) focused on textual inference in any domain. Condoravdi et al. (2003) first recognized the importance of handling entailment and contradiction for text understanding, but they rely on a strict logical definition of these phenomena and do not report empirical results. To our knowledge, Harabagiu et al. (2006) provide the first empirical results for contradiction detection, but they focus on specific kinds of contradiction: those featuring negation and those formed by paraphrases. They constructed two corpora for evaluating their system. One was created by overtly nega</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second PASCAL recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Cer</author>
<author>Trond Grenager</author>
<author>David Hall</author>
<author>Chloe Kiddon</author>
<author>Bill MacCartney</author>
<author>MarieCatherine de Marneffe</author>
<author>Daniel Ramage</author>
<author>Eric Yeh</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning alignments and leveraging natural logic.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<marker>Chambers, Cer, Grenager, Hall, Kiddon, MacCartney, de Marneffe, Ramage, Yeh, Manning, 2007</marker>
<rawString>Nathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe Kiddon, Bill MacCartney, MarieCatherine de Marneffe, Daniel Ramage, Eric Yeh, and Christopher D. Manning. 2007. Learning alignments and leveraging natural logic. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>Verbocean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP-04.</booktitle>
<contexts>
<context position="10849" citStr="Chklovski and Pantel, 2004" startWordPosition="1682" endWordPosition="1685">s well as world knowledge (WK). We consider contradictions in category (1) easy because they can often be automatically detected without full sentence comprehension. For example, if words in the two passages are antonyms and the sentences are reasonably similar, especially in polarity, a contradiction occurs. Additionally, little external information is needed to gain broad coverage of antonymy, negation, and numeric mismatch contradictions; each involves only a closed set of words or data that can be obtained using existing resources and techniques (e.g., WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)). However, contradictions in category (2) are more difficult to detect automatically because they require precise models of sentence meaning. For instance, to find the contradiction in example 8 (table 1), it is necessary to learn that X said Y did nothing wrong and X accuses Y are incompatible. Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Example 9 provides an even more </context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. Verbocean: Mining the web for fine-grained semantic verb relations. In Proceedings of EMNLP-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cleo Condoravdi</author>
<author>Dick Crouch</author>
<author>Valeria de Pavia</author>
<author>Reinhard Stolle</author>
<author>Daniel G Bobrow</author>
</authors>
<date>2003</date>
<booktitle>Entailment, intensionality and text understanding. Workshop on Text Meaning</booktitle>
<marker>Condoravdi, Crouch, de Pavia, Stolle, Bobrow, 2003</marker>
<rawString>Cleo Condoravdi, Dick Crouch, Valeria de Pavia, Reinhard Stolle, and Daniel G. Bobrow. 2003. Entailment, intensionality and text understanding. Workshop on Text Meaning (2003 May 31).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2001</date>
<booktitle>In Proceedings of COLT-2001.</booktitle>
<contexts>
<context position="17407" citStr="Crammer and Singer, 2001" startWordPosition="2708" endWordPosition="2712">tation, collocations in WordNet and named entities are collapsed, causing entities and multiword relations to become single nodes. 4.2 Alignment between graphs The second stage provides an alignment between text and hypothesis graphs, consisting of a mapping from each node in the hypothesis to a unique node in the text or to null. The scoring measure uses node similarity (irrespective of polarity) and structural information based on the dependency graphs. Similarity measures and structural information are combined via weights learned using the passiveaggressive online learning algorithm MIRA (Crammer and Singer, 2001). Alignment weights were learned using manually annotated RTE development sets (see Chambers et al., 2007). 4.3 Filtering non-coreferent events Contradiction features are extracted based on mismatches between the text and hypothesis. Therefore, we must first remove pairs of sentences which do not describe the same event, and thus cannot be contradictory to one another. In the following example, it is necessary to recognize that Plutos moon is not the same as the moon Titan; otherwise conflicting diameters result in labeling the pair a contradiction. T: Plutos moon, which is only about 25 miles</context>
</contexts>
<marker>Crammer, Singer, 2001</marker>
<rawString>Koby Crammer and Yoram Singer. 2001. Ultraconservative online algorithms for multiclass problems. In Proceedings of COLT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>MLCW 2005, LNAI Volume 3944,</booktitle>
<pages>177190</pages>
<editor>In Quinonero-Candela et al., editor,</editor>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="4296" citStr="Dagan et al., 2006" startWordPosition="637" endWordPosition="640">ent. The importance of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., 1997). Recent work in text understanding has not focused on this issue, but it must be tackled in a successful contradiction system. Our system includes event coreference, and we present the first detailed examination of contradiction detection performance, on the basis of our typology. 2 Related work Little work has been done on contradiction detection. The PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) focused on textual inference in any domain. Condoravdi et al. (2003) first recognized the importance of handling entailment and contradiction for text understanding, but they rely on a strict logical definition of these phenomena and do not report empirical results. To our knowledge, Harabagiu et al. (2006) provide the first empirical results for contradiction detection, but they focus on specific kinds of contradiction: those featuring negation and those formed by paraphrases. They constructed two corpora for evaluating their system. One was </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Quinonero-Candela et al., editor, MLCW 2005, LNAI Volume 3944, pages 177190. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-06).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10809" citStr="Fellbaum, 1998" startWordPosition="1679" endWordPosition="1680"> subtle lexical contrasts, as well as world knowledge (WK). We consider contradictions in category (1) easy because they can often be automatically detected without full sentence comprehension. For example, if words in the two passages are antonyms and the sentences are reasonably similar, especially in polarity, a contradiction occurs. Additionally, little external information is needed to gain broad coverage of antonymy, negation, and numeric mismatch contradictions; each involves only a closed set of words or data that can be obtained using existing resources and techniques (e.g., WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)). However, contradictions in category (2) are more difficult to detect automatically because they require precise models of sentence meaning. For instance, to find the contradiction in example 8 (table 1), it is necessary to learn that X said Y did nothing wrong and X accuses Y are incompatible. Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distri</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Firbas</author>
</authors>
<title>On the concept of communicative dynamism in the theory of functional sentence perspective.</title>
<date>1971</date>
<booktitle>Brno Studies in English,</booktitle>
<pages>7--2347</pages>
<contexts>
<context position="19408" citStr="Firbas, 1971" startWordPosition="3038" endWordPosition="3039">s (e.g., John of IBM ... John works for IBM). Thus, we do not filter verb roots that are indicative of such relations. As shown in table 4, this strategy improves results on RTE data. For real world data, however, the assumption of directionality made in this strategy is unfounded, and we cannot assume that one sentence will be short and the other more complex. Assuming two sentences of comparable complexity, we hypothesize that modeling topicality could be used to assess whether the sentences describe the same event. There is a continuum of topicality from the start to the end of a sentence (Firbas, 1971). We thus originally defined the topicality of an NP by nw where n is the nth NP in the sentence. Additionally, we accounted for multiple clauses by weighting each clause equally; in example 4 in table 1, Australia receives the same weight as Prime Minister because each begins a clause. However, this weighting was not supported empirically, and we thus use a simpler, unweighted model. The topicality score of a sentence is calculated as a normalized score across all aligned NPs.3 The text and hypothesis are topically related if either sentence score is above a tuned threshold. Modeling topicali</context>
</contexts>
<marker>Firbas, 1971</marker>
<rawString>Jan Firbas. 1971. On the concept of communicative dynamism in the theory of functional sentence perspective. Brno Studies in English, 7:2347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Ido Dagan</author>
<author>Bernardo Magnini</author>
<author>Bill Dolan</author>
</authors>
<title>The third PASCAL recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="4346" citStr="Giampiccolo et al., 2007" startWordPosition="645" endWordPosition="648">s recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., 1997). Recent work in text understanding has not focused on this issue, but it must be tackled in a successful contradiction system. Our system includes event coreference, and we present the first detailed examination of contradiction detection performance, on the basis of our typology. 2 Related work Little work has been done on contradiction detection. The PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) focused on textual inference in any domain. Condoravdi et al. (2003) first recognized the importance of handling entailment and contradiction for text understanding, but they rely on a strict logical definition of these phenomena and do not report empirical results. To our knowledge, Harabagiu et al. (2006) provide the first empirical results for contradiction detection, but they focus on specific kinds of contradiction: those featuring negation and those formed by paraphrases. They constructed two corpora for evaluating their system. One was created by overtly negating each entailment in the</context>
</contexts>
<marker>Giampiccolo, Dagan, Magnini, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Ido Dagan, Bernardo Magnini, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>Finley Lacatusu</author>
</authors>
<title>Negation, contrast, and contradiction in text processing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI-06).</booktitle>
<contexts>
<context position="4655" citStr="Harabagiu et al. (2006)" startWordPosition="692" endWordPosition="695"> and we present the first detailed examination of contradiction detection performance, on the basis of our typology. 2 Related work Little work has been done on contradiction detection. The PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) focused on textual inference in any domain. Condoravdi et al. (2003) first recognized the importance of handling entailment and contradiction for text understanding, but they rely on a strict logical definition of these phenomena and do not report empirical results. To our knowledge, Harabagiu et al. (2006) provide the first empirical results for contradiction detection, but they focus on specific kinds of contradiction: those featuring negation and those formed by paraphrases. They constructed two corpora for evaluating their system. One was created by overtly negating each entailment in the RTE2 data, producing a balanced dataset (LCC negation). To avoid overtraining, negative markers were also added to each nonentailment, ensuring that they did not create contradictions. The other was produced by paraphrasing the hypothesis sentences from LCC negation, removing the negation (LCC paraphrase): </context>
<context position="13652" citStr="Harabagiu et al. (2006)" startWordPosition="2119" endWordPosition="2122">se datasets contain pairs consisting of a short text and a onesentence hypothesis. Table 2 gives the number of contradictions in each dataset. The RTE datasets are balanced between entailments and non-entailments, and even in these datasets targeting inference, there are few contradictions. Using our guidelines, RTE3 test was annotated by NIST as part of the RTE3 Pilot task in which systems made a 3-way decision as to whether pairs of sentences were entailed, contradictory, or neither (Voorhees, 2008).1 Our annotations and those of NIST were performed on the original RTE datasets, contrary to Harabagiu et al. (2006). Because their corpora are constructed using negation and paraphrase, they are unlikely to cover all types of contradictions in section 3.2. We might hypothesize that rewriting explicit negations commonly occurs via the substitution of antonyms. Imagine, e.g.: H: Bill has finished his math. 1 Information about this task as well as data can be found at http://nlp.stanford.edu/RTE3-pilot/. Type RTE sets Real corpus 1 Antonym 15.0 9.2 Negation 8.8 17.6 Numeric 8.8 29.0 2 Factive/Modal 5.0 6.9 Structure 16.3 3.1 Lexical 18.8 21.4 WK 27.5 13.0 Table 3: Percentages of contradiction types in the RTE</context>
<context position="27276" citStr="Harabagiu et al. (2006)" startWordPosition="4317" endWordPosition="4320">radictions conservatively. We found reasonable inter-annotator agreement between NIST and our post-hoc annotation of RTE3 test ( = 0.81), showing that, even with limited context, humans tend to Precision Recall Accuracy RTE1 dev1 70.37 40.43 RTE1 dev2 72.41 38.18 RTE2 dev 64.00 28.83 RTE3 dev 61.90 31.71 Neg dev 74.07 78.43 75.49 Neg test 62.97 62.50 62.74 LCC negation 75.63 RTE1 test 42.22 26.21 RTE3 test 22.95 19.44 Avg. RTE3 test 10.72 11.69 Table 5: Precision and recall figures for contradiction detection. Accuracy is given for balanced datasets only. LCC negation refers to performance of Harabagiu et al. (2006); Avg. RTE3 test refers to mean performance of the 12 submissions to the RTE3 Pilot. agree on contradictions.4 The results on the test sets show that performance drops on new data, highlighting the difficulty in generalizing from a small corpus of positive contradiction examples, as well as underlining the complexity of building a broad coverage system. This drop in accuracy on the test sets is greater than that of many RTE systems, suggesting that generalizing for contradiction is more difficult than for entailment. Particularly when addressing contradictions that require lexical and world kn</context>
</contexts>
<marker>Harabagiu, Hickl, Lacatusu, 2006</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2006. Negation, contrast, and contradiction in text processing. In Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCCs GROUNDHOG system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="3212" citStr="Hickl et al., 2006" startWordPosition="469" endWordPosition="472">the structure of assertions, discrepancies based on world-knowledge, and lexical contrasts. (1) Police specializing in explosives defused the rockets. Some 100 people were working inside the plant. (2) 100 people were injured. This pair is contradictory: defused rockets cannot go off, and thus cannot injure anyone. Detecting contradictions appears to be a harder task than detecting entailments. Here, it is relatively easy to identify the lack of entailment: the first sentence involves no injuries, so the second is unlikely to be entailed. Most entailment systems function as weak proof theory (Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007), but contradictions require deeper inferences and model building. While mismatching information between sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 \x0crefer to the same event. The importance of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenar</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with LCCs GROUNDHOG system. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Humphreys</author>
<author>Robert Gaizauskas</author>
<author>Saliha Azzam</author>
</authors>
<title>Event coreference for information extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Operational Factors in Pratical, Robust Anaphora Resolution for Unrestricted Texts, 35th ACL meeting.</booktitle>
<contexts>
<context position="3866" citStr="Humphreys et al., 1997" startWordPosition="570" endWordPosition="573">otto et al., 2007), but contradictions require deeper inferences and model building. While mismatching information between sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 \x0crefer to the same event. The importance of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., 1997). Recent work in text understanding has not focused on this issue, but it must be tackled in a successful contradiction system. Our system includes event coreference, and we present the first detailed examination of contradiction detection performance, on the basis of our typology. 2 Related work Little work has been done on contradiction detection. The PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) focused on textual inference in any domain. Condoravdi et al. (2003) first recognized the importance of handling entail</context>
</contexts>
<marker>Humphreys, Gaizauskas, Azzam, 1997</marker>
<rawString>Kevin Humphreys, Robert Gaizauskas, and Saliha Azzam. 1997. Event coreference for information extraction. In Proceedings of the Workshop on Operational Factors in Pratical, Robust Anaphora Resolution for Unrestricted Texts, 35th ACL meeting.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="16692" citStr="Klein and Manning, 2003" startWordPosition="2601" endWordPosition="2604">p.stanford.edu/projects/contradiction. 1042 \x0cof an event is acquired over time (e.g., a rising death toll) or various parties have divergent views of an event (e.g., example 9 in table 1). 4 System overview Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al., 2006), but adds a stage for event coreference decision. 4.1 Linguistic analysis The first stage computes linguistic representations containing information about the semantic content of the passages. The text and hypothesis are converted to typed dependency graphs produced by the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). To improve the dependency graph as a pseudo-semantic representation, collocations in WordNet and named entities are collapsed, causing entities and multiword relations to become single nodes. 4.2 Alignment between graphs The second stage provides an alignment between text and hypothesis graphs, consisting of a mapping from each node in the hypothesis to a unique node in the text or to null. The scoring measure uses node similarity (irrespective of polarity) and structural information based on the dependency graphs. Similarity measures and structural information are</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics (NAACL06).</booktitle>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the North American Association of Computational Linguistics (NAACL06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Abdessamad Echihabi</author>
</authors>
<title>An unsupervised approach to recognizing discourse relations.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11236" citStr="Marcu and Echihabi, 2002" startWordPosition="1742" endWordPosition="1745">antonymy, negation, and numeric mismatch contradictions; each involves only a closed set of words or data that can be obtained using existing resources and techniques (e.g., WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004)). However, contradictions in category (2) are more difficult to detect automatically because they require precise models of sentence meaning. For instance, to find the contradiction in example 8 (table 1), it is necessary to learn that X said Y did nothing wrong and X accuses Y are incompatible. Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. Example 9 provides an even more difficult instance of contradiction created by a lexical discrepancy. Structural issues also create contradictions (examples 6 and 7). Lexical complexities and variations in the function of arguments across verbs can make recognizing these contradictions complicated. Even when similar verbs are used and argument differences exist, structural differences may indicate non-entailment or </context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing relative polarity for textual inference.</title>
<date>2006</date>
<booktitle>In Proceedings of ICoS-5.</booktitle>
<contexts>
<context position="24913" citStr="Nairn et al., 2006" startWordPosition="3931" endWordPosition="3934">tures. The context in which a verb phrase is embedded may give rise to contradiction, as in example 5 (table 1). Negation influences some factivity patterns: Bill forgot to take his wallet contradicts Bill took his wallet while Bill did not forget to take his wallet does not contradict Bill took his wallet. For each text/hypothesis pair, we check the (grand)parent of the text word aligned to the hypothesis verb, and generate a feature based on its factiv1044 \x0city class. Factivity classes are formed by clustering our expansion of the PARC lists of factive, implicative and non-factive verbs (Nairn et al., 2006) according to how they create contradiction. Modality features. Simple patterns of modal reasoning are captured by mapping the text and hypothesis to one of six modalities ((not )possible, (not )actual, (not )necessary), according to the presence of predefined modality markers such as can or maybe. A feature is produced if the text/hypothesis modality pair gives rise to a contradiction. For instance, the following pair will be mapped to the contradiction judgment (possible, not possible): T: The trial court may allow the prevailing party reasonable attorney fees as part of costs. H: The prevai</context>
</contexts>
<marker>Nairn, Condoravdi, Karttunen, 2006</marker>
<rawString>Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative polarity for textual inference. In Proceedings of ICoS-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivia Sanchez-Graillet</author>
<author>Massimo Poesio</author>
</authors>
<title>Discovering contradiction protein-protein interactions in text.</title>
<date>2007</date>
<booktitle>In Proceedings of BioNLP</booktitle>
<contexts>
<context position="28745" citStr="Sanchez-Graillet and Poesio (2007)" startWordPosition="4559" endWordPosition="4562">tions in RTE3 dev but are unable to identify any such contradictions in RTE3 test. Additionally, we found that the precision of category (2) features was less than that of category (1) features. Structural features, for example, caused us to tag 36 non-contradictions as contradictions in RTE3 test, over 75% of the precision errors. Despite these issues, we achieve much higher precision and recall than the average submission to the RTE3 Pilot task on detecting contradictions, as shown in the last two lines of table 5. 4 This stands in contrast with the low inter-annotator agreement reported by Sanchez-Graillet and Poesio (2007) for contradictions in protein-protein interactions. The only hypothesis we have to explain this contrast is the difficulty of scientific material. 1045 \x0cType RTE3 dev RTE3 test 1 Antonym 25.0 (3/12) 42.9 (3/7) Negation 71.4 (5/7) 60.0 (3/5) Numeric 71.4 (5/7) 28.6 (2/7) 2 Factive/Modal 25.0 (1/4) 10.0 (1/10) Structure 46.2 (6/13) 21.1 (4/19) Lexical 13.3 (2/15) 0.0 (0/12) WK 18.2 (4/22) 8.3 (1/12) Table 6: Recall by contradiction type. 7 Error analysis and discussion One significant issue in contradiction detection is lack of feature generalization. This problem is especially apparent for </context>
</contexts>
<marker>Sanchez-Graillet, Poesio, 2007</marker>
<rawString>Olivia Sanchez-Graillet and Massimo Poesio. 2007. Discovering contradiction protein-protein interactions in text. In Proceedings of BioNLP 2007: Biological, translational, and clinical language processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
<author>Arul Menezes</author>
<author>Rion Snow</author>
</authors>
<title>Microsoft research at rte-2: Syntactic contributions in the entailment task: an implementation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="3440" citStr="Vanderwende et al., 2006" startWordPosition="503" endWordPosition="506">ured. This pair is contradictory: defused rockets cannot go off, and thus cannot injure anyone. Detecting contradictions appears to be a harder task than detecting entailments. Here, it is relatively easy to identify the lack of entailment: the first sentence involves no injuries, so the second is unlikely to be entailed. Most entailment systems function as weak proof theory (Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007), but contradictions require deeper inferences and model building. While mismatching information between sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 \x0crefer to the same event. The importance of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., 1997). Recent work in text understanding has not focused on this issue, but it must be tackled in a successful contradiction system. Our system includes event coreference, and we </context>
</contexts>
<marker>Vanderwende, Menezes, Snow, 2006</marker>
<rawString>Lucy Vanderwende, Arul Menezes, and Rion Snow. 2006. Microsoft research at rte-2: Syntactic contributions in the entailment task: an implementation. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Contradictions and justifications: Extensions to the textual entailment task.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="13535" citStr="Voorhees, 2008" startWordPosition="2101" endWordPosition="2102">.3 Contradiction corpora Following the guidelines above, we annotated the RTE datasets for contradiction. These datasets contain pairs consisting of a short text and a onesentence hypothesis. Table 2 gives the number of contradictions in each dataset. The RTE datasets are balanced between entailments and non-entailments, and even in these datasets targeting inference, there are few contradictions. Using our guidelines, RTE3 test was annotated by NIST as part of the RTE3 Pilot task in which systems made a 3-way decision as to whether pairs of sentences were entailed, contradictory, or neither (Voorhees, 2008).1 Our annotations and those of NIST were performed on the original RTE datasets, contrary to Harabagiu et al. (2006). Because their corpora are constructed using negation and paraphrase, they are unlikely to cover all types of contradictions in section 3.2. We might hypothesize that rewriting explicit negations commonly occurs via the substitution of antonyms. Imagine, e.g.: H: Bill has finished his math. 1 Information about this task as well as data can be found at http://nlp.stanford.edu/RTE3-pilot/. Type RTE sets Real corpus 1 Antonym 15.0 9.2 Negation 8.8 17.6 Numeric 8.8 29.0 2 Factive/M</context>
</contexts>
<marker>Voorhees, 2008</marker>
<rawString>Ellen Voorhees. 2008. Contradictions and justifications: Extensions to the textual entailment task. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Zaenen</author>
<author>Lauri Karttunen</author>
<author>Richard S Crouch</author>
</authors>
<title>Local textual inference: can it be defined or circumscribed?</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</booktitle>
<contexts>
<context position="7597" citStr="Zaenen et al. (2005)" startWordPosition="1161" endWordPosition="1164">match turned violent when a woman stabbed her partner because she didnt want to watch the game. (2) A woman passionately wanted to watch the game. We also mark as contradictions pairs reporting contradictory statements. The following sentences refer to the same event (de Menezes in a subway station), and display incompatible views of this event: (1) Eyewitnesses said de Menezes had jumped over the turnstile at Stockwell subway station. (2) The documents leaked to ITV News suggest that Menezes walked casually into the subway station. This example contains an embedded contradiction. Contrary to Zaenen et al. (2005), we argue that recognizing embedded contradictions is important for the application of a contradiction detection system: if John thinks that he is incompetent, and his boss believes that John is not being given a chance, one would like to detect that the targeted information in the two sentences is contradictory, even though the two sentences can be true simultaneously. 3.2 Typology of contradictions Contradictions may arise from a number of different constructions, some overt and others that are com1040 \x0cID Type Text Hypothesis 1 Antonym Capital punishment is a catalyst for more crime. Ca</context>
</contexts>
<marker>Zaenen, Karttunen, Crouch, 2005</marker>
<rawString>Annie Zaenen, Lauri Karttunen, and Richard S. Crouch. 2005. Local textual inference: can it be defined or circumscribed? In ACL 2005 Workshop on Empirical Modeling of Semantic Equivalence and Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Shallow semantics in fast textual entailment rule learners.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</booktitle>
<contexts>
<context position="3261" citStr="Zanzotto et al., 2007" startWordPosition="477" endWordPosition="481">sed on world-knowledge, and lexical contrasts. (1) Police specializing in explosives defused the rockets. Some 100 people were working inside the plant. (2) 100 people were injured. This pair is contradictory: defused rockets cannot go off, and thus cannot injure anyone. Detecting contradictions appears to be a harder task than detecting entailments. Here, it is relatively easy to identify the lack of entailment: the first sentence involves no injuries, so the second is unlikely to be entailed. Most entailment systems function as weak proof theory (Hickl et al., 2006; MacCartney et al., 2006; Zanzotto et al., 2007), but contradictions require deeper inferences and model building. While mismatching information between sentences is often a good cue of non-entailment (Vanderwende et al., 2006), it is not sufficient for contradiction detection which requires more precise comprehension of the consequences of sentences. Assessing event coreference is also essential: for texts to contradict, they must 1039 \x0crefer to the same event. The importance of event coreference was recognized in the MUC information extraction tasks in which it was key to identify scenarios related to the same event (Humphreys et al., </context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2007</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2007. Shallow semantics in fast textual entailment rule learners. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>