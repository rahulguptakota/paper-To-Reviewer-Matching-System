<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.8749375">
b&apos;Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 266275, Jeju Island, Korea, 1214 July 2012. c
</bodyText>
<sectionHeader confidence="0.587282" genericHeader="abstract">
2012 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.801358">
Large Scale Decipherment for Out-of-Domain Machine Translation
</title>
<author confidence="0.914072">
Qing Dou and Kevin Knight
</author>
<affiliation confidence="0.971184666666667">
Information Sciences Institute
Department of Computer Science
University of Southern California
</affiliation>
<email confidence="0.998373">
{qdou,knight}@isi.edu
</email>
<sectionHeader confidence="0.990854" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999078833333333">
We apply slice sampling to Bayesian de-
cipherment and use our new decipherment
framework to improve out-of-domain machine
translation. Compared with the state of the
art algorithm, our approach is highly scalable
and produces better results, which allows us
to decipher ciphertext with billions of tokens
and hundreds of thousands of word types with
high accuracy. We decipher a large amount
of monolingual data to improve out-of-domain
translation and achieve significant gains of up
to 3.8 BLEU points.
</bodyText>
<sectionHeader confidence="0.998254" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999474230769231">
Nowadays, state of the art statistical machine trans-
lation (SMT) systems are built using large amounts
of bilingual parallel corpora. Those corpora are
used to estimate probabilities of word-to-word trans-
lation, word sequences rearrangement, and even
syntactic transformation. Unfortunately, as paral-
lel corpora are expensive and not available for ev-
ery domain, performance of SMT systems drops
significantly when translating out-of-domain texts
(Callison-Burch et al., 2008).
In general, it is easier to obtain in-domain mono-
lingual corpora. Is it possible to use domain specific
monolingual data to improve an MT system trained
on parallel texts from a different domain? Some re-
searchers have attempted to do this by adding a do-
main specific dictionary (Wu et al., 2008), or mining
unseen words (Daume and Jagarlamudi, 2011) us-
ing one of several translation lexicon induction tech-
niques (Haghighi et al., 2008; Koehn and Knight,
2002; Rapp, 1995). However, a dictionary is not al-
ways available, and it is difficult to assign probabil-
ities to a translation lexicon.
(Ravi and Knight, 2011b) have shown that one
can use decipherment to learn a full translation
model from non-parallel data. Their approach is able
to find translations, and assign probabilities to them.
But their work also has certain limitations. First of
all, the corpus they use to build the translation sys-
tem has a very small vocabulary. Secondly, although
their algorithm is able to handle word substitution
ciphers with limited vocabulary, its deciphering ac-
curacy is low.
The contributions of this work are:
We improve previous decipherment work by in-
troducing a more efficient sampling algorithm.
In experiments, our new method improves de-
ciphering accuracy from 82.5% to 88.1% on
(Ravi and Knight, 2011b)s domain specific
data set. Furthermore, we also solve a very
large word substitution cipher built from the
English Gigaword corpus and achieve 92.2%
deciphering accuracy on news text.
With the ability to handle a much larger vocab-
ulary, we learn a domain specific translation ta-
ble from a large amount of monolingual data
and use the translation table to improve out-of-
domain machine translation. In experiments,
we observe significant gains of up to 3.8 BLEU
points. Unlike previous works, the translation
table we build from monolingual data do not
only contain unseen words but also words seen
in parallel data.
</bodyText>
<page confidence="0.998893">
266
</page>
<sectionHeader confidence="0.350051" genericHeader="method">
\x0c2 Word Substitution Ciphers
</sectionHeader>
<bodyText confidence="0.968966111111111">
Before we present our new decipherment frame-
work, we quickly review word substitution decipher-
ment.
Recently, there has been an increasing interest in
decipherment work (Ravi and Knight, 2011a; Ravi
and Knight, 2008). While letter substitution ciphers
can be solved easily, nobody has been able to solve
a word substitution cipher with high accuracy.
As shown in Figure 1, a word substitution cipher
is generated by replacing each word in a natural lan-
guage (plaintext) sequence with a cipher token ac-
cording to a substitution table. The mapping in the
table is deterministic each plaintext word type is
only encoded with one unique cipher token. Solv-
ing a word substitution cipher means recovering the
original plaintext from the ciphertext without know-
ing the substitution table. The only thing we rely on
is knowledge about the underlying language.
</bodyText>
<figureCaption confidence="0.7932435">
Figure 1: Encoding and Decipherment of a Word Substi-
tution Cipher
</figureCaption>
<bodyText confidence="0.992453071428572">
How can we solve a word substitution cipher?
The approach is similar to those taken by cryptana-
lysts who try to recover keys that convert encrypted
texts to readable texts. Suppose we observe a large
cipher string f and want to decipher it into English e.
We can follow the work in (Ravi and Knight, 2011b)
and assume that the cipher string f is generated in
the following way:
Generate English plaintext sequence e =
e1, e2...en with probability P(e).
Replace each English plaintext token ei with a
cipher token fi with probability P(fi|ei).
Based on the above generative story, we write the
probability of the cipher string f as:
</bodyText>
<equation confidence="0.999946">
P(f) =
e
P(e)
n
i
P(fi|ei) (1)
</equation>
<bodyText confidence="0.999472885714286">
We use this equation as an objective function for
maximum likelihood training. In the equation, P(e)
is given by an ngram language model, which is
trained using a large amount of monolingual texts.
The rest of the task is to manipulate channel prob-
abilities P(fi|ei) so that the probability of the ob-
served texts P(f) is maximized.
Theoretically, we can directly apply EM, as pro-
posed in (Knight et al., 2006), or Bayesian decipher-
ment (Ravi and Knight, 2011a) to solve the prob-
lem. However, unlike letter substitution ciphers,
word substitution ciphers pose much greater chal-
lenges to algorithm scalability. To solve a word sub-
stitution cipher, the EM algorithm has a computa-
tional complexity of O(N V 2 R) and the com-
plexity of Bayesian method is O(N V R), where
V is the size of plaintext vocabulary, N is the length
of ciphertext, and R is the number of iterations. In
the world of word substitution ciphers, both V and
N are very large, making these approaches impracti-
cal. (Ravi and Knight, 2011b) propose several mod-
ifications to the existing algorithms. However, the
modified algorithms are only an approximation of
the original algorithms and produce poor decipher-
ing accuracy, and they are still unable to handle very
large scale ciphers.
To address the above problems, we propose the
following two new improvements to previous deci-
pherment methods.
We apply slice sampling (Neal, 2000) to scale
up to ciphers with a very large vocabulary.
Instead of deciphering using the original ci-
phertext, we break the ciphertext into bigrams,
collect their counts, and use the bigrams with
their counts for decipherment.
</bodyText>
<page confidence="0.994885">
267
</page>
<bodyText confidence="0.998994333333333">
\x0cThe new improvements allow us to solve a word
substitution cipher with billions of tokens and hun-
dreds of thousands of word types. Through better
approximation, we achieve a significant increase in
deciphering accuracy. In the following section, we
present details of our new approach.
</bodyText>
<sectionHeader confidence="0.681077" genericHeader="method">
3 Slice Sampling for Bayesian
</sectionHeader>
<subsectionHeader confidence="0.854582">
Decipherment
</subsectionHeader>
<bodyText confidence="0.998753333333333">
In this section, we first give an introduction to
Bayesian decipherment and then describe how to use
slice sampling for it.
</bodyText>
<subsectionHeader confidence="0.999298">
3.1 Bayesian Decipherment
</subsectionHeader>
<bodyText confidence="0.997579583333333">
Bayesian inference has been widely used in natural
language processing (Goldwater and Griffiths, 2007;
Blunsom et al., 2009; Ravi and Knight, 2011b). It is
very attractive for problems like word substitution
ciphers for the following reasons. First, there are
no memory bottlenecks as compared to EM, which
has an O(N V 2) space complexity. Second, priors
encourage the model to learn a sparse distribution.
The inference is usually performed using Gibbs
sampling. For decipherment, a Gibbs sampler keeps
drawing samples from plaintext sequences accord-
ing to derivation probability P(d):
</bodyText>
<equation confidence="0.99995525">
P(d) = P(e)
n
i
P(ci|ei) (2)
</equation>
<bodyText confidence="0.8792775">
In Bayesian inference, P(e) is still given by an
ngram language model, while the channel probabil-
ity is modeled by the Chinese Restaurant Process
(CRP):
</bodyText>
<equation confidence="0.9919335">
P(ci|ei) =
prior + count(ci, ei)
+ count(ei)
(3)
</equation>
<bodyText confidence="0.998361555555556">
Where prior is the base distribution (we set prior
to 1
C in all our experiments, where C is the number
of word types in ciphertext), and count, also called
cache, records events that occurred in the history.
Each sampling operation involves changing a plain-
text token ei, which has V possible choices, where
V is the plaintext vocabulary size, and the final sam-
ple is chosen with probability P(d)
</bodyText>
<equation confidence="0.981406666666667">
V
n=1 P(d)
.
</equation>
<subsectionHeader confidence="0.999258">
3.2 Slice Sampling
</subsectionHeader>
<bodyText confidence="0.954694">
With Gibbs sampling, one has to evaluate all possi-
ble plaintext word types (10k1M) for each sam-
ple decision. This become intractable when the vo-
cabulary is large and the ciphertext is long. Slice
sampling (Neal, 2000) can solve this problem by au-
tomatically adjusting the number of samples to be
considered for each sampling operation.
Suppose the derivation probability for current
sample is P(current s). Then slice sampling draws
a sample in two steps:
Select a threshold T uniformly from the range
{0, P(current s)}.
Draw a new sample new s uniformly from a
pool of candidates: {new s|P(new s) &amp;gt; T}.
From the above two steps, we can see that given a
threshold T, we only need to consider those samples
whose probability is higher than the threshold. This
will lead to a significant reduction on the number
of samples to be considered, if probabilities of the
most samples are below T. In practice, the first step
is easy to implement, while it is difficult to make the
second step efficient. An obvious way to collect can-
didate samples is to go over all possible samples and
record those with probabilities higher than T. How-
ever, doing this will not save any time. Fortunately,
for Bayesian decipherment, we are able to complete
the second step efficiently.
According to Equation 1, the probability of the
current sample is given by a language model P(e)
and a channel model P(c|e). The language model
is usually an ngram language model. Suppose our
current sample current s contains English tokens
X, Y , and Z at position i 1, i, and i + 1 respec-
tively. Let ci be the cipher token at position i. To
obtain a new sample, we just need to change token
Y to Y . Since the rest of the sample stays the same,
we only need to calculate the probability of any tri-
gram 1: P(XY Z) and the channel model probabil-
ity: P(ci|Y ), and multiply them together as shown
in Equation 4.
</bodyText>
<equation confidence="0.912895">
P(XY
Z) P(ci|Y
) (4)
1
</equation>
<bodyText confidence="0.848608">
The probability is given by a bigram language model.
</bodyText>
<page confidence="0.990687">
268
</page>
<bodyText confidence="0.970653833333333">
\x0cIn slice sampling, each sampling operation has
two steps. For the first step, we choose a thresh-
old T uniformly between 0 and P(XY Z)P(ci|Y ).
For the second step, there are two cases.
First, we notice that two types of Y are more
likely to pass the threshold T: (1) Those that have
a very high trigram probability , and (2) those that
have high channel model probability. To find can-
didates that have high trigram probability, we build
sorted lists ranked by P(XY Z), which can be pre-
computed off-line. We only keep the top K En-
glish words for each of the sorted list. When the
last item YK in the list satisfies P(XYkZ) prior &amp;lt;
T, We draw a sample in the following way: set
A = {Y |P(XY Z) prior &amp;gt; T} and set B =
{Y |count(ci, Y ) &amp;gt; 0}, then we only need to sam-
ple Y uniformly from A B until Equation 4 is
greater than T. 2
Second, what happens when the last item YK in
the list does not satisfy P(XYkZ) prior &amp;lt; T?
Then we always choose a word Y randomly and ac-
cept it as a new sample if Equation 4 is greater than
T.
Our algorithm alternates between the two cases.
The actual number of choices the algorithm looks at
depends on the K and the total number of possible
choices. In different experiments, we find that when
K = 500, the algorithm only looks at 0.06% of all
possible choices. When K = 2000, this further re-
duces to 0.007%.
</bodyText>
<subsectionHeader confidence="0.999253">
3.3 Deciphering with Bigrams
</subsectionHeader>
<bodyText confidence="0.9955115">
Since we can decipher with a bigram language
model, we posit that a frequency list of ciphertext
bigrams may contain enough information for deci-
pherment. In our letter substitution experiments, we
find that breaking ciphertext into bigrams doesnt
hurt decipherment accuracy. Table 1 shows how full
English sentences in the original data are broken into
bigrams and their counts.
Instead of doing sampling on full sentences, we
treat each bigram as a full sentence. There are
</bodyText>
<page confidence="0.970387">
2
</page>
<bodyText confidence="0.976358428571429">
It is easy to prove that all other candidates that are not in
the sorted list and with count(ci, Y
) = 0 have a upper bound
probability: P(XYkZ) prior. Therefore, they are ignored
when P(XYkZ) prior &amp;lt; T.
man they took our land .
they took our arable land .
</bodyText>
<construct confidence="0.9602998">
took our 2
they took 2
land . 2
man they 1
arable land 1
</construct>
<tableCaption confidence="0.977275">
Table 1: Converting full sentences to bigrams
</tableCaption>
<bodyText confidence="0.976739333333334">
two advantages to use bigrams and their counts for
decipherment.
First of all, the bigrams and counts are a much
more compact representation of the original cipher-
text with full sentences. For instance, after breaking
a billion tokens from the English Gigaword corpus,
we find only 29m bigrams and 58m tokens, which
is only 1/17 of the original text. In practice, we fur-
ther discard all bigrams with low frequency, which
makes the ciphertext even shorter.
Secondly, using bigrams significantly reduces the
number of sorted lists (from |V |2 to 2|V |) mentioned
in the previous section. The number of lists reduces
from |V |2 to 2|V  |because words in a bigram only
have one neighbor. Therefore, for any word W in a
bigram, we need only 2|V  |lists (words to the right
of W and words to the left of W) instead of |V |2
lists (pairs of words that surround W).
</bodyText>
<subsectionHeader confidence="0.962482">
3.4 Iterative Sampling
</subsectionHeader>
<bodyText confidence="0.999103352941176">
Although we can directly apply slice sampling on
a large number of bigrams, we find that gradually
including less frequent bigrams into a sampling pro-
cess saves deciphering time we call this iterative
sampling:
Break the ciphertext into bigrams and collect
their counts
Keep bigrams whose counts are greater than a
threshold . Then initialize the first sample
randomly and use slice sampling to perform
maximum likelihood training. In the end, ex-
tract a translation table T according to the final
sample.
Lower the threshold to include more bi-
grams into the sampling process. Initialize the
first sample using the translation table obtained
from the previous sampling run (for each ci-
</bodyText>
<page confidence="0.986131">
269
</page>
<bodyText confidence="0.773206333333333">
\x0cpher token f, choose a plaintext token e whose
P(e|f) is the largest). Perform sampling again.
Repeat until = 1.
</bodyText>
<subsectionHeader confidence="0.978322">
3.5 Parallel Sampling
</subsectionHeader>
<bodyText confidence="0.998923333333333">
Inspired by (Newman et al., 2009), our parallel sam-
pling procedure is described below:
Collect bigrams and their counts from cipher-
text and split the bigrams into N parts.
Run slice sampling on each part for 5 iterations
independently.
Combine counts from each part to form a new
count table and run sampling again on each part
using the new table.3
</bodyText>
<sectionHeader confidence="0.956368" genericHeader="method">
4 Decipherment Experiments
</sectionHeader>
<bodyText confidence="0.999145142857143">
In this section, we evaluate our new sampling algo-
rithm in two different experiments. In the first ex-
periment, we compare our method with (Ravi and
Knight, 2011b) on their data set to prove correct-
ness of our approach. In the second experiment, we
scale up to the whole English Gigaword corpus and
achieve a much higher deciphering accuracy.
</bodyText>
<subsectionHeader confidence="0.995635">
4.1 Deciphering Transtac Corpus
</subsectionHeader>
<subsubsectionHeader confidence="0.922434">
4.1.1 Data
</subsubsectionHeader>
<bodyText confidence="0.998828923076923">
We split the Transtac corpus the same way it was
split in (Ravi and Knight, 2011b). The data used to
create ciphertext consists of 1 million tokens, and
3397 word types. The data for language model
training contains 2.7 million tokens and 25761 word
types.4 The ciphertext is created by replacing each
English word with a cipher word.
We use a bigram language model for decipher-
ment training. When the training terminates, a trans-
lation table with probability P(c|e) is built based on
the counts collected from the final sample. For de-
coding, we employ a trigram language model using
full sentences. We use Moses (Koehn et al., 2007)
</bodyText>
<page confidence="0.972953">
3
</page>
<bodyText confidence="0.933274333333333">
Except for combining the counts to form a new count table,
other parameters remain the same. For instance, each part i has
its own prior set to 1
Ci
, where Ci is the number of word types
in that part of ciphertext.
</bodyText>
<page confidence="0.955867">
4
</page>
<table confidence="0.883397666666667">
In practice, we replaced singletons with a UNK symbol,
leaving around 16904 word types.
Method Deciphering Accuracy
Ravi and Knight 80.0 (with bigram LM)
82.5 (with trigram LM)
Slice Sampling 88.1 (with bigram LM)
</table>
<tableCaption confidence="0.827663">
Table 2: Decipherment Accuracy on Transtac Corpus
</tableCaption>
<figure confidence="0.959519777777778">
from (Ravi and Knight, 2011b)
Gold Decoded
man ive come to file
a complaint against
some people .
man ive come to hand
a telephone lines some
people .
man they took our land
.
man they took our
farm .
they took our arable
land .
they took our slide
door .
okay man . okay man .
eighty donums . miflih donums .
</figure>
<tableCaption confidence="0.976407">
Table 3: Sample Decoding Results on Transtac Corpus
</tableCaption>
<bodyText confidence="0.85625425">
from (Ravi and Knight, 2011b)
to perform the decoding. We set the distortion limit
to 0 and cube the translation probabilities. Essen-
tially, Moses tries to find an English sequence e that
</bodyText>
<sectionHeader confidence="0.537746" genericHeader="method">
maximizes P(e) P(c|e)3
4.1.2 Results
</sectionHeader>
<bodyText confidence="0.998788214285714">
We evaluate the performance of our algorithm
by decipherment accuracy, which measures the per-
centage of correctly deciphered cipher tokens. Table
2 compares the deciphering accuracy with the state
of the art algorithm.
Results show that our algorithm improves the de-
ciphering accuracy to 88.1%, which amounts to 33%
reduction in error rate. This justifies our claim: do-
ing better approximation using slice sampling im-
proves decipherment accuracy.
Table 3 shows the first 5 decoding results and
compares them with the gold plaintext. From the ta-
ble we can see that the algorithm recovered the ma-
jority of the plaintext correctly.
</bodyText>
<subsectionHeader confidence="0.998967">
4.2 Deciphering Gigaword Corpus
</subsectionHeader>
<bodyText confidence="0.99804">
To prove the scalability of our new approach, we ap-
ply it to solve a much larger word substitution cipher
built from English Gigaword corpus. The corpus
contains news articles from different news agencies
</bodyText>
<page confidence="0.967435">
270
</page>
<bodyText confidence="0.886583">
\x0cand has a much larger vocabulary compared with the
Transtac corpus.
</bodyText>
<subsubsectionHeader confidence="0.945981">
4.2.1 Data
</subsubsectionHeader>
<bodyText confidence="0.9999225">
We split the corpus into two parts chronologically.
Each part contains approximately 1.2 billion tokens.
We uses the first part to build a word substitution
cipher, which is 10k times longer than the one in the
previous experiment, and the second part to build a
bigram language model. 5
</bodyText>
<subsubsectionHeader confidence="0.772015">
4.2.2 Results
</subsubsectionHeader>
<bodyText confidence="0.999194666666667">
We first use a single machine and apply iterative
sampling to solve a 68 million token cipher. Then
we use the result from the first step to initialize our
parallel sampling process, which uses as many as
100 machines. For evaluation, we calculate deci-
phering accuracy over the first 1000 sentences (33k
tokens).
After 2000 iterations of the parallel sampling pro-
cess, the deciphering accuracy reaches 92.2%. Fig-
ure 2 shows the learning curve of the algorithm. It
can be seen from the graph that both token and type
accuracy increase as more and more data becomes
</bodyText>
<figureCaption confidence="0.855051">
available.
Figure 2: Learning curve for a very large word substitu-
tion cipher: Both token and type accuracy rise as more
and more ciphertext becomes available.
</figureCaption>
<page confidence="0.971838">
5
</page>
<bodyText confidence="0.971786">
Before building the language model, we replace low fre-
quency word types with an UNK symbol, leaving 129k
unique word types.
</bodyText>
<sectionHeader confidence="0.6398085" genericHeader="method">
5 Improving Out-of-Domain Machine
Translation
</sectionHeader>
<bodyText confidence="0.977248181818182">
Domain specific machine translation (MT) is a chal-
lenge for statistical machine translation (SMT) sys-
tems trained on parallel corpora. It is common to see
a significant drop in translation quality when trans-
lating out-of-domain texts. Although it is hard to
find parallel corpora for any specific domain, it is
relatively easy to find domain specific monolingual
corpora. In this section, we show how to use our new
decipherment framework to learn a domain specific
translation table and use it to improve out-of-domain
translations.
</bodyText>
<subsectionHeader confidence="0.901173">
5.1 Baseline SMT System
</subsectionHeader>
<bodyText confidence="0.996087842105263">
We build a state of the art phrase-based SMT system
using Moses (Koehn et al., 2007). The baseline sys-
tem has 3 models: a translation model, a reordering
model, and a language model. The language model
can be trained on monolingual data, and the rest are
trained on parallel data. By default, Moses uses the
following 8 features to score a candidate translation:
direct and inverse translation probabilities
direct and inverse lexical weighting
phrase penalty
a language model
a re-ordering model
word penalty
Each of the 8 features has its own weight, which
can be tuned on a held-out set using minimum error
rate training. (Och, 2003). In the following sections,
we describe how to use decipherment to learn do-
main specific translation probabilities, and use the
new features to improve the baseline.
</bodyText>
<subsectionHeader confidence="0.996725">
5.2 Learning a New Translation Table with
Decipherment
</subsectionHeader>
<bodyText confidence="0.998925333333333">
From a decipherment perspective, machine transla-
tion is a much more complex task than solving a
word substitution cipher and poses three major chal-
lenges:
Mappings between languages are nondetermin-
istic, as words can have multiple translations
</bodyText>
<page confidence="0.984436">
271
</page>
<bodyText confidence="0.97126528">
\x0c Re-ordering of words
Insertion and deletion of words
Fortunately, our decipherment model does not as-
sume deterministic mapping and is able to discover
multiple translations. For the reordering problem,
we treat Spanish as a simple word substitution for
French. Despite the simplification in the assump-
tion, we still expect to learn a useful word-to-word
lexicon via decipherment and use the lexicon to im-
prove our baseline.
Problem formulation: By ignoring word re-
orderings, we can formulate MT decipherment prob-
lem as word substitution decipherment. We view
source language f as ciphertext and target language
e as plaintext. Our goal is to decipher f into e and
estimate translation probabilities based on the deci-
pherment.
Probabilistic decipherment: Similar to solving
a word substitution cipher, all we have to do here is
to estimate the translation model parameters P(f|e)
using a large amount of monolingual data in f and
e respectively. According to Equation 5, our objec-
tive is to estimate the model parameters so that the
probability of source text P(f) is maximized.
arg max
</bodyText>
<equation confidence="0.9787068">
e
P(e)
n
i
P(fi|ei) (5)
</equation>
<bodyText confidence="0.998543285714286">
Building a translation table: Once the sampling
process completes, we estimate translation probabil-
ity P(f|e) from the final sample using maximum
likelihood estimation. We also decipher from the re-
verse direction to estimate P(e|f). Finally, we build
a phrase table by taking translation pairs seen in both
decipherments.
</bodyText>
<subsectionHeader confidence="0.999176">
5.3 Combining Phrase Tables
</subsectionHeader>
<bodyText confidence="0.9970862">
We now have two phrase tables: one learnt from par-
allel corpus and one learnt from non-parallel mono-
lingual corpus through decipherment. The phrase ta-
ble learnt through decipherment only contains word
to word translations, and each translation option
only has two scores. Moses has a function to decode
with multiple phrase tables, so we just need to add
the newly learnt phrase table and specify two more
weights for the scores in it. During decoding, if a
source word only appears in the decipherment table,
</bodyText>
<table confidence="0.846414777777778">
Train
French: 28.5 million tokens
Spanish: 26.6 million tokens
Tune
French: 28k tokens
Spanish: 26k tokens
Test
French: 30k tokens
Spanish: 28k tokens
</table>
<tableCaption confidence="0.997695">
Table 4: Europarl Training, Tuning, and Testing Data
</tableCaption>
<bodyText confidence="0.9566968">
that tables translation will be used. If a source word
exists in both tables, Moses will create two separate
decoding paths and choose the best one after taking
other features into account. If a word is not seen in
either of the tables, it is copied literally to the output.
</bodyText>
<sectionHeader confidence="0.926905" genericHeader="method">
6 MT Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.96804">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.979132">
In our MT experiments, we translate French into
Spanish and use the following corpora to learn our
translation systems:
Europarl Corpus (Koehn, 2005): The Europarl
parallel corpus is extracted from the proceed-
ings of the European Parliament and includes
versions in 11 European languages. The cor-
pus contains articles from the political domain
and is used to train our baseline system. We
use the 6th version of the corpus. After clean-
ing, there are 1.3 million lines left for training.
We use the last 2k lines for tuning and testing
(1k for each), and the rest for training. Details
of training, tuning, and testing data are listed in
</bodyText>
<tableCaption confidence="0.6650175">
Table 4.
EMEA Corpus (Tiedemann, 2009): EMEA is
</tableCaption>
<bodyText confidence="0.994121916666666">
a parallel corpus made out of PDF documents
from the European Medicines Agency. It con-
tains articles from the medical domain, which
is a good test bed for out-of-domain tasks. We
use the first 2k pairs of sentences for tuning
and testing (1k for each), and use the rest (1.1
million lines) for decipherment training. We
split the training corpus in ways that no parallel
sentences are included in the training set. The
splitting methods are listed in Table 5.
For decipherment training, we use lexical transla-
tion tables learned from the Europarl corpus to ini-
</bodyText>
<page confidence="0.996835">
272
</page>
<table confidence="0.914704428571429">
\x0cComparable EMEA :
French: Every odd line, 8.7 million tokens
Spanish: Every even line, 8.1 million tokens
Non-parallel EMEA:
French: First 550k sentences, 9.1 million tokens
Spanish: Last 550k sentences, 7.7 million to-
kens
</table>
<tableCaption confidence="0.978507">
Table 5: EMEA Decipherment Training Data
tialize our sampling process.
</tableCaption>
<subsectionHeader confidence="0.665723">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.886501666666667">
BLEU (Papineni et al., 2002) is used as a standard
evaluation metric. We compare the following 3 sys-
tems in our experiments, and present the results in
</bodyText>
<tableCaption confidence="0.86167">
Table 6.
</tableCaption>
<figure confidence="0.6702374">
Baseline: Trained on Europarl
Decipher-CP: Trained on Europarl + Compa-
rable EMEA
Decipher-NP: Trained on Europarl + Non-
Parallel EMEA
</figure>
<bodyText confidence="0.961105727272727">
Our baseline system achieves 38.2 BLEU score
on Europarl test set. In the second row of Table
6, the test set changes to EMEA, and the baseline
BLEU score drops to 24.9. In the third row, the base-
line score rises to 30.5 with a language model built
from EMEA corpus. Although it is much higher
than the previous baseline, we further improve it
by including a new phrase table learnt from domain
specific monolingual data. In a real out-of-domain
task, we are unlikely to have any parallel data to
tune weights for the new phrase table. Therefore,
we can only set it manually. In experiments, each
score in the new phrase table has a weight of 5, and
the BLEU score rises up to 33.2. In the fourth row
of the table, we assume that there is a small amount
of domain specific parallel data for tuning. With
better weights, our baseline BLEU score increases
to 37.3, and our combined systems increase to 41.1
and 39.7 respectively. In the last row of the table, we
compare the combined systems with an even better
baseline. This time, the baseline is given half of the
EMEA tuning set for training and uses the other half
</bodyText>
<table confidence="0.983011181818182">
French Spanish P(fr|es) P(es|fr)
&amp;lt; &amp;lt; 0.32 1.00
hepatique hepatico 0.88 0.08
hepatica 0.76 0.85
injectable inyectable 0.91 0.92
dl dl 1.00 0.70
&amp;gt; &amp;gt; 0.32 1.00
ribavirine ribavirina 0.40 1.00
olanzapine olanzapina 0.57 1.00
clairance aclaramiento 0.99 0.64
pelliculess recubiertos 1.00 1.00
</table>
<figure confidence="0.8335628">
pharmaco-
cinetique
farmaco-
cinetico
1.00 1.00
</figure>
<tableCaption confidence="0.96736">
Table 7: 10 most frequent OOV words in the table learnt
</tableCaption>
<bodyText confidence="0.965785384615385">
from non-parallel EMEA corpus
for weight tuning. Results show that our combined
systems still outperform the baseline.
The phrase table learnt from monolingual data
consists of both observed and unknown words. Ta-
ble 7 shows the top 10 most frequent OOV words
in the table learnt from non-parallel EMEA corpus.
Among the 10 words, 9 have correct translations. It
is interesting to see that our algorithm finds mul-
tiple correct translations for the word hepatique.
The only mistake in the table is sensible as French
word pellicules is translated into recubiertos con
pelcula in Spanish.
</bodyText>
<sectionHeader confidence="0.984573" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99968675">
We apply slice sampling to Bayesian Decipherment
and show significant improvement in deciphering
accuracy compared with the state of the art algo-
rithm. Our method is not only accurate but also
highly scalable. In experiments, we decipher at the
scale of the English Gigaword corpus, which con-
tains over billions of tokens and hundreds of thou-
sands word types. We further show the value of
our new decipherment algorithm by using it to im-
prove out-of-domain translation. In the future, we
will work with more language pairs, especially those
with significant word re-orderings. Moreover, the
monolingual corpora used in the experiments are far
smaller than what our algorithm can handle. We will
continue to work in scenarios where large amount of
monolingual data is readily available.
</bodyText>
<page confidence="0.96133">
273
</page>
<figure confidence="0.994298041666667">
\x0cTrain Data Tune Data Tune LM Test Data Test LM Baseline
Decipher-
CP
Decipher-
NP
Europarl Europarl Europarl Europarl Europarl 38.2
Europarl Europarl Europarl EMEA Europarl 24.9
Europarl Europarl Europarl EMEA EMEA 30.5
33.2
(+2.7)
32.4
(+1.9)
Europarl EMEA EMEA EMEA EMEA 37.3
41.1
(+3.8)
39.7
(+2.4)
Europarl +
EMEA
EMEA EMEA EMEA EMEA 67.4
68.7
(+1.3)
68.7
(+1.3)
</figure>
<tableCaption confidence="0.984776">
Table 6: MT experiment results: The table shows how much the combined systems outperform the baseline system in
</tableCaption>
<bodyText confidence="0.978452333333333">
different experiments. Each row has a different set of training, tuning, and testing data. Baseline is trained on parallel
data only. Tune LM and Test LM specify language models used for tuning and testing respectively. Decipher-CP and
Decipher-NP use a phrase table learnt from comparable and non-parallel EMEA corpus respectively.
</bodyText>
<sectionHeader confidence="0.998432" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9895145">
This work was supported by NSF Grant 0904684.
The authors would like to thank Philip Koehen,
David Chiang, Jason Riesa, Ashish Vaswani, and
Hui Zhang for their comments and suggestions.
</bodyText>
<sectionHeader confidence="0.989304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998649213114754">
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP. Associa-
tion for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics.
Hal Daume, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies. Association for Com-
putational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics. Association
for Computational Linguistics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT. Association for Computational Linguistics.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipher-
ment problems. In Proceedings of the COLING/ACL
2006 Main Conference Poster Sessions. Association
for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In Pro-
ceedings of the ACL-02 Workshop on Unsupervised
Lexical Acquisition. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Interac-
tive Poster and Demonstration Sessions. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In In Proceedings of the
Tenth Machine Translation Summit, Phuket, Thailand.
Asia-Pacific Association for Machine Translation.
Radford Neal. 2000. Slice sampling. Annals of Statis-
tics, 31.
David Newman, Arthur Asuncion, Padhrai Smyth, and
Max Welling. 2009. Distributed algorithms for topic
models. Journal of Machine Learning Research, 10.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.973588">
274
</page>
<reference confidence="0.999578189189189">
\x0cKishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguistics.
Association for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing. Associ-
ation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2011a. Bayesian infer-
ence for Zodiac and other homophonic ciphers. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2011b. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies. Association for
Computational Linguistics.
Jorg Tiedemann. 2009. News from OPUS a collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing V, volume 309 of Current Issues in Linguistic
Theory. John Benjamins.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.97586">
275
</page>
<figure confidence="0.249763">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.821987">
<note confidence="0.972537666666667">b&apos;Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 266275, Jeju Island, Korea, 1214 July 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.964659">Large Scale Decipherment for Out-of-Domain Machine Translation</title>
<author confidence="0.999348">Qing Dou</author>
<author confidence="0.999348">Kevin Knight</author>
<affiliation confidence="0.999830666666667">Information Sciences Institute Department of Computer Science University of Southern California</affiliation>
<email confidence="0.999811">qdou@isi.edu</email>
<email confidence="0.999811">knight@isi.edu</email>
<abstract confidence="0.994076307692308">We apply slice sampling to Bayesian decipherment and use our new decipherment framework to improve out-of-domain machine translation. Compared with the state of the art algorithm, our approach is highly scalable and produces better results, which allows us to decipher ciphertext with billions of tokens and hundreds of thousands of word types with high accuracy. We decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Chris Dyer</author>
<author>Miles Osborne</author>
</authors>
<title>A Gibbs sampler for phrasal synchronous grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7218" citStr="Blunsom et al., 2009" startWordPosition="1149" endWordPosition="1152">pherment. 267 \x0cThe new improvements allow us to solve a word substitution cipher with billions of tokens and hundreds of thousands of word types. Through better approximation, we achieve a significant increase in deciphering accuracy. In the following section, we present details of our new approach. 3 Slice Sampling for Bayesian Decipherment In this section, we first give an introduction to Bayesian decipherment and then describe how to use slice sampling for it. 3.1 Bayesian Decipherment Bayesian inference has been widely used in natural language processing (Goldwater and Griffiths, 2007; Blunsom et al., 2009; Ravi and Knight, 2011b). It is very attractive for problems like word substitution ciphers for the following reasons. First, there are no memory bottlenecks as compared to EM, which has an O(N V 2) space complexity. Second, priors encourage the model to learn a sparse distribution. The inference is usually performed using Gibbs sampling. For decipherment, a Gibbs sampler keeps drawing samples from plaintext sequences according to derivation probability P(d): P(d) = P(e) n i P(ci|ei) (2) In Bayesian inference, P(e) is still given by an ngram language model, while the channel probability is mo</context>
</contexts>
<marker>Blunsom, Cohn, Dyer, Osborne, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Osborne. 2009. A Gibbs sampler for phrasal synchronous grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1449" citStr="Callison-Burch et al., 2008" startWordPosition="199" endWordPosition="202"> decipher a large amount of monolingual data to improve out-of-domain translation and achieve significant gains of up to 3.8 BLEU points. 1 Introduction Nowadays, state of the art statistical machine translation (SMT) systems are built using large amounts of bilingual parallel corpora. Those corpora are used to estimate probabilities of word-to-word translation, word sequences rearrangement, and even syntactic transformation. Unfortunately, as parallel corpora are expensive and not available for every domain, performance of SMT systems drops significantly when translating out-of-domain texts (Callison-Burch et al., 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi an</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
<author>Jagadeesh Jagarlamudi</author>
</authors>
<title>Domain adaptation for machine translation by mining unseen words.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="1800" citStr="Daume and Jagarlamudi, 2011" startWordPosition="259" endWordPosition="262"> translation, word sequences rearrangement, and even syntactic transformation. Unfortunately, as parallel corpora are expensive and not available for every domain, performance of SMT systems drops significantly when translating out-of-domain texts (Callison-Burch et al., 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi and Knight, 2011b) have shown that one can use decipherment to learn a full translation model from non-parallel data. Their approach is able to find translations, and assign probabilities to them. But their work also has certain limitations. First of all, the corpus they use to build the translation system has a very small vocabulary. Secondly, althou</context>
</contexts>
<marker>Daume, Jagarlamudi, 2011</marker>
<rawString>Hal Daume, III and Jagadeesh Jagarlamudi. 2011. Domain adaptation for machine translation by mining unseen words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Tom Griffiths</author>
</authors>
<title>A fully Bayesian approach to unsupervised part-of-speech tagging.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7196" citStr="Goldwater and Griffiths, 2007" startWordPosition="1145" endWordPosition="1148">rams with their counts for decipherment. 267 \x0cThe new improvements allow us to solve a word substitution cipher with billions of tokens and hundreds of thousands of word types. Through better approximation, we achieve a significant increase in deciphering accuracy. In the following section, we present details of our new approach. 3 Slice Sampling for Bayesian Decipherment In this section, we first give an introduction to Bayesian decipherment and then describe how to use slice sampling for it. 3.1 Bayesian Decipherment Bayesian inference has been widely used in natural language processing (Goldwater and Griffiths, 2007; Blunsom et al., 2009; Ravi and Knight, 2011b). It is very attractive for problems like word substitution ciphers for the following reasons. First, there are no memory bottlenecks as compared to EM, which has an O(N V 2) space complexity. Second, priors encourage the model to learn a sparse distribution. The inference is usually performed using Gibbs sampling. For decipherment, a Gibbs sampler keeps drawing samples from plaintext sequences according to derivation probability P(d): P(d) = P(e) n i P(ci|ei) (2) In Bayesian inference, P(e) is still given by an ngram language model, while the cha</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>Sharon Goldwater and Tom Griffiths. 2007. A fully Bayesian approach to unsupervised part-of-speech tagging. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1885" citStr="Haghighi et al., 2008" startWordPosition="273" endWordPosition="276">, as parallel corpora are expensive and not available for every domain, performance of SMT systems drops significantly when translating out-of-domain texts (Callison-Burch et al., 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi and Knight, 2011b) have shown that one can use decipherment to learn a full translation model from non-parallel data. Their approach is able to find translations, and assign probabilities to them. But their work also has certain limitations. First of all, the corpus they use to build the translation system has a very small vocabulary. Secondly, although their algorithm is able to handle word substitution ciphers with limited vocabular</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL08: HLT. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Anish Nair</author>
<author>Nishit Rathod</author>
<author>Kenji Yamada</author>
</authors>
<title>Unsupervised analysis for decipherment problems.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5397" citStr="Knight et al., 2006" startWordPosition="855" endWordPosition="858"> Replace each English plaintext token ei with a cipher token fi with probability P(fi|ei). Based on the above generative story, we write the probability of the cipher string f as: P(f) = e P(e) n i P(fi|ei) (1) We use this equation as an objective function for maximum likelihood training. In the equation, P(e) is given by an ngram language model, which is trained using a large amount of monolingual texts. The rest of the task is to manipulate channel probabilities P(fi|ei) so that the probability of the observed texts P(f) is maximized. Theoretically, we can directly apply EM, as proposed in (Knight et al., 2006), or Bayesian decipherment (Ravi and Knight, 2011a) to solve the problem. However, unlike letter substitution ciphers, word substitution ciphers pose much greater challenges to algorithm scalability. To solve a word substitution cipher, the EM algorithm has a computational complexity of O(N V 2 R) and the complexity of Bayesian method is O(N V R), where V is the size of plaintext vocabulary, N is the length of ciphertext, and R is the number of iterations. In the world of word substitution ciphers, both V and N are very large, making these approaches impractical. (Ravi and Knight, 2011b) propo</context>
</contexts>
<marker>Knight, Nair, Rathod, Yamada, 2006</marker>
<rawString>Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Yamada. 2006. Unsupervised analysis for decipherment problems. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1909" citStr="Koehn and Knight, 2002" startWordPosition="277" endWordPosition="280">re expensive and not available for every domain, performance of SMT systems drops significantly when translating out-of-domain texts (Callison-Burch et al., 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi and Knight, 2011b) have shown that one can use decipherment to learn a full translation model from non-parallel data. Their approach is able to find translations, and assign probabilities to them. But their work also has certain limitations. First of all, the corpus they use to build the translation system has a very small vocabulary. Secondly, although their algorithm is able to handle word substitution ciphers with limited vocabulary, its deciphering accur</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 Workshop on Unsupervised Lexical Acquisition. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics.</booktitle>
<location>Alexandra</location>
<contexts>
<context position="15607" citStr="Koehn et al., 2007" startWordPosition="2640" endWordPosition="2643">ranstac corpus the same way it was split in (Ravi and Knight, 2011b). The data used to create ciphertext consists of 1 million tokens, and 3397 word types. The data for language model training contains 2.7 million tokens and 25761 word types.4 The ciphertext is created by replacing each English word with a cipher word. We use a bigram language model for decipherment training. When the training terminates, a translation table with probability P(c|e) is built based on the counts collected from the final sample. For decoding, we employ a trigram language model using full sentences. We use Moses (Koehn et al., 2007) 3 Except for combining the counts to form a new count table, other parameters remain the same. For instance, each part i has its own prior set to 1 Ci , where Ci is the number of word types in that part of ciphertext. 4 In practice, we replaced singletons with a UNK symbol, leaving around 16904 word types. Method Deciphering Accuracy Ravi and Knight 80.0 (with bigram LM) 82.5 (with trigram LM) Slice Sampling 88.1 (with bigram LM) Table 2: Decipherment Accuracy on Transtac Corpus from (Ravi and Knight, 2011b) Gold Decoded man ive come to file a complaint against some people . man ive come to h</context>
<context position="19472" citStr="Koehn et al., 2007" startWordPosition="3294" endWordPosition="3297"> translation (MT) is a challenge for statistical machine translation (SMT) systems trained on parallel corpora. It is common to see a significant drop in translation quality when translating out-of-domain texts. Although it is hard to find parallel corpora for any specific domain, it is relatively easy to find domain specific monolingual corpora. In this section, we show how to use our new decipherment framework to learn a domain specific translation table and use it to improve out-of-domain translations. 5.1 Baseline SMT System We build a state of the art phrase-based SMT system using Moses (Koehn et al., 2007). The baseline system has 3 models: a translation model, a reordering model, and a language model. The language model can be trained on monolingual data, and the rest are trained on parallel data. By default, Moses uses the following 8 features to score a candidate translation: direct and inverse translation probabilities direct and inverse lexical weighting phrase penalty a language model a re-ordering model word penalty Each of the 8 features has its own weight, which can be tuned on a held-out set using minimum error rate training. (Och, 2003). In the following sections, we describe how to </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation. In</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit,</booktitle>
<location>Phuket,</location>
<contexts>
<context position="23135" citStr="Koehn, 2005" startWordPosition="3889" endWordPosition="3890">n tokens Tune French: 28k tokens Spanish: 26k tokens Test French: 30k tokens Spanish: 28k tokens Table 4: Europarl Training, Tuning, and Testing Data that tables translation will be used. If a source word exists in both tables, Moses will create two separate decoding paths and choose the best one after taking other features into account. If a word is not seen in either of the tables, it is copied literally to the output. 6 MT Experiments and Results 6.1 Data In our MT experiments, we translate French into Spanish and use the following corpora to learn our translation systems: Europarl Corpus (Koehn, 2005): The Europarl parallel corpus is extracted from the proceedings of the European Parliament and includes versions in 11 European languages. The corpus contains articles from the political domain and is used to train our baseline system. We use the 6th version of the corpus. After cleaning, there are 1.3 million lines left for training. We use the last 2k lines for tuning and testing (1k for each), and the rest for training. Details of training, tuning, and testing data are listed in Table 4. EMEA Corpus (Tiedemann, 2009): EMEA is a parallel corpus made out of PDF documents from the European Me</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In In Proceedings of the Tenth Machine Translation Summit, Phuket, Thailand. Asia-Pacific Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2000</date>
<journal>Annals of Statistics,</journal>
<volume>31</volume>
<contexts>
<context position="6384" citStr="Neal, 2000" startWordPosition="1021" endWordPosition="1022">intext vocabulary, N is the length of ciphertext, and R is the number of iterations. In the world of word substitution ciphers, both V and N are very large, making these approaches impractical. (Ravi and Knight, 2011b) propose several modifications to the existing algorithms. However, the modified algorithms are only an approximation of the original algorithms and produce poor deciphering accuracy, and they are still unable to handle very large scale ciphers. To address the above problems, we propose the following two new improvements to previous decipherment methods. We apply slice sampling (Neal, 2000) to scale up to ciphers with a very large vocabulary. Instead of deciphering using the original ciphertext, we break the ciphertext into bigrams, collect their counts, and use the bigrams with their counts for decipherment. 267 \x0cThe new improvements allow us to solve a word substitution cipher with billions of tokens and hundreds of thousands of word types. Through better approximation, we achieve a significant increase in deciphering accuracy. In the following section, we present details of our new approach. 3 Slice Sampling for Bayesian Decipherment In this section, we first give an intro</context>
<context position="8561" citStr="Neal, 2000" startWordPosition="1377" endWordPosition="1378"> (we set prior to 1 C in all our experiments, where C is the number of word types in ciphertext), and count, also called cache, records events that occurred in the history. Each sampling operation involves changing a plaintext token ei, which has V possible choices, where V is the plaintext vocabulary size, and the final sample is chosen with probability P(d) V n=1 P(d) . 3.2 Slice Sampling With Gibbs sampling, one has to evaluate all possible plaintext word types (10k1M) for each sample decision. This become intractable when the vocabulary is large and the ciphertext is long. Slice sampling (Neal, 2000) can solve this problem by automatically adjusting the number of samples to be considered for each sampling operation. Suppose the derivation probability for current sample is P(current s). Then slice sampling draws a sample in two steps: Select a threshold T uniformly from the range {0, P(current s)}. Draw a new sample new s uniformly from a pool of candidates: {new s|P(new s) &amp;gt; T}. From the above two steps, we can see that given a threshold T, we only need to consider those samples whose probability is higher than the threshold. This will lead to a significant reduction on the number of samp</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Radford Neal. 2000. Slice sampling. Annals of Statistics, 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Newman</author>
<author>Arthur Asuncion</author>
<author>Padhrai Smyth</author>
<author>Max Welling</author>
</authors>
<title>Distributed algorithms for topic models.</title>
<date>2009</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>10</volume>
<contexts>
<context position="14245" citStr="Newman et al., 2009" startWordPosition="2408" endWordPosition="2411">rams and collect their counts Keep bigrams whose counts are greater than a threshold . Then initialize the first sample randomly and use slice sampling to perform maximum likelihood training. In the end, extract a translation table T according to the final sample. Lower the threshold to include more bigrams into the sampling process. Initialize the first sample using the translation table obtained from the previous sampling run (for each ci269 \x0cpher token f, choose a plaintext token e whose P(e|f) is the largest). Perform sampling again. Repeat until = 1. 3.5 Parallel Sampling Inspired by (Newman et al., 2009), our parallel sampling procedure is described below: Collect bigrams and their counts from ciphertext and split the bigrams into N parts. Run slice sampling on each part for 5 iterations independently. Combine counts from each part to form a new count table and run sampling again on each part using the new table.3 4 Decipherment Experiments In this section, we evaluate our new sampling algorithm in two different experiments. In the first experiment, we compare our method with (Ravi and Knight, 2011b) on their data set to prove correctness of our approach. In the second experiment, we scale up</context>
</contexts>
<marker>Newman, Asuncion, Smyth, Welling, 2009</marker>
<rawString>David Newman, Arthur Asuncion, Padhrai Smyth, and Max Welling. 2009. Distributed algorithms for topic models. Journal of Machine Learning Research, 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="20024" citStr="Och, 2003" startWordPosition="3387" endWordPosition="3388">t phrase-based SMT system using Moses (Koehn et al., 2007). The baseline system has 3 models: a translation model, a reordering model, and a language model. The language model can be trained on monolingual data, and the rest are trained on parallel data. By default, Moses uses the following 8 features to score a candidate translation: direct and inverse translation probabilities direct and inverse lexical weighting phrase penalty a language model a re-ordering model word penalty Each of the 8 features has its own weight, which can be tuned on a held-out set using minimum error rate training. (Och, 2003). In the following sections, we describe how to use decipherment to learn domain specific translation probabilities, and use the new features to improve the baseline. 5.2 Learning a New Translation Table with Decipherment From a decipherment perspective, machine translation is a much more complex task than solving a word substitution cipher and poses three major challenges: Mappings between languages are nondeterministic, as words can have multiple translations 271 \x0c Re-ordering of words Insertion and deletion of words Fortunately, our decipherment model does not assume deterministic mappin</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cKishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="24573" citStr="Papineni et al., 2002" startWordPosition="4128" endWordPosition="4131">on lines) for decipherment training. We split the training corpus in ways that no parallel sentences are included in the training set. The splitting methods are listed in Table 5. For decipherment training, we use lexical translation tables learned from the Europarl corpus to ini272 \x0cComparable EMEA : French: Every odd line, 8.7 million tokens Spanish: Every even line, 8.1 million tokens Non-parallel EMEA: French: First 550k sentences, 9.1 million tokens Spanish: Last 550k sentences, 7.7 million tokens Table 5: EMEA Decipherment Training Data tialize our sampling process. 6.2 Results BLEU (Papineni et al., 2002) is used as a standard evaluation metric. We compare the following 3 systems in our experiments, and present the results in Table 6. Baseline: Trained on Europarl Decipher-CP: Trained on Europarl + Comparable EMEA Decipher-NP: Trained on Europarl + NonParallel EMEA Our baseline system achieves 38.2 BLEU score on Europarl test set. In the second row of Table 6, the test set changes to EMEA, and the baseline BLEU score drops to 24.9. In the third row, the baseline score rises to 30.5 with a language model built from EMEA corpus. Although it is much higher than the previous baseline, we further i</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>\x0cKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1922" citStr="Rapp, 1995" startWordPosition="281" endWordPosition="282">ilable for every domain, performance of SMT systems drops significantly when translating out-of-domain texts (Callison-Burch et al., 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi and Knight, 2011b) have shown that one can use decipherment to learn a full translation model from non-parallel data. Their approach is able to find translations, and assign probabilities to them. But their work also has certain limitations. First of all, the corpus they use to build the translation system has a very small vocabulary. Secondly, although their algorithm is able to handle word substitution ciphers with limited vocabulary, its deciphering accuracy is low. T</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Attacking decipherment problems optimally with low-order n-gram models.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3623" citStr="Ravi and Knight, 2008" startWordPosition="551" endWordPosition="554"> a domain specific translation table from a large amount of monolingual data and use the translation table to improve out-ofdomain machine translation. In experiments, we observe significant gains of up to 3.8 BLEU points. Unlike previous works, the translation table we build from monolingual data do not only contain unseen words but also words seen in parallel data. 266 \x0c2 Word Substitution Ciphers Before we present our new decipherment framework, we quickly review word substitution decipherment. Recently, there has been an increasing interest in decipherment work (Ravi and Knight, 2011a; Ravi and Knight, 2008). While letter substitution ciphers can be solved easily, nobody has been able to solve a word substitution cipher with high accuracy. As shown in Figure 1, a word substitution cipher is generated by replacing each word in a natural language (plaintext) sequence with a cipher token according to a substitution table. The mapping in the table is deterministic each plaintext word type is only encoded with one unique cipher token. Solving a word substitution cipher means recovering the original plaintext from the ciphertext without knowing the substitution table. The only thing we rely on is knowl</context>
</contexts>
<marker>Ravi, Knight, 2008</marker>
<rawString>Sujith Ravi and Kevin Knight. 2008. Attacking decipherment problems optimally with low-order n-gram models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Bayesian inference for Zodiac and other homophonic ciphers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="2063" citStr="Ravi and Knight, 2011" startWordPosition="303" endWordPosition="306">, 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi and Knight, 2011b) have shown that one can use decipherment to learn a full translation model from non-parallel data. Their approach is able to find translations, and assign probabilities to them. But their work also has certain limitations. First of all, the corpus they use to build the translation system has a very small vocabulary. Secondly, although their algorithm is able to handle word substitution ciphers with limited vocabulary, its deciphering accuracy is low. The contributions of this work are: We improve previous decipherment work by introducing a more efficient sampling algorithm. In experiments, </context>
<context position="3598" citStr="Ravi and Knight, 2011" startWordPosition="547" endWordPosition="550">ger vocabulary, we learn a domain specific translation table from a large amount of monolingual data and use the translation table to improve out-ofdomain machine translation. In experiments, we observe significant gains of up to 3.8 BLEU points. Unlike previous works, the translation table we build from monolingual data do not only contain unseen words but also words seen in parallel data. 266 \x0c2 Word Substitution Ciphers Before we present our new decipherment framework, we quickly review word substitution decipherment. Recently, there has been an increasing interest in decipherment work (Ravi and Knight, 2011a; Ravi and Knight, 2008). While letter substitution ciphers can be solved easily, nobody has been able to solve a word substitution cipher with high accuracy. As shown in Figure 1, a word substitution cipher is generated by replacing each word in a natural language (plaintext) sequence with a cipher token according to a substitution table. The mapping in the table is deterministic each plaintext word type is only encoded with one unique cipher token. Solving a word substitution cipher means recovering the original plaintext from the ciphertext without knowing the substitution table. The only </context>
<context position="5446" citStr="Ravi and Knight, 2011" startWordPosition="863" endWordPosition="866">cipher token fi with probability P(fi|ei). Based on the above generative story, we write the probability of the cipher string f as: P(f) = e P(e) n i P(fi|ei) (1) We use this equation as an objective function for maximum likelihood training. In the equation, P(e) is given by an ngram language model, which is trained using a large amount of monolingual texts. The rest of the task is to manipulate channel probabilities P(fi|ei) so that the probability of the observed texts P(f) is maximized. Theoretically, we can directly apply EM, as proposed in (Knight et al., 2006), or Bayesian decipherment (Ravi and Knight, 2011a) to solve the problem. However, unlike letter substitution ciphers, word substitution ciphers pose much greater challenges to algorithm scalability. To solve a word substitution cipher, the EM algorithm has a computational complexity of O(N V 2 R) and the complexity of Bayesian method is O(N V R), where V is the size of plaintext vocabulary, N is the length of ciphertext, and R is the number of iterations. In the world of word substitution ciphers, both V and N are very large, making these approaches impractical. (Ravi and Knight, 2011b) propose several modifications to the existing algorith</context>
<context position="7241" citStr="Ravi and Knight, 2011" startWordPosition="1153" endWordPosition="1156">new improvements allow us to solve a word substitution cipher with billions of tokens and hundreds of thousands of word types. Through better approximation, we achieve a significant increase in deciphering accuracy. In the following section, we present details of our new approach. 3 Slice Sampling for Bayesian Decipherment In this section, we first give an introduction to Bayesian decipherment and then describe how to use slice sampling for it. 3.1 Bayesian Decipherment Bayesian inference has been widely used in natural language processing (Goldwater and Griffiths, 2007; Blunsom et al., 2009; Ravi and Knight, 2011b). It is very attractive for problems like word substitution ciphers for the following reasons. First, there are no memory bottlenecks as compared to EM, which has an O(N V 2) space complexity. Second, priors encourage the model to learn a sparse distribution. The inference is usually performed using Gibbs sampling. For decipherment, a Gibbs sampler keeps drawing samples from plaintext sequences according to derivation probability P(d): P(d) = P(e) n i P(ci|ei) (2) In Bayesian inference, P(e) is still given by an ngram language model, while the channel probability is modeled by the Chinese Re</context>
<context position="14749" citStr="Ravi and Knight, 2011" startWordPosition="2494" endWordPosition="2497">f) is the largest). Perform sampling again. Repeat until = 1. 3.5 Parallel Sampling Inspired by (Newman et al., 2009), our parallel sampling procedure is described below: Collect bigrams and their counts from ciphertext and split the bigrams into N parts. Run slice sampling on each part for 5 iterations independently. Combine counts from each part to form a new count table and run sampling again on each part using the new table.3 4 Decipherment Experiments In this section, we evaluate our new sampling algorithm in two different experiments. In the first experiment, we compare our method with (Ravi and Knight, 2011b) on their data set to prove correctness of our approach. In the second experiment, we scale up to the whole English Gigaword corpus and achieve a much higher deciphering accuracy. 4.1 Deciphering Transtac Corpus 4.1.1 Data We split the Transtac corpus the same way it was split in (Ravi and Knight, 2011b). The data used to create ciphertext consists of 1 million tokens, and 3397 word types. The data for language model training contains 2.7 million tokens and 25761 word types.4 The ciphertext is created by replacing each English word with a cipher word. We use a bigram language model for decip</context>
<context position="16119" citStr="Ravi and Knight, 2011" startWordPosition="2731" endWordPosition="2734">ple. For decoding, we employ a trigram language model using full sentences. We use Moses (Koehn et al., 2007) 3 Except for combining the counts to form a new count table, other parameters remain the same. For instance, each part i has its own prior set to 1 Ci , where Ci is the number of word types in that part of ciphertext. 4 In practice, we replaced singletons with a UNK symbol, leaving around 16904 word types. Method Deciphering Accuracy Ravi and Knight 80.0 (with bigram LM) 82.5 (with trigram LM) Slice Sampling 88.1 (with bigram LM) Table 2: Decipherment Accuracy on Transtac Corpus from (Ravi and Knight, 2011b) Gold Decoded man ive come to file a complaint against some people . man ive come to hand a telephone lines some people . man they took our land . man they took our farm . they took our arable land . they took our slide door . okay man . okay man . eighty donums . miflih donums . Table 3: Sample Decoding Results on Transtac Corpus from (Ravi and Knight, 2011b) to perform the decoding. We set the distortion limit to 0 and cube the translation probabilities. Essentially, Moses tries to find an English sequence e that maximizes P(e) P(c|e)3 4.1.2 Results We evaluate the performance of our algor</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011a. Bayesian inference for Zodiac and other homophonic ciphers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="2063" citStr="Ravi and Knight, 2011" startWordPosition="303" endWordPosition="306">, 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi and Knight, 2011b) have shown that one can use decipherment to learn a full translation model from non-parallel data. Their approach is able to find translations, and assign probabilities to them. But their work also has certain limitations. First of all, the corpus they use to build the translation system has a very small vocabulary. Secondly, although their algorithm is able to handle word substitution ciphers with limited vocabulary, its deciphering accuracy is low. The contributions of this work are: We improve previous decipherment work by introducing a more efficient sampling algorithm. In experiments, </context>
<context position="3598" citStr="Ravi and Knight, 2011" startWordPosition="547" endWordPosition="550">ger vocabulary, we learn a domain specific translation table from a large amount of monolingual data and use the translation table to improve out-ofdomain machine translation. In experiments, we observe significant gains of up to 3.8 BLEU points. Unlike previous works, the translation table we build from monolingual data do not only contain unseen words but also words seen in parallel data. 266 \x0c2 Word Substitution Ciphers Before we present our new decipherment framework, we quickly review word substitution decipherment. Recently, there has been an increasing interest in decipherment work (Ravi and Knight, 2011a; Ravi and Knight, 2008). While letter substitution ciphers can be solved easily, nobody has been able to solve a word substitution cipher with high accuracy. As shown in Figure 1, a word substitution cipher is generated by replacing each word in a natural language (plaintext) sequence with a cipher token according to a substitution table. The mapping in the table is deterministic each plaintext word type is only encoded with one unique cipher token. Solving a word substitution cipher means recovering the original plaintext from the ciphertext without knowing the substitution table. The only </context>
<context position="5446" citStr="Ravi and Knight, 2011" startWordPosition="863" endWordPosition="866">cipher token fi with probability P(fi|ei). Based on the above generative story, we write the probability of the cipher string f as: P(f) = e P(e) n i P(fi|ei) (1) We use this equation as an objective function for maximum likelihood training. In the equation, P(e) is given by an ngram language model, which is trained using a large amount of monolingual texts. The rest of the task is to manipulate channel probabilities P(fi|ei) so that the probability of the observed texts P(f) is maximized. Theoretically, we can directly apply EM, as proposed in (Knight et al., 2006), or Bayesian decipherment (Ravi and Knight, 2011a) to solve the problem. However, unlike letter substitution ciphers, word substitution ciphers pose much greater challenges to algorithm scalability. To solve a word substitution cipher, the EM algorithm has a computational complexity of O(N V 2 R) and the complexity of Bayesian method is O(N V R), where V is the size of plaintext vocabulary, N is the length of ciphertext, and R is the number of iterations. In the world of word substitution ciphers, both V and N are very large, making these approaches impractical. (Ravi and Knight, 2011b) propose several modifications to the existing algorith</context>
<context position="7241" citStr="Ravi and Knight, 2011" startWordPosition="1153" endWordPosition="1156">new improvements allow us to solve a word substitution cipher with billions of tokens and hundreds of thousands of word types. Through better approximation, we achieve a significant increase in deciphering accuracy. In the following section, we present details of our new approach. 3 Slice Sampling for Bayesian Decipherment In this section, we first give an introduction to Bayesian decipherment and then describe how to use slice sampling for it. 3.1 Bayesian Decipherment Bayesian inference has been widely used in natural language processing (Goldwater and Griffiths, 2007; Blunsom et al., 2009; Ravi and Knight, 2011b). It is very attractive for problems like word substitution ciphers for the following reasons. First, there are no memory bottlenecks as compared to EM, which has an O(N V 2) space complexity. Second, priors encourage the model to learn a sparse distribution. The inference is usually performed using Gibbs sampling. For decipherment, a Gibbs sampler keeps drawing samples from plaintext sequences according to derivation probability P(d): P(d) = P(e) n i P(ci|ei) (2) In Bayesian inference, P(e) is still given by an ngram language model, while the channel probability is modeled by the Chinese Re</context>
<context position="14749" citStr="Ravi and Knight, 2011" startWordPosition="2494" endWordPosition="2497">f) is the largest). Perform sampling again. Repeat until = 1. 3.5 Parallel Sampling Inspired by (Newman et al., 2009), our parallel sampling procedure is described below: Collect bigrams and their counts from ciphertext and split the bigrams into N parts. Run slice sampling on each part for 5 iterations independently. Combine counts from each part to form a new count table and run sampling again on each part using the new table.3 4 Decipherment Experiments In this section, we evaluate our new sampling algorithm in two different experiments. In the first experiment, we compare our method with (Ravi and Knight, 2011b) on their data set to prove correctness of our approach. In the second experiment, we scale up to the whole English Gigaword corpus and achieve a much higher deciphering accuracy. 4.1 Deciphering Transtac Corpus 4.1.1 Data We split the Transtac corpus the same way it was split in (Ravi and Knight, 2011b). The data used to create ciphertext consists of 1 million tokens, and 3397 word types. The data for language model training contains 2.7 million tokens and 25761 word types.4 The ciphertext is created by replacing each English word with a cipher word. We use a bigram language model for decip</context>
<context position="16119" citStr="Ravi and Knight, 2011" startWordPosition="2731" endWordPosition="2734">ple. For decoding, we employ a trigram language model using full sentences. We use Moses (Koehn et al., 2007) 3 Except for combining the counts to form a new count table, other parameters remain the same. For instance, each part i has its own prior set to 1 Ci , where Ci is the number of word types in that part of ciphertext. 4 In practice, we replaced singletons with a UNK symbol, leaving around 16904 word types. Method Deciphering Accuracy Ravi and Knight 80.0 (with bigram LM) 82.5 (with trigram LM) Slice Sampling 88.1 (with bigram LM) Table 2: Decipherment Accuracy on Transtac Corpus from (Ravi and Knight, 2011b) Gold Decoded man ive come to file a complaint against some people . man ive come to hand a telephone lines some people . man they took our land . man they took our farm . they took our arable land . they took our slide door . okay man . okay man . eighty donums . miflih donums . Table 3: Sample Decoding Results on Transtac Corpus from (Ravi and Knight, 2011b) to perform the decoding. We set the distortion limit to 0 and cube the translation probabilities. Essentially, Moses tries to find an English sequence e that maximizes P(e) P(c|e)3 4.1.2 Results We evaluate the performance of our algor</context>
</contexts>
<marker>Ravi, Knight, 2011</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011b. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorg Tiedemann</author>
</authors>
<title>News from OPUS a collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>In Recent Advances in Natural Language Processing V,</booktitle>
<volume>309</volume>
<publisher>John Benjamins.</publisher>
<contexts>
<context position="23661" citStr="Tiedemann, 2009" startWordPosition="3980" endWordPosition="3981"> use the following corpora to learn our translation systems: Europarl Corpus (Koehn, 2005): The Europarl parallel corpus is extracted from the proceedings of the European Parliament and includes versions in 11 European languages. The corpus contains articles from the political domain and is used to train our baseline system. We use the 6th version of the corpus. After cleaning, there are 1.3 million lines left for training. We use the last 2k lines for tuning and testing (1k for each), and the rest for training. Details of training, tuning, and testing data are listed in Table 4. EMEA Corpus (Tiedemann, 2009): EMEA is a parallel corpus made out of PDF documents from the European Medicines Agency. It contains articles from the medical domain, which is a good test bed for out-of-domain tasks. We use the first 2k pairs of sentences for tuning and testing (1k for each), and use the rest (1.1 million lines) for decipherment training. We split the training corpus in ways that no parallel sentences are included in the training set. The splitting methods are listed in Table 5. For decipherment training, we use lexical translation tables learned from the Europarl corpus to ini272 \x0cComparable EMEA : Fren</context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Jorg Tiedemann. 2009. News from OPUS a collection of multilingual parallel corpora with tools and interfaces. In Recent Advances in Natural Language Processing V, volume 309 of Current Issues in Linguistic Theory. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Chengqing Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1746" citStr="Wu et al., 2008" startWordPosition="251" endWordPosition="254"> to estimate probabilities of word-to-word translation, word sequences rearrangement, and even syntactic transformation. Unfortunately, as parallel corpora are expensive and not available for every domain, performance of SMT systems drops significantly when translating out-of-domain texts (Callison-Burch et al., 2008). In general, it is easier to obtain in-domain monolingual corpora. Is it possible to use domain specific monolingual data to improve an MT system trained on parallel texts from a different domain? Some researchers have attempted to do this by adding a domain specific dictionary (Wu et al., 2008), or mining unseen words (Daume and Jagarlamudi, 2011) using one of several translation lexicon induction techniques (Haghighi et al., 2008; Koehn and Knight, 2002; Rapp, 1995). However, a dictionary is not always available, and it is difficult to assign probabilities to a translation lexicon. (Ravi and Knight, 2011b) have shown that one can use decipherment to learn a full translation model from non-parallel data. Their approach is able to find translations, and assign probabilities to them. But their work also has certain limitations. First of all, the corpus they use to build the translatio</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Hua Wu, Haifeng Wang, and Chengqing Zong. 2008. Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>