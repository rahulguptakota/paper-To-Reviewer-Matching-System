<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.438402">
b&amp;quot;Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917926,
Uppsala, Sweden, 11-16 July 2010. c
</bodyText>
<sectionHeader confidence="0.370882" genericHeader="abstract">
2010 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.841981">
Cross-Language Document Summarization Based on Machine
Translation Quality Prediction
</title>
<author confidence="0.983876">
Xiaojun Wan, Huiying Li and Jianguo Xiao
</author>
<affiliation confidence="0.9343535">
Institute of Compute Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistics (Peking University), MOE, China
</affiliation>
<email confidence="0.994242">
{wanxiaojun,lihuiying,xiaojianguo}@icst.pku.edu.cn
</email>
<sectionHeader confidence="0.989156" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998652478260869">
Cross-language document summarization is a
task of producing a summary in one language
for a document set in a different language. Ex-
isting methods simply use machine translation
for document translation or summary transla-
tion. However, current machine translation
services are far from satisfactory, which re-
sults in that the quality of the cross-language
summary is usually very poor, both in read-
ability and content. In this paper, we propose
to consider the translation quality of each sen-
tence in the English-to-Chinese cross-language
summarization process. First, the translation
quality of each English sentence in the docu-
ment set is predicted with the SVM regression
method, and then the quality score of each sen-
tence is incorporated into the summarization
process. Finally, the English sentences with
high translation quality and high informative-
ness are selected and translated to form the
Chinese summary. Experimental results dem-
onstrate the effectiveness and usefulness of the
proposed approach.
</bodyText>
<sectionHeader confidence="0.998143" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999195375">
Given a document or document set in one source
language, cross-language document summariza-
tion aims to produce a summary in a different
target language. In this study, we focus on Eng-
lish-to-Chinese document summarization for the
purpose of helping Chinese readers to quickly
understand the major content of an English docu-
ment or document set. This task is very impor-
tant in the field of multilingual information ac-
cess.
Till now, most previous work focuses on
monolingual document summarization, but
cross-language document summarization has re-
ceived little attention in the past years. A
straightforward way for cross-language docu-
ment summarization is to translate the summary
from the source language to the target language
by using machine translation services. However,
though machine translation techniques have been
advanced a lot, the machine translation quality is
far from satisfactory, and in many cases, the
translated texts are hard to understand. Therefore,
the translated summary is likely to be hard to
understand by readers, i.e., the summary quality
is likely to be very poor. For example, the trans-
lated Chinese sentence for an ordinary English
sentence (It is also Mr Baker who is making the
most of presidential powers to dispense lar-
gesse.) by using Google Translate is ,
.
The translated sentence is hard to understand
because it contains incorrect translations and it is
very disfluent. If such sentences are selected into
the summary, the quality of the summary would
be very poor.
In order to address the above problem, we
propose to consider the translation quality of the
English sentences in the summarization process.
In particular, the translation quality of each Eng-
lish sentence is predicted by using the SVM re-
gression method, and then the predicted MT
quality score of each sentence is incorporated
into the sentence evaluation process, and finally
both informative and easy-to-translate sentences
are selected and translated to form the Chinese
summary.
An empirical evaluation is conducted to evalu-
ate the performance of machine translation qual-
ity prediction, and a user study is performed to
evaluate the cross-language summary quality.
The results demonstrate the effectiveness of the
proposed approach.
The rest of this paper is organized as follows:
Section 2 introduces related work. The system is
overviewed in Section 3. In Sections 4 and 5, we
present the detailed algorithms and evaluation
</bodyText>
<page confidence="0.990994">
917
</page>
<bodyText confidence="0.9878315">
\x0cresults of machine translation quality prediction
and cross-language summarization, respectively.
We discuss in Section 6 and conclude this paper
in Section 7.
</bodyText>
<sectionHeader confidence="0.8426955" genericHeader="related work">
2 Related Work
2.1 Machine Translation Quality Prediction
</sectionHeader>
<bodyText confidence="0.997285073170732">
Machine translation evaluation aims to assess the
correctness and quality of the translation. Usu-
ally, the human reference translation is provided,
and various methods and metrics have been de-
veloped for comparing the system-translated text
and the human reference text. For example, the
BLEU metric, the NIST metric and their relatives
are all based on the idea that the more shared
substrings the system-translated text has with the
human reference translation, the better the trans-
lation is. Blatz et al. (2003) investigate training
sentence-level confidence measures using a vari-
ety of fuzzy match scores. Albrecht and Hwa
(2007) rely on regression algorithms and refer-
ence-based features to measure the quality of
sentences.
Transition evaluation without using reference
translations has also been investigated. Quirk
(2004) presents a supervised method for training
a sentence level confidence measure on transla-
tion output using a human-annotated corpus.
Features derived from the source sentence and
the target sentence (e.g. sentence length, perplex-
ity, etc.) and features about the translation proc-
ess are leveraged. Gamon et al. (2005) investi-
gate the possibility of evaluating MT quality and
fluency at the sentence level in the absence of
reference translations, and they can improve on
the correlation between language model perplex-
ity scores and human judgment by combing these
perplexity scores with class probabilities from a
machine-learned classifier. Specia et al. (2009)
use the ICM theory to identify the threshold to
map a continuous predicted score into good or
bad categories. Chae and Nenkova (2009) use
surface syntactic features to assess the fluency of
machine translation results.
In this study, we further predict the translation
quality of an English sentence before the ma-
chine translation process, i.e., we do not leverage
reference translation and the target sentence.
</bodyText>
<subsectionHeader confidence="0.998393">
2.2 Document Summarization
</subsectionHeader>
<bodyText confidence="0.997761322033898">
Document summarization methods can be gener-
ally categorized into extraction-based methods
and abstraction-based methods. In this paper, we
focus on extraction-based methods. Extraction-
based summarization methods usually assign
each sentence a saliency score and then rank the
sentences in a document or document set.
For single document summarization, the sen-
tence score is usually computed by empirical
combination of a number of statistical and lin-
guistic feature values, such as term frequency,
sentence position, cue words, stigma words,
topic signature (Luhn 1969; Lin and Hovy, 2000).
The summary sentences can also be selected by
using machine learning methods (Kupiec et al.,
1995; Amini and Gallinari, 2002) or graph-based
methods (ErKan and Radev, 2004; Mihalcea and
Tarau, 2004). Other methods include mutual re-
inforcement principle (Zha 2002; Wan et al.,
2007).
For multi-document summarization, the cen-
troid-based method (Radev et al., 2004) is a typi-
cal method, and it scores sentences based on
cluster centroids, position and TFIDF features.
NeATS (Lin and Hovy, 2002) makes use of new
features such as topic signature to select impor-
tant sentences. Machine Learning based ap-
proaches have also been proposed for combining
various sentence features (Wong et al., 2008).
The influences of input difficulty on summariza-
tion performance have been investigated in
(Nenkova and Louis, 2008). Graph-based meth-
ods have also been used to rank sentences in a
document set. For example, Mihalcea and Tarau
(2005) extend the TextRank algorithm to com-
pute sentence importance in a document set.
Cluster-level information has been incorporated
in the graph model to better evaluate sentences
(Wan and Yang, 2008). Topic-focused or query
biased multi-document summarization has also
been investigated (Wan et al., 2006). Wan et al.
(2010) propose the EUSUM system for extract-
ing easy-to-understand English summaries for
non-native readers.
Several pilot studies have been performed for
the cross-language summarization task by simply
using document translation or summary transla-
tion. Leuski et al. (2003) use machine translation
for English headline generation for Hindi docu-
ments. Lim et al. (2004) propose to generate a
Japanese summary without using a Japanese
summarization system, by first translating Japa-
nese documents into Korean documents, and
then extracting summary sentences by using Ko-
rean summarizer, and finally mapping Korean
summary sentences to Japanese summary sen-
tences. Chalendar et al. (2005) focuses on se-
mantic analysis and sentence generation tech-
niques for cross-language summarization. Orasan
</bodyText>
<page confidence="0.99409">
918
</page>
<bodyText confidence="0.999133571428571">
\x0cand Chiorean (2008) propose to produce summa-
ries with the MMR method from Romanian news
articles and then automatically translate the
summaries into English. Cross language query
based summarization has been investigated in
(Pingali et al., 2007), where the query and the
documents are in different languages. Other re-
lated work includes multilingual summarization
(Lin et al., 2005), which aims to create summa-
ries from multiple sources in multiple languages.
Siddharthan and McKeown (2005) use the in-
formation redundancy in multilingual input to
correct errors in machine translation and thus
improve the quality of multilingual summaries.
</bodyText>
<sectionHeader confidence="0.997338" genericHeader="method">
3 The Proposed Approach
</sectionHeader>
<bodyText confidence="0.99848605">
Previous methods for cross-language summariza-
tion usually consist of two steps: one step for
summarization and one step for translation. Dif-
ferent order of the two steps can lead to the fol-
lowing two basic English-to-Chinese summariza-
tion methods:
Late Translation (LateTrans): Firstly, an
English summary is produced for the English
document set by using existing summarization
methods. Then, the English summary is auto-
matically translated into the corresponding Chi-
nese summary by using machine translation ser-
vices.
Early Translation (EarlyTrans): Firstly, the
English documents are translated into Chinese
documents by using machine translation services.
Then, a Chinese summary is produced for the
translated Chinese documents.
Generally speaking, the LateTrans method has
a few advantages over the EarlyTrans method:
</bodyText>
<sectionHeader confidence="0.523383" genericHeader="method">
1) The LateTrans method is much more effi-
</sectionHeader>
<bodyText confidence="0.9953034">
cient than the EarlyTrans method, because only a
very few summary sentences are required to be
translated in the LateTrans method, whereas all
the sentences in the documents are required to be
translated in the EarlyTrans method.
</bodyText>
<sectionHeader confidence="0.526593" genericHeader="method">
2) The LateTrans method is deemed to be
</sectionHeader>
<bodyText confidence="0.999648518518519">
more effective than the EarlyTrans method, be-
cause the translation errors of the sentences have
great influences on the summary sentence extrac-
tion in the EarlyTrans method.
Thus in this study, we adopt the LateTrans
method as our baseline method. We also adopt
the late translation strategy for our proposed ap-
proach.
In the baseline method, a translated Chinese
sentence is selected into the summary because
the original English sentence is informative.
However, an informative and fluent English sen-
tence is likely to be translated into an uninforma-
tive and disfluent Chinese sentence, and there-
fore, this sentence cannot be selected into the
summary.
In order to address the above problem of exist-
ing methods, our proposed approach takes into
account a novel factor of each sentence for cross-
language summary extraction. Each English sen-
tence is associated with a score indicating its
translation quality. An English sentence with
high translation quality score is more likely to be
selected into the original English summary, and
such English summary can be translated into a
better Chinese summary. Figure 1 gives the ar-
chitecture of our proposed approach.
</bodyText>
<figureCaption confidence="0.7257115">
Figure 1: Architecture of our proposed ap-
proach
</figureCaption>
<bodyText confidence="0.979943764705882">
Seen from the figure, our proposed approach
consists of four main steps: 1) The machine
translation quality score of each English sentence
is predicted by using regression methods; 2) The
informativeness score of each English sentence is
computed by using existing methods; 3) The
English summary is produced by making use of
both the machine translation quality score and
the informativeness score; 4) The extracted Eng-
lish summary is translated into Chinese summary
by using machine translation services.
In this study, we adopt Google Translate1
for
English-to-Chinese translation. Google Translate
is one of the state-of-the-art commercial machine
translation systems used today. It applies statisti-
cal learning techniques to build a translation
</bodyText>
<figure confidence="0.99253265">
1
http://translate.google.com/translate_t
English
Sentences
Sentence
MT Quality
Prediction
Sentence
Informativeness
Evaluation
English
Summary
Extraction
EN-to-CN
Machine
Translation
Chinese Summary
Informativeness score
English summary
MT quality score
</figure>
<page confidence="0.997361">
919
</page>
<bodyText confidence="0.996992125">
\x0cmodel based on both monolingual text in the tar-
get language and aligned text consisting of ex-
amples of human translations between the lan-
guages.
The first step and the evaluation results will be
described in Section 4, and the other steps and
the evaluation results will be described together
in Section 5.
</bodyText>
<sectionHeader confidence="0.96711" genericHeader="method">
4 Machine Translation Quality Predic-
</sectionHeader>
<bodyText confidence="0.4263">
tion
</bodyText>
<subsectionHeader confidence="0.984584">
4.1 Methodology
</subsectionHeader>
<bodyText confidence="0.997194333333333">
In this study, machine translation (MT) quality
reflects both the translation accuracy and the flu-
ency of the translated sentence. An English sen-
tence with high MT quality score is likely to be
translated into an accurate and fluent Chinese
sentence, which can be easily read and under-
stand by Chinese readers. The MT quality pre-
diction is a task of mapping an English sentence
to a numerical value corresponding to a quality
level. The larger the value is, the more accurately
and fluently the sentence can be translated into
Chinese sentence.
As introduced in Section 2.1, several related
work has used regression and classification
methods for MT quality prediction without refer-
ence translations. In our approach, the MT qual-
ity of each sentence in the documents is also pre-
dicted without reference translations. The differ-
ence between our task and previous work is that
previous work can make use of both features in
source sentence and features in target sentence,
while our task only leverages features in source
sentence, because in the late translation strategy,
the English sentences in the documents have not
been translated yet at this step.
In this study, we adopt the -support vector re-
gression (-SVR) method (Vapnik 1995) for the
sentence-level MT quality prediction task. The
SVR algorithm is firmly grounded in the frame-
work of statistical learning theory (VC theory).
The goal of a regression algorithm is to fit a flat
function to the given training data points.
Formally, given a set of training data points
D={(xi,yi) |i=1,2,...,n}Rd
R, where xi is input
feature vector and yi is associated score, the goal
is to fit a function f which approximates the rela-
tion inherited between the data set points. The
standard form is:
</bodyText>
<equation confidence="0.978204393442623">
=
=
+
+
n
i
i
n
i
i
T
b
w
C
C
w
w
1
*
1
,
,
, 2
1
min*
Subject to
i
i
i
T
y
b
x
f
w
+
+
)
(
*
)
( i
i
T
i b
x
f
w
y
+
.
,...,
1
,
0
,
, *
n
i
i
i =
</equation>
<bodyText confidence="0.997616631578947">
The constant C&gt;0 is a parameter for determin-
ing the trade-off between the flatness of f and the
amount up to which deviations larger than are
tolerated.
In the experiments, we use the LIBSVM tool
(Chang and Lin, 2001) with the RBF kernel for
the task, and we use the parameter selection tool
of 10-fold cross validation via grid search to find
the best parameters on the training set with re-
spect to mean squared error (MSE), and then use
the best parameters to train on the whole training
set.
We use the following two groups of features
for each sentence: the first group includes several
basic features, and the second group includes
several parse based features2
. They are all de-
rived based on the source English sentence.
The basic features are as follows:
</bodyText>
<listItem confidence="0.956957714285714">
1) Sentence length: It refers to the number of
words in the sentence.
2) Sub-sentence number: It refers to the num-
ber of sub-sentences in the sentence. We
simply use the punctuation marks as indica-
tors of sub-sentences.
3) Average sub-sentence length: It refers to
the average number of words in the sub-
sentences within the sentence.
4) Percentage of nouns and adjectives: It re-
fers to the percentage of noun words or ad-
jective words in the in the sentence.
5) Number of question words: It refers to the
number of question words (who, whom,
</listItem>
<bodyText confidence="0.993450125">
whose, when, where, which, how, why, what)
in the sentence.
We use the Stanford Lexicalized Parser (Klein
and Manning, 2002) with the provided English
PCFG model to parse a sentence into a parse tree.
The output tree is a context-free phrase structure
grammar representation of the sentence. The
parse features are then selected as follows:
</bodyText>
<listItem confidence="0.918986">
1) Depth of the parse tree: It refers to the
depth of the generated parse tree.
2) Number of SBARs in the parse tree:
SBAR is defined as a clause introduced by a
</listItem>
<bodyText confidence="0.6239675">
(possibly empty) subordinating conjunction.
It is an indictor of sentence complexity.
</bodyText>
<page confidence="0.927129">
2
</page>
<bodyText confidence="0.961808">
Other features, including n-gram frequency, perplexity
features, etc., are not useful in our study. MT features are
not used because Google Translate is used as a black box.
</bodyText>
<page confidence="0.925845">
920
</page>
<listItem confidence="0.77233375">
\x0c3) Number of NPs in the parse tree: It refers
to the number of noun phrases in the parse
tree.
4) Number of VPs in the parse tree: It refers
</listItem>
<bodyText confidence="0.970163111111111">
to the number of verb phrases in the parse
tree.
All the above feature values are scaled by us-
ing the provided svm-scale program.
At this step, each English sentence si can be
associated with a MT quality score TransScore(si)
predicted by the -SVR method. The score is fi-
nally normalized by dividing by the maximum
score.
</bodyText>
<subsectionHeader confidence="0.973214">
4.2 Evaluation
</subsectionHeader>
<subsubsectionHeader confidence="0.929911">
4.2.1 Evaluation Setup
</subsubsectionHeader>
<bodyText confidence="0.996069641025641">
In the experiments, we first constructed the gold-
standard dataset in the following way:
DUC2001 provided 309 English news articles
for document summarization tasks, and the arti-
cles were grouped into 30 document sets. The
news articles were selected from TREC-9. We
chose five document sets (d04, d05, d06, d08,
d11) with 54 news articles out of the DUC2001
document sets. The documents were then split
into sentences and we used 1736 sentences for
evaluation. All the sentences were automatically
translated into Chinese sentences by using the
Google Translate service.
Two Chinese college students were employed
for data annotation. They read the original Eng-
lish sentence and the translated Chinese sentence,
and then manually labeled the overall translation
quality score for each sentence, separately. The
translation quality is an overall measure for both
the translation accuracy and the readability of the
translated sentence. The score ranges between 1
and 5, and 1 means very bad, and 5 means
very good, and 3 means normal. The correla-
tion between the two sets of labeled scores is
0.646. The final translation quality score was the
average of the scores provided by the two anno-
tators.
After annotation, we randomly separated the
labeled sentence set into a training set of 1428
sentences and a test set of 308 sentences. We
then used the LIBSVM tool for training and test-
ing.
Two metrics were used for evaluating the pre-
diction results. The two metrics are as follows:
Mean Square Error (MSE): This metric is a
measure of how correct each of the prediction
values is on average, penalizing more severe er-
rors more heavily. Given the set of prediction
scores for the test sentences: }
</bodyText>
<equation confidence="0.877199333333333">
,...
1
|
{
n
i
y
Y i =
= , and
</equation>
<bodyText confidence="0.93547">
the manually assigned scores for the sentences:
</bodyText>
<equation confidence="0.955098">
}
,...
1
|
{ n
i
y
Y i =
</equation>
<bodyText confidence="0.755874">
= , the MSE of the prediction result
is defined as
</bodyText>
<equation confidence="0.835876166666667">
=
=
n
i
i
i y
y
n
Y
MSE
1
2
)
(
1
)
(
Pearsons Correlation Coefficient (): This
</equation>
<bodyText confidence="0.659284">
metric is a measure of whether the trends of pre-
diction values matched the trends for human-
labeled data. The coefficient between Y and Y
</bodyText>
<equation confidence="0.955275304347826">
is
defined as
y
y
n
i
i
i
s
ns
y
y
y
y
1
)
)(
(
=
=
where y and y are the sample means of Y and
Y
, y
</equation>
<bodyText confidence="0.70278875">
s and y
s are the sample standard deviations
of Y and Y
.
</bodyText>
<subsubsectionHeader confidence="0.989321">
4.2.2 Evaluation Results
</subsubsectionHeader>
<bodyText confidence="0.9608633">
Table 1 shows the prediction results. We can see
that the overall results are promising. And the
correlation is moderately high. The results are
acceptable because we only make use of the fea-
tures derived from the source sentence. The re-
sults guarantee that the use of MT quality scores
in the summarization process is feasible.
We can also see that both the basic features
and the parse features are beneficial to the over-
all prediction results.
</bodyText>
<table confidence="0.997901">
Feature Set MSE
Basic features 0.709 0.399
Parse features 0.702 0.395
All features 0.683 0.433
</table>
<tableCaption confidence="0.959782">
Table 1: Prediction results
</tableCaption>
<figure confidence="0.378666">
5 Cross-Language Document Summari-
zation
</figure>
<subsectionHeader confidence="0.962819">
5.1 Methodology
</subsectionHeader>
<bodyText confidence="0.998246363636364">
In this section, we first compute the informative-
ness score for each sentence. The score reflect
how the sentence expresses the major topic in the
documents. Various existing methods can be
used for computing the score. In this study, we
adopt the centroid-based method.
The centroid-based method is the algorithm
used in the MEAD system. The method uses a
heuristic and simple way to sum the sentence
scores computed based on different features. The
score for each sentence is a linear combination of
</bodyText>
<page confidence="0.984298">
921
</page>
<bodyText confidence="0.995429185185185">
\x0cthe weights computed based on the following
three features:
Centroid-based Weight. The sentences close
to the centroid of the document set are usually
more important than the sentences farther away.
And the centroid weight C(si) of a sentence si is
calculated as the cosine similarity between the
sentence text and the concatenated text for the
whole document set D. The weight is then nor-
malized by dividing the maximal weight.
Sentence Position. The leading several sen-
tences of a document are usually important. So
we calculate for each sentence a weight to reflect
its position priority as P(si)=1-(i-1)/n, where i is
the sequence of the sentence si and n is the total
number of sentences in the document. Obviously,
i ranges from 1 to n.
First Sentence Similarity. Because the first
sentence of a document is very important, a sen-
tence similar to the first sentence is also impor-
tant. Thus we use the cosine similarity value be-
tween a sentence and the corresponding first sen-
tence in the same document as the weight F(si)
for sentence si.
After all the above weights are calculated for
each sentence, we sum all the weights and get the
overall score for the sentence as follows:
</bodyText>
<equation confidence="0.994472285714286">
)
(
)
(
)
(
)
( i
i
i
i s
F
s
P
s
C
s
InfoScore
+
+
=
</equation>
<bodyText confidence="0.997022454545455">
where , and are parameters reflecting the
importance of different features. We empirically
set ===1.
After the informativeness scores for all sen-
tences are computed, the score of each sentence
is normalized by dividing by the maximum score.
After we obtain the MT quality score and the
informativeness score of each sentence in the
document set, we linearly combine the two
scores to get the overall score of each sentence.
Formally, let TransScore(si)[0,1] and Info-
Score(si)[0,1] denote the MT quality score and
the informativeness score of sentence si, the
overall score of the sentence is:
where [0,1] is a parameter controlling the
influences of the two factors. If is set to 0, the
summary is extracted without considering the
MT quality factor. In the experiments, we em-
pirically set the parameter to 0.3 in order to bal-
ance the two factors of content informativeness
and translation quality.
For multi-document summarization, some sen-
tences are highly overlapping with each other,
and thus we apply the same greedy algorithm in
(Wan et al., 2006) to penalize the sentences
highly overlapping with other highly scored sen-
tences, and finally the informative, novel, and
easy-to-translate sentences are chosen into the
English summary.
Finally, the sentences in the English summary
are translated into the corresponding Chinese
sentences by using Google Translate, and the
Chinese summary is formed.
</bodyText>
<subsectionHeader confidence="0.997903">
5.2 Evaluation
</subsectionHeader>
<subsubsectionHeader confidence="0.54465">
5.2.1 Evaluation Setup
</subsubsectionHeader>
<bodyText confidence="0.99717411627907">
In this experiment, we used the document sets
provided by DUC2001 for evaluation. As men-
tioned in Section 4.2.1, DUC2001 provided 30
English document sets for generic multi-
document summarization. The average document
number per document set was 10. The sentences
in each article have been separated and the sen-
tence information has been stored into files. Ge-
neric reference English summaries were pro-
vided by NIST annotators for evaluation. In our
study, we aimed to produce Chinese summaries
for the English document sets. The summary
length was limited to five sentences, i.e. each
summary consisted of five sentences.
The DUC2001 dataset was divided into the
following two datasets:
Ideal Dataset: We have manually labeled the
MT quality scores for the sentences in five
document sets (d04-d11), and we directly used
the manually labeled scores in the summarization
process. The ideal dataset contained these five
document sets.
Real Dataset: The MT quality scores for the
sentences in the remaining 25 document sets
were automatically predicted by using the
learned SVM regression model. And we used the
automatically predicted scores in the summariza-
tion process. The real dataset contained these 25
document sets.
We performed two evaluation procedures: one
based on the ideal dataset to validate the
feasibility of the proposed approach, and
the other based on the real dataset to
demonstrate the effectiveness of the proposed
approach in real applications.
To date, various methods and metrics have
been developed for English summary evaluation
by comparing system summary with reference
summary, such as the pyramid method (Nenkova
et al., 2007) and the ROUGE metrics (Lin and
Hovy, 2003). However, such methods or metrics
cannot be directly used for evaluating Chinese
summary without reference Chinese summary.
</bodyText>
<figure confidence="0.961405947368421">
)
(
)
(
)
1
(
)
( i
i
i s
TransScore
s
InfoScore
s
re
OverallSco
+
=
</figure>
<page confidence="0.995852">
922
</page>
<bodyText confidence="0.958306972972973">
\x0cInstead, we developed an evaluation protocol as
follows:
The evaluation was based on human scoring.
Four Chinese college students participated in the
evaluation as subjects. We have developed a
friendly tool for helping the subjects to evaluate
each Chinese summary from the following three
aspects:
Content: This aspect indicates how much a
summary reflects the major content of the docu-
ment set. After reading a summary, each user can
select a score between 1 and 5 for the summary.
1 means very uninformative and 5 means
very informative.
Readability: This aspect indicates the read-
ability level of the whole summary. After reading
a summary, each user can select a score between
1 and 5 for the summary. 1 means hard to read,
and 5 means easy to read.
Overall: This aspect indicates the overall
quality of a summary. After reading a summary,
each user can select a score between 1 and 5 for
the summary. 1 means very bad, and 5 means
very good.
We performed the evaluation procedures on
the ideal dataset and the read dataset, separately.
During each evaluation procedure, we compared
our proposed approach (=0.3) with the baseline
approach without considering the MT quality
factor (=0). And the two summaries produced
by the two systems for the same document set
were presented in the same interface, and then
the four subjects assigned scores to each sum-
mary after they read and compared the two
summaries. And the assigned scores were finally
averaged across the documents sets and across
the subjects.
</bodyText>
<subsubsectionHeader confidence="0.957278">
5.2.2 Evaluation Results
</subsubsectionHeader>
<bodyText confidence="0.998836733333333">
Table 2 shows the evaluation results on the ideal
dataset with 5 document sets. We can see that
based on the manually labeled MT quality scores,
the Chinese summaries produced by our pro-
posed approach are significantly better than that
produced by the baseline approach over all three
aspects. All subjects agree that our proposed ap-
proach can produce more informative and easy-
to-read Chinese summaries than the baseline ap-
proach.
Table 3 shows the evaluation results on the
real dataset with 25 document sets. We can see
that based on the automatically predicted MT
quality scores, the Chinese summaries produced
by our proposed approach are significantly better
than that produced by the baseline approach over
the readability aspect and the overall aspect. Al-
most all subjects agree that our proposed ap-
proach can produce more easy-to-read and high-
quality Chinese summaries than the baseline ap-
proach.
Comparing the evaluation results in the two
tables, we can find that the performance differ-
ence between the two approaches on the ideal
dataset is bigger than that on the real dataset, es-
pecially on the content aspect. The results dem-
onstrate that the more accurate the MT quality
scores are, the more significant the performance
improvement is.
Overall, the proposed approach is effective to
</bodyText>
<table confidence="0.8823279">
produce good-quality Chinese summaries for
English document sets.
Baseline Approach Proposed Approach
content readability overall content readability overall
Subject1 3.2 2.6 2.8 3.4 3.0 3.4
Subject2 3.0 3.2 3.2 3.4 3.6 3.4
Subject3 3.4 2.8 3.2 3.6 3.8 3.8
Subject4 3.2 3.0 3.2 3.8 3.8 3.8
Average 3.2 2.9 3.1 3.55*
3.55*
</table>
<page confidence="0.346903">
3.6*
</page>
<tableCaption confidence="0.969361">
Table 2: Evaluation results on the ideal dataset (5 document sets)
</tableCaption>
<table confidence="0.972622714285714">
Baseline Approach Proposed Approach
content readability overall content readability overall
Subject1 2.64 2.56 2.60 2.80 3.24 2.96
Subject2 3.60 2.76 3.36 3.52 3.28 3.64
Subject3 3.52 3.72 3.44 3.56 3.80 3.48
Subject4 3.16 2.96 3.12 3.16 3.44 3.52
Average 3.23 3.00 3.13 3.26 3.44*
</table>
<page confidence="0.600469">
3.40*
</page>
<tableCaption confidence="0.999473">
Table 3: Evaluation results on the real dataset (25 document sets)
</tableCaption>
<page confidence="0.434872">
(*
</page>
<bodyText confidence="0.990429">
indicates the difference between the average score of the proposed approach and that of the baseline approach
is statistically significant by using t-test.)
</bodyText>
<page confidence="0.995524">
923
</page>
<sectionHeader confidence="0.57174" genericHeader="method">
\x0c5.2.3 Example Analysis
</sectionHeader>
<bodyText confidence="0.648740285714286">
In this section, we give two running examples to
better show the effectiveness of our proposed
approach. The Chinese sentences and the original
English sentences in the summary are presented
together. The normalized MT quality score for
each sentence is also given at the end of the Chi-
nese sentence.
Document set 1: D04 from the ideal dataset
Summary by baseline approach:
s1: ,73
(37),-
(0.56)
(US INSURERS expect to pay out an estimated Dollars 7.3bn
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far
the costliest disaster the industry has ever faced. )
s2: ,,
,
(0.67)
(THERE are growing signs that Hurricane Andrew, unwelcome as
it was for the devastated inhabitants of Florida and Louisiana, may
in the end do no harm to the re-election campaign of President
</bodyText>
<figure confidence="0.912322">
George Bush.)
s3: ,,
4000&amp;#39; (0.44)
(GENERAL ACCIDENT said yesterday that insurance claims
arising from Hurricane Andrew could &apos;cost it as much as Dollars
40m&apos;.)
s4: ,,4
(0.56)
(In the Bahamas, government spokesman Mr Jimmy Curry said
four deaths had been reported on outlying eastern islands.)
s5: 1.6,,
,
(0.44)
(New Orleans, with a population of 1.6m, is particularly vulnerable
because the city lies below sea level, has the Mississippi River
running through its centre and a large lake immediately to the north.)
Summary by proposed approach:
s1: ,73
(37),-
(0.56)
</figure>
<sectionHeader confidence="0.293827" genericHeader="method">
(US INSURERS expect to pay out an estimated Dollars 7.3bn
</sectionHeader>
<bodyText confidence="0.790848463768116">
(Pounds 3.7bn) in Florida as a result of Hurricane Andrew - by far
the costliest disaster the industry has ever faced.)
s2: ,,
,
(0.67)
(THERE are growing signs that Hurricane Andrew, unwelcome as
it was for the devastated inhabitants of Florida and Louisiana, may
in the end do no harm to the re-election campaign of President
George Bush.)
s3: ,,4
(0.56)
(In the Bahamas, government spokesman Mr Jimmy Curry said
four deaths had been reported on outlying eastern islands.)
s4: ,
(0.89)
(The brunt of the losses are likely to be concentrated among US
insurers, industry analysts said yesterday.)
s5: ,(1.0)
(In north Miami, damage is minimal.)
Document set 2: D54 from the real dataset
Summary by baseline approach:
s1: 116,,
(0.57)
(Two propositions on California&apos;s Nov. 6 ballot would, among other
things, limit the terms of statewide officeholders and state legisla-
tors.)
s2:
(0.36)
(One reason is that term limits would open up politics to many
people now excluded from office by career incumbents.)
s3: ,
,(0.20)
(Proposals to limit the terms of members of Congress and of state
legislators are popular and getting more so, according to the pundits
and the polls.)
s4:
,(0.24)
(State statutes that bar first-time candidates from running for Con-
gress have been held to add to the qualifications set forth in the
Constitution and have been invalidated.)
s5: ,,
,(0.20)
(Another argument is that a citizen Congress with its continuing
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.)
Summary by proposed approach:
s1: 11 6 ,,
(0.57)
(Two propositions on California&apos;s Nov. 6 ballot would, among other
things, limit the terms of statewide officeholders and state legisla-
tors.)
s2:
(0.36)
(One reason is that term limits would open up politics to many
people now excluded from office by career incumbents.)
s3: ,,
,(0.20)
(Another argument is that a citizen Congress with its continuing
flow of fresh faces into Washington would result in better govern-
ment than that provided by representatives with lengthy tenure.)
s4: ,,
,(0.39)
(There are two solid reasons for congressional term limitation that
economists, at least those of the public-choice persuasion, should
fully appreciate.)
s5: ,,
(0.47)
(The root of the problems with Congress is that, barring major
scandal, it is almost impossible to defeat an incumbent.)
</bodyText>
<sectionHeader confidence="0.997826" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999845">
In this study, we adopt the late translation strat-
egy for cross-document summarization. As men-
tioned earlier, the late translation strategy has
some advantages over the early translation strat-
egy. However, in the early translation strategy,
we can use the features derived from both the
source English sentence and the target Chinese
sentence to improve the MT quality prediction
results.
Overall, the framework of our proposed ap-
proach can be easily adapted for cross-document
summarization with the early translation strategy.
</bodyText>
<page confidence="0.983133">
924
</page>
<bodyText confidence="0.993810857142857">
\x0cAnd an empirical comparison between the two
strategies is left as our future work.
Though this study focuses on English-to-
Chinese document summarization, cross-
language summarization tasks for other lan-
guages can also be solved by using our proposed
approach.
</bodyText>
<sectionHeader confidence="0.98443" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9996065">
In this study we propose a novel approach to ad-
dress the cross-language document summariza-
tion task. Our proposed approach predicts the
MT quality score of each English sentence and
then incorporates the score into the summariza-
tion process. The user study results verify the
effectiveness of the approach.
In future work, we will manually translate
English reference summaries into Chinese refer-
ence summaries, and then adopt the ROUGE
metrics to perform automatic evaluation of the
extracted Chinese summaries by comparing them
with the Chinese reference summaries. Moreover,
we will further improve the sentences MT qual-
ity by using sentence compression or sentence
reduction techniques.
</bodyText>
<sectionHeader confidence="0.976361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.6881395">
This work was supported by NSFC (60873155),
Beijing Nova Program (2008B03), NCET
(NCET-08-0006), RFDP (20070001059) and
National High-tech R&amp;D Program
</reference>
<bodyText confidence="0.947809333333333">
(2008AA01Z421). We thank the students for
participating in the user study. We also thank the
anonymous reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.990682" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998877552631579">
J. Albrecht and R. Hwa. 2007. A re-examination of
machine learning approaches for sentence-level mt
evaluation. In Proceedings of ACL2007.
M. R. Amini, P. Gallinari. 2002. The Use of Unla-
beled Data to Improve Supervised Learning for
Text Summarization. In Proceedings of SIGIR2002.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C.
Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003. Confidence estimation for statistical machine
translation. Johns Hopkins Summer Workshop Fi-
nal Report.
J. Chae and A. Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case studies
of machine translation and human-written text. In
Proceedings of EACL2009.
G. de Chalendar, R. Besancon, O. Ferret, G. Grefen-
stette, and O. Mesnard. 2005. Crosslingual summa-
rization with thematic extraction, syntactic sen-
tence simplification, and bilingual generation. In
Workshop on Crossing Barriers in Text Summari-
zation Research, 5th International Conference on
Recent Advances in Natural Language Processing
(RANLP2005).
C.-C. Chang and C.-J. Lin. 2001. LIBSVM : a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/~cjlin/libsvm
G. ErKan, D. R. Radev. LexPageRank. 2004. Prestige
in Multi-Document Text Summarization. In Pro-
ceedings of EMNLP2004.
M. Gamon, A. Aue, and M. Smets. 2005. Sentence-
level MT evaluation without reference translations:
beyond language modeling. In Proceedings of
EAMT2005.
D. Klein and C. D. Manning. 2002. Fast Exact Infer-
ence with a Factored Model for Natural Language
Parsing. In Proceedings of NIPS2002.
J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable
Document Summarizer. In Proceedings of
SIGIR1995.
A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. Och,
E. Hovy. 2003. Cross-lingual C*ST*RD: English
access to Hindi information. ACM Transactions on
Asian Language Information Processing, 2(3):
245-269.
J.-M. Lim, I.-S. Kang, J.-H. Lee. 2004. Multi-
document summarization using cross-language
texts. In Proceedings of NTCIR-4.
C. Y. Lin, E. Hovy. 2000. The Automated Acquisition
of Topic Signatures for Text Summarization. In
Proceedings of the 17th Conference on Computa-
tional Linguistics.
C..-Y. Lin and E.. H. Hovy. 2002. From Single to
Multi-document Summarization: A Prototype Sys-
tem and its Evaluation. In Proceedings of ACL-02.
C.-Y. Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence
Statistics. In Proceedings of HLT-NAACL -03.
C.-Y. Lin, L. Zhou, and E. Hovy. 2005. Multilingual
summarization evaluation 2005: automatic evalua-
tion report. In Proceedings of MSE (ACL-2005
Workshop).
H. P. Luhn. 1969. The Automatic Creation of litera-
ture Abstracts. IBM Journal of Research and De-
velopment, 2(2).
R. Mihalcea, P. Tarau. 2004. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP2004.
R. Mihalcea and P. Tarau. 2005. A language inde-
pendent algorithm for single and multiple docu-
ment summarization. In Proceedings of IJCNLP-05.
A. Nenkova and A. Louis. 2008. Can you summarize
this? Identifying correlates of input difficulty for
generic multi-document summarization. In Pro-
ceedings of ACL-08:HLT.
A. Nenkova, R. Passonneau, and K. McKeown. 2007.
The Pyramid method: incorporating human content
selection variation in summarization evaluation.
</reference>
<page confidence="0.941653">
925
</page>
<reference confidence="0.99935746">
\x0cACM Transactions on Speech and Language Proc-
essing (TSLP), 4(2).
C. Orasan, and O. A. Chiorean. 2008. Evaluation of a
Crosslingual Romanian-English Multi-document
Summariser. In Proceedings of 6th Language Re-
sources and Evaluation Conference (LREC2008).
P. Pingali, J. Jagarlamudi and V. Varma. 2007. Ex-
periments in cross language query focused multi-
document summarization. In Workshop on Cross
Lingual Information Access Addressing the Infor-
mation Need of Multilingual Societies in
IJCAI2007.
C. Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of
LREC2004.
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40: 919-938.
A. Siddharthan and K. McKeown. 2005. Improving
multilingual summarization: using redundancy in
the input to correct MT errors. In Proceedings of
HLT/EMNLP-2005.
L. Specia, Z. Wang, M. Turchi, J. Shawe-Taylor, C.
Saunders. 2009. Improving the Confidence of Ma-
chine Translation Quality Estimates. In MT Summit
2009 (Machine Translation Summit XII).
V. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
X. Wan, H. Li and J. Xiao. 2010. EUSUM: extracting
easy-to-understand English summaries for non-
native readers. In Proceedings of SIGIR2010.
X. Wan, J. Yang and J. Xiao. 2006. Using cross-
document random walks for topic-focused multi-
documetn summarization. In Proceedings of
WI2006.
X. Wan and J. Yang. 2008. Multi-document summari-
zation using cluster-based link analysis. In Pro-
ceedings of SIGIR-08.
X. Wan, J. Yang and J. Xiao. 2007. Towards an Itera-
tive Reinforcement Approach for Simultaneous
Document Summarization and Keyword Extraction.
In Proceedings of ACL2007.
K.-F. Wong, M. Wu and W. Li. 2008. Extractive sum-
marization using supervised and semi-supervised
learning. In Proceedings of COLING-08.
H. Y. Zha. 2002. Generic Summarization and Key-
phrase Extraction Using Mutual Reinforcement
Principle and Sentence Clustering. In Proceedings
of SIGIR2002.
</reference>
<page confidence="0.986561">
926
</page>
<figure confidence="0.246408">
\x0c&amp;quot;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684540">
<note confidence="0.917423333333333">b&amp;quot;Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 917926, Uppsala, Sweden, 11-16 July 2010. c 2010 Association for Computational Linguistics</note>
<title confidence="0.9814275">Cross-Language Document Summarization Based on Machine Translation Quality Prediction</title>
<author confidence="0.995156">Xiaojun Wan</author>
<author confidence="0.995156">Huiying Li</author>
<author confidence="0.995156">Jianguo Xiao</author>
<affiliation confidence="0.976223">Institute of Compute Science and Technology, Peking University, Beijing 100871, China</affiliation>
<address confidence="0.97835">Key Laboratory of Computational Linguistics (Peking University), MOE, China</address>
<email confidence="0.987111">wanxiaojun@icst.pku.edu.cn</email>
<email confidence="0.987111">lihuiying@icst.pku.edu.cn</email>
<email confidence="0.987111">xiaojianguo@icst.pku.edu.cn</email>
<abstract confidence="0.995934583333333">Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informativeness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Albrecht</author>
<author>R Hwa</author>
</authors>
<title>This work was supported by NSFC (60873155), Beijing Nova Program</title>
<date>2007</date>
<booktitle>(2008B03), NCET (NCET-08-0006), RFDP (20070001059) and National High-tech R&amp;D Program</booktitle>
<contexts>
<context position="4861" citStr="Albrecht and Hwa (2007)" startWordPosition="727" endWordPosition="730">anslation evaluation aims to assess the correctness and quality of the translation. Usually, the human reference translation is provided, and various methods and metrics have been developed for comparing the system-translated text and the human reference text. For example, the BLEU metric, the NIST metric and their relatives are all based on the idea that the more shared substrings the system-translated text has with the human reference translation, the better the translation is. Blatz et al. (2003) investigate training sentence-level confidence measures using a variety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and reference-based features to measure the quality of sentences. Transition evaluation without using reference translations has also been investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>This work was supported by NSFC (60873155), Beijing Nova Program (2008B03), NCET (NCET-08-0006), RFDP (20070001059) and National High-tech R&amp;D Program J. Albrecht and R. Hwa. 2007. A re-examination of machine learning approaches for sentence-level mt evaluation. In Proceedings of ACL2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Amini</author>
<author>P Gallinari</author>
</authors>
<title>The Use of Unlabeled Data to Improve Supervised Learning for Text Summarization.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR2002.</booktitle>
<contexts>
<context position="6875" citStr="Amini and Gallinari, 2002" startWordPosition="1027" endWordPosition="1030">and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficul</context>
</contexts>
<marker>Amini, Gallinari, 2002</marker>
<rawString>M. R. Amini, P. Gallinari. 2002. The Use of Unlabeled Data to Improve Supervised Learning for Text Summarization. In Proceedings of SIGIR2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence estimation for statistical machine translation. Johns Hopkins Summer Workshop Final Report.</title>
<date>2003</date>
<contexts>
<context position="4742" citStr="Blatz et al. (2003)" startWordPosition="710" endWordPosition="713">ection 6 and conclude this paper in Section 7. 2 Related Work 2.1 Machine Translation Quality Prediction Machine translation evaluation aims to assess the correctness and quality of the translation. Usually, the human reference translation is provided, and various methods and metrics have been developed for comparing the system-translated text and the human reference text. For example, the BLEU metric, the NIST metric and their relatives are all based on the idea that the more shared substrings the system-translated text has with the human reference translation, the better the translation is. Blatz et al. (2003) investigate training sentence-level confidence measures using a variety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and reference-based features to measure the quality of sentences. Transition evaluation without using reference translations has also been investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process ar</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2003. Confidence estimation for statistical machine translation. Johns Hopkins Summer Workshop Final Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chae</author>
<author>A Nenkova</author>
</authors>
<title>Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL2009.</booktitle>
<contexts>
<context position="5853" citStr="Chae and Nenkova (2009)" startWordPosition="876" endWordPosition="879">e and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabilities from a machine-learned classifier. Specia et al. (2009) use the ICM theory to identify the threshold to map a continuous predicted score into good or bad categories. Chae and Nenkova (2009) use surface syntactic features to assess the fluency of machine translation results. In this study, we further predict the translation quality of an English sentence before the machine translation process, i.e., we do not leverage reference translation and the target sentence. 2.2 Document Summarization Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a doc</context>
</contexts>
<marker>Chae, Nenkova, 2009</marker>
<rawString>J. Chae and A. Nenkova. 2009. Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text. In Proceedings of EACL2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G de Chalendar</author>
<author>R Besancon</author>
<author>O Ferret</author>
<author>G Grefenstette</author>
<author>O Mesnard</author>
</authors>
<title>Crosslingual summarization with thematic extraction, syntactic sentence simplification, and bilingual generation.</title>
<date>2005</date>
<booktitle>In Workshop on Crossing Barriers in Text Summarization Research, 5th International Conference on Recent Advances in Natural Language Processing (RANLP2005).</booktitle>
<marker>de Chalendar, Besancon, Ferret, Grefenstette, Mesnard, 2005</marker>
<rawString>G. de Chalendar, R. Besancon, O. Ferret, G. Grefenstette, and O. Mesnard. 2005. Crosslingual summarization with thematic extraction, syntactic sentence simplification, and bilingual generation. In Workshop on Crossing Barriers in Text Summarization Research, 5th International Conference on Recent Advances in Natural Language Processing (RANLP2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM : a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP2004.</booktitle>
<contexts>
<context position="15285" citStr="Chang and Lin, 2001" startWordPosition="2375" endWordPosition="2378">y, given a set of training data points D={(xi,yi) |i=1,2,...,n}Rd R, where xi is input feature vector and yi is associated score, the goal is to fit a function f which approximates the relation inherited between the data set points. The standard form is: = = + + n i i n i i T b w C C w w 1 * 1 , , , 2 1 min* Subject to i i i T y b x f w + + ) ( * ) ( i i T i b x f w y + . ,..., 1 , 0 , , * n i i i = The constant C&gt;0 is a parameter for determining the trade-off between the flatness of f and the amount up to which deviations larger than are tolerated. In the experiments, we use the LIBSVM tool (Chang and Lin, 2001) with the RBF kernel for the task, and we use the parameter selection tool of 10-fold cross validation via grid search to find the best parameters on the training set with respect to mean squared error (MSE), and then use the best parameters to train on the whole training set. We use the following two groups of features for each sentence: the first group includes several basic features, and the second group includes several parse based features2 . They are all derived based on the source English sentence. The basic features are as follows: 1) Sentence length: It refers to the number of words i</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C. Chang and C.-J. Lin. 2001. LIBSVM : a library for support vector machines. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm G. ErKan, D. R. Radev. LexPageRank. 2004. Prestige in Multi-Document Text Summarization. In Proceedings of EMNLP2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>A Aue</author>
<author>M Smets</author>
</authors>
<title>Sentencelevel MT evaluation without reference translations: beyond language modeling.</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT2005.</booktitle>
<contexts>
<context position="5374" citStr="Gamon et al. (2005)" startWordPosition="801" endWordPosition="804">training sentence-level confidence measures using a variety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and reference-based features to measure the quality of sentences. Transition evaluation without using reference translations has also been investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabilities from a machine-learned classifier. Specia et al. (2009) use the ICM theory to identify the threshold to map a continuous predicted score into good or bad categories. Chae and Nenkova (2009) use surface syntactic features to assess the fluency of machine translation results. In this study, we further predict t</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>M. Gamon, A. Aue, and M. Smets. 2005. Sentencelevel MT evaluation without reference translations: beyond language modeling. In Proceedings of EAMT2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast Exact Inference with a Factored Model for Natural Language Parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS2002.</booktitle>
<contexts>
<context position="16496" citStr="Klein and Manning, 2002" startWordPosition="2587" endWordPosition="2590">er of words in the sentence. 2) Sub-sentence number: It refers to the number of sub-sentences in the sentence. We simply use the punctuation marks as indicators of sub-sentences. 3) Average sub-sentence length: It refers to the average number of words in the subsentences within the sentence. 4) Percentage of nouns and adjectives: It refers to the percentage of noun words or adjective words in the in the sentence. 5) Number of question words: It refers to the number of question words (who, whom, whose, when, where, which, how, why, what) in the sentence. We use the Stanford Lexicalized Parser (Klein and Manning, 2002) with the provided English PCFG model to parse a sentence into a parse tree. The output tree is a context-free phrase structure grammar representation of the sentence. The parse features are then selected as follows: 1) Depth of the parse tree: It refers to the depth of the generated parse tree. 2) Number of SBARs in the parse tree: SBAR is defined as a clause introduced by a (possibly empty) subordinating conjunction. It is an indictor of sentence complexity. 2 Other features, including n-gram frequency, perplexity features, etc., are not useful in our study. MT features are not used because </context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>D. Klein and C. D. Manning. 2002. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Proceedings of NIPS2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>F Chen</author>
</authors>
<title>A.Trainable Document Summarizer.</title>
<date>1995</date>
<booktitle>In Proceedings of SIGIR1995.</booktitle>
<contexts>
<context position="6847" citStr="Kupiec et al., 1995" startWordPosition="1023" endWordPosition="1026">action-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The </context>
</contexts>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>J. Kupiec, J. Pedersen, F. Chen. 1995. A.Trainable Document Summarizer. In Proceedings of SIGIR1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Leuski</author>
<author>C-Y Lin</author>
<author>L Zhou</author>
<author>U Germann</author>
<author>F J Och</author>
<author>E Hovy</author>
</authors>
<title>Cross-lingual C*ST*RD: English access to Hindi information.</title>
<date>2003</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>245--269</pages>
<contexts>
<context position="8264" citStr="Leuski et al. (2003)" startWordPosition="1238" endWordPosition="1241">mple, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary translation. Leuski et al. (2003) use machine translation for English headline generation for Hindi documents. Lim et al. (2004) propose to generate a Japanese summary without using a Japanese summarization system, by first translating Japanese documents into Korean documents, and then extracting summary sentences by using Korean summarizer, and finally mapping Korean summary sentences to Japanese summary sentences. Chalendar et al. (2005) focuses on semantic analysis and sentence generation techniques for cross-language summarization. Orasan 918 \x0cand Chiorean (2008) propose to produce summaries with the MMR method from Ro</context>
</contexts>
<marker>Leuski, Lin, Zhou, Germann, Och, Hovy, 2003</marker>
<rawString>A. Leuski, C.-Y. Lin, L. Zhou, U. Germann, F. J. Och, E. Hovy. 2003. Cross-lingual C*ST*RD: English access to Hindi information. ACM Transactions on Asian Language Information Processing, 2(3): 245-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-M Lim</author>
<author>I-S Kang</author>
<author>J-H Lee</author>
</authors>
<title>Multidocument summarization using cross-language texts.</title>
<date>2004</date>
<booktitle>In Proceedings of NTCIR-4.</booktitle>
<contexts>
<context position="8359" citStr="Lim et al. (2004)" startWordPosition="1253" endWordPosition="1256">document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary translation. Leuski et al. (2003) use machine translation for English headline generation for Hindi documents. Lim et al. (2004) propose to generate a Japanese summary without using a Japanese summarization system, by first translating Japanese documents into Korean documents, and then extracting summary sentences by using Korean summarizer, and finally mapping Korean summary sentences to Japanese summary sentences. Chalendar et al. (2005) focuses on semantic analysis and sentence generation techniques for cross-language summarization. Orasan 918 \x0cand Chiorean (2008) propose to produce summaries with the MMR method from Romanian news articles and then automatically translate the summaries into English. Cross languag</context>
</contexts>
<marker>Lim, Kang, Lee, 2004</marker>
<rawString>J.-M. Lim, I.-S. Kang, J.-H. Lee. 2004. Multidocument summarization using cross-language texts. In Proceedings of NTCIR-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>The Automated Acquisition of Topic Signatures for Text Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="6748" citStr="Lin and Hovy, 2000" startWordPosition="1007" endWordPosition="1010">e. 2.2 Document Summarization Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based a</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>C. Y. Lin, E. Hovy. 2000. The Automated Acquisition of Topic Signatures for Text Summarization. In Proceedings of the 17th Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C -Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>From Single to Multi-document Summarization: A Prototype System and its Evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-02.</booktitle>
<contexts>
<context position="7242" citStr="Lin and Hovy, 2002" startWordPosition="1084" endWordPosition="1087">e values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate </context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>C..-Y. Lin and E.. H. Hovy. 2002. From Single to Multi-document Summarization: A Prototype System and its Evaluation. In Proceedings of ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL -03.</booktitle>
<contexts>
<context position="25412" citStr="Lin and Hovy, 2003" startWordPosition="4118" endWordPosition="4121">ession model. And we used the automatically predicted scores in the summarization process. The real dataset contained these 25 document sets. We performed two evaluation procedures: one based on the ideal dataset to validate the feasibility of the proposed approach, and the other based on the real dataset to demonstrate the effectiveness of the proposed approach in real applications. To date, various methods and metrics have been developed for English summary evaluation by comparing system summary with reference summary, such as the pyramid method (Nenkova et al., 2007) and the ROUGE metrics (Lin and Hovy, 2003). However, such methods or metrics cannot be directly used for evaluating Chinese summary without reference Chinese summary. ) ( ) ( ) 1 ( ) ( i i i s TransScore s InfoScore s re OverallSco + = 922 \x0cInstead, we developed an evaluation protocol as follows: The evaluation was based on human scoring. Four Chinese college students participated in the evaluation as subjects. We have developed a friendly tool for helping the subjects to evaluate each Chinese summary from the following three aspects: Content: This aspect indicates how much a summary reflects the major content of the document set. </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.-Y. Lin and E.H. Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of HLT-NAACL -03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>L Zhou</author>
<author>E Hovy</author>
</authors>
<title>Multilingual summarization evaluation 2005: automatic evaluation report.</title>
<date>2005</date>
<booktitle>In Proceedings of MSE (ACL-2005 Workshop).</booktitle>
<contexts>
<context position="9171" citStr="Lin et al., 2005" startWordPosition="1372" endWordPosition="1375">g Korean summarizer, and finally mapping Korean summary sentences to Japanese summary sentences. Chalendar et al. (2005) focuses on semantic analysis and sentence generation techniques for cross-language summarization. Orasan 918 \x0cand Chiorean (2008) propose to produce summaries with the MMR method from Romanian news articles and then automatically translate the summaries into English. Cross language query based summarization has been investigated in (Pingali et al., 2007), where the query and the documents are in different languages. Other related work includes multilingual summarization (Lin et al., 2005), which aims to create summaries from multiple sources in multiple languages. Siddharthan and McKeown (2005) use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries. 3 The Proposed Approach Previous methods for cross-language summarization usually consist of two steps: one step for summarization and one step for translation. Different order of the two steps can lead to the following two basic English-to-Chinese summarization methods: Late Translation (LateTrans): Firstly, an English summary is produced</context>
</contexts>
<marker>Lin, Zhou, Hovy, 2005</marker>
<rawString>C.-Y. Lin, L. Zhou, and E. Hovy. 2005. Multilingual summarization evaluation 2005: automatic evaluation report. In Proceedings of MSE (ACL-2005 Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of literature Abstracts.</title>
<date>1969</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="6727" citStr="Luhn 1969" startWordPosition="1005" endWordPosition="1006">get sentence. 2.2 Document Summarization Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Mac</context>
</contexts>
<marker>Luhn, 1969</marker>
<rawString>H. P. Luhn. 1969. The Automatic Creation of literature Abstracts. IBM Journal of Research and Development, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>TextRank: Bringing Order into Texts.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP2004.</booktitle>
<contexts>
<context position="6948" citStr="Mihalcea and Tarau, 2004" startWordPosition="1038" endWordPosition="1041"> methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Lo</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>R. Mihalcea, P. Tarau. 2004. TextRank: Bringing Order into Texts. In Proceedings of EMNLP2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>A language independent algorithm for single and multiple document summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP-05.</booktitle>
<contexts>
<context position="7675" citStr="Mihalcea and Tarau (2005)" startWordPosition="1153" endWordPosition="1156">t summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary translation. Leuski et al. (2003) use machin</context>
</contexts>
<marker>Mihalcea, Tarau, 2005</marker>
<rawString>R. Mihalcea and P. Tarau. 2005. A language independent algorithm for single and multiple document summarization. In Proceedings of IJCNLP-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>A Louis</author>
</authors>
<title>Can you summarize this? Identifying correlates of input difficulty for generic multi-document summarization.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT.</booktitle>
<contexts>
<context position="7558" citStr="Nenkova and Louis, 2008" startWordPosition="1133" endWordPosition="1136">d Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-lan</context>
</contexts>
<marker>Nenkova, Louis, 2008</marker>
<rawString>A. Nenkova and A. Louis. 2008. Can you summarize this? Identifying correlates of input difficulty for generic multi-document summarization. In Proceedings of ACL-08:HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
<author>K McKeown</author>
</authors>
<title>The Pyramid method: incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<booktitle>x0cACM Transactions on Speech and Language Processing (TSLP),</booktitle>
<pages>4--2</pages>
<contexts>
<context position="25369" citStr="Nenkova et al., 2007" startWordPosition="4110" endWordPosition="4113">cally predicted by using the learned SVM regression model. And we used the automatically predicted scores in the summarization process. The real dataset contained these 25 document sets. We performed two evaluation procedures: one based on the ideal dataset to validate the feasibility of the proposed approach, and the other based on the real dataset to demonstrate the effectiveness of the proposed approach in real applications. To date, various methods and metrics have been developed for English summary evaluation by comparing system summary with reference summary, such as the pyramid method (Nenkova et al., 2007) and the ROUGE metrics (Lin and Hovy, 2003). However, such methods or metrics cannot be directly used for evaluating Chinese summary without reference Chinese summary. ) ( ) ( ) 1 ( ) ( i i i s TransScore s InfoScore s re OverallSco + = 922 \x0cInstead, we developed an evaluation protocol as follows: The evaluation was based on human scoring. Four Chinese college students participated in the evaluation as subjects. We have developed a friendly tool for helping the subjects to evaluate each Chinese summary from the following three aspects: Content: This aspect indicates how much a summary refle</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid method: incorporating human content selection variation in summarization evaluation. \x0cACM Transactions on Speech and Language Processing (TSLP), 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Orasan</author>
<author>O A Chiorean</author>
</authors>
<title>Evaluation of a Crosslingual Romanian-English Multi-document Summariser.</title>
<date>2008</date>
<booktitle>In Proceedings of 6th Language Resources and Evaluation Conference (LREC2008).</booktitle>
<marker>Orasan, Chiorean, 2008</marker>
<rawString>C. Orasan, and O. A. Chiorean. 2008. Evaluation of a Crosslingual Romanian-English Multi-document Summariser. In Proceedings of 6th Language Resources and Evaluation Conference (LREC2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pingali</author>
<author>J Jagarlamudi</author>
<author>V Varma</author>
</authors>
<title>Experiments in cross language query focused multidocument summarization.</title>
<date>2007</date>
<booktitle>In Workshop on Cross Lingual Information Access Addressing the Information Need of Multilingual Societies in IJCAI2007.</booktitle>
<contexts>
<context position="9034" citStr="Pingali et al., 2007" startWordPosition="1351" endWordPosition="1354">a Japanese summarization system, by first translating Japanese documents into Korean documents, and then extracting summary sentences by using Korean summarizer, and finally mapping Korean summary sentences to Japanese summary sentences. Chalendar et al. (2005) focuses on semantic analysis and sentence generation techniques for cross-language summarization. Orasan 918 \x0cand Chiorean (2008) propose to produce summaries with the MMR method from Romanian news articles and then automatically translate the summaries into English. Cross language query based summarization has been investigated in (Pingali et al., 2007), where the query and the documents are in different languages. Other related work includes multilingual summarization (Lin et al., 2005), which aims to create summaries from multiple sources in multiple languages. Siddharthan and McKeown (2005) use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries. 3 The Proposed Approach Previous methods for cross-language summarization usually consist of two steps: one step for summarization and one step for translation. Different order of the two steps can lead t</context>
</contexts>
<marker>Pingali, Jagarlamudi, Varma, 2007</marker>
<rawString>P. Pingali, J. Jagarlamudi and V. Varma. 2007. Experiments in cross language query focused multidocument summarization. In Workshop on Cross Lingual Information Access Addressing the Information Need of Multilingual Societies in IJCAI2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC2004.</booktitle>
<contexts>
<context position="5057" citStr="Quirk (2004)" startWordPosition="755" endWordPosition="756"> system-translated text and the human reference text. For example, the BLEU metric, the NIST metric and their relatives are all based on the idea that the more shared substrings the system-translated text has with the human reference translation, the better the translation is. Blatz et al. (2003) investigate training sentence-level confidence measures using a variety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and reference-based features to measure the quality of sentences. Transition evaluation without using reference translations has also been investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabi</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>C. Quirk. 2004. Training a sentence-level machine translation confidence measure. In Proceedings of LREC2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Y Jing</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing and Management,</journal>
<volume>40</volume>
<pages>919--938</pages>
<contexts>
<context position="7113" citStr="Radev et al., 2004" startWordPosition="1062" endWordPosition="1065">t summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to comp</context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>D. R. Radev, H. Y. Jing, M. Stys and D. Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40: 919-938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
<author>K McKeown</author>
</authors>
<title>Improving multilingual summarization: using redundancy in the input to correct MT errors.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP-2005.</booktitle>
<contexts>
<context position="9279" citStr="Siddharthan and McKeown (2005)" startWordPosition="1388" endWordPosition="1391">. Chalendar et al. (2005) focuses on semantic analysis and sentence generation techniques for cross-language summarization. Orasan 918 \x0cand Chiorean (2008) propose to produce summaries with the MMR method from Romanian news articles and then automatically translate the summaries into English. Cross language query based summarization has been investigated in (Pingali et al., 2007), where the query and the documents are in different languages. Other related work includes multilingual summarization (Lin et al., 2005), which aims to create summaries from multiple sources in multiple languages. Siddharthan and McKeown (2005) use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries. 3 The Proposed Approach Previous methods for cross-language summarization usually consist of two steps: one step for summarization and one step for translation. Different order of the two steps can lead to the following two basic English-to-Chinese summarization methods: Late Translation (LateTrans): Firstly, an English summary is produced for the English document set by using existing summarization methods. Then, the English summary is automati</context>
</contexts>
<marker>Siddharthan, McKeown, 2005</marker>
<rawString>A. Siddharthan and K. McKeown. 2005. Improving multilingual summarization: using redundancy in the input to correct MT errors. In Proceedings of HLT/EMNLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>Z Wang</author>
<author>M Turchi</author>
<author>J Shawe-Taylor</author>
<author>C Saunders</author>
</authors>
<title>Improving the Confidence of Machine Translation Quality Estimates.</title>
<date>2009</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="5719" citStr="Specia et al. (2009)" startWordPosition="853" endWordPosition="856"> a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabilities from a machine-learned classifier. Specia et al. (2009) use the ICM theory to identify the threshold to map a continuous predicted score into good or bad categories. Chae and Nenkova (2009) use surface syntactic features to assess the fluency of machine translation results. In this study, we further predict the translation quality of an English sentence before the machine translation process, i.e., we do not leverage reference translation and the target sentence. 2.2 Document Summarization Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-b</context>
</contexts>
<marker>Specia, Wang, Turchi, Shawe-Taylor, Saunders, 2009</marker>
<rawString>L. Specia, Z. Wang, M. Turchi, J. Shawe-Taylor, C. Saunders. 2009. Improving the Confidence of Machine Translation Quality Estimates. In MT Summit 2009 (Machine Translation Summit XII).</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="14412" citStr="Vapnik 1995" startWordPosition="2185" endWordPosition="2186">sification methods for MT quality prediction without reference translations. In our approach, the MT quality of each sentence in the documents is also predicted without reference translations. The difference between our task and previous work is that previous work can make use of both features in source sentence and features in target sentence, while our task only leverages features in source sentence, because in the late translation strategy, the English sentences in the documents have not been translated yet at this step. In this study, we adopt the -support vector regression (-SVR) method (Vapnik 1995) for the sentence-level MT quality prediction task. The SVR algorithm is firmly grounded in the framework of statistical learning theory (VC theory). The goal of a regression algorithm is to fit a flat function to the given training data points. Formally, given a set of training data points D={(xi,yi) |i=1,2,...,n}Rd R, where xi is input feature vector and yi is associated score, the goal is to fit a function f which approximates the relation inherited between the data set points. The standard form is: = = + + n i i n i i T b w C C w w 1 * 1 , , , 2 1 min* Subject to i i i T y b x f w + + ) ( </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>H Li</author>
<author>J Xiao</author>
</authors>
<title>EUSUM: extracting easy-to-understand English summaries for nonnative readers.</title>
<date>2010</date>
<booktitle>In Proceedings of SIGIR2010.</booktitle>
<contexts>
<context position="7997" citStr="Wan et al. (2010)" startWordPosition="1201" endWordPosition="1204">d for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary translation. Leuski et al. (2003) use machine translation for English headline generation for Hindi documents. Lim et al. (2004) propose to generate a Japanese summary without using a Japanese summarization system, by first translating Japanese documents into Korean documents, and then extracting summary sentences by using Korean summarizer, and finally mapping Ko</context>
</contexts>
<marker>Wan, Li, Xiao, 2010</marker>
<rawString>X. Wan, H. Li and J. Xiao. 2010. EUSUM: extracting easy-to-understand English summaries for nonnative readers. In Proceedings of SIGIR2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Using crossdocument random walks for topic-focused multidocumetn summarization.</title>
<date>2006</date>
<booktitle>In Proceedings of WI2006.</booktitle>
<contexts>
<context position="7978" citStr="Wan et al., 2006" startWordPosition="1197" endWordPosition="1200">e also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary translation. Leuski et al. (2003) use machine translation for English headline generation for Hindi documents. Lim et al. (2004) propose to generate a Japanese summary without using a Japanese summarization system, by first translating Japanese documents into Korean documents, and then extracting summary sentences by using Korean summarizer, and</context>
<context position="23326" citStr="Wan et al., 2006" startWordPosition="3796" endWordPosition="3799">t TransScore(si)[0,1] and InfoScore(si)[0,1] denote the MT quality score and the informativeness score of sentence si, the overall score of the sentence is: where [0,1] is a parameter controlling the influences of the two factors. If is set to 0, the summary is extracted without considering the MT quality factor. In the experiments, we empirically set the parameter to 0.3 in order to balance the two factors of content informativeness and translation quality. For multi-document summarization, some sentences are highly overlapping with each other, and thus we apply the same greedy algorithm in (Wan et al., 2006) to penalize the sentences highly overlapping with other highly scored sentences, and finally the informative, novel, and easy-to-translate sentences are chosen into the English summary. Finally, the sentences in the English summary are translated into the corresponding Chinese sentences by using Google Translate, and the Chinese summary is formed. 5.2 Evaluation 5.2.1 Evaluation Setup In this experiment, we used the document sets provided by DUC2001 for evaluation. As mentioned in Section 4.2.1, DUC2001 provided 30 English document sets for generic multidocument summarization. The average doc</context>
</contexts>
<marker>Wan, Yang, Xiao, 2006</marker>
<rawString>X. Wan, J. Yang and J. Xiao. 2006. Using crossdocument random walks for topic-focused multidocumetn summarization. In Proceedings of WI2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
</authors>
<title>Multi-document summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGIR-08.</booktitle>
<contexts>
<context position="7872" citStr="Wan and Yang, 2008" startWordPosition="1183" endWordPosition="1186">of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary translation. Leuski et al. (2003) use machine translation for English headline generation for Hindi documents. Lim et al. (2004) propose to generate a Japanese summary without using a Japanese summarization system, by first translating Japan</context>
</contexts>
<marker>Wan, Yang, 2008</marker>
<rawString>X. Wan and J. Yang. 2008. Multi-document summarization using cluster-based link analysis. In Proceedings of SIGIR-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL2007.</booktitle>
<contexts>
<context position="7031" citStr="Wan et al., 2007" startWordPosition="1051" endWordPosition="1054">re and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>X. Wan, J. Yang and J. Xiao. 2007. Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction. In Proceedings of ACL2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-F Wong</author>
<author>M Wu</author>
<author>W Li</author>
</authors>
<title>Extractive summarization using supervised and semi-supervised learning.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING-08.</booktitle>
<contexts>
<context position="7441" citStr="Wong et al., 2008" startWordPosition="1116" endWordPosition="1119">ods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting eas</context>
</contexts>
<marker>Wong, Wu, Li, 2008</marker>
<rawString>K.-F. Wong, M. Wu and W. Li. 2008. Extractive summarization using supervised and semi-supervised learning. In Proceedings of COLING-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Y Zha</author>
</authors>
<title>Generic Summarization and Keyphrase Extraction Using Mutual Reinforcement Principle and Sentence Clustering.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR2002.</booktitle>
<contexts>
<context position="7012" citStr="Zha 2002" startWordPosition="1049" endWordPosition="1050">liency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sent</context>
</contexts>
<marker>Zha, 2002</marker>
<rawString>H. Y. Zha. 2002. Generic Summarization and Keyphrase Extraction Using Mutual Reinforcement Principle and Sentence Clustering. In Proceedings of SIGIR2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>