<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000051">
<bodyText confidence="0.8486725">
b&amp;apos;Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 354358,
Jeju, Republic of Korea, 8-14 July 2012. c
</bodyText>
<sectionHeader confidence="0.55957" genericHeader="abstract">
2012 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.886526">
Fully Abstractive Approach to Guided Summarization
</title>
<author confidence="0.918531">
Pierre-Etienne Genest, Guy Lapalme
</author>
<affiliation confidence="0.422681">
RALI-DIRO
Universite de Montreal
</affiliation>
<address confidence="0.688925333333333">
P.O. Box 6128, Succ. Centre-Ville
Montreal, Quebec
Canada, H3C 3J7
</address>
<email confidence="0.994302">
{genestpe,lapalme}@iro.umontreal.ca
</email>
<sectionHeader confidence="0.990623" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999732142857143">
This paper shows that full abstraction can be
accomplished in the context of guided sum-
marization. We describe a work in progress
that relies on Information Extraction, statis-
tical content selection and Natural Language
Generation. Early results already demonstrate
the effectiveness of the approach.
</bodyText>
<sectionHeader confidence="0.998327" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997246272727273">
In the last decade, automatic text summarization has
been dominated by extractive approaches that rely
purely on shallow statistics. In the latest evalu-
ation campaign of the Text Analysis Conference1
(TAC), the top systems were considered only barely
acceptable by human assessment (Owczarzak and
Dang, 2011). The field is also getting saturated near
what appears to be a ceiling in performance. Sys-
tems that claim to be very different from one an-
other have all become statistically indistinguishable
in evaluation results. An experiment (Genest et al.,
2009) found a performance ceiling to pure sentence
extraction that is very low compared to regular (ab-
stractive) human summaries, but not that much bet-
ter than the current best automatic systems.
Abstractive summarization has been explored to
some extent in recent years: sentence compression
(Knight and Marcu, 2000) (Cohn and Lapata, 2009),
sentence fusion (Barzilay and McKeown, 2005) or
revision (Tanaka et al., 2009), and a generation-
based approach that could be called sentence split-
ting (Genest and Lapalme, 2011). They are all
</bodyText>
<page confidence="0.797921">
1
</page>
<note confidence="0.45151">
www.nist.gov/tac
</note>
<bodyText confidence="0.999050166666667">
rewriting techniques based on syntactical analysis,
offering little improvement over extractive methods
in the content selection process.
We believe that a fully abstractive approach with a
separate process for the analysis of the text, the con-
tent selection, and the generation of the summary
has the most potential for generating summaries at a
level comparable to human. For the foreseeable fu-
ture, we think that such a process for full abstraction
is impossible in the general case, since it is almost
equivalent to perfect text understanding. In specific
domains, however, an approximation of full abstrac-
tion is possible.
This paper shows that full abstraction can be ac-
complished in the context of guided summarization.
We propose a methodology that relies on Informa-
tion Extraction and Natural Language Generation,
and discuss our early results.
</bodyText>
<sectionHeader confidence="0.993865" genericHeader="method">
2 Guided Summarization
</sectionHeader>
<bodyText confidence="0.998174384615385">
The stated goal of the guided summarization task
at TAC is to motivate a move towards abstractive
approaches. It is an oriented multidocument sum-
marization task in which a category is attributed
to a cluster of 10 source documents to be summa-
rized in 100 words or less. There are five cate-
gories: Accidents and Natural Disasters, Attacks,
Health and Safety, Endangered Resources, and In-
vestigations/Trials. Each category is associated with
a list of aspects to address in the summary. Figure 1
shows the aspects for the Attacks category. We use
this specification of categories and aspects to accom-
plish domain-specific summarization.
</bodyText>
<page confidence="0.999632">
354
</page>
<listItem confidence="0.944122625">
\x0c2.1 WHAT: what happened
2.2 WHEN: date, time, other temporal placement markers
2.3 WHERE: physical location
2.4 PERPETRATORS: individuals or groups responsible for the attack
2.5 WHY: reasons for the attack
2.6 WHO AFFECTED: casualties (death, injury), or individuals otherwise negatively affected
2.7 DAMAGES: damages caused by the attack
2.8 COUNTERMEASURES: countermeasures, rescue efforts, prevention efforts, other reactions
</listItem>
<figureCaption confidence="0.986518">
Figure 1: Aspects for TACs guided summarization task, category 2: Attacks
</figureCaption>
<sectionHeader confidence="0.958684" genericHeader="method">
3 Fully Abstractive Approach
</sectionHeader>
<bodyText confidence="0.998554">
Guided summarization categories and aspects define
an information need, and using Information Extrac-
tion (IE) seems appropriate to address it. The idea
to use an IE system for summarization can be traced
back to the FRUMP system (DeJong, 1982), which
generates brief summaries about various kinds of
stories; (White et al., 2001) also wrote abstractive
summaries using the output of an IE system applied
to events such as natural disasters. In both cases, the
end result is a generated summary from the informa-
tion available. A lot of other work has instead used
IE to improve the performance of extraction-based
systems, like (Barzilay and Lee, 2004) and (Ji et al.,
2010).
What is common to all these approaches is that
the IE system is designed for a specific purpose, sep-
arate from summarization. However, to properly ad-
dress each aspect requires a system designed specifi-
cally for that task. To our knowledge, tailoring IE to
the needs of abstractive summarization has not been
done before. Our methodology uses a rule-based,
custom-designed IE module, integrated with Con-
tent Selection and Generation in order to write short,
well-written abstractive summaries.
Before tackling these, we perform some prepro-
cessing on the cluster of documents. It includes:
cleaning up and normalization of the input using reg-
ular expressions, sentence segmentation, tokeniza-
tion and lemmatization using GATE (Cunningham
et al., 2002), syntactical parsing and dependency
parsing (collapsed) using the Stanford Parser (de
Marneffe et al., 2006), and Named Entity Recogni-
tion using Stanford NER (Finkel et al., 2005). We
have also developed a date resolution engine that fo-
cuses on days of the week and relative terms.
</bodyText>
<subsectionHeader confidence="0.998977">
3.1 Information Extraction
</subsectionHeader>
<bodyText confidence="0.998752833333333">
Our architecture is based on Abstraction Schemes.
An abstraction scheme consists of IE rules, con-
tent selection heuristics and one or more genera-
tion patterns, all created by hand. Each abstrac-
tion scheme is designed to address a theme or sub-
category. Thus, rules that extract information for
the same aspect within the same scheme will share a
similar meaning. An abstraction scheme aims to an-
swer one or more aspects of its category, and more
than one scheme can be linked to the same aspect.
Figure 2 shows two of the schemes that we have
created. For the scheme killing, the IE rules would
match X as the perpetrator and Y as a victim for
all of the following phrases: X killed Y, Y was
assassinated by X, and the murder of X
by Y. Other schemes have similar structure and pur-
pose, such as wounding, abducting, damaging
and destroying. To create extraction rules for a
scheme, we must find several verbs and nouns shar-
ing a similar meaning and identify the syntactical
position of the roles we are interested in. Three re-
sources have helped us in designing extraction rules:
a thesaurus to find semantically related nouns and
verbs; VerbNet (Kipper et al., 2006), which provides
amongst other things the semantic roles of the syn-
tactical dependents of verbs; and a hand-crafted list
of aspect-relevant word stems provided by the team
that made CLASSY (Conroy et al., 2010).
Schemes and their extraction rules can also be
quite different from this first example, as shown with
the scheme event. This scheme gathers the basic in-
formation about the attack event: WHAT category of
attack, WHEN and WHERE it occurred. A list of key
words is used to identify words that imply an attack
event, while a list of EVENT NOUNs is used to iden-
tify specifically words that refer to a type of attack.
</bodyText>
<page confidence="0.981152">
355
</page>
<table confidence="0.5311345">
\x0cScheme: killing
Information Extraction
</table>
<equation confidence="0.916787166666667">
SUBJ(kill, X) WHO(X)
OBJ(kill, Y) WHO AFFECTED(Y)
SUBJ(assassinate, X) WHO(X)
OBJ(assassinate, Y) WHO AFFECTED(Y)
.
.
.
PREP OF(murder, Y) WHO AFFECTED(Y)
PREP BY(murder, X) WHO(X)
.
.
.
</equation>
<table confidence="0.99413175">
Content Selection Select best candidates for kill verb, WHO(X) and WHO AFFECTED(Y)
Generation X kill verb Y
Scheme: event
Information Extraction
PREP IN(key word, X), LOCATION(X) WHERE(X)
PREP IN(key word, X), ORGANIZATION(X) WHERE(X)
PREP AT(key word, X), LOCATION(X) WHERE(X)
PREP AT(key word, X), ORGANIZATION(X) WHERE(X)
DEP(key word, Y), DATE(Y) WHEN(Y)
EVENT NOUN(Z) WHAT(Z)
Content Selection Select best candidates for at or in, WHERE(X), WHEN(Y) and WHAT(Z)
Generation On Y, Z occurred at/in X
</table>
<figureCaption confidence="0.954082">
Figure 2: Abstraction schemes killing and event. The information extraction rules translate preprocessing annota-
</figureCaption>
<bodyText confidence="0.98380075">
tions into candidate answers for a specific aspect. Content selection determines which candidate will be included in the
generated sentence for each aspect. Finally, a pattern is used to determine the structure of the generated sentence. No-
tation: word or lemma, variable, group of words, PREDICATE OR ASPECT. Note that the predicate DEP matches
any syntactical dependency and that key words refer to a premade list of category-relevant verbs and nouns.
</bodyText>
<subsectionHeader confidence="0.999852">
3.2 Content Selection
</subsectionHeader>
<bodyText confidence="0.999043789473684">
A large number of candidates are found by the IE
rules for each aspect. The content selection module
selects the best ones and sends them to the genera-
tion module. The basic heuristic is to select the can-
didate most often mentioned for an aspect, and simi-
larly for the choice of a preposition or a verb for gen-
eration. More than one candidate may be selected
for the aspect WHO AFFECTED, the victims of
the attack. Several heuristics are used to avoid re-
dundancies and uninformative answers.
News articles may contain references to more
than one event of a given category, but our sum-
maries describe only one. To avoid mixing candi-
dates from two different event instances that might
appear in the same cluster of documents, we rely on
dates. The ancestors of a date in the dependency
tree are associated with that date, and excluded from
the summary if the main event occurs on a different
date.
</bodyText>
<subsectionHeader confidence="0.997823">
3.3 Generation
</subsectionHeader>
<bodyText confidence="0.99968">
The text of a summary must be fluid and feel natu-
ral, while being straightforward and concise. From
our observation of human-written summaries, it also
does not require a great deal of originality to be
considered excellent by human standards. Thus,
we have designed straightforward generation pat-
terns for each scheme. They are implemented us-
ing the SimpleNLG realizer (Gatt and Reiter, 2009),
which takes a sentence structure and words in their
root form as input and gives a sentence with re-
solved agreements and sentence markers as output.
The greatest difficulty in the structure is in realizing
noun phrases. The content selection module selects
a lemma that should serve as noun phrase head, and
its number, modifiers and specifier must be deter-
mined during generation. Frequencies and heuristics
are again used to identify appropriate modifiers, this
time from all those used with that head within the
source documents. We apply the constraint that the
</bodyText>
<page confidence="0.99612">
356
</page>
<bodyText confidence="0.9707304">
\x0cOn April 20, 1999, a massacre occurred at Columbine High School.
Two student gunmen killed 12 students, a teacher and themselves.
On November 2, 2004, a brutal murder occurred in Amsterdam.
A gunman stabbed and shot Dutch filmmaker Theo van Gogh.
A policeman and the suspect were wounded.
</bodyText>
<table confidence="0.2159045">
On February 14, 2005, a suicide car bombing occurred in Beirut.
Former Lebanese Prime Minister Rafik Hariri and 14 others were killed.
</table>
<figureCaption confidence="0.952915">
Figure 3: Brief fully abstractive summaries on clusters D1001A-A, D1039G-A and D1043H-A, respectively on the
</figureCaption>
<bodyText confidence="0.985717090909091">
Columbine massacre, the murder of Theo van Gogh and the assassination of Rafik Hariri.
combination of number and modifiers chosen must
appear at least once as an IE rule match.
As for any generated text, a good summary also
requires a text plan (Hovy, 1988) (McKeown, 1985).
Ours consists of an ordering of the schemes. For ex-
ample, an Attack summary begins with the scheme
event. This ordering also determines which scheme
to favor in the case of redundancy, e.g. given that a
building was both damaged and destroyed, only the
fact that is was destroyed will be mentioned.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999782882352941">
We have implemented this fully abstractive summa-
rization methodology. The abstraction schemes and
text plan for the Attack category are written in an
XML document, designed to easily allow the addi-
tion of more schemes and the design of new cate-
gories. The language processing of the source docu-
ments and the domain-specific knowledge are com-
pletely separate in the program.
Our system, which is meant as a proof of concept,
can generate useful summaries for the Attack cate-
gory, as can be seen in Figure 3. The key elements
of information are present in each case, stated in a
way that is easy to understand.
These short summaries have a high density of in-
formation, in terms of how much content from the
source documents they cover for a given number of
words. For example, using the most widely used
content metric, Pyramid (Nenkova et al., 2007), the
two sentences generated for the cluster D1001A-
A contain 8 Semantic Content Units (SCU) for a
weighted total of 30 out of a maximum of 56, for
a raw Pyramid score of 0.54. Only 3 of the 43 auto-
matic summaries beat this score on this cluster that
year (the average was 0.31). Note that the sum-
maries that we compare against contain up to 100
words, whereas ours is only 21 words long. We con-
clude that our method has the potential for creating
summaries with much greater information density
than the current state of the art.
In fact, our approach does not only have the po-
tential to increase a summarys coverage, but also its
linguistic quality and the reader satisfaction as well,
since the most relevant information now appears at
the beginning of the summary.
</bodyText>
<sectionHeader confidence="0.98244" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.997622625">
We have developed and implemented a fully abstrac-
tive summarization methodology in the context of
guided summarization. The higher density of infor-
mation in our short summaries is one key to address
the performance ceiling of extractive summarization
methods. Although fully abstractive summarization
is a daunting challenge, our work shows the feasibil-
ity and usefulness of this new direction for summa-
rization research.
We are now expanding the variety and complexity
of the abstraction schemes and generation patterns
to deal with more aspects and other categories. We
should then be able to compare on a greater scale
the output of our system with the ones produced by
other automatic systems and by humans on all the
clusters used at TAC 2010 and 2011.
</bodyText>
<sectionHeader confidence="0.994899" genericHeader="references">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.997459857142857">
The authors want to thank Dr. Eduard Hovy, of ISI,
and Prof. Kathy McKeown, of Columbia Univer-
sity, for fruitful discussions on abstractive summa-
rization, and Dr. Judith Schlesinger and Dr. John
Conroy, both of the IDA / Center for Computing Sci-
ences, for providing us with their hand-crafted list of
category- and aspect-relevant keywords.
</bodyText>
<page confidence="0.994109">
357
</page>
<reference confidence="0.990129747572815">
\x0cReferences
R. Barzilay and L. Lee. 2004. Catching the Drift: Prob-
abilistic Content Models, with Applications to Gen-
eration and Summarization. eprint arXiv:cs/0405039,
May.
Regina Barzilay and Kathleen R. McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297328.
Trevor Cohn and Mirella Lapata. 2009. Sentence
compression as tree transduction. J. Artif. Int. Res.,
34(1):637674.
John M. Conroy, Judith D. Schlesinger, Peter A. Rankel,
and Dianne P. OLeary. 2010. CLASSY 2010: Sum-
marization and metrics. In Proceedings of the Third
Text Analysis Conference, Gaithersburg, Maryland,
USA. National Institute of Standards and Technology.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics, Philadelphia, PA, USA.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the IEEE / ACL 2006 Workshop on
Spoken Language Technology. The Stanford Natural
Language Processing Group.
Gerald DeJong, 1982. An Overview of the FRUMP Sys-
tem, pages 149176. Lawrence Erlbaum.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL 05,
pages 363370, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a Re-
alisation Engine for Practical Applications. In ENLG
09: Proceedings of the 12th European Workshop on
Natural Language Generation, pages 9093, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Pierre-Etienne Genest and Guy Lapalme. 2011. Frame-
work for Abstractive Summarization using Text-to-
Text Generation. In Proceedings of the Workshop on
Monolingual Text-To-Text Generation, pages 6473,
Portland, Oregon, USA, June. Association for Com-
putational Linguistics.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi Yousfi-
Monod. 2009. HexTac: the Creation of a Manual Ex-
tractive Run. In Proceedings of the Second Text Anal-
ysis Conference, Gaithersburg, Maryland, USA. Na-
tional Institute of Standards and Technology.
Eduard H. Hovy. 1988. Planning coherent multisenten-
tial text. In Proceedings of the 26th annual meeting
on Association for Computational Linguistics, pages
163169, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Heng Ji, Juan Liu, Benoit Favre, Dan Gillick, and Dilek
Hakkani-Tur. 2010. Re-ranking summaries based
on cross-document information extraction. In Pu-Jen
Cheng, Min-Yen Kan, Wai Lam, and Preslav Nakov,
editors, Information Retrieval Technology, volume
6458 of Lecture Notes in Computer Science, pages
432442. Springer Berlin / Heidelberg. 10.1007/978-
3-642-17187-1 42.
Karen Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2006. Extending VerbNet with Novel
Verb Classes. In LREC 2006.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National Con-
ference on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 703710. AAAI Press.
Kathleen R. McKeown. 1985. Discourse strategies for
generating natural-language text. Artif. Intell., 27:1
41, September.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Trans. Speech Lang. Process., 4, May.
Karolina Owczarzak and Hoa Trang Dang. 2011.
Overview of the TAC 2011 summarization track:
Guided task and aesop task. In Proceedings of the
Fourth Text Analysis Conference, Gaithersburg, Mary-
land, USA. National Institute of Standards and Tech-
nology. http://www.nist.gov/tac/publications/.
Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa,
Tadashi Kumano, and Naoto Kato. 2009. Syntax-
driven sentence revision for broadcast news summa-
rization. In Proceedings of the 2009 Workshop on Lan-
guage Generation and Summarisation, UCNLG+Sum
09, pages 3947, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Multi-
document summarization via information extraction.
In Proceedings of the first international conference on
Human language technology research, HLT 01, pages
17, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.981116">
358
</page>
<figure confidence="0.249644">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.452963">
<note confidence="0.982151">b&amp;apos;Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 354358, Jeju, Republic of Korea, 8-14 July 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.941599">Fully Abstractive Approach to Guided Summarization</title>
<author confidence="0.992616">Pierre-Etienne Genest</author>
<author confidence="0.992616">Guy Lapalme</author>
<affiliation confidence="0.850686333333333">RALI-DIRO Universite de Montreal P.O. Box 6128, Succ. Centre-Ville</affiliation>
<address confidence="0.99344">Montreal, Quebec Canada, H3C 3J7</address>
<email confidence="0.99529">genestpe@iro.umontreal.ca</email>
<email confidence="0.99529">lapalme@iro.umontreal.ca</email>
<abstract confidence="0.982151625">This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences R Barzilay</author>
<author>L Lee</author>
</authors>
<title>Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization. eprint arXiv:cs/0405039,</title>
<date>2004</date>
<contexts>
<context position="4554" citStr="Barzilay and Lee, 2004" startWordPosition="685" endWordPosition="688">ies and aspects define an information need, and using Information Extraction (IE) seems appropriate to address it. The idea to use an IE system for summarization can be traced back to the FRUMP system (DeJong, 1982), which generates brief summaries about various kinds of stories; (White et al., 2001) also wrote abstractive summaries using the output of an IE system applied to events such as natural disasters. In both cases, the end result is a generated summary from the information available. A lot of other work has instead used IE to improve the performance of extraction-based systems, like (Barzilay and Lee, 2004) and (Ji et al., 2010). What is common to all these approaches is that the IE system is designed for a specific purpose, separate from summarization. However, to properly address each aspect requires a system designed specifically for that task. To our knowledge, tailoring IE to the needs of abstractive summarization has not been done before. Our methodology uses a rule-based, custom-designed IE module, integrated with Content Selection and Generation in order to write short, well-written abstractive summaries. Before tackling these, we perform some preprocessing on the cluster of documents. I</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>\x0cReferences R. Barzilay and L. Lee. 2004. Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization. eprint arXiv:cs/0405039, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1683" citStr="Barzilay and McKeown, 2005" startWordPosition="240" endWordPosition="243">field is also getting saturated near what appears to be a ceiling in performance. Systems that claim to be very different from one another have all become statistically indistinguishable in evaluation results. An experiment (Genest et al., 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al., 2009), and a generationbased approach that could be called sentence splitting (Genest and Lapalme, 2011). They are all 1 www.nist.gov/tac rewriting techniques based on syntactical analysis, offering little improvement over extractive methods in the content selection process. We believe that a fully abstractive approach with a separate process for the analysis of the text, the content selection, and the generation of the summary has the most potential for generating summaries at a level comparable to human. For the foreseeable future, we think that such a process fo</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="1637" citStr="Cohn and Lapata, 2009" startWordPosition="234" endWordPosition="237">sessment (Owczarzak and Dang, 2011). The field is also getting saturated near what appears to be a ceiling in performance. Systems that claim to be very different from one another have all become statistically indistinguishable in evaluation results. An experiment (Genest et al., 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al., 2009), and a generationbased approach that could be called sentence splitting (Genest and Lapalme, 2011). They are all 1 www.nist.gov/tac rewriting techniques based on syntactical analysis, offering little improvement over extractive methods in the content selection process. We believe that a fully abstractive approach with a separate process for the analysis of the text, the content selection, and the generation of the summary has the most potential for generating summaries at a level comparable to human. For the fores</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2009. Sentence compression as tree transduction. J. Artif. Int. Res., 34(1):637674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Peter A Rankel</author>
<author>Dianne P OLeary</author>
</authors>
<title>CLASSY 2010: Summarization and metrics.</title>
<date>2010</date>
<booktitle>In Proceedings of the Third Text Analysis Conference,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, Maryland, USA.</location>
<contexts>
<context position="7012" citStr="Conroy et al., 2010" startWordPosition="1094" endWordPosition="1097">imilar structure and purpose, such as wounding, abducting, damaging and destroying. To create extraction rules for a scheme, we must find several verbs and nouns sharing a similar meaning and identify the syntactical position of the roles we are interested in. Three resources have helped us in designing extraction rules: a thesaurus to find semantically related nouns and verbs; VerbNet (Kipper et al., 2006), which provides amongst other things the semantic roles of the syntactical dependents of verbs; and a hand-crafted list of aspect-relevant word stems provided by the team that made CLASSY (Conroy et al., 2010). Schemes and their extraction rules can also be quite different from this first example, as shown with the scheme event. This scheme gathers the basic information about the attack event: WHAT category of attack, WHEN and WHERE it occurred. A list of key words is used to identify words that imply an attack event, while a list of EVENT NOUNs is used to identify specifically words that refer to a type of attack. 355 \x0cScheme: killing Information Extraction SUBJ(kill, X) WHO(X) OBJ(kill, Y) WHO AFFECTED(Y) SUBJ(assassinate, X) WHO(X) OBJ(assassinate, Y) WHO AFFECTED(Y) . . . PREP OF(murder, Y) </context>
</contexts>
<marker>Conroy, Schlesinger, Rankel, OLeary, 2010</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, Peter A. Rankel, and Dianne P. OLeary. 2010. CLASSY 2010: Summarization and metrics. In Proceedings of the Third Text Analysis Conference, Gaithersburg, Maryland, USA. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hamish Cunningham</author>
<author>Diana Maynard</author>
<author>Kalina Bontcheva</author>
<author>Valentin Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5326" citStr="Cunningham et al., 2002" startWordPosition="805" endWordPosition="808">. However, to properly address each aspect requires a system designed specifically for that task. To our knowledge, tailoring IE to the needs of abstractive summarization has not been done before. Our methodology uses a rule-based, custom-designed IE module, integrated with Content Selection and Generation in order to write short, well-written abstractive summaries. Before tackling these, we perform some preprocessing on the cluster of documents. It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE (Cunningham et al., 2002), syntactical parsing and dependency parsing (collapsed) using the Stanford Parser (de Marneffe et al., 2006), and Named Entity Recognition using Stanford NER (Finkel et al., 2005). We have also developed a date resolution engine that focuses on days of the week and relative terms. 3.1 Information Extraction Our architecture is based on Abstraction Schemes. An abstraction scheme consists of IE rules, content selection heuristics and one or more generation patterns, all created by hand. Each abstraction scheme is designed to address a theme or subcategory. Thus, rules that extract information f</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology. The Stanford Natural Language Processing Group.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proceedings of the IEEE / ACL 2006 Workshop on Spoken Language Technology. The Stanford Natural Language Processing Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald DeJong</author>
</authors>
<title>An Overview of the FRUMP System,</title>
<date>1982</date>
<pages>149176</pages>
<institution>Lawrence Erlbaum.</institution>
<contexts>
<context position="4146" citStr="DeJong, 1982" startWordPosition="620" endWordPosition="621"> the attack 2.5 WHY: reasons for the attack 2.6 WHO AFFECTED: casualties (death, injury), or individuals otherwise negatively affected 2.7 DAMAGES: damages caused by the attack 2.8 COUNTERMEASURES: countermeasures, rescue efforts, prevention efforts, other reactions Figure 1: Aspects for TACs guided summarization task, category 2: Attacks 3 Fully Abstractive Approach Guided summarization categories and aspects define an information need, and using Information Extraction (IE) seems appropriate to address it. The idea to use an IE system for summarization can be traced back to the FRUMP system (DeJong, 1982), which generates brief summaries about various kinds of stories; (White et al., 2001) also wrote abstractive summaries using the output of an IE system applied to events such as natural disasters. In both cases, the end result is a generated summary from the information available. A lot of other work has instead used IE to improve the performance of extraction-based systems, like (Barzilay and Lee, 2004) and (Ji et al., 2010). What is common to all these approaches is that the IE system is designed for a specific purpose, separate from summarization. However, to properly address each aspect r</context>
</contexts>
<marker>DeJong, 1982</marker>
<rawString>Gerald DeJong, 1982. An Overview of the FRUMP System, pages 149176. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL 05,</booktitle>
<pages>363370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5506" citStr="Finkel et al., 2005" startWordPosition="832" endWordPosition="835">done before. Our methodology uses a rule-based, custom-designed IE module, integrated with Content Selection and Generation in order to write short, well-written abstractive summaries. Before tackling these, we perform some preprocessing on the cluster of documents. It includes: cleaning up and normalization of the input using regular expressions, sentence segmentation, tokenization and lemmatization using GATE (Cunningham et al., 2002), syntactical parsing and dependency parsing (collapsed) using the Stanford Parser (de Marneffe et al., 2006), and Named Entity Recognition using Stanford NER (Finkel et al., 2005). We have also developed a date resolution engine that focuses on days of the week and relative terms. 3.1 Information Extraction Our architecture is based on Abstraction Schemes. An abstraction scheme consists of IE rules, content selection heuristics and one or more generation patterns, all created by hand. Each abstraction scheme is designed to address a theme or subcategory. Thus, rules that extract information for the same aspect within the same scheme will share a similar meaning. An abstraction scheme aims to answer one or more aspects of its category, and more than one scheme can be li</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL 05, pages 363370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Ehud Reiter</author>
</authors>
<title>SimpleNLG: a Realisation Engine for Practical Applications. In</title>
<date>2009</date>
<booktitle>ENLG 09: Proceedings of the 12th European Workshop on Natural Language Generation,</booktitle>
<pages>9093</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10054" citStr="Gatt and Reiter, 2009" startWordPosition="1589" endWordPosition="1592"> appear in the same cluster of documents, we rely on dates. The ancestors of a date in the dependency tree are associated with that date, and excluded from the summary if the main event occurs on a different date. 3.3 Generation The text of a summary must be fluid and feel natural, while being straightforward and concise. From our observation of human-written summaries, it also does not require a great deal of originality to be considered excellent by human standards. Thus, we have designed straightforward generation patterns for each scheme. They are implemented using the SimpleNLG realizer (Gatt and Reiter, 2009), which takes a sentence structure and words in their root form as input and gives a sentence with resolved agreements and sentence markers as output. The greatest difficulty in the structure is in realizing noun phrases. The content selection module selects a lemma that should serve as noun phrase head, and its number, modifiers and specifier must be determined during generation. Frequencies and heuristics are again used to identify appropriate modifiers, this time from all those used with that head within the source documents. We apply the constraint that the 356 \x0cOn April 20, 1999, a mas</context>
</contexts>
<marker>Gatt, Reiter, 2009</marker>
<rawString>Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a Realisation Engine for Practical Applications. In ENLG 09: Proceedings of the 12th European Workshop on Natural Language Generation, pages 9093, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Etienne Genest</author>
<author>Guy Lapalme</author>
</authors>
<title>Framework for Abstractive Summarization using Text-toText Generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>6473</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="1816" citStr="Genest and Lapalme, 2011" startWordPosition="262" endWordPosition="265">her have all become statistically indistinguishable in evaluation results. An experiment (Genest et al., 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al., 2009), and a generationbased approach that could be called sentence splitting (Genest and Lapalme, 2011). They are all 1 www.nist.gov/tac rewriting techniques based on syntactical analysis, offering little improvement over extractive methods in the content selection process. We believe that a fully abstractive approach with a separate process for the analysis of the text, the content selection, and the generation of the summary has the most potential for generating summaries at a level comparable to human. For the foreseeable future, we think that such a process for full abstraction is impossible in the general case, since it is almost equivalent to perfect text understanding. In specific domain</context>
</contexts>
<marker>Genest, Lapalme, 2011</marker>
<rawString>Pierre-Etienne Genest and Guy Lapalme. 2011. Framework for Abstractive Summarization using Text-toText Generation. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, pages 6473, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre-Etienne Genest</author>
<author>Guy Lapalme</author>
<author>Mehdi YousfiMonod</author>
</authors>
<title>HexTac: the Creation of a Manual Extractive Run.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second Text Analysis Conference,</booktitle>
<institution>National Institute of Standards and Technology.</institution>
<location>Gaithersburg, Maryland, USA.</location>
<contexts>
<context position="1301" citStr="Genest et al., 2009" startWordPosition="182" endWordPosition="185">emonstrate the effectiveness of the approach. 1 Introduction In the last decade, automatic text summarization has been dominated by extractive approaches that rely purely on shallow statistics. In the latest evaluation campaign of the Text Analysis Conference1 (TAC), the top systems were considered only barely acceptable by human assessment (Owczarzak and Dang, 2011). The field is also getting saturated near what appears to be a ceiling in performance. Systems that claim to be very different from one another have all become statistically indistinguishable in evaluation results. An experiment (Genest et al., 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al., 2009), and a generationbased approach that could be called sentence splitting (Genest and Lapalme, 2011). They are all 1 www.nist.gov/tac rewriting techniques based on syntactical analysis,</context>
</contexts>
<marker>Genest, Lapalme, YousfiMonod, 2009</marker>
<rawString>Pierre-Etienne Genest, Guy Lapalme, and Mehdi YousfiMonod. 2009. HexTac: the Creation of a Manual Extractive Run. In Proceedings of the Second Text Analysis Conference, Gaithersburg, Maryland, USA. National Institute of Standards and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Planning coherent multisentential text.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>163169</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="11420" citStr="Hovy, 1988" startWordPosition="1814" endWordPosition="1815">Amsterdam. A gunman stabbed and shot Dutch filmmaker Theo van Gogh. A policeman and the suspect were wounded. On February 14, 2005, a suicide car bombing occurred in Beirut. Former Lebanese Prime Minister Rafik Hariri and 14 others were killed. Figure 3: Brief fully abstractive summaries on clusters D1001A-A, D1039G-A and D1043H-A, respectively on the Columbine massacre, the murder of Theo van Gogh and the assassination of Rafik Hariri. combination of number and modifiers chosen must appear at least once as an IE rule match. As for any generated text, a good summary also requires a text plan (Hovy, 1988) (McKeown, 1985). Ours consists of an ordering of the schemes. For example, an Attack summary begins with the scheme event. This ordering also determines which scheme to favor in the case of redundancy, e.g. given that a building was both damaged and destroyed, only the fact that is was destroyed will be mentioned. 4 Results and Discussion We have implemented this fully abstractive summarization methodology. The abstraction schemes and text plan for the Attack category are written in an XML document, designed to easily allow the addition of more schemes and the design of new categories. The la</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Eduard H. Hovy. 1988. Planning coherent multisentential text. In Proceedings of the 26th annual meeting on Association for Computational Linguistics, pages 163169, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Juan Liu</author>
<author>Benoit Favre</author>
<author>Dan Gillick</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>Re-ranking summaries based on cross-document information extraction.</title>
<date>2010</date>
<booktitle>In Pu-Jen Cheng, Min-Yen Kan, Wai Lam, and Preslav Nakov, editors, Information Retrieval Technology,</booktitle>
<volume>6458</volume>
<pages>432442</pages>
<publisher>Springer</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="4576" citStr="Ji et al., 2010" startWordPosition="690" endWordPosition="693">ormation need, and using Information Extraction (IE) seems appropriate to address it. The idea to use an IE system for summarization can be traced back to the FRUMP system (DeJong, 1982), which generates brief summaries about various kinds of stories; (White et al., 2001) also wrote abstractive summaries using the output of an IE system applied to events such as natural disasters. In both cases, the end result is a generated summary from the information available. A lot of other work has instead used IE to improve the performance of extraction-based systems, like (Barzilay and Lee, 2004) and (Ji et al., 2010). What is common to all these approaches is that the IE system is designed for a specific purpose, separate from summarization. However, to properly address each aspect requires a system designed specifically for that task. To our knowledge, tailoring IE to the needs of abstractive summarization has not been done before. Our methodology uses a rule-based, custom-designed IE module, integrated with Content Selection and Generation in order to write short, well-written abstractive summaries. Before tackling these, we perform some preprocessing on the cluster of documents. It includes: cleaning u</context>
</contexts>
<marker>Ji, Liu, Favre, Gillick, Hakkani-Tur, 2010</marker>
<rawString>Heng Ji, Juan Liu, Benoit Favre, Dan Gillick, and Dilek Hakkani-Tur. 2010. Re-ranking summaries based on cross-document information extraction. In Pu-Jen Cheng, Min-Yen Kan, Wai Lam, and Preslav Nakov, editors, Information Retrieval Technology, volume 6458 of Lecture Notes in Computer Science, pages 432442. Springer Berlin / Heidelberg. 10.1007/978-3-642-17187-1 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>Extending VerbNet with Novel Verb Classes. In</title>
<date>2006</date>
<booktitle>LREC</booktitle>
<contexts>
<context position="6802" citStr="Kipper et al., 2006" startWordPosition="1060" endWordPosition="1063">ted. For the scheme killing, the IE rules would match X as the perpetrator and Y as a victim for all of the following phrases: X killed Y, Y was assassinated by X, and the murder of X by Y. Other schemes have similar structure and purpose, such as wounding, abducting, damaging and destroying. To create extraction rules for a scheme, we must find several verbs and nouns sharing a similar meaning and identify the syntactical position of the roles we are interested in. Three resources have helped us in designing extraction rules: a thesaurus to find semantically related nouns and verbs; VerbNet (Kipper et al., 2006), which provides amongst other things the semantic roles of the syntactical dependents of verbs; and a hand-crafted list of aspect-relevant word stems provided by the team that made CLASSY (Conroy et al., 2010). Schemes and their extraction rules can also be quite different from this first example, as shown with the scheme event. This scheme gathers the basic information about the attack event: WHAT category of attack, WHEN and WHERE it occurred. A list of key words is used to identify words that imply an attack event, while a list of EVENT NOUNs is used to identify specifically words that ref</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2006</marker>
<rawString>Karen Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending VerbNet with Novel Verb Classes. In LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticsbased summarization - step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>703710</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1613" citStr="Knight and Marcu, 2000" startWordPosition="230" endWordPosition="233">ly acceptable by human assessment (Owczarzak and Dang, 2011). The field is also getting saturated near what appears to be a ceiling in performance. Systems that claim to be very different from one another have all become statistically indistinguishable in evaluation results. An experiment (Genest et al., 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al., 2009), and a generationbased approach that could be called sentence splitting (Genest and Lapalme, 2011). They are all 1 www.nist.gov/tac rewriting techniques based on syntactical analysis, offering little improvement over extractive methods in the content selection process. We believe that a fully abstractive approach with a separate process for the analysis of the text, the content selection, and the generation of the summary has the most potential for generating summaries at a level comparable</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticsbased summarization - step one: Sentence compression. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 703710. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Discourse strategies for generating natural-language text.</title>
<date>1985</date>
<journal>Artif. Intell.,</journal>
<volume>27</volume>
<pages>41</pages>
<contexts>
<context position="11436" citStr="McKeown, 1985" startWordPosition="1816" endWordPosition="1817">gunman stabbed and shot Dutch filmmaker Theo van Gogh. A policeman and the suspect were wounded. On February 14, 2005, a suicide car bombing occurred in Beirut. Former Lebanese Prime Minister Rafik Hariri and 14 others were killed. Figure 3: Brief fully abstractive summaries on clusters D1001A-A, D1039G-A and D1043H-A, respectively on the Columbine massacre, the murder of Theo van Gogh and the assassination of Rafik Hariri. combination of number and modifiers chosen must appear at least once as an IE rule match. As for any generated text, a good summary also requires a text plan (Hovy, 1988) (McKeown, 1985). Ours consists of an ordering of the schemes. For example, an Attack summary begins with the scheme event. This ordering also determines which scheme to favor in the case of redundancy, e.g. given that a building was both damaged and destroyed, only the fact that is was destroyed will be mentioned. 4 Results and Discussion We have implemented this fully abstractive summarization methodology. The abstraction schemes and text plan for the Attack category are written in an XML document, designed to easily allow the addition of more schemes and the design of new categories. The language processin</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R. McKeown. 1985. Discourse strategies for generating natural-language text. Artif. Intell., 27:1 41, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The pyramid method: Incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>4</volume>
<contexts>
<context position="12610" citStr="Nenkova et al., 2007" startWordPosition="2018" endWordPosition="2021">esign of new categories. The language processing of the source documents and the domain-specific knowledge are completely separate in the program. Our system, which is meant as a proof of concept, can generate useful summaries for the Attack category, as can be seen in Figure 3. The key elements of information are present in each case, stated in a way that is easy to understand. These short summaries have a high density of information, in terms of how much content from the source documents they cover for a given number of words. For example, using the most widely used content metric, Pyramid (Nenkova et al., 2007), the two sentences generated for the cluster D1001AA contain 8 Semantic Content Units (SCU) for a weighted total of 30 out of a maximum of 56, for a raw Pyramid score of 0.54. Only 3 of the 43 automatic summaries beat this score on this cluster that year (the average was 0.31). Note that the summaries that we compare against contain up to 100 words, whereas ours is only 21 words long. We conclude that our method has the potential for creating summaries with much greater information density than the current state of the art. In fact, our approach does not only have the potential to increase a </context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. 2007. The pyramid method: Incorporating human content selection variation in summarization evaluation. ACM Trans. Speech Lang. Process., 4, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Hoa Trang Dang</author>
</authors>
<date>2011</date>
<contexts>
<context position="1050" citStr="Owczarzak and Dang, 2011" startWordPosition="141" endWordPosition="144"> This paper shows that full abstraction can be accomplished in the context of guided summarization. We describe a work in progress that relies on Information Extraction, statistical content selection and Natural Language Generation. Early results already demonstrate the effectiveness of the approach. 1 Introduction In the last decade, automatic text summarization has been dominated by extractive approaches that rely purely on shallow statistics. In the latest evaluation campaign of the Text Analysis Conference1 (TAC), the top systems were considered only barely acceptable by human assessment (Owczarzak and Dang, 2011). The field is also getting saturated near what appears to be a ceiling in performance. Systems that claim to be very different from one another have all become statistically indistinguishable in evaluation results. An experiment (Genest et al., 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fu</context>
</contexts>
<marker>Owczarzak, Dang, 2011</marker>
<rawString>Karolina Owczarzak and Hoa Trang Dang. 2011.</rawString>
</citation>
<citation valid="false">
<title>Overview of the TAC 2011 summarization track: Guided task and aesop task.</title>
<booktitle>In Proceedings of the Fourth Text Analysis Conference,</booktitle>
<location>Gaithersburg, Maryland, USA.</location>
<marker></marker>
<rawString>Overview of the TAC 2011 summarization track: Guided task and aesop task. In Proceedings of the Fourth Text Analysis Conference, Gaithersburg, Maryland, USA. National Institute of Standards and Technology. http://www.nist.gov/tac/publications/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Tanaka</author>
<author>Akinori Kinoshita</author>
<author>Takeshi Kobayakawa</author>
<author>Tadashi Kumano</author>
<author>Naoto Kato</author>
</authors>
<title>Syntaxdriven sentence revision for broadcast news summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Language Generation and Summarisation, UCNLG+Sum 09,</booktitle>
<pages>3947</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1717" citStr="Tanaka et al., 2009" startWordPosition="246" endWordPosition="249"> appears to be a ceiling in performance. Systems that claim to be very different from one another have all become statistically indistinguishable in evaluation results. An experiment (Genest et al., 2009) found a performance ceiling to pure sentence extraction that is very low compared to regular (abstractive) human summaries, but not that much better than the current best automatic systems. Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al., 2009), and a generationbased approach that could be called sentence splitting (Genest and Lapalme, 2011). They are all 1 www.nist.gov/tac rewriting techniques based on syntactical analysis, offering little improvement over extractive methods in the content selection process. We believe that a fully abstractive approach with a separate process for the analysis of the text, the content selection, and the generation of the summary has the most potential for generating summaries at a level comparable to human. For the foreseeable future, we think that such a process for full abstraction is impossible i</context>
</contexts>
<marker>Tanaka, Kinoshita, Kobayakawa, Kumano, Kato, 2009</marker>
<rawString>Hideki Tanaka, Akinori Kinoshita, Takeshi Kobayakawa, Tadashi Kumano, and Naoto Kato. 2009. Syntaxdriven sentence revision for broadcast news summarization. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, UCNLG+Sum 09, pages 3947, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Tanya Korelsky</author>
<author>Claire Cardie</author>
<author>Vincent Ng</author>
<author>David Pierce</author>
<author>Kiri Wagstaff</author>
</authors>
<title>Multidocument summarization via information extraction.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research, HLT 01,</booktitle>
<pages>17</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4232" citStr="White et al., 2001" startWordPosition="631" endWordPosition="634"> injury), or individuals otherwise negatively affected 2.7 DAMAGES: damages caused by the attack 2.8 COUNTERMEASURES: countermeasures, rescue efforts, prevention efforts, other reactions Figure 1: Aspects for TACs guided summarization task, category 2: Attacks 3 Fully Abstractive Approach Guided summarization categories and aspects define an information need, and using Information Extraction (IE) seems appropriate to address it. The idea to use an IE system for summarization can be traced back to the FRUMP system (DeJong, 1982), which generates brief summaries about various kinds of stories; (White et al., 2001) also wrote abstractive summaries using the output of an IE system applied to events such as natural disasters. In both cases, the end result is a generated summary from the information available. A lot of other work has instead used IE to improve the performance of extraction-based systems, like (Barzilay and Lee, 2004) and (Ji et al., 2010). What is common to all these approaches is that the IE system is designed for a specific purpose, separate from summarization. However, to properly address each aspect requires a system designed specifically for that task. To our knowledge, tailoring IE t</context>
</contexts>
<marker>White, Korelsky, Cardie, Ng, Pierce, Wagstaff, 2001</marker>
<rawString>Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng, David Pierce, and Kiri Wagstaff. 2001. Multidocument summarization via information extraction. In Proceedings of the first international conference on Human language technology research, HLT 01, pages 17, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>