Proceedings of NAACL HLT 2007, pages 65–72,
Rochester, NY, April 2007. c
2007 Association for Computational Linguistics
Structured Local Training and Biased Potential Functions for Conditional
Random Fields with Application to Coreference Resolution
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Conditional Random Fields (CRFs) have shown
great success for problems involving structured out-
put variables. However, for many real-world NLP
applications, exact maximum-likelihood training is
intractable because computing the global normal-
ization factor even approximately can be extremely
hard. In addition, optimizing likelihood often does
not correlate with maximizing task-specific evalu-
ation measures. In this paper, we present a novel
training procedure, structured local training, that
maximizes likelihood while exploiting the benefits
of global inference during training: hidden vari-
ables are used to capture interactions between lo-
cal inference and global inference. Furthermore,
we introduce biased potential functions that empir-
ically drive CRFs towards performance improve-
ments w.r.t. the preferred evaluation measure for
the learning task. We report promising experimen-
tal results on two coreference data sets using two
task-specific evaluation measures.
1 Introduction
Undirected graphical models such as Conditional
Random Fields (CRFs) (Lafferty et al., 2001) have
shown great success for problems involving struc-
tured output variables (e.g. Wellner et al. (2004),
Finkel et al. (2005)). For many real-world NLP ap-
plications, however, the required graph structure can
be very complex, and computing the global normal-
ization factor even approximately can be extremely
hard. Previous approaches for training CRFs have
either (1) opted for a training method that no longer
maximizes the likelihood, (e.g. McCallum and Well-
ner (2004), Roth and Yih (2005))1, or (2) opted for a
1
Both McCallum and Wellner (2004) and Roth and Yih
(2005) used the voted perceptron algorithm (Collins, 2002) to
train intractable CRFs.
simplified graph structure to avoid intractable global
normalization (e.g. Roth and Yih (2005), Wellner et
al. (2004)).
Solutions of the first type replace the computation
of the global normalization factor

y p(y|x) with
argmaxy p(y|x) during training, since finding an
argmax of a probability distribution is often an eas-
ier problem than finding the entire probability distri-
bution. Training via the voted perceptron algorithm
(Collins, 2002) or using a max-margin criterion also
correspond to the first option (e.g. McCallum and
Wellner (2004), Finley and Joachims (2005)). But
without the global normalization, the maximum-
likelihood criterion motivated by the maximum en-
tropy principle (Berger et al., 1996) is no longer a
feasible option as an optimization criterion.
The second solution simplifies the graph struc-
ture for training, and applies complex global infer-
ence only for testing. In spite of the discrepancy
between the training model and the testing model,
it has been empirically shown that (1) performing
global inference only during testing can improve
performance (e.g. Finkel et al. (2005), Roth and Yih
(2005)), and (2) full-blown global training can of-
ten perform worse due to insufficient training data
(e.g. Punyakanok et al. (2005)). Importantly, how-
ever, attempts to reduce the discrepancy between the
training and test models — by judiciously adding the
effect of global inference to the training — have pro-
duced substantial performance improvements over
locally trained models (e.g. Cohen and Carvalho
(2005), Sutton and McCallum (2005a)).
In this paper, we present structured local training,
a novel training procedure for maximum-likelihood
65
training of undirected graphical models, such as
CRFs. The procedure maximizes likelihood while
exploiting the benefits of global inference during
training by capturing the interactions between local
inference and global inference via hidden variables.
Furthermore, we introduce biased potential func-
tions that redefine the likelihood for CRFs so that
the performance of CRFs trained under the max-
imum likelihood criterion correlates better empiri-
cally with the preferred evaluation measures such as
F-score and MUC-score.
We focus on the problem of coreference resolu-
tion; however, our approaches are general and can
be extended to other NLP applications with struc-
tured output. Our approaches also extend to non-
conditional graphical models such as Markov Ran-
dom Fields. In experiments on two coreference data
sets, structured local training reduces the error rate
significantly (3.5%) for one coreference data set and
minimally (≤ 1%) for the other. Experiments using
biased potential functions increase recall uniformly
and significantly for both data sets and both task-
specific evaluation measures. Results for the com-
bination of the two techniques are promising, but
mixed: pairwise F1 increases by 0.8-5.5% for both
data sets; MUC F1 increases by 3.5% for one data
set, but slightly hurts performance for the second
data set.
In §2, we describe structured local training, and
follow with experimental results in §3. In §4, we
describe biased potential functions and follow with
experimental results in §5. We discuss related work
in §6.
2 Structured Local Training
2.1 Definitions
For clarity, we define the following terms that we
will use throughout the paper.
• local inference: 2 Inference factored into smaller
independent pieces, without considering the
structure of the output space.
• global inference: Inference applied on the entire
set of output variables, considering the structure
of the output space.
2
In this paper, inference refers to the operation of finding the
argmax in particular.
• local training: Training that does not invoke
global inference at each iteration.
• global training: Training that does invoke global
inference at each iteration.
2.2 A Motivating Example for Coreference
Resolution
In this section, we present an example of the coref-
erence resolution problem to motivate our approach.
It has been shown that global inference-based train-
ing for coreference resolution outperforms training
with local inference only (e.g. Finley and Joachims
(2005), McCallum and Wellner (2004)). In particu-
lar, the output of coreference resolution must obey
equivalence relations, and exploiting such structural
constraints on the output space during training can
improve performance. Consider the coreference res-
olution task for the following text.
It was after the passage of this act, that Mary(1)
’s attitude
towards Elizabeth(1)
became overtly hostile. The deliber-
ations surrounding the act seem to have revived all Mary’s
memories of the humiliations she had suffered at the
hands of Anne Boleyn. At the same time, Elizabeth(2)
’s
continuing prevarications over religion confirmed that she
was indeed her mother’s daughter.
In the above text, the “she” in the last sen-
tence is coreferent with both mentions of
“Elizabeth”. However, when we consider
“she” and “Elizabeth(1)” in isolation from the
remaining coreference chain, it can be difficult for
a machine learning method to determine whether
the pair is coreferent or not. Indeed, such a
pair may not look very different from the pair
“she” and “Mary(1)” in terms of feature vectors.
It is much easier, however, to determine that
“she” and “Elizabeth(2)” are coreferent, or that
“Elizabeth(1)” and “Elizabeth(2)” are coreferent.
Only by taking the transitive closure of these pair-
wise coreference relations does it become clear that
“she” and “Elizabeth(1)” are coreferent. In other
words, global training might handle potentially
confusing coreference cases better because it allows
parameter learning (for each pairwise coreference
decision) to be informed by global inference.
We argue that, with appropriate modification to
the learning instances, local training is adequate for
the coreference resolution task. Specifically, we pro-
pose that confusing pairs in the training data — such
66
as “she” and “Elizabeth(1)” — be learned as not-
coreferent, so long as the global inference step can
fix this error by exploiting the structure of the out-
put space, i.e. by exploiting the equivalence rela-
tions. This is the key idea of structured local train-
ing, which we elaborate formally in the following
section.
2.3 A Hidden-Variable Model
In this section, we present a general description of
structured local training. Let y be a vector of out-
put variables for structured output, and let x be a
vector of input variables. In order to capture the in-
teractions between global inference and local infer-
ence, we introduce hidden variables h, |h| = |y|,
so that the global inference for p(y, h|x) can be fac-
tored into two components using the product rule, as
follows:
p(y, h|x) = p(y|h, x) p(h|x)
= p(y|h) p(h|x)
The second component p(h|x) on the right hand side
corresponds to the local model, for which the infer-
ence factorizes into smaller independent pieces, e.g.
argmaxhp(h|x) = {argmaxhi
φ(hi, x)}. And the
first component p(y|h, x) on the right hand side cor-
responds to the global model, whose inference may
not factorize nicely. Further, we assume that y is in-
dependent of x given h, so that p(y|h, x) = p(y|h).
That is to say, h captures sufficient information from
x, so that given h, global inference of y only de-
pends on h. The quantity of p(y|x) then is given by
marginalizing out h as follows:
p(y|x) =

h
p(y, h|x)
Intuitively, the hidden variables h represent the lo-
cal decisions that can lead to a good y after global
inference is applied. In the case of coreference reso-
lution, one natural factorization would be that global
inference is a clustering algorithm, and local infer-
ence is a classification decision on each pair of noun
phrases (or mentions).3 In this paper, we assume
3
Formally, we define each yi ∈ y to be the coreference de-
cision for the ith pair of mentions, and xi ∈ x be the input
regarding the ith pair of mentions. Then hi corresponds to the
local coreference decision that can lead to a good coreference
decision yi after the clustering algorithm has been applied.
that we only parameterize the local model p(h|x),
although it would be possible to extend the parame-
terization to the global model as well, depending on
the particular application under consideration. The
similarity between a pair of mentions is parameter-
ized via log-linear models. However, once we have
the similarity scores extracted via local inference,
the clustering algorithm does not require further pa-
rameterization.
For training, we apply the standard Expectation-
Maximization (EM) algorithm (Dempster et al.,
1977) as follows:
• E Step: Compute a distribution

P(t)
= P(h|y, x, θ(t−1)
)
• M Step: Set θ(t) to θ that maximizes
E
P (t) [logP(y, h|x, θ)]
By repeatedly applying the above two steps for
t = 1, 2, ..., the value of θ converges to the local
maxima of the conditional log likelihood L(θ) =
logP(y|x, θ).
2.4 Application to Coreference Resolution
For yi ∈ y (and hi ∈ h) in the coreference resolution
task, yi = 1 (and hi = 1) corresponds to ith pair of
mentions being coreferent, and yi = 0 (and hi = 0)
corresponds to ith pair being not coreferent.
[Local Model P(h|x)] For the local model, we de-
fine cliques as individual nodes,4 and parameterize
each clique potential as
φ(hi, x) = φ(hi, xi) = exp

k
λkfk(hi, xi)
Let Φ(h|x) ≡

i φ(hi, xi). Then,
P(h|x) =
Φ(h, x)

h Φ(h, x)
Notice that in this model, finding argmaxhP(h|x)
corresponds to simply finding argmaxhi
φ(hi, xi) in-
dependently for each hi ∈ h.
4
Each node in the graphical representation of CRFs corre-
sponds to the coreferent decision for each pair of mentions. This
corresponds to the “Model 3” of McCallum and Wellner (2004).
67
ALGORITHM-1
INPUT: x, true labeling y∗
, current local model P(h|x)
GOAL: Find the highest confidence labeling y
such that y∗
= single-link-clustering(y
)
h∗
← argmaxhP(h|x)
h
← single-link-clustering(h∗
)
construct a graph G = (V, E), where
E = {h
i : h
i ∈ h
s.t. y∗
i = 1}
V = {v : v is a NP referred by a h
i ∈ E}
with edge cost costh
i
= φ(h
i, xi) if h
i = y∗
i
with edge cost costh
i
= 0 if h
i = y∗
i
find a minimum spanning tree(or forest) M of G
for each h
i ∈ h
if h
i = y∗
i
y
i ← h∗
i
else if h
i ∈ M
y
i ← 1
else
y
i ← 0
end for
return y
Figure 1: Algorithm to find the highest confidence labeling y
that can be clustered to the true labeling y∗
[Global Model P(y|h)] For the global model, we
assume a deterministic clustering algorithm is given.
In particular, we focus on single-link clustering, as it
has been shown to be effective for coreference reso-
lution (e.g. Ng and Cardie (2002)). With single-link
clustering, P(y|h) = 1 if h can be clustered to y,
and P(y|h) = 0 if h cannot be clustered to y.5
[Computation of the E-step] The E-step requires
computation of the distribution of P(h|y, x, θ(t−1)),
which we will simply denote as P(h|y, x), since all
our distributions are implicitly conditioned on the
model parameters θ.
P(h|y, x) =
P(h, y|x)
P(y|x)
∝ P(y|h) P(h|x)
Notice that when computing P(h|y, x), the denomi-
nator P(y|x) stays as a constant for different values
of h. The E-step requires enumeration of all possible
values of h, but it is intractable with our formulation,
because inference for the global model P(y|h) does
not factor out nicely. Therefore, we must resort to an
5
Single-link clustering simply takes the transitive closure,
and does not consider the distance metric. In a pilot study, we
also tried a variant of a stochastic clustering algorithm that takes
into account the distance metric (set as the probabilities from
the local model) for the global model, but the performance was
worse.
ALGORITHM-2
INPUT: x, true labeling y∗
, current local model P(h|x)
GOAL: Find a high confidence labeling y
that is
close to the true labeling y∗
h∗
← argmaxhP(h|x)
h
← single-link-clustering(h∗
)
for each h
i ∈ h
if h
i = y∗
i
y
i ← h∗
i
else
y
i ← y∗
i
end for
return y
Figure 2: Algorithm to find a high confidence labeling y
that
is close to the true labeling y∗
approximation method. Neal and Hinton (1998) an-
alyze and motivate various approximate EM training
methods. One popular choice in practice is called
“Viterbi training”, a variant of the EM algorithm,
which has been shown effective in many NLP ap-
plications. Viterbi training approximates the distri-
bution by assigning all probability mass to a single
best assignment. The algorithm for this is shown in
Figure 1.
We propose another approximation option for the
E-step that is given by Figure 2. Intuitively, when
the current local model misses positive coreference
decisions, the first algorithm constructs a y that is
closest to h
for single-link clustering to recover the
true labeling y∗, while the second algorithm con-
structs a y that is closer to y∗ by preserving all of
the missing positive coreference decisions. 6
[Computation of M-step] Because P(y|h) is not
parameterized, finding argmaxθ P(y, h|x) reduces
to finding argmaxθ P(h|x), which is standard CRF
training. In order to speed up the training, we start
convex optimization for CRFs using the parame-
ter values θ(t−1) from the previous M-step. For
the very first iteration of EM, we start by setting
P(y∗|x) = 1 for E-step, so that the first M-step will
finds argmaxθ P(y∗|x).
6
In a pilot study, we found that ALGORITHM-2 per-
forms slightly better than ALGORITHM-1. We also tried two
other approximation options, but none performed as well as
ALGORITHM-2. One of them removes the confusing sub-
instances and has the effect of setting a uniform distribution on
those sub-instances. The other computes the actual distribution
on a subset of sub-instances. For brevity, we only present ex-
perimental results using ALGORITHM-2 in this paper.
68
[Inference on the test data] It is intractable to
marginalize out h from P(y, h|x). Therefore, sim-
ilar to the Viterbi-training in the E-step, we approx-
imate the distribution of h by argmaxhP(h|X).
3 Experiments–I
Data set: We evaluate our approach with two
coreference data sets: MUC6 (MUC-6, 1995) and
MPQA7(Wiebe et al., 2005). For the MUC6 data set,
we extract noun phrases (mentions) automatically,
but for MPQA, we assume mentions for corefer-
ence resolution are given as in Stoyanov and Cardie
(2006). For MUC6, we use the standard training/test
data split. For MPQA, we use 150 documents for
training, and 50 documents for testing.
Configuration: We follow Ng and Cardie (2002)
for feature vector construction for each pair of men-
tions,8 and Finley and Joachims (2005) for con-
structing a training/testing instance for each docu-
ment: a training/testing instance consists of all pairs
of mentions in a document. Then, a single pair of
mentions is a sub-instance. We use the Mallet9 im-
plementation of CRFs, and set a Gaussian prior of
1.0 for all experiments. At each M-step, we train
CRFs starting from the parameters from the previous
M-step. We train CRFs up to 200 iterations, but be-
cause we start training CRFs from the previous pa-
rameters, the convergence from the second M-step
becomes much faster. We apply up to 5 EM itera-
tions, and choose best performing θ(t), 2 ≤ t ≤ 5
based on the performance on the training data.10
Hypothesis: For the baseline (BASE) we employ
the locally trained model for pairwise decisions
without global inference. Clustering is applied only
at test time, in order to make the assignment on the
output variables coherent. We hypothesize that for
the baseline, maximizing the likelihood for training
will correlate more with the pairwise accuracy of the
7
Available at http://nrrc.mitre.org/NRRC/publications.htm.
8
In particular, our feature set corresponds to “All Features”
in Ng and Cardie (2002), and we discretized numeric values.
9
Available at http://mallet.cs.umass.edu.
10
Selecting θ(t)
on a separate tuning data would be better, but
the data for MUC6 in particular is very limited. Notice that we
don’t pick θ1
when reporting the performance of SLT, because
it is identical to the baseline.
MUC6
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 1.50 59.2 56.2 57.7 1.18 38.0 85.6 52.6
SLT 1.28 49.8 67.3 57.2 1.35 26.4 84.3 40.2
MPQA
after clustering before clustering
e % R % P % F % e % R % P % F %
BASE 9.83 75.8 57.0 65.1 7.05 52.1 83.4 64.1
SLT 6.39 62.1 80.6 70.2 7.39 43.7 90.1 58.9
Table 1: Performance of Structured Local Training: SLT re-
duces error rate (e %) after applying single-link clustering.
incoherent decisions before clustering than the pair-
wise accuracy of the coherent decisions after cluster-
ing. We also hypothesize that by performing struc-
tured local training (SLT), maximizing the likeli-
hood will correlate more with the pairwise accuracy
after clustering.
Results: Experimental results are shown in Ta-
ble 1. We report error rate (error rate = 100 −
accuracy) on the pairwise decisions (e %), and F1-
score (F %) on the coreferent pairs.11 For compar-
ison, we show numbers from both after and before
single-link clustering is applied. As hypothesized,
the error rate of BASE increases after clustering,
while the error rate of SLT decreases after cluster-
ing. Moreover, the error rate of SLT is considerably
lower than that of BASE after clustering. However,
the F1-score does not correlate with the error rate.
That is, a lower error rate does not always lead to a
higher F1-score, which motivates the Biased Poten-
tial Functions that we introduce in the next section.
Notice that when we compare the precision/recall
breakdown after clustering, SLT has higher precision
and lower recall than BASE.
4 Biased Potential Functions
We introduce biased potential functions for train-
ing CRFs to empirically favor preferred evaluation
measures for the learning task, such as F-score and
MUC-score that have been considered hard for tradi-
11
Error rate and F1-score on the coreferent pairs are not ideal
measures for the quality of clustering, however, we show them
here in order to contrast the effect of SLT. We present MUC-
scores for the same experimental settings in Table 3.
69
tional likelihood-based methods to optimize for. In-
tuitively, biased potential functions emphasize those
sub-components of an instance that can be of greater
importance than the rest of an instance.
4.1 Definitions
The conditional probability of P(y|x)12 for CRFs is
given by (Lafferty et al., 2001)
P(y|x) =

i φ(Ci, x)

y

i φ(Ci, x)
where φ(Ci, x) is a potential function defined over
each clique Ci. Potential functions are typically pa-
rameterized in an exponential form as follows.
φ(Ci, x) = exp

k
λkfk(Ci, x)
where λk are the parameters and fk(·) are fea-
ture indicator functions. Because the Hammersley-
Clifford theorem (1971) for undirected graphical
models holds for any non-negative potential func-
tions, we propose alternative potential functions as
follows.
ψ(Ci, x) =

βφ(Ci, x) if µ(Ci, x) = true
φ(Ci, x) otherwise
where β is a non-negative bias factor, and µ(Ci, x)
is a predicate (or an indicator function) to check cer-
tain properties on (Ci, x).13 Examples of possible
µ(·) would be whether the true assignment for Ci
in the training data contains certain class values, or
whether the current observation indexed by Ci has
particular characteristics. More specific details will
be given in §4.2.
Training and testing with biased potential func-
tions is mostly identical to the traditional log-linear
formulations by φ(·) as defined above, except for
small and straightforward modifications to the com-
putation of the likelihood and the derivative of the
likelihood.
12
For the local model described in Section 2, y should be
replaced with h. We use y in this section however, as it is a
more conventional notation in general.
13
In our problem formulation, cliques are individual nodes,
and potential functions are defined over the observations in-
dexed by the current i only: i.e. φ(Ci, x) = φ(yi, xi),
µ(Ci, x) = µ(yi, xi) and ψ(Ci, x) = ψ(yi, xi).
The key idea for biased potential functions is
nothing new, as it is conceptually similar to in-
stance weighting for problems with non-structured
output (e.g. Aha and Goldstone (1992), Cardie et al.
(1997)). However, biased potential functions differ
technically in that they emphasize desired subcom-
ponents without altering the i.i.d. assumption, and
still weight each instance alike. Despite the con-
ceptual simplicity, we are not aware of any previ-
ous work that explored biased potential functions for
problems with structured output.
4.2 Applications to Coreference Resolution
[Bias on Coreferent Pairs] For coreference res-
olution, pairs that are coreferent are in a minority
class14, and biased potential functions can mitigate
this skewed data problem, by amplifying the clique
potentials that correspond to coreferent pairs. We
define µ(yi, xi) to be true if and only if the true as-
signment for yi in the training data is ’coreferent’.
Notice that µ(·) does not depend on what particu-
lar value yi might take, but only depends on the true
value of yi in the training data. For testing, µ(yi, xi)
will be always false.15
[Bias on Closer Coreferent Pairs] For corefer-
ence resolution, we hypothesize that coreferent pairs
for closer mentions have more significance, because
they tend to have clearer linguistic clues to deter-
mine coreference. We further hypothesize that by
emphasizing only close coreferent pairs, we can
have our model favor the MUC score. For this, we
define µ(yi, xi) to be true if and only if xi is for a
pair of mentions that are the closest coreferent pair.
5 Experiments–II
Data sets and configurations for experiments are
identical to those used in §3.
Hypothesis: We hypothesize that using biased po-
tential functions, maximizing the likelihood for
training can correlate better with F1-score or MUC-
score than the pairwise accuracy. In particular,
14
Only 1.72% of the pairs are coreferent in the MUC6 data,
and about 12% are coreferent in the MPQA data.
15
Notice that µ(yi, xi) changes the surface of the likelihood
for training, but does not affect the inference of finding the
argmax in our local model. That is, argmaxyi
φ(yi, xi) =
argmaxyi
ψ(yi, xi) (with yi replaced with hi).
70
MUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.18 38.0 85.6 52.6 59.0 75.8 66.4
BASIC-P11.5
1.20 38.9 82.1 52.8 64.2 71.8 67.8
BASIC-P13.0
1.32 46.9 71.3 56.6 68.9 64.3 66.5
BASIC-Pa1.5
1.15 44.2 79.9 56.9 62.1 68.7 65.2
BASIC-Pa3.0
1.44 52.5 62.9 57.2 70.9 60.5 65.3
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 7.05 52.1 83.4 64.1 75.6 81.5 78.4
BASIC-P11.5
7.18 54.6 79.6 64.8 77.7 76.5 77.1
BASIC-P13.0
7.22 59.9 75.4 66.8 83.3 71.7 77.1
BASIC-Pa1.5
7.65 59.7 72.2 65.4 79.8 73.2 76.4
BASIC-Pa3.0
8.22 69.2 65.1 67.1 85.8 67.8 75.7
Table 2: Performance of Biased Potential Functions: pairwise
scores are taken before single-link-clustering is applied.
we hypothesize that biasing on every coreferent
pair will correlate more with F1-score, and bias-
ing on close coreferent pairs will correlate more
with MUC-score. In general, we expect that bias-
ing on coreferent pairs will boost recall, potentially
decreasing precision.
Results [BPF]: Experimental results for biased
potential functions, without structured local train-
ing, are shown in Table 2. BASIC-P1β denotes local
training with biased potential on the closest corefer-
ent pairs with bias factor β, and BASIC-Paβ denotes
local training with biased potential on the all coref-
erent pairs with bias factor β, where β = 1.5 or 3.0.
For brevity, we only show pairwise numbers before
applying single-link-clustering.16 As hypothesized,
biased potential functions in general boost recall at
the cost of precision. Also, for a fixed value of
β, BASIC-P1β gives better MUC-F1 than BASIC-
Paβ, and BASIC-Paβ gives better pairwise-F1 than
BASIC-P1β for both data sets.
Results [SLT+BPF]: Experimental results that
combine SLT and BPF are shown in Table 3. Sim-
ilarly as before, SLT-Pxβ denotes SLT with biased
potential scheme Px, with bias factor β. For brevity,
16
This is because we showed in §3 that basic local training
does not correlate well with pairwise scores after clustering, and
in order to see the direct effect of biased potential functions, we
examine pairwise numbers before clustering.
MUC6
pairwise MUC
e % R % P % F % R % P % F %
BASE 1.50 59.2 56.2 57.7 59.0 75.8 66.4
SLT 1.28 49.8 67.3 57.2 56.3 77.8 65.3
SLT-P11.5
1.19 52.8 70.6 60.4 59.3 74.6 66.1
SLT-P13.0
1.42 63.5 57.9 60.6 67.5 70.7 69.1
SLT-Pa1.5
1.43 58.6 58.5 58.5* 64.0 73.6 68.5
SLT-Pa3.0
1.71 65.2 50.3 56.8 70.5 69.3 69.9*
MPQA
pairwise MUC
e % R % P % F % R % P % F %
BASE 9.83 75.8 57.0 65.1 75.6 81.5 78.4
SLT 6.39 62.1 80.6 70.2 69.1 88.2 77.5
SLT-P11.5
6.54 64.9 77.4 70.6* 72.2 84.5 77.9*
SLT-P13.0
9.09 77.2 59.6 67.3 78.4 79.5 78.9
SLT-Pa1.5
6.74 65.2 75.7 70.1 72.4 87.2 79.1
SLT-Pa3.0
14.71 78.2 43.9 56.2 80.5 73.8 77.0
Table 3: Performance of Biased Potential Functions with
Structured Local Training: All numbers are taken after single-
link clustering.
we only show numbers after applying single-link-
clustering. Unlike the results shown in Table 2,
for a fixed value of β, SLT-P1β correlates better
with pairwise-F1, and SLT-Paβ correlates better with
MUC-F1. This indicates that when biased poten-
tial functions are used in conjunction with SLT, the
effect of biased potential functions can be different
from the case without SLT. Comparing F1-scores in
Table 2 and Table 3, we see that the combination of
biased potential functions with SLT improves per-
formance in general. In particular, SLT-P13.0 and
SLT-Pa1.5 consistently improve performance over
BASE on both data sets, for both pairwise-F1 and
MUC-F1. We present performance scores for all
variations of configurations for reference, but we
also mark the particular configuration SLT-Pxβ (by
‘*’ on F1-scores) that is chosen when selecting the
configuration based on the performance on the train-
ing data for each performance measure. To con-
clude, structured local training with biased poten-
tial functions bring a substantial improvement for
MUC-F1 score, from 66.4% to 69.9% for MUC6
data set. For pairwise-F1, the performance increase
from 57.7% to 58.5% for MUC6, and from 65.1% to
70.6% for MPQA.17
17
Performance on the MPQA data for MUC-F1 is slightly
decreased from 78.4% to 77.9%. Note the MUC scores for the
71
6 Related Work
Structured local training is motivated by recent re-
search that has shown that reducing the discrep-
ancy between the training model and testing model
can improve the performance without incurring the
heavy computational overhead of full-blown global
inference-based training. 18 (e.g. Cohen and Car-
valho (2005), Sutton and McCallum (2005a), Sutton
and McCallum (2005b)). Our work differs in that
(1) we use hidden variables to capture the interac-
tions between local inference and global inference,
(2) we present an application to coreference resolu-
tion, while previous work has shown applications for
variants of sequence tagging. McCallum and Well-
ner (2004) showed a global training approach with
CRFs for coreference resolution, but they used the
voted perceptron algorithm for training, which no
longer maximizes the likelihood. In addition, they
assume that all and only those noun phrases involved
in coreference resolution are given.
The performance of our system on MUC6 data
set is comparable to previously reported systems.
Using the same feature set, Ng and Cardie (2002)
reports 64.5% of MUC-score, while our system
achieved 69.9%. Ng and Cardie (2002) reports
70.4% of MUC-score using hand-selected features.
With an additional feature selection or feature induc-
tion step, the performance of our system might fur-
ther improve. McCallum and Wellner (2004) reports
73.42% of MUC-score on MUC6 data set, but their
experiments assumed perfect identification of all and
only those noun phrases involved in a coreference
relation, thus substantially simplifying the task.
7 Conclusion
We present a novel training procedure, structured
local training, that maximizes likelihood while
exploiting the benefits of global inference during
training. This is achieved by incorporating hidden
variables to capture the interactions between local
MPQA baseline are already quite high to begin with.
18
The computational cost for SLT in our experiments were
about twice of the cost for the local training of the baseline. This
is the case because M-step converges very fast from the second
EM iteration, by initializing CRFs using parameters from the
previous M-step. Biased potential functions hardly adds extra
computational cost. In practice, BPFs reduce training time sub-
stantially: we observed that the higher the bias is, the quicker
CRFs converge.
inference and global inference. In addition, we
introduce biased potential functions that allow
CRFs to empirically favor performance measures
such as F1-score or MUC-score. We focused on the
application of coreference resolution in this paper,
but the key ideas of our approaches can be extended
to other applications, and other machine learning
techniques motivated by Markov networks.
Acknowledgments We thank the reviewers as well
as Eric Breck and Ves Stoyanov for their many helpful com-
ments. This work was supported by the Advanced Research and
Development Activity (ARDA), by NSF Grants BCS-0624277,
IIS-0535099, and IIS-0208028, and by gifts from Google and
the Xerox Foundation.
References
D.W. Aha and R.L. Goldstone. 1992. Concept learning and flexible weighting. In
Proc. of the Fourteenth Annual Conference of the Cognitive Science Society.
A. Berger, S.D. Pietra, V.D. Pietra 1996. A Maximum Entropy Approach to
Natural Language Processing. In Computational Linguistics,22.
C. Cardie and N. Howe. 1997. Improving Minority Class Prediction Using Case-
Specific Feature Weights. In ICML.
W.W. Cohen and V. Carvalho. 2005. Stacked Sequential Learning. In IJCAI.
M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models:
Theory and Experiments with Perceptron Algorithms. In EMNLP.
A.P. Dempster, N. M. Laird and D. B. Rubin. 1977. Maximum Likelihood from
Incomplete Data via the EM Algorithm. In Journal of the Loyal Statistical
Society, B.39.
J. Finkel, T. Grenager and C. D. Manning. 2005. Incorporating Non-local Infor-
mation Into Information Extraction Systems By Gibbs Sampling. In ACL.
T. Finley and T. Joachims. 2005. Supervised Clustering with Support Vector
Machines. In ICML.
J. Hammersley and P. Clifford. 1971. Markov fields on finite graphs and lattices.
Unpublished manuscript.
J. Lafferty, A. McCallum and F. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML.
A. McCallum and B. Wellner. 2004. Conditional Models of Identity Uncertainty
with Application to Noun Coreference. In NIPS.
MUC-6 1995. In Proc. of the Sixth Message Understanding Conference (MUC-6)
Morgan Kaufmann.
R. M. Neal and G. E. Hinton. 1998. A view of the EM algorithm that justies
incremental, sparse, and other variants. In Learning in Graphical Models,
Kluwer.
V. Ng and C. Cardie. 2002. Improving Machine Learning Approaches to Coref-
erence Resolution. In ACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak 2005. Learning and Inference
over Constrained Output. In IJCAI.
D. Roth and W. Yih. 2005. Integer Linear Programming Inference for Conditional
Random Fields. In ICML.
V. Stoyanov and C. Cardie. 2006. Partially Supervised Coreference Resolution
for Opinion Summarization through Structured Rule Learning. In EMNLP.
C. Sutton and A. McCallum. 2005. Fast, Piecewise Training for Discriminative
Finite-state and Parsing Models. In Technical Report IR-403, University of
Massachusetts.
C. Sutton and A. McCallum. 2005. Piecewise Training for Undirected Models.
In UAI.
B. Wellner, A. McCallum, F. Peng and M. Hay. 2004. An Integrated, Conditional
Model of Information Extraction and Coreference with Application to Citation
Matching. In UAI.
J. Wiebe and T. Wilson and C. Cardie 2005. Annotating Expressions of Opinions
and Emotions in Language. In Language Resources and Evaluation, volume
39, issue 2-3.
72
