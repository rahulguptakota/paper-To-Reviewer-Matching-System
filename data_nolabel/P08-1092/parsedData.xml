<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<bodyText confidence="0.261593">
b&amp;apos;Proceedings of ACL-08: HLT, pages 807815,
Columbus, Ohio, USA, June 2008. c
</bodyText>
<sectionHeader confidence="0.355226" genericHeader="abstract">
2008 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.98465">
An Unsupervised Approach to Biography Production using Wikipedia
</title>
<author confidence="0.992366">
Fadi Biadsy, Julia Hirschberg and Elena Filatova*
</author>
<affiliation confidence="0.924731">
Department of Computer Science
Columbia University, New York, NY 10027, USA
</affiliation>
<email confidence="0.990508">
{fadi,julia}@cs.columbia.edu
</email>
<affiliation confidence="0.334038">
*InforSense LLC
</affiliation>
<address confidence="0.887734">
Cambridge, MA 02141, USA
</address>
<email confidence="0.991791">
efilatova@inforsense.com
</email>
<sectionHeader confidence="0.99056" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998054625">
We describe an unsupervised approach to
multi-document sentence-extraction based
summarization for the task of producing
biographies. We utilize Wikipedia to auto-
matically construct a corpus of biographical
sentences and TDT4 to construct a corpus
of non-biographical sentences. We build a
biographical-sentence classifier from these
corpora and an SVM regression model for
sentence ordering from the Wikipedia corpus.
We evaluate our work on the DUC2004
evaluation data and with human judges.
Overall, our system significantly outperforms
all systems that participated in DUC2004,
according to the ROUGE-L metric, and is
preferred by human subjects.
</bodyText>
<sectionHeader confidence="0.998236" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998970825">
Producing biographies by hand is a labor-intensive
task, generally done only for famous individuals.
The process is particularly difficult when persons of
interest are not well known and when information
must be gathered from a wide variety of sources. We
present an automatic, unsupervised, multi-document
summarization (MDS) approach based on extractive
techniques to producing biographies, answering the
question Who is X?
There is growing interest in automatic MDS in
general due in part to the explosion of multilingual
and multimedia data available online. The goal of
MDS is to automatically produce a concise, well-
organized, and fluent summary of a set of docu-
ments on the same topic. MDS strategies have been
employed to produce both generic summaries and
query-focused summaries. Due to the complexity
of text generation, most summarization systems em-
ploy sentence-extraction techniques, in which the
most relevant sentences from one or more docu-
ments are selected to represent the summary. This
approach is guaranteed to produce grammatical sen-
tences, although they must subsequently be ordered
appropriately to produce a coherent summary.
In this paper we describe a sentence-extraction
based MDS procedure to produce biographies from
online resources automatically. We make use of
Wikipedia, the largest free multilingual encyclope-
dia on the internet, to build a biographical-sentence
classifier and a component for ordering sentences in
the output summary. Section 2 presents an overview
of our system. In Section 3 we describe our cor-
pus and in Section 4 we discuss the components of
our system in more detail. In Section 5, we present
an evaluation of our work on the Document Under-
standing Conference of 2004 (DUC2004), the biog-
raphy task (task 5) test set. In Section 6 we com-
pare our research with previous work on biography
generation. We conclude in Section 7 and identify
directions for future research.
</bodyText>
<sectionHeader confidence="0.971379" genericHeader="method">
2 System Overview
</sectionHeader>
<bodyText confidence="0.998403571428572">
In this section, we present an overview of our biog-
raphy extraction system. We assume as input a set of
documents retrieved by an information retrieval en-
gine from a query consisting of the name of the per-
son for whom the biography is desired. We further
assume that these documents have been tagged with
Named Entities (NE)s with coreferences resolved
</bodyText>
<page confidence="0.990252">
807
</page>
<bodyText confidence="0.997759484848485">
\x0cusing a system such as NYUs 2005 ACE system
(Grishman et al., 2005), which we used for our ex-
periments. Our task is to produce a concise biogra-
phy from these documents.
First, we need to select the most important bio-
graphical sentences for the target person. To do so,
we first extract from the input documents all sen-
tences that contain some reference to the target per-
son according to the coreference assignment algo-
rithm; this reference may be the targets name or
a coreferential full NP or pronominal referring ex-
pression, such as the President or he. We call these
sentences hypothesis sentences. We hypothesize that
most biographical sentences will contain a refer-
ence to the target. However, some of these sentences
may be irrelevant to a biography; therefore, we filter
them using a binary classifier that retains only bio-
graphical sentences. These biographical sentences
may also include redundant information; therefore,
we cluster them and choose one sentence from each
cluster to represent the information in that cluster.
Since some of these sentences have more salient bi-
ographical information than others and since manu-
ally produced biographies tend to include informa-
tion in a certain order, we reorder our summary sen-
tences using an SVM regression model trained on
biographies. Finally, the first reference to the tar-
get person in the initial sentence in the reordering
is rewritten using the longest coreference in our hy-
pothesis sentences which contains the targets full
name. We then trim the output to a threshold to pro-
duce a biography of a certain length for evaluation
against the DUC2004 systems.
</bodyText>
<sectionHeader confidence="0.94766" genericHeader="method">
3 Training Data
</sectionHeader>
<bodyText confidence="0.999728636363636">
One of the difficulties inherent in automatic biog-
raphy generation is the lack of training data. One
might collect training data by manually annotating
a suitable corpus containing biographical and non-
biographical data about a person, as in (Zhou et al.,
2004). However, such annotation is labor intensive.
To avoid this problem, we adopt an unsupervised ap-
proach. We use Wikipedia biographies as our corpus
of biographical sentences. We collect our non-
biographical sentences from the English newswire
documents in the TDT4 corpus.1 While each corpus
</bodyText>
<equation confidence="0.564145">
1
http://projects.ldc.upenn.edu/TDT4
</equation>
<bodyText confidence="0.99733725">
may contain positive and negative examples, we as-
sume that most sentences in Wikipedia biographies
are biographical and that the majority of TDT4 sen-
tences are non-biographical.
</bodyText>
<subsectionHeader confidence="0.99965">
3.1 Constructing the Biographical Corpus
</subsectionHeader>
<bodyText confidence="0.925513837837838">
To automatically collect our biographical sentences,
we first download the xml version of Wikipedia
and extract only the documents whose authors used
the Wikipedia biography template when creating
their biography. There are 16,906 biographies in
Wikipedia that used this template. We next apply
simple text processing techniques to clean the text.
We select at most the first 150 sentences from each
page, to avoid sentences that are not critically impor-
tant to the biography. For each of these sentences we
perform the following steps:
1. We identify the biographys subject from its ti-
tle, terming this name the target person.
2. We run NYUs 2005 ACE system (Grish-
man et al., 2005) to tag NEs and do coref-
erence resolution. There are 43 unique NE
tags in our corpora, including PER Individual,
ORG Educational, and so on, and TIMEX tags
for all dates.
3. For each sentence, we replace each NE by its
tag name and type ([name-type subtype]) as as-
signed by the NYU tagger. This modified sen-
tence we term a class-based/lexical sentence.
4. Each non-pronominal referring expression
(e.g., George W. Bush, the US president) that
is tagged as coreferential with the target per-
son is replaced by our own [TARGET PER] tag
and every pronoun P that refers to the target
person is replaced by [TARGET P], where P is
the pronoun itself. This allows us to general-
ize our sentences while retaining a) the essen-
tial distinction between this NE (and its role in
the sentence) and all other NEs in the sentence,
and b) the form of referring expressions.
5. Sentences containing no reference to the tar-
get person are assumed to be irrelevant and re-
moved from the corpus, as are sentences with
</bodyText>
<page confidence="0.99438">
808
</page>
<bodyText confidence="0.9334155">
\x0cfewer than 4 tokens; short sentences are un-
likely to contain useful information beyond the
target reference.
For example, given sentences from the Wikipedia
biography of Martin Luther King, Jr. we produce
class-based/lexical sentences as follows:
Martin Luther King, Jr., was born on January 15, 1929, in Atlanta,
Georgia. He was the son of Reverend Martin Luther King, Sr. and
Alberta Williams King. He had an older sister, Willie Christine
(September 11, 1927) and a younger brother, Albert Daniel.
[TARGET PER], was born on [TIMEX], in [GPE PopulationCenter].
[TARGET HE] was the son of [PER Individual] and [PER Individual].
[TARGET HE] had an older sister, [PER Individual] ([TIMEX]) and a
younger brother, [PER Individual].
</bodyText>
<subsectionHeader confidence="0.998995">
3.2 Constructing the Non-Biographical Corpus
</subsectionHeader>
<bodyText confidence="0.999709888888889">
We use the TDT4 corpus to identify non-
biographical sentences. Again, we run NYUs 2005
ACE system to tag NEs and do coreference resolu-
tion on each news story in TDT4. Since we have
no target name for these stories, we select an NE
tagged as PER Individual at random from all NEs in
the story to represent the target person. We exclude
any sentence with no reference to this target person
and produce class-based/lexical sentences as above.
</bodyText>
<sectionHeader confidence="0.943014" genericHeader="method">
4 Our Biography Extraction System
</sectionHeader>
<subsectionHeader confidence="0.998961">
4.1 Classifying Biographical Sentences
</subsectionHeader>
<bodyText confidence="0.971533055555556">
Using the biographical and non-biographical cor-
pora described in Section 3, we train a binary classi-
fier to determine whether a new sentence should be
included in a biography or not. For our experiments
we extracted 30,002 sentences from Wikipedia bi-
ographies and held out 2,108 sentences for test-
ing. Similarly. we extracted 23,424 sentences from
TDT4, and held out 2,108 sentences for testing.
For each sentence, we then extract the frequency of
three class-based/lexical features unigram, bia-
gram, and trigram and two POS features the
frequency of unigram and bigram POS. To reduce
the dimensionality of our feature space, we first sort
the features in decreasing order of Chi-square statis-
tics computed from the contingency tables of the ob-
served frequencies from the training data. We then
take the highest 30-80% features, where the num-
ber of features used is determined empirically for
</bodyText>
<table confidence="0.96593225">
Classifier Accuracy F-Measure
SVM 87.6% 0.87
M. nave Bayes 84.1% 0.84
C4.5 81.8% 0.82
</table>
<tableCaption confidence="0.998532">
Table 1: Binary classification results: Wikipedia bi-
</tableCaption>
<bodyText confidence="0.989710695652174">
ography class-based/lexical sentences vs. TDT4 class-
based/lexical sentences
each feature type. This process identifies features
that significantly contribute to the classification task.
We extract 3K class-based/lexical unigrams, 5.5K
bigrams, 3K trigrams, 20 POS unigrams, and 166
POS bigrams.
Using the training data described above, we ex-
perimented with three different classification algo-
rithms using the Weka machine learning toolkit
(Witten et al., 1999): multinomial nave Bayes,
SVM with linear kernel, and C4.5. Weka also pro-
vides a classification confidence score that repre-
sents how confident the classifier is on each classi-
fied sample, which we will make use of as well.
Table 1 presents the classification results on our
4,216 held-out test-set sentences. These results are
quite promising. However, we should note that they
may not necessarily represent the successful clas-
sification of biographical vs. non-biographical sen-
tences but rather the classification of Wikipedia sen-
tences vs. TDT4 sentences. We will validate these
results for our full systems in Section 5.
</bodyText>
<subsectionHeader confidence="0.996281">
4.2 Removing Redundant Sentences
</subsectionHeader>
<bodyText confidence="0.999617">
Typically, redundancy removal is a standard com-
ponent in MDS systems. In sentence-extraction
based summarizers, redundant sentences are defined
as those which include the same information with-
out introducing new information and identified by
some form of lexically-based clustering. We use
an implementation of a single-link nearest neighbor
clustering technique based on stem-overlap (Blair-
Goldensohn et al., 2004b) to cluster the sentences
classified as biographical by our classifier, and then
select the sentence from each cluster that maximizes
the confidence score returned by the classifier as the
representative for that cluster.
</bodyText>
<subsectionHeader confidence="0.999524">
4.3 Sentence Reordering
</subsectionHeader>
<bodyText confidence="0.990871">
It is essential for MDS systems in the extraction
framework to choose the order in which sentences
</bodyText>
<page confidence="0.990978">
809
</page>
<bodyText confidence="0.999003704545455">
\x0cshould be presented in the final summary. Present-
ing more important information earlier in a sum-
mary is a general strategy for most domains, al-
though importance may be difficult to determine re-
liably. Similar to (Barzilay and Lee, 2004), we au-
tomatically learn how to order our biographical sen-
tences by observing the typical order of presentation
of information in a particular domain. We observe
that our Wikipedia biographies tend to follow a gen-
eral presentation template, in which birth informa-
tion is mentioned before death information, infor-
mation about current professional position and af-
filiations usually appear early in the biography, and
nuclear family members are typically mentioned be-
fore more distant relations. Learning how to order
information from these biographies however would
require that we learn to identify particular types of
biographical information in sentences.
We directly use the position of each sentence in
each Wikipedia biography as a way of determin-
ing where sentences containing similar information
about different target individuals should appear in
their biographies. We represent the absolute posi-
tion of each sentence in its biography as an inte-
ger and train an SVM regression model with RBF
kernel, from the class/lexical features of the sen-
tence to its position. We represent each sentence by
a feature vector whose elements correspond to the
frequency of unigrams and bigrams of class-based
items (e.g., GPE, PER) (cf. Section 3) and lexical
items; for example, the unigrams born, became, and
[GPE State-or-Province], and the bigrams was born,
[TARGET PER] died and [TARGET PER] joined
would be good candidates for such features.
To minimize the dimensionality of our regres-
sion space, we constrained our feature choice to
those features that are important to distinguish bi-
ographical sentences, which we term biographical
terms. Since we want these biographical terms to
impact the regression function, we define these to
be phrases that consist of at least one lexical item
that occurs in many biographies but rarely more than
once in any given biography. We compute the bio-
graphical term score as in the following equation:
</bodyText>
<equation confidence="0.984021888888889">
bio score(t)=
 |Dt |
 |D |
P
dDt
(1 n(t)d
maxt(n(t)d) )
 |D |
(1)
</equation>
<bodyText confidence="0.9983112">
where D is the set of 16,906 Wikipedia biographies,
n(t)d is the number of occurrences of term t in doc-
ument d, and Dt = {d D : t d}. The left factor
represents the document frequency of term t, and the
right factor calculates how infrequent the term is in
each biography that contains t at least once.2 We or-
der the unigrams and bigrams in the biographies by
their biographical term scores and select the high-
est 1K unigrams and 500 bigrams; these thresholds
were determined empirically.
</bodyText>
<subsectionHeader confidence="0.995984">
4.4 Reference Rewriting
</subsectionHeader>
<bodyText confidence="0.999377428571429">
We observe that news articles typically mention bio-
graphical information that occurs early in Wikipedia
biographies when they mention individuals for the
first time in a story (e.g. Stephen Hawking, the Cam-
bridge University physicist). We take advantage of
the fact that the coreference resolution system we
use tags full noun phrases including appositives as
part of NEs. Therefore, we initially search for the
sentence that contains the longest identified NE (of
type PER) that includes the target persons full name
and is coreferential with the target according to the
reference resolution system; we denote this NE NE-
NP. If this sentence has already been classified as
a biographical sentence by our classifier, we simply
boost its rank in the summary to first. Otherwise,
when we order our sentences, we replace the refer-
ence to the target person in the first sentence by NE-
NP. For example, if the first sentence in the biogra-
phy we have produced for Jimmy Carter is He was
born in 1947 and a sentence not chosen for inclusion
in our biography Jimmy Carter, former U.S. Presi-
dent, visited the University of California last year.
contains the NE-NP, and Jimmy Carter and He are
coreferential, then the first sentence in our biography
will be rewritten as Jimmy Carter, former U.S. Presi-
dent, was born in 1947. Note that, in the evaluations
presented in Section 5, sentence order was modified
by this process in only eight summaries.
</bodyText>
<sectionHeader confidence="0.996102" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.996985">
To evaluate our biography generation system, we use
the document sets created for the biography evalua-
</bodyText>
<page confidence="0.934144">
2
</page>
<bodyText confidence="0.9691646">
We considered various approaches to feature selection here,
such as comparing term frequency between our biographical
and non-biographical corpora. However, terms such as killed
and died, which are useful biographical terms, also occur fre-
quently in our non-biographical corpus.
</bodyText>
<figure confidence="0.979259708333333">
810
\x0cROUGE-L Average_F
0.25
0.275
0.3
0.325
0.35
0 1 2 3 4 5 6 7 8 9 10 11 12
SVM
reg. only
top-DUC2004 C4.5 SVM
SVM +
SVM reg.
MNB +
SVM reg
MNB
C4.5 +
SVM reg.
SVM +
baseline order
C4.5 +
baseline order
MNB +
baseline order
</figure>
<figureCaption confidence="0.8801605">
Figure 1: Comparing our approaches against the top performing system in DUC2004 according to ROUGE-L (dia-
mond).
</figureCaption>
<bodyText confidence="0.982721684210527">
tion (task 5) of DUC2004.3 The task for systems
participating in this evalution was Given each doc-
ument cluster and a question of the form Who is
X?, where X is the name of a person or group of
people, create a short summary (no longer than 665
bytes) of the cluster that responds to the question.
NIST assessors chose 50 clusters of TREC docu-
ments such that all the documents in a given cluster
provide at least part of the answer to this question.
Each cluster contained on average 10 documents.
NIST had 4 human summaries written for each clus-
ter. A baseline summary was also created for each
cluster by extracting the first 665 bytes of the most
recent document in the cluster. 22 systems partici-
pated in the competition, producing a total of 22 au-
tomatic summaries (restricted to 665 bytes) for each
cluster. We evaluate our system against the top per-
forming of these 22 systems, according to ROUGE-
L, which we denote top-DUC2004.4
</bodyText>
<subsectionHeader confidence="0.995779">
5.1 Automatic Evaluation Using ROUGE
</subsectionHeader>
<bodyText confidence="0.998160833333333">
As noted in Section 4.1, we experimented with a
number of learning algorithms when building our
biographical-sentence classifier. For each machine
learning algorithm tested, we build a system that ini-
tially classifies the input list of sentences into bio-
graphical and non-biographical sentences and then
</bodyText>
<page confidence="0.79874">
3
</page>
<footnote confidence="0.276391">
http://duc.nist.gov/duc2004
</footnote>
<page confidence="0.951309">
4
</page>
<bodyText confidence="0.998197297297297">
Note that this system out-performed 19 of the 22 systems
on ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2
(p &amp;lt; .05) (Blair-Goldensohn et al., 2004a). No ROUGE metric
produced scores where this system scored significantly worse
than any other system. See Figure 2 below for a comparison
of all DUC2004 systems with our top system where all systems
are evaluated using ROUGE-L-1.5.5.
removes redundant sentences. Next, we produce
three versions of each system: one which imple-
ments a baseline ordering procedure, in which sen-
tences from the clusters are ordered by their ap-
pearance in their source document (e.g. any sen-
tence which occurred first in its original document
is placed first in the summary, with ties ordered ran-
domly within the set), a second which orders the
biographical sentences by the confidence score ob-
tained from the classifier, and a third which uses the
SVM regression as the reordering component. Fi-
nally, we run our reference rewriting component on
each and trim the output to 665 bytes.
We evaluate first using the ROUGE-L metric (Lin
and Hovy, 2003) with a 95% (ROUGE computed)
confidence interval for all systems and compared
these to the ROUGE-L score of the best-performing
DUC2004 system.5 The higher the ROUGE score,
the closer the summary is to the DUC2004 human
reference summaries. As shown in Figure 1, our
best performing system is the multinomial nave
Bayes classifier (MNB) using the classifier confi-
dence scores to order the sentences in the biography.
This system significantly outperforms the top ranked
DUC2004 system (top-DUC2004).6 The success of
this particularly learning algorithm on our task may
be due to: (1) the nature of our feature space n-
gram frequencies are modeled properly by a multi-
nomial distribution; (2) the simplicity of this classi-
fier particularly given our large feature dimensional-
</bodyText>
<page confidence="0.968896">
5
</page>
<bodyText confidence="0.996733">
We used the same version (1.5.5) of the ROUGE metric to
compute scores for the DUC systems and baseline also.
</bodyText>
<page confidence="0.991029">
6
</page>
<bodyText confidence="0.9938915">
Significance for each pair of systems was determined by
paired t-test and calculated at the .05 significance level.
</bodyText>
<page confidence="0.984745">
811
</page>
<bodyText confidence="0.998046884615384">
\x0city; and (3) the robustness of nave Bayes with re-
spect to noisy data: Not all sentences in Wikipedia
biographies are biographical sentences and some
sentences in TDT4 are biographical.
While the SVM regression reordering component
has a slight negative impact on the performance
of the MNB system, the difference between the
two versions is not significant. Note however, that
both the C4.5 and the SVM versions of our system
are improved by the SVM regression sentence re-
ordering. While neither performs better than top-
DUC2004 without this component, the C4.5 system
with SVM reordering is significantly better than top-
DUC2004 and the performance of the SVM sys-
tem with SVM regression is comparable to top-
DUC2004. In fact, when we use only the SVM
regression model to rank the hypothesis sentences,
without employing any classifier, then remove re-
dundant sentences, rewrite and trim the results, we
find that, interestingly, this approach also outper-
forms top-DUC2004, although the difference is not
statistically significant. However, we believe that
this is an area worth pursuing in future, with more
sophisticated features.
The following biography of Brian Jones was pro-
duced by our MNB system and then the sentences
were ordered using the SVM regression model:
Born in Bristol in 1947, Brian Jones, the co-pilot on the
Breitling mission, learned to fly at 16, dropping out of
school a year later to join the Royal Air Force. After earn-
ing his commercial balloon flying license, Jones became
a ballooning instructor in 1989 and was certified as an ex-
aminer for balloon flight licenses by the British Civil Avi-
ation Authority. He helped organize Breitlings most re-
cent around-the-world attempts, in 1997 and 1998. Jones,
52, replaces fellow British flight engineer Tony Brown.
Jones, who is to turn 52 next week, is actually the teams
third co-pilot. After 13 years of service, he joined a cater-
ing business and, in the 1980s,...
Figure 2 illustrates the performance of our MNB
system with classifier confidence score sentence or-
dering when compared to mean ROUGE-L-1.5.5
scores of DUC2004 human-generated summaries
and the 22 DUC2004 systems summaries across all
summary tasks. Human summaries are labeled A-
H, DUC2004 systems 1-22, and our MNB system
is marked by the rectangle. Results are sorted by
mean ROUGE-L score. Note that our system perfor-
mance is actually comparable in ROUGE-L score to
one of the human summary generators and is signif-
icantly better that all DUC2004 systems, including
top-DUC2004, which is System 1 in the figure.
</bodyText>
<subsectionHeader confidence="0.999198">
5.2 Manual Evaluation
</subsectionHeader>
<bodyText confidence="0.998827925">
ROUGE evaluation is based on n-gram overlap be-
tween the automatically produced summary and the
human reference summaries. Thus, it is not able to
measure how fluent or coherent a summary is. Sen-
tence ordering is one factor in determining fluency
and coherence. So, we conducted two experiments
to measure these qualities, one comparing our top-
performing system according to ROUGE-L score
(MNB) vs. the top-performing DUC2004 system
(top-DUC2004) and another comparing our top sys-
tem with two different ordering methods, classifier-
based and SVM regression.7 In each experiment,
summaries were trimmed to 665 bytes.
In the first experiment, three native American En-
glish speakers were presented with the 50 questions
(Who is X?). For each question they were given a
pair of summaries (presented in random order): one
was the output of our MNB system and the other
was the summary produced by the top-DUC2004
system. Subjects were asked to decide which sum-
mary was more responsive in form and content to the
question or whether both were equally responsive.
85.3% (128/150) of subject judgments preferred one
summary over the other. 100/128 (78.1%) of these
judgments preferred the summaries produced by our
MNB system over those produced by top-DUC2004.
If we compute the majority vote, there were 42/50
summaries in which at least two subjects made the
same choice. 37/42 (88.1%) of these majority judg-
ments preferred our systems summary (using bino-
mial test, p = 4.4e7). We used the weighted kappa
statistic with quadratic weighting (Cohen, 1968)
to determine the inter-rater agreement, obtaining a
mean pairwise of 0.441.
Recall from Section 5.1 that our SVM regression
reordering component slightly decreases the aver-
age ROUGE score (although not significantly) for
our MNB system. For our human evaluations, we
decided to evaluate the quality of the presentation
of our summaries with and without this compo-
</bodyText>
<page confidence="0.993912">
7
</page>
<bodyText confidence="0.98731475">
Note that top-DUC2004 was ranked sixth in the DUC 2004
manual evaluation, with no system performing significantly
better for coverage and only 1 system performing significantly
better for responsiveness.
</bodyText>
<page confidence="0.943621">
812
</page>
<figure confidence="0.996492375">
\x0cROUGE-L Average_F
0.2
0.25
0.3
0.35
0.4
0.45
B E H G F A D C * 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 BL 18 19 20 21 22
</figure>
<figureCaption confidence="0.997714">
Figure 2: ROUGE-L scores for DUC2004 human summaries (A-H), our MNB system (rectangle), and the DUC2004
</figureCaption>
<bodyText confidence="0.995696">
competing systems (1-22 anonymized), with the baseline system labeled BL.
nent to see if this reordering component affected hu-
man judgments even if it did not improve ROUGE
scores. For each question, we produced two sum-
maries from the sentences classified as biographi-
cal by the MNB classifier, one ordered by the con-
fidence score obtained by the MNB, in decreasing
order, and the other ordered by the SVM regression
values, in increasing order. Note that, in three cases,
the summary sentences were ordered identically by
both procedures, so we used only 47 summaries
for this evaluation. Three (different) native Amer-
ican English speakers were presented with the 47
questions for which sentence ordering differed. For
each question they were given the two summaries
(presented in random order) and asked to determine
which biography they preferred.
We found inter-rater agreement for these judg-
ments using Fleiss kappa (Fleiss, 1971) to be only
moderate (=0.362). However, when we computed
the majority vote for each question, we found that
61.7% (29/47) preferred the SVM regression order-
ing over the MNB classifier confidence score order-
ing. Although this difference is not statistically sig-
nificant, again we find the SVM regression ordering
results encouraging enough to motivate our further
research on improving such ordering procedures.
</bodyText>
<sectionHeader confidence="0.999478" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999377657894737">
The DUC2004 system achieving the highest over-
all ROUGE score, our top-DUC2004 in Section 5,
was Blair-Goldensohn et al. (2004a)s DefScriber,
which treats Who is X? as a definition question
and targets definitional themes (e.g. genus-species)
found in the input document collections which in-
clude references to the target person. Extracted sen-
tences are then rewritten using a reference rewriting
system (Nenkova and McKeown, 2003) which at-
tempts to shorten subsequent references to the tar-
get. Sentences are ordered in the summary based
on a weighted combination of topic centrality, lex-
ical cohesion, and topic coverage scores. A simi-
lar approach is explored in Biryukov et al. (2005),
which uses Topic Signatures (Lin and Hovy, 2000)
constructed around the target individuals name to
identify sentences to be included in the biography.
Zhou et al. (2004)s biography generation system,
like ours, trains biographical and non-biographical
sentence classifiers to select sentences to be included
in the biography. Their system is trained on a hand-
annotated corpus of 130 biographies of 12 people,
tagged with 9 biographical elements (e.g., bio, ed-
ucation, nationality) and uses binary unigram and
bigram lexical and unigram part-of-speech features
for classification. Duboue et al. (2003) also ad-
dress the problem of learning content selection rules
for biography. They learn rules from two corpora,
a semi-structured corpus with lists of biographical
facts about show business celebrities and a corpus
of free-text biographies about the same celebrities.
Filatova et al. (2005) learn text features typical
of biographical descriptions by deducing biograph-
ical and occupation-related activities automatically
by compariing descriptions of people with differ-
ent occupations. Weischedel et al. (2004) models
kernel-fact features typical for biographies using lin-
guistic and semantic processing. Linguistic features
</bodyText>
<page confidence="0.974482">
813
</page>
<bodyText confidence="0.999516363636364">
\x0care derived from predicate-argument structures de-
duced from parse trees, and semantic features are the
set of biography-related relations and events defined
in the ACE guidelines (Doddington et al., 2004).
Sentences containing kernel facts are ranked using
probabilities estimated from a corpus of manually
created biographies, including Wikipedia, to esti-
mate the conditional distribution of relevant material
given a kernel fact and a background corpus.
The problem of ordering sentences and preserv-
ing coherence in MDS is addressed by Barzi-
lay et al. (2001), who combine chronological order-
ing of events with cohesion metrics. SVM regres-
sion has recently been used by (Li et al., 2007) for
sentence ranking for general MDS. The authors cal-
culated a similarity score for each sentence to the
human summaries and then regress numeric features
(e.g., the centroid) from each sentence to this score.
Barzilay and Lee (2004) use HMMs to capture topic
shift within a particular domain; sequence of topic
shifts then guides the subsequent ordering of sen-
tences within the summary.
</bodyText>
<sectionHeader confidence="0.970575" genericHeader="discussions">
7 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.99951371875">
In this paper, we describe a MDS system for produc-
ing biographies, given a target name. We present an
unsupervised approach using Wikipedia biography
pages and a general news corpus (TDT4) to automat-
ically construct training data for our system. We em-
ploy a NE tagger and a coreference resolution sys-
tem to extract class-based and lexical features from
each sentence which we use to train a binary classi-
fier to identify biographical sentences. We also train
an SVM regression model to reorder the sentences
and then employ a rewriting heuristic to create the
final summary.
We compare versions of our system based upon
three machine learning algorithms and two sentence
reordering strategies plus a baseline. Our best per-
forming system uses the multinomial nave Bayes
(MNB) classifier with classifier confidence score re-
ordering. However, our SVM regression reorder-
ing improves summaries produced by the other two
classifiers and is preferred by human judges. We
compare our MNB system on the DUC2004 bi-
ography task (task 5) to other DUC2004 systems
and to human-generated summaries. Our system
out-performs all DUC2004 systems significantly,
according to ROUGE-L-1.5.5. When presented
with summaries produced by our system and sum-
maries produced by the best-performing (according
to ROUGE scores) of the DUC2004 systems, human
judges (majority vote of 3) prefer our systems bi-
ographies in 88.1% of cases.
In addition to its high performance, our approach
has the following advantages: It employs no manual
annotation but relies upon identifying appropriately
different corpora to represent our training corpus.
It employs class-based as well as lexical features
where the classes are obtained automatically from
an ACE NE tagger. It utilizes automatic corefer-
ence resolution to identify sentences containing ref-
erences to the target person. Our sentence reorder-
ing approaches make use of either classifier confi-
dence scores or ordering learned automatically from
the actual ordering of sentences in Wikipedia biogra-
phies to determine the order of presentation of sen-
tences in our summaries.
Since our task is to produce concise summaries,
one focus of our future research will be to simplify
the sentences we extract before classifying them
as biographical or non-biographical. This proce-
dure should also help to remove irrelevant informa-
tion from sentences. Recall that our SVM regres-
sion model for sentence ordering was trained using
only biographical class-based/lexical items. In fu-
ture, we would also like to experiment with more
linguistically-informed features. While Wikipedia
does not enforce any particular ordering of infor-
mation in biographies, and while different biogra-
phies may emphasize different types of information,
it would appear that the success of our automatically
derived ordering procedures may capture some un-
derlying shared view of how biographies are written.
The same underlying views may also apply to do-
mains such as organization descriptions or types of
historical events. In future we plan to explore such a
generalization of our procedures to such domains.
</bodyText>
<sectionHeader confidence="0.978293" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.991804571428571">
We thank Kathy McKeown, Andrew Rosenberg, Wisam Dakka, and the
Speech and NLP groups at Columbia for useful discussions. This mate-
rial is based upon work supported by the Defense Advanced Research
Projects Agency (DARPA) under Contract No. HR001106C0023 (ap-
proved for public release, distribution unlimited). Any opinions, find-
ings and conclusions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the views of DARPA.
</bodyText>
<page confidence="0.999023">
814
</page>
<reference confidence="0.998178011904762">
\x0cReferences
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
NAACL-HLT.
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2001. Sentence ordering in multidocument sum-
marization. In Proceedings of the First Human Lan-
guage Technology Conference, San Diego, California.
Maria Biryukov, Roxana Angheluta, and Marie-Francine
Moens. 2005. Multidocument question answering
text summarization using topic signatures. In Pro-
ceedings of the 5th Dutch-Belgium Information Re-
trieval Workshop, Utrecht, the Netherlands.
Sasha Blair-Goldensohn, David Evans, Vasileios Hatzi-
vassiloglou, Kathleen McKeown, Ani Nenkova, Re-
becca Passonneau, Barry Schiffman, Andrew Schlaik-
jer, Advaith Siddharthan, and Sergey Siegelman.
2004a. Columbia University at DUC 2004. In Pro-
ceedings of the 4th Document Understanding Confer-
ence, Boston, Massachusetts, USA.
Sasha Blair-Goldensohn, Kathy McKeown, and Andrew
Schlaikjer. 2004b. Answering definitional questions:
A hybrid approach. In Mark Maybury, editor, New
Directions In Question Answering, chapter 4. AAAI
Press.
J. Cohen. 1968. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or partial
credit. volume 70, pages 213220.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extraction
program - tasks, data, and evaluation. In Proceedings
of the LREC Conference, Canary Islands, Spain, July.
Pablo Duboue and Kathleen McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Proceedings of the Conference
on Empirical Methods for Natural Language Process-
ing, pages 121128, Sapporo, Japan, July.
Elena Filatova and John Prager. 2005. Tell me what
you do and Ill tell you what you are: Learning
occupation-related activities for biographies. In Pro-
ceedings of the Joint Human Language Technology
Conference and Conference on Empirical Methods in
Natural Language Processing, pages 113120, Van-
couver, Canada, October.
J. L. Fleiss. 1971. Measuring nominal scale agreement
among many raters. volume 76, No. 5, pages 378382.
Ralph Grishman, David Westbrook, and Adam Meyers.
2005. Nyus english ace 2005 system description. In
ACE 05 Evaluation Workshop, Gaithersburg, MD.
Sujian Li, You Ouyang, Wei Wang, and Bin Sun. 2007.
Multi-document summarization using support vector
regression. In http://duc.nist.gov/pubs/2007papers.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th International Con-
ference on Computational Linguistics, pages 495501,
Saarbrucken, Germany, July.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Language Technology
Conference, Edmonton, Canada.
Ani Nenkova and Kathleen McKeown. 2003. References
to named entities: A corpus study. In Proceedings
of the Joint Human Language Technology Conference
and North American chapter of the Association for
Computational Linguistics Annual Meeting, Edmon-
ton, Canada, May.
Ralph Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A
hybrid approach to answering biographical questions.
In Mark Maybury, editor, New Directions In Question
Answering, chapter 5. AAAI Press.
I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and
S. Cunningham. 1999. Weka: Practical machine
learning tools and techniques with java implementa-
tion. In International Workshop: Emerging Knowl-
edge Engineering and Connectionist-Based Informa-
tion Systems, pages 192196.
Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004.
Multi-document biography summarization. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 434441,
Barcelona, Spain.
</reference>
<page confidence="0.9763">
815
</page>
<figure confidence="0.252506">
\x0c&amp;apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.573002">
<note confidence="0.845662666666667">b&amp;apos;Proceedings of ACL-08: HLT, pages 807815, Columbus, Ohio, USA, June 2008. c 2008 Association for Computational Linguistics</note>
<title confidence="0.982568">An Unsupervised Approach to Biography Production using Wikipedia</title>
<author confidence="0.994871">Fadi Biadsy</author>
<author confidence="0.994871">Julia Hirschberg</author>
<author confidence="0.994871">Elena Filatova</author>
<affiliation confidence="0.99984">Department of Computer Science</affiliation>
<address confidence="0.883278">Columbia University, New York, NY 10027, USA</address>
<email confidence="0.999668">fadi@cs.columbia.edu</email>
<email confidence="0.999668">julia@cs.columbia.edu</email>
<affiliation confidence="0.99615">InforSense LLC</affiliation>
<address confidence="0.999898">Cambridge, MA 02141, USA</address>
<email confidence="0.999922">efilatova@inforsense.com</email>
<abstract confidence="0.996340647058824">We describe an unsupervised approach to multi-document sentence-extraction based summarization for the task of producing biographies. We utilize Wikipedia to automatically construct a corpus of biographical sentences and TDT4 to construct a corpus of non-biographical sentences. We build a biographical-sentence classifier from these corpora and an SVM regression model for sentence ordering from the Wikipedia corpus. We evaluate our work on the DUC2004 evaluation data and with human judges. Overall, our system significantly outperforms all systems that participated in DUC2004, according to the ROUGE-L metric, and is preferred by human subjects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Catching the drift: Probabilistic content models, with applications to generation and summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="11960" citStr="Barzilay and Lee, 2004" startWordPosition="1878" endWordPosition="1881">ap (BlairGoldensohn et al., 2004b) to cluster the sentences classified as biographical by our classifier, and then select the sentence from each cluster that maximizes the confidence score returned by the classifier as the representative for that cluster. 4.3 Sentence Reordering It is essential for MDS systems in the extraction framework to choose the order in which sentences 809 \x0cshould be presented in the final summary. Presenting more important information earlier in a summary is a general strategy for most domains, although importance may be difficult to determine reliably. Similar to (Barzilay and Lee, 2004), we automatically learn how to order our biographical sentences by observing the typical order of presentation of information in a particular domain. We observe that our Wikipedia biographies tend to follow a general presentation template, in which birth information is mentioned before death information, information about current professional position and affiliations usually appear early in the biography, and nuclear family members are typically mentioned before more distant relations. Learning how to order information from these biographies however would require that we learn to identify pa</context>
<context position="29167" citStr="Barzilay and Lee (2004)" startWordPosition="4676" endWordPosition="4679">anually created biographies, including Wikipedia, to estimate the conditional distribution of relevant material given a kernel fact and a background corpus. The problem of ordering sentences and preserving coherence in MDS is addressed by Barzilay et al. (2001), who combine chronological ordering of events with cohesion metrics. SVM regression has recently been used by (Li et al., 2007) for sentence ranking for general MDS. The authors calculated a similarity score for each sentence to the human summaries and then regress numeric features (e.g., the centroid) from each sentence to this score. Barzilay and Lee (2004) use HMMs to capture topic shift within a particular domain; sequence of topic shifts then guides the subsequent ordering of sentences within the summary. 7 Discussion and Future Work In this paper, we describe a MDS system for producing biographies, given a target name. We present an unsupervised approach using Wikipedia biography pages and a general news corpus (TDT4) to automatically construct training data for our system. We employ a NE tagger and a coreference resolution system to extract class-based and lexical features from each sentence which we use to train a binary classifier to iden</context>
</contexts>
<marker>Barzilay, Lee, 2004</marker>
<rawString>\x0cReferences Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
<author>Kathleen McKeown</author>
</authors>
<title>Sentence ordering in multidocument summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of the First Human Language Technology Conference,</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="28805" citStr="Barzilay et al. (2001)" startWordPosition="4615" endWordPosition="4619">nguistic and semantic processing. Linguistic features 813 \x0care derived from predicate-argument structures deduced from parse trees, and semantic features are the set of biography-related relations and events defined in the ACE guidelines (Doddington et al., 2004). Sentences containing kernel facts are ranked using probabilities estimated from a corpus of manually created biographies, including Wikipedia, to estimate the conditional distribution of relevant material given a kernel fact and a background corpus. The problem of ordering sentences and preserving coherence in MDS is addressed by Barzilay et al. (2001), who combine chronological ordering of events with cohesion metrics. SVM regression has recently been used by (Li et al., 2007) for sentence ranking for general MDS. The authors calculated a similarity score for each sentence to the human summaries and then regress numeric features (e.g., the centroid) from each sentence to this score. Barzilay and Lee (2004) use HMMs to capture topic shift within a particular domain; sequence of topic shifts then guides the subsequent ordering of sentences within the summary. 7 Discussion and Future Work In this paper, we describe a MDS system for producing </context>
</contexts>
<marker>Barzilay, Elhadad, McKeown, 2001</marker>
<rawString>Regina Barzilay, Noemie Elhadad, and Kathleen McKeown. 2001. Sentence ordering in multidocument summarization. In Proceedings of the First Human Language Technology Conference, San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Biryukov</author>
<author>Roxana Angheluta</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Multidocument question answering text summarization using topic signatures.</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th Dutch-Belgium Information Retrieval Workshop,</booktitle>
<location>Utrecht, the Netherlands.</location>
<contexts>
<context position="27010" citStr="Biryukov et al. (2005)" startWordPosition="4356" endWordPosition="4359">ur top-DUC2004 in Section 5, was Blair-Goldensohn et al. (2004a)s DefScriber, which treats Who is X? as a definition question and targets definitional themes (e.g. genus-species) found in the input document collections which include references to the target person. Extracted sentences are then rewritten using a reference rewriting system (Nenkova and McKeown, 2003) which attempts to shorten subsequent references to the target. Sentences are ordered in the summary based on a weighted combination of topic centrality, lexical cohesion, and topic coverage scores. A similar approach is explored in Biryukov et al. (2005), which uses Topic Signatures (Lin and Hovy, 2000) constructed around the target individuals name to identify sentences to be included in the biography. Zhou et al. (2004)s biography generation system, like ours, trains biographical and non-biographical sentence classifiers to select sentences to be included in the biography. Their system is trained on a handannotated corpus of 130 biographies of 12 people, tagged with 9 biographical elements (e.g., bio, education, nationality) and uses binary unigram and bigram lexical and unigram part-of-speech features for classification. Duboue et al. (200</context>
</contexts>
<marker>Biryukov, Angheluta, Moens, 2005</marker>
<rawString>Maria Biryukov, Roxana Angheluta, and Marie-Francine Moens. 2005. Multidocument question answering text summarization using topic signatures. In Proceedings of the 5th Dutch-Belgium Information Retrieval Workshop, Utrecht, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen McKeown</author>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
<author>Barry Schiffman</author>
<author>Andrew Schlaikjer</author>
</authors>
<date>2004</date>
<booktitle>Advaith Siddharthan, and Sergey Siegelman. 2004a.</booktitle>
<institution>Columbia University at DUC</institution>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="18104" citStr="Blair-Goldensohn et al., 2004" startWordPosition="2906" endWordPosition="2909">r system against the top performing of these 22 systems, according to ROUGEL, which we denote top-DUC2004.4 5.1 Automatic Evaluation Using ROUGE As noted in Section 4.1, we experimented with a number of learning algorithms when building our biographical-sentence classifier. For each machine learning algorithm tested, we build a system that initially classifies the input list of sentences into biographical and non-biographical sentences and then 3 http://duc.nist.gov/duc2004 4 Note that this system out-performed 19 of the 22 systems on ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2 (p &amp;lt; .05) (Blair-Goldensohn et al., 2004a). No ROUGE metric produced scores where this system scored significantly worse than any other system. See Figure 2 below for a comparison of all DUC2004 systems with our top system where all systems are evaluated using ROUGE-L-1.5.5. removes redundant sentences. Next, we produce three versions of each system: one which implements a baseline ordering procedure, in which sentences from the clusters are ordered by their appearance in their source document (e.g. any sentence which occurred first in its original document is placed first in the summary, with ties ordered randomly within the set), </context>
<context position="26450" citStr="Blair-Goldensohn et al. (2004" startWordPosition="4268" endWordPosition="4271">nd inter-rater agreement for these judgments using Fleiss kappa (Fleiss, 1971) to be only moderate (=0.362). However, when we computed the majority vote for each question, we found that 61.7% (29/47) preferred the SVM regression ordering over the MNB classifier confidence score ordering. Although this difference is not statistically significant, again we find the SVM regression ordering results encouraging enough to motivate our further research on improving such ordering procedures. 6 Related Work The DUC2004 system achieving the highest overall ROUGE score, our top-DUC2004 in Section 5, was Blair-Goldensohn et al. (2004a)s DefScriber, which treats Who is X? as a definition question and targets definitional themes (e.g. genus-species) found in the input document collections which include references to the target person. Extracted sentences are then rewritten using a reference rewriting system (Nenkova and McKeown, 2003) which attempts to shorten subsequent references to the target. Sentences are ordered in the summary based on a weighted combination of topic centrality, lexical cohesion, and topic coverage scores. A similar approach is explored in Biryukov et al. (2005), which uses Topic Signatures (Lin and H</context>
</contexts>
<marker>Blair-Goldensohn, Evans, Hatzivassiloglou, McKeown, Nenkova, Passonneau, Schiffman, Schlaikjer, 2004</marker>
<rawString>Sasha Blair-Goldensohn, David Evans, Vasileios Hatzivassiloglou, Kathleen McKeown, Ani Nenkova, Rebecca Passonneau, Barry Schiffman, Andrew Schlaikjer, Advaith Siddharthan, and Sergey Siegelman. 2004a. Columbia University at DUC 2004. In Proceedings of the 4th Document Understanding Conference, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Blair-Goldensohn</author>
<author>Kathy McKeown</author>
<author>Andrew Schlaikjer</author>
</authors>
<title>Answering definitional questions: A hybrid approach.</title>
<date>2004</date>
<booktitle>New Directions In Question Answering, chapter 4.</booktitle>
<editor>In Mark Maybury, editor,</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="18104" citStr="Blair-Goldensohn et al., 2004" startWordPosition="2906" endWordPosition="2909">r system against the top performing of these 22 systems, according to ROUGEL, which we denote top-DUC2004.4 5.1 Automatic Evaluation Using ROUGE As noted in Section 4.1, we experimented with a number of learning algorithms when building our biographical-sentence classifier. For each machine learning algorithm tested, we build a system that initially classifies the input list of sentences into biographical and non-biographical sentences and then 3 http://duc.nist.gov/duc2004 4 Note that this system out-performed 19 of the 22 systems on ROUGE-1 and 20 of 22 on ROUGE-L and ROUGE-W-1.2 (p &amp;lt; .05) (Blair-Goldensohn et al., 2004a). No ROUGE metric produced scores where this system scored significantly worse than any other system. See Figure 2 below for a comparison of all DUC2004 systems with our top system where all systems are evaluated using ROUGE-L-1.5.5. removes redundant sentences. Next, we produce three versions of each system: one which implements a baseline ordering procedure, in which sentences from the clusters are ordered by their appearance in their source document (e.g. any sentence which occurred first in its original document is placed first in the summary, with ties ordered randomly within the set), </context>
<context position="26450" citStr="Blair-Goldensohn et al. (2004" startWordPosition="4268" endWordPosition="4271">nd inter-rater agreement for these judgments using Fleiss kappa (Fleiss, 1971) to be only moderate (=0.362). However, when we computed the majority vote for each question, we found that 61.7% (29/47) preferred the SVM regression ordering over the MNB classifier confidence score ordering. Although this difference is not statistically significant, again we find the SVM regression ordering results encouraging enough to motivate our further research on improving such ordering procedures. 6 Related Work The DUC2004 system achieving the highest overall ROUGE score, our top-DUC2004 in Section 5, was Blair-Goldensohn et al. (2004a)s DefScriber, which treats Who is X? as a definition question and targets definitional themes (e.g. genus-species) found in the input document collections which include references to the target person. Extracted sentences are then rewritten using a reference rewriting system (Nenkova and McKeown, 2003) which attempts to shorten subsequent references to the target. Sentences are ordered in the summary based on a weighted combination of topic centrality, lexical cohesion, and topic coverage scores. A similar approach is explored in Biryukov et al. (2005), which uses Topic Signatures (Lin and H</context>
</contexts>
<marker>Blair-Goldensohn, McKeown, Schlaikjer, 2004</marker>
<rawString>Sasha Blair-Goldensohn, Kathy McKeown, and Andrew Schlaikjer. 2004b. Answering definitional questions: A hybrid approach. In Mark Maybury, editor, New Directions In Question Answering, chapter 4. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit.</title>
<date>1968</date>
<volume>70</volume>
<pages>213220</pages>
<contexts>
<context position="24159" citStr="Cohen, 1968" startWordPosition="3893" endWordPosition="3894">to decide which summary was more responsive in form and content to the question or whether both were equally responsive. 85.3% (128/150) of subject judgments preferred one summary over the other. 100/128 (78.1%) of these judgments preferred the summaries produced by our MNB system over those produced by top-DUC2004. If we compute the majority vote, there were 42/50 summaries in which at least two subjects made the same choice. 37/42 (88.1%) of these majority judgments preferred our systems summary (using binomial test, p = 4.4e7). We used the weighted kappa statistic with quadratic weighting (Cohen, 1968) to determine the inter-rater agreement, obtaining a mean pairwise of 0.441. Recall from Section 5.1 that our SVM regression reordering component slightly decreases the average ROUGE score (although not significantly) for our MNB system. For our human evaluations, we decided to evaluate the quality of the presentation of our summaries with and without this compo7 Note that top-DUC2004 was ranked sixth in the DUC 2004 manual evaluation, with no system performing significantly better for coverage and only 1 system performing significantly better for responsiveness. 812 \x0cROUGE-L Average_F 0.2 </context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>J. Cohen. 1968. Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial credit. volume 70, pages 213220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The automatic content extraction program - tasks, data, and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC Conference, Canary Islands,</booktitle>
<location>Spain,</location>
<contexts>
<context position="28449" citStr="Doddington et al., 2004" startWordPosition="4562" endWordPosition="4565">s of free-text biographies about the same celebrities. Filatova et al. (2005) learn text features typical of biographical descriptions by deducing biographical and occupation-related activities automatically by compariing descriptions of people with different occupations. Weischedel et al. (2004) models kernel-fact features typical for biographies using linguistic and semantic processing. Linguistic features 813 \x0care derived from predicate-argument structures deduced from parse trees, and semantic features are the set of biography-related relations and events defined in the ACE guidelines (Doddington et al., 2004). Sentences containing kernel facts are ranked using probabilities estimated from a corpus of manually created biographies, including Wikipedia, to estimate the conditional distribution of relevant material given a kernel fact and a background corpus. The problem of ordering sentences and preserving coherence in MDS is addressed by Barzilay et al. (2001), who combine chronological ordering of events with cohesion metrics. SVM regression has recently been used by (Li et al., 2007) for sentence ranking for general MDS. The authors calculated a similarity score for each sentence to the human summ</context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction program - tasks, data, and evaluation. In Proceedings of the LREC Conference, Canary Islands, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Duboue</author>
<author>Kathleen McKeown</author>
</authors>
<title>Statistical acquisition of content selection rules for natural language generation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing,</booktitle>
<pages>121128</pages>
<location>Sapporo, Japan,</location>
<marker>Duboue, McKeown, 2003</marker>
<rawString>Pablo Duboue and Kathleen McKeown. 2003. Statistical acquisition of content selection rules for natural language generation. In Proceedings of the Conference on Empirical Methods for Natural Language Processing, pages 121128, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>John Prager</author>
</authors>
<title>Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>113120</pages>
<location>Vancouver, Canada,</location>
<marker>Filatova, Prager, 2005</marker>
<rawString>Elena Filatova and John Prager. 2005. Tell me what you do and Ill tell you what you are: Learning occupation-related activities for biographies. In Proceedings of the Joint Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 113120, Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<volume>76</volume>
<pages>378382</pages>
<contexts>
<context position="25899" citStr="Fleiss, 1971" startWordPosition="4185" endWordPosition="4186">ce score obtained by the MNB, in decreasing order, and the other ordered by the SVM regression values, in increasing order. Note that, in three cases, the summary sentences were ordered identically by both procedures, so we used only 47 summaries for this evaluation. Three (different) native American English speakers were presented with the 47 questions for which sentence ordering differed. For each question they were given the two summaries (presented in random order) and asked to determine which biography they preferred. We found inter-rater agreement for these judgments using Fleiss kappa (Fleiss, 1971) to be only moderate (=0.362). However, when we computed the majority vote for each question, we found that 61.7% (29/47) preferred the SVM regression ordering over the MNB classifier confidence score ordering. Although this difference is not statistically significant, again we find the SVM regression ordering results encouraging enough to motivate our further research on improving such ordering procedures. 6 Related Work The DUC2004 system achieving the highest overall ROUGE score, our top-DUC2004 in Section 5, was Blair-Goldensohn et al. (2004a)s DefScriber, which treats Who is X? as a defin</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>J. L. Fleiss. 1971. Measuring nominal scale agreement among many raters. volume 76, No. 5, pages 378382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<title>Nyus english ace 2005 system description.</title>
<date>2005</date>
<booktitle>In ACE 05 Evaluation Workshop,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="3458" citStr="Grishman et al., 2005" startWordPosition="522" endWordPosition="525"> task (task 5) test set. In Section 6 we compare our research with previous work on biography generation. We conclude in Section 7 and identify directions for future research. 2 System Overview In this section, we present an overview of our biography extraction system. We assume as input a set of documents retrieved by an information retrieval engine from a query consisting of the name of the person for whom the biography is desired. We further assume that these documents have been tagged with Named Entities (NE)s with coreferences resolved 807 \x0cusing a system such as NYUs 2005 ACE system (Grishman et al., 2005), which we used for our experiments. Our task is to produce a concise biography from these documents. First, we need to select the most important biographical sentences for the target person. To do so, we first extract from the input documents all sentences that contain some reference to the target person according to the coreference assignment algorithm; this reference may be the targets name or a coreferential full NP or pronominal referring expression, such as the President or he. We call these sentences hypothesis sentences. We hypothesize that most biographical sentences will contain a re</context>
<context position="6518" citStr="Grishman et al., 2005" startWordPosition="1011" endWordPosition="1015">t download the xml version of Wikipedia and extract only the documents whose authors used the Wikipedia biography template when creating their biography. There are 16,906 biographies in Wikipedia that used this template. We next apply simple text processing techniques to clean the text. We select at most the first 150 sentences from each page, to avoid sentences that are not critically important to the biography. For each of these sentences we perform the following steps: 1. We identify the biographys subject from its title, terming this name the target person. 2. We run NYUs 2005 ACE system (Grishman et al., 2005) to tag NEs and do coreference resolution. There are 43 unique NE tags in our corpora, including PER Individual, ORG Educational, and so on, and TIMEX tags for all dates. 3. For each sentence, we replace each NE by its tag name and type ([name-type subtype]) as assigned by the NYU tagger. This modified sentence we term a class-based/lexical sentence. 4. Each non-pronominal referring expression (e.g., George W. Bush, the US president) that is tagged as coreferential with the target person is replaced by our own [TARGET PER] tag and every pronoun P that refers to the target person is replaced by</context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook, and Adam Meyers. 2005. Nyus english ace 2005 system description. In ACE 05 Evaluation Workshop, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujian Li</author>
<author>You Ouyang</author>
<author>Wei Wang</author>
<author>Bin Sun</author>
</authors>
<title>Multi-document summarization using support vector regression.</title>
<date>2007</date>
<booktitle>In http://duc.nist.gov/pubs/2007papers.</booktitle>
<contexts>
<context position="28933" citStr="Li et al., 2007" startWordPosition="4638" endWordPosition="4641"> and semantic features are the set of biography-related relations and events defined in the ACE guidelines (Doddington et al., 2004). Sentences containing kernel facts are ranked using probabilities estimated from a corpus of manually created biographies, including Wikipedia, to estimate the conditional distribution of relevant material given a kernel fact and a background corpus. The problem of ordering sentences and preserving coherence in MDS is addressed by Barzilay et al. (2001), who combine chronological ordering of events with cohesion metrics. SVM regression has recently been used by (Li et al., 2007) for sentence ranking for general MDS. The authors calculated a similarity score for each sentence to the human summaries and then regress numeric features (e.g., the centroid) from each sentence to this score. Barzilay and Lee (2004) use HMMs to capture topic shift within a particular domain; sequence of topic shifts then guides the subsequent ordering of sentences within the summary. 7 Discussion and Future Work In this paper, we describe a MDS system for producing biographies, given a target name. We present an unsupervised approach using Wikipedia biography pages and a general news corpus </context>
</contexts>
<marker>Li, Ouyang, Wang, Sun, 2007</marker>
<rawString>Sujian Li, You Ouyang, Wei Wang, and Bin Sun. 2007. Multi-document summarization using support vector regression. In http://duc.nist.gov/pubs/2007papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>The automated acquisition of topic signatures for text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>495501</pages>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="27060" citStr="Lin and Hovy, 2000" startWordPosition="4364" endWordPosition="4367">al. (2004a)s DefScriber, which treats Who is X? as a definition question and targets definitional themes (e.g. genus-species) found in the input document collections which include references to the target person. Extracted sentences are then rewritten using a reference rewriting system (Nenkova and McKeown, 2003) which attempts to shorten subsequent references to the target. Sentences are ordered in the summary based on a weighted combination of topic centrality, lexical cohesion, and topic coverage scores. A similar approach is explored in Biryukov et al. (2005), which uses Topic Signatures (Lin and Hovy, 2000) constructed around the target individuals name to identify sentences to be included in the biography. Zhou et al. (2004)s biography generation system, like ours, trains biographical and non-biographical sentence classifiers to select sentences to be included in the biography. Their system is trained on a handannotated corpus of 130 biographies of 12 people, tagged with 9 biographical elements (e.g., bio, education, nationality) and uses binary unigram and bigram lexical and unigram part-of-speech features for classification. Duboue et al. (2003) also address the problem of learning content se</context>
</contexts>
<marker>Lin, Hovy, 2000</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In Proceedings of the 18th International Conference on Computational Linguistics, pages 495501, Saarbrucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Language Technology Conference,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="19033" citStr="Lin and Hovy, 2003" startWordPosition="3060" endWordPosition="3063">m: one which implements a baseline ordering procedure, in which sentences from the clusters are ordered by their appearance in their source document (e.g. any sentence which occurred first in its original document is placed first in the summary, with ties ordered randomly within the set), a second which orders the biographical sentences by the confidence score obtained from the classifier, and a third which uses the SVM regression as the reordering component. Finally, we run our reference rewriting component on each and trim the output to 665 bytes. We evaluate first using the ROUGE-L metric (Lin and Hovy, 2003) with a 95% (ROUGE computed) confidence interval for all systems and compared these to the ROUGE-L score of the best-performing DUC2004 system.5 The higher the ROUGE score, the closer the summary is to the DUC2004 human reference summaries. As shown in Figure 1, our best performing system is the multinomial nave Bayes classifier (MNB) using the classifier confidence scores to order the sentences in the biography. This system significantly outperforms the top ranked DUC2004 system (top-DUC2004).6 The success of this particularly learning algorithm on our task may be due to: (1) the nature of ou</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 Language Technology Conference, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>References to named entities: A corpus study.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Human Language Technology Conference and North American chapter of the Association for Computational Linguistics Annual Meeting,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="26755" citStr="Nenkova and McKeown, 2003" startWordPosition="4313" endWordPosition="4316">rence is not statistically significant, again we find the SVM regression ordering results encouraging enough to motivate our further research on improving such ordering procedures. 6 Related Work The DUC2004 system achieving the highest overall ROUGE score, our top-DUC2004 in Section 5, was Blair-Goldensohn et al. (2004a)s DefScriber, which treats Who is X? as a definition question and targets definitional themes (e.g. genus-species) found in the input document collections which include references to the target person. Extracted sentences are then rewritten using a reference rewriting system (Nenkova and McKeown, 2003) which attempts to shorten subsequent references to the target. Sentences are ordered in the summary based on a weighted combination of topic centrality, lexical cohesion, and topic coverage scores. A similar approach is explored in Biryukov et al. (2005), which uses Topic Signatures (Lin and Hovy, 2000) constructed around the target individuals name to identify sentences to be included in the biography. Zhou et al. (2004)s biography generation system, like ours, trains biographical and non-biographical sentence classifiers to select sentences to be included in the biography. Their system is t</context>
</contexts>
<marker>Nenkova, McKeown, 2003</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2003. References to named entities: A corpus study. In Proceedings of the Joint Human Language Technology Conference and North American chapter of the Association for Computational Linguistics Annual Meeting, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Jinxi Xu</author>
<author>Ana Licuanan</author>
</authors>
<title>A hybrid approach to answering biographical questions.</title>
<date>2004</date>
<booktitle>New Directions In Question Answering, chapter 5.</booktitle>
<editor>In Mark Maybury, editor,</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="28122" citStr="Weischedel et al. (2004)" startWordPosition="4517" endWordPosition="4520">ses binary unigram and bigram lexical and unigram part-of-speech features for classification. Duboue et al. (2003) also address the problem of learning content selection rules for biography. They learn rules from two corpora, a semi-structured corpus with lists of biographical facts about show business celebrities and a corpus of free-text biographies about the same celebrities. Filatova et al. (2005) learn text features typical of biographical descriptions by deducing biographical and occupation-related activities automatically by compariing descriptions of people with different occupations. Weischedel et al. (2004) models kernel-fact features typical for biographies using linguistic and semantic processing. Linguistic features 813 \x0care derived from predicate-argument structures deduced from parse trees, and semantic features are the set of biography-related relations and events defined in the ACE guidelines (Doddington et al., 2004). Sentences containing kernel facts are ranked using probabilities estimated from a corpus of manually created biographies, including Wikipedia, to estimate the conditional distribution of relevant material given a kernel fact and a background corpus. The problem of orderi</context>
</contexts>
<marker>Weischedel, Xu, Licuanan, 2004</marker>
<rawString>Ralph Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A hybrid approach to answering biographical questions. In Mark Maybury, editor, New Directions In Question Answering, chapter 5. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
<author>L Trigg</author>
<author>M Hall</author>
<author>G Holmes</author>
<author>S Cunningham</author>
</authors>
<title>Weka: Practical machine learning tools and techniques with java implementation.</title>
<date>1999</date>
<booktitle>In International Workshop: Emerging Knowledge Engineering and Connectionist-Based Information Systems,</booktitle>
<pages>192196</pages>
<contexts>
<context position="10298" citStr="Witten et al., 1999" startWordPosition="1624" endWordPosition="1627">mined empirically for Classifier Accuracy F-Measure SVM 87.6% 0.87 M. nave Bayes 84.1% 0.84 C4.5 81.8% 0.82 Table 1: Binary classification results: Wikipedia biography class-based/lexical sentences vs. TDT4 classbased/lexical sentences each feature type. This process identifies features that significantly contribute to the classification task. We extract 3K class-based/lexical unigrams, 5.5K bigrams, 3K trigrams, 20 POS unigrams, and 166 POS bigrams. Using the training data described above, we experimented with three different classification algorithms using the Weka machine learning toolkit (Witten et al., 1999): multinomial nave Bayes, SVM with linear kernel, and C4.5. Weka also provides a classification confidence score that represents how confident the classifier is on each classified sample, which we will make use of as well. Table 1 presents the classification results on our 4,216 held-out test-set sentences. These results are quite promising. However, we should note that they may not necessarily represent the successful classification of biographical vs. non-biographical sentences but rather the classification of Wikipedia sentences vs. TDT4 sentences. We will validate these results for our ful</context>
</contexts>
<marker>Witten, Frank, Trigg, Hall, Holmes, Cunningham, 1999</marker>
<rawString>I. Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, and S. Cunningham. 1999. Weka: Practical machine learning tools and techniques with java implementation. In International Workshop: Emerging Knowledge Engineering and Connectionist-Based Information Systems, pages 192196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Miruna Ticrea</author>
<author>Eduard Hovy</author>
</authors>
<title>Multi-document biography summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>434441</pages>
<location>Barcelona,</location>
<contexts>
<context position="5289" citStr="Zhou et al., 2004" startWordPosition="822" endWordPosition="825">n biographies. Finally, the first reference to the target person in the initial sentence in the reordering is rewritten using the longest coreference in our hypothesis sentences which contains the targets full name. We then trim the output to a threshold to produce a biography of a certain length for evaluation against the DUC2004 systems. 3 Training Data One of the difficulties inherent in automatic biography generation is the lack of training data. One might collect training data by manually annotating a suitable corpus containing biographical and nonbiographical data about a person, as in (Zhou et al., 2004). However, such annotation is labor intensive. To avoid this problem, we adopt an unsupervised approach. We use Wikipedia biographies as our corpus of biographical sentences. We collect our nonbiographical sentences from the English newswire documents in the TDT4 corpus.1 While each corpus 1 http://projects.ldc.upenn.edu/TDT4 may contain positive and negative examples, we assume that most sentences in Wikipedia biographies are biographical and that the majority of TDT4 sentences are non-biographical. 3.1 Constructing the Biographical Corpus To automatically collect our biographical sentences, </context>
<context position="27181" citStr="Zhou et al. (2004)" startWordPosition="4383" endWordPosition="4386">es) found in the input document collections which include references to the target person. Extracted sentences are then rewritten using a reference rewriting system (Nenkova and McKeown, 2003) which attempts to shorten subsequent references to the target. Sentences are ordered in the summary based on a weighted combination of topic centrality, lexical cohesion, and topic coverage scores. A similar approach is explored in Biryukov et al. (2005), which uses Topic Signatures (Lin and Hovy, 2000) constructed around the target individuals name to identify sentences to be included in the biography. Zhou et al. (2004)s biography generation system, like ours, trains biographical and non-biographical sentence classifiers to select sentences to be included in the biography. Their system is trained on a handannotated corpus of 130 biographies of 12 people, tagged with 9 biographical elements (e.g., bio, education, nationality) and uses binary unigram and bigram lexical and unigram part-of-speech features for classification. Duboue et al. (2003) also address the problem of learning content selection rules for biography. They learn rules from two corpora, a semi-structured corpus with lists of biographical facts</context>
</contexts>
<marker>Zhou, Ticrea, Hovy, 2004</marker>
<rawString>Liang Zhou, Miruna Ticrea, and Eduard Hovy. 2004. Multi-document biography summarization. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 434441, Barcelona, Spain.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>