Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 867–876,
Honolulu, October 2008. c
2008 Association for Computational Linguistics
Sparse Multi-Scale Grammars
for Discriminative Latent Variable Parsing
Slav Petrov and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{petrov, klein}@eecs.berkeley.edu
Abstract
We present a discriminative, latent variable
approach to syntactic parsing in which rules
exist at multiple scales of refinement. The
model is formally a latent variable CRF gram-
mar over trees, learned by iteratively splitting
grammar productions (not categories). Dif-
ferent regions of the grammar are refined to
different degrees, yielding grammars which
are three orders of magnitude smaller than
the single-scale baseline and 20 times smaller
than the split-and-merge grammars of Petrov
et al. (2006). In addition, our discriminative
approach integrally admits features beyond lo-
cal tree configurations. We present a multi-
scale training method along with an efficient
CKY-style dynamic program. On a variety of
domains and languages, this method produces
the best published parsing accuracies with the
smallest reported grammars.
1 Introduction
In latent variable approaches to parsing (Matsuzaki
et al., 2005; Petrov et al., 2006), one models an ob-
served treebank of coarse parse trees using a gram-
mar over more refined, but unobserved, derivation
trees. The parse trees represent the desired output
of the system, while the derivation trees represent
the typically much more complex underlying syntac-
tic processes. In recent years, latent variable meth-
ods have been shown to produce grammars which
are as good as, or even better than, earlier parsing
work (Collins, 1999; Charniak, 2000). In particular,
in Petrov et al. (2006) we exhibited a very accurate
category-splitting approach, in which a coarse ini-
tial grammar is refined by iteratively splitting each
grammar category into two subcategories using the
EM algorithm. Of course, each time the number of
grammar categories is doubled, the number of bi-
nary productions is increased by a factor of eight.
As a result, while our final grammars used few cat-
egories, the number of total active (non-zero) pro-
ductions was still substantial (see Section 7). In ad-
dition, it is reasonable to assume that some genera-
tively learned splits have little discriminative utility.
In this paper, we present a discriminative approach
which addresses both of these limitations.
We introduce multi-scale grammars, in which
some productions reference fine categories, while
others reference coarse categories (see Figure 2).
We use the general framework of hidden variable
CRFs (Lafferty et al., 2001; Koo and Collins, 2005),
where gradient-based optimization maximizes the
likelihood of the observed variables, here parse
trees, summing over log-linearly scored derivations.
With multi-scale grammars, it is natural to refine
productions rather than categories. As a result, a
category such as NP can be complex in some re-
gions of the grammar while remaining simpler in
other regions. Additionally, we exploit the flexibility
of the discriminative framework both to improve the
treatment of unknown words as well as to include
span features (Taskar et al., 2004), giving the bene-
fit of some input features integrally in our dynamic
program. Our multi-scale grammars are 3 orders
of magnitude smaller than the fully-split baseline
grammar and 20 times smaller than the generative
split-and-merge grammars of Petrov et al. (2006).
867
In addition, we exhibit the best parsing numbers on
several metrics, for several domains and languages.
Discriminative parsing has been investigated be-
fore, such as in Johnson (2001), Clark and Curran
(2004), Henderson (2004), Koo and Collins (2005),
Turian et al. (2007), Finkel et al. (2008), and, most
similarly, in Petrov and Klein (2008). However, in
all of these cases, the final parsing performance fell
short of the best generative models by several per-
centage points or only short sentences were used.
Only in combination with a generative model was
a discriminative component able to produce high
parsing accuracies (Charniak and Johnson, 2005;
Huang, 2008). Multi-scale grammars, in contrast,
give higher accuracies using smaller grammars than
previous work in this direction, outperforming top
generative models in grammar size and in parsing
accuracy.
2 Latent Variable Parsing
Treebanks are typically not annotated with fully de-
tailed syntactic structure. Rather, they present only
a coarse trace of the true underlying processes. As
a result, learning a grammar for parsing requires
the estimation of a more highly articulated model
than the naive CFG embodied by such treebanks.
A manual approach might take the category NP and
subdivide it into one subcategory NPˆS for subjects
and another subcategory NPˆVP for objects (John-
son, 1998; Klein and Manning, 2003). However,
rather than devising linguistically motivated features
or splits, latent variable parsing takes a fully auto-
mated approach, in which each symbol is split into
unconstrained subcategories.
2.1 Latent Variable Grammars
Latent variable grammars augment the treebank
trees with latent variables at each node. This cre-
ates a set of (exponentially many) derivations over
split categories for each of the original parse trees
over unsplit categories. For each observed category
A we now have a set of latent subcategories Ax. For
example, NP might be split into NP1 through NP8.
The parameters of the refined productions
Ax → By Cz, where Ax is a subcategory of A, By
of B, and Cz of C, can then be estimated in var-
ious ways; past work has included both generative
(Matsuzaki et al., 2005; Liang et al., 2007) and dis-
criminative approaches (Petrov and Klein, 2008).
We take the discriminative log-linear approach here.
Note that the comparison is only between estimation
methods, as Smith and Johnson (2007) show that the
model classes are the same.
2.2 Log-Linear Latent Variable Grammars
In a log-linear latent variable grammar, each pro-
duction r = Ax → By Cz is associated with a
multiplicative weight φr (Johnson, 2001; Petrov and
Klein, 2008) (sometimes we will use the log-weight
θr when convenient). The probability of a derivation
t of a sentence w is proportional to the product of the
weights of its productions r:
P(t|w) ∝
Y
r∈t
φr
The score of a parse T is then the sum of the scores
of its derivations:
P(T|w) =
X
t∈T
P(t|w)
3 Hierarchical Refinement
Grammar refinement becomes challenging when the
number of subcategories is large. If each category
is split into k subcategories, each (binary) produc-
tion will be split into k3. The resulting memory lim-
itations alone can prevent the practical learning of
highly split grammars (Matsuzaki et al., 2005). This
issue was partially addressed in Petrov et al. (2006),
where categories were repeatedly split and some
splits were re-merged if the gains were too small.
However, while the grammars are indeed compact
at the (sub-)category level, they are still dense at the
production level, which we address here.
As in Petrov et al. (2006), we arrange our subcat-
egories into a hierarchy, as shown in Figure 1. In
practice, the construction of the hierarchy is tightly
coupled to a split-based learning process (see Sec-
tion 5). We use the naming convention that an origi-
nal category A becomes A0 and A1 in the first round;
A0 then becoming A00 and A01 in the second round,
and so on. We will use x̂ ≻ x to indicate that the
subscript or subcategory x is a refinement of x̂.1 We
1
Conversely, x̂ is a coarser version of x, or, in the language
of Petrov and Klein (2007), x̂ is a projection of x.
868
+ 7 . 3
+ 5 . 0
+ 7 . 3
+ 1 2
+ 2 . 1
S i n g l e  s c a l e p r o d u c t i o n s
+ 5 . 0
+ 5 . 0
+ 7 . 3
+ 2 . 1
+ 2 . 1
+ 2 . 1
+ 2 . 1
M
u l t i  s c a l e p r o d u c t i o n s
+ 2 . 1
+ 1 2
+ 5 . 0
→
0 0
1
0 1 0 0 1 1
+ 5 . 0 + 5 . 0 + 7 . 3 + 1 2
→
0 0 0 1 0 0 1 0 1 1 1 0 1 1 1
0 0 1 0 1 0 0 1 1
+ 2 . 1 + 2 . 1
+ 2 . 1
+ 2 . 1
θr̄ θr̂
0 0
0
*
1
1 0
0 1 1 1
0
*
0 1
D T 0 0 0 → t h e
D T 0 0 1 → t h e
D T 0 1 0 → t h e
D T 0 1 1 → t h e
D T 1 0 0 → t h e
D T 1 0 1 → t h e
D T 1 1 0 → t h e
D T 1 1 1 → t h e
D T 0 0 → t h e
D T 0 1 0 → t h e
D T 0 1 1 → t h e
D T 1 → t h e
}
}
+ 1 2
Figure 1: Multi-scale refinement of the DT → the production. The multi-scale grammar can be encoded much more
compactly than the equally expressive single scale grammar by using only the shaded features along the fringe.
will also say that x̂ dominates x, and x will refer to
fully refined subcategories. The same terminology
can be applied to (binary) productions, which split
into eight refinements each time the subcategories
are split in two.
The core observation leading to multi-scale gram-
mars is that when we look at the refinements of a
production, many are very similar in weight. It is
therefore advantageous to record productions only at
the level where they are distinct from their children
in the hierarchy.
4 Multi-Scale Grammars
A multi-scale grammar is a grammar in which some
productions reference fine categories, while others
reference coarse categories. As an example, con-
sider the multi-scale grammar in Figure 2, where the
NP category has been split into two subcategories
(NP0, NP1) to capture subject and object distinc-
tions. Since it can occur in subject and object po-
sition, the production NP → it has remained unsplit.
In contrast, in a single-scale grammar, two produc-
tions NP0 → it and NP1 → it would have been nec-
essary. We use * as a wildcard, indicating that NP∗
can combine with any other NP, while NP1 can only
combine with other NP1. Whenever subcategories
of different granularity are combined, the resulting
constituent takes the more specific label.
In terms of its structure, a multi-scale grammar is
a set of productions over varyingly refined symbols,
where each production is associated with a weight.
Consider the refinement of the production shown in
Figure 1. The original unsplit production (at top)
would naively be split into a tree of many subpro-
ductions (downward in the diagram) as the grammar
categories are incrementally split. However, it may
be that many of the fully refined productions share
the same weights. This will be especially common
in the present work, where we go out of our way to
achieve it (see Section 5). For example, in Figure 1,
the productions DTx → the have the same weight
for all categories DTx which refine DT1.2 A multi-
scale grammar can capture this behavior with just 4
productions, while the single-scale grammar has 8
productions. For binary productions the savings will
of course be much higher.
In terms of its semantics, a multi-scale grammar is
simply a compact encoding of a fully refined latent
variable grammar, in which identically weighted re-
finements of productions have been collapsed to the
coarsest possible scale. Therefore, rather than at-
tempting to control the degree to which categories
are split, multi-scale grammars simply encode pro-
ductions at varying scales. It is hence natural to
speak of refining productions, while considering
the categories to exist at all degrees of refinement.
Multi-scale grammars enable the use of coarse (even
unsplit) categories in some regions of the grammar,
while requiring very specific subcategories in others,
as needed. As we will see in the following, this flex-
ibility results in a tremendous reduction of grammar
parameters, as well as improved parsing time, be-
cause the vast majority of productions end up only
partially split.
Since a multi-scale grammar has productions
which can refer to different levels of the category
hierarchy, there must be constraints on their coher-
ence. Specifically, for each fully refined produc-
tion, exactly one of its dominating coarse produc-
tions must be in the grammar. More formally, the
multi-scale grammar partitions the space of fully re-
fined base rules such that each r maps to a unique
2
We define dominating productions and refining productions
analogously as for subcategories.
869
i t
s a w
V P 0
N P 1
V *
S *
N P 0
V 0 N P
*
h e r
s a w
V P 0
N P 1
V
*
S *
N P 0
V 0 N P 1
V P 0
N P 1
V
*
S *
V P
*
N P 0
h e r
N P 1
s
h e
N P 0
i t
N P
*
s a w
V 0
L
e x i c o n :
G
r
a m m a
r :
V P * V P *
s
h e
N P 0
i t
N P
*
Figure 2: In multi-scale grammars, the categories exist
at varying degrees of refinement. The grammar in this
example enforces the correct usage of she and her, while
allowing the use of it in both subject and object position.
dominating rule r̂, and for all base rules r′ such that
r̂ ≻ r′, r′ maps to r̂ as well. This constraint is al-
ways satisfied if the multi-scale grammar consists of
fringes of the production refinement hierarchies, in-
dicated by the shading in Figure 1.
A multi-scale grammar straightforwardly assigns
scores to derivations in the corresponding fully re-
fined single scale grammar: simply map each refined
derivation rule to its dominating abstraction in the
multi-scale grammar and give it the corresponding
weight. The fully refined grammar is therefore triv-
ially (though not compactly) reconstructable from
its multi-scale encoding.
It is possible to directly define a derivational se-
mantics for multi-scale grammars which does not
appeal to the underlying single scale grammar.
However, in the present work, we use our multi-
scale grammars only to compute expectations of the
underlying grammars in an efficient, implicit way.
5 Learning Sparse Multi-Scale Grammars
We now consider how to discriminatively learn
multi-scale grammars by iterative splitting produc-
tions. There are two main concerns. First, be-
cause multi-scale grammars are most effective when
many productions share the same weight, sparsity
is very desirable. In the present work, we exploit
L1-regularization, though other techniques such as
structural zeros (Mohri and Roark, 2006) could
also potentially be used. Second, training requires
repeated parsing, so we use coarse-to-fine chart
caching to greatly accelerate each iteration.
5.1 Hierarchical Training
We learn discriminative multi-scale grammars in an
iterative fashion (see Figure 1). As in Petrov et al.
(2006), we start with a simple X-bar grammar from
an input treebank. The parameters θ of the grammar
(production log-weights for now) are estimated in a
log-linear framework by maximizing the penalized
log conditional likelihood Lcond − R(θ), where:
Lcond(θ) = log
Y
i
P(Ti|wi)
R(θ) =
X
r
|θr|
We directly optimize this non-convex objective
function using a numerical gradient based method
(LBFGS (Nocedal and Wright, 1999) in our imple-
mentation). To handle the non-diferentiability of the
L1-regularization term R(θ) we use the orthant-wise
method of Andrew and Gao (2007). Fitting the log-
linear model involves the following derivatives:
∂Lcond(θ)
∂θr
=
X
i

Eθ [fr(t)|Ti] − Eθ[fr(t)|wi]

where the first term is the expected count fr of a pro-
duction r in derivations corresponding to the correct
parse tree Ti and the second term is the expected
count of the production in all derivations of the sen-
tence wi. Note that r may be of any scale. As we
will show below, these expectations can be com-
puted exactly using marginals from the chart of the
inside/outside algorithm (Lari and Young, 1990).
Once the base grammar has been estimated, all
categories are split in two, meaning that all binary
productions are split in eight. When splitting an al-
ready refined grammar, we only split productions
whose log-weight in the previous grammar deviates
from zero.3 This creates a refinement hierarchy over
productions. Each newly split production r is given
a unique feature, as well as inheriting the features of
its parent productions r̂ ≻ r:
φr = exp
 X
r̂≻r
θr̂

The parent productions r̂ are then removed from the
grammar and the new features are fit as described
3
L1-regularization drives more than 95% of the feature
weights to zero in each round.
870
V P
N P
S
N P
D T N N V B D D T N N
V P
i k j
S 0 → N P 1 V P 0 1
I(S0, i, j)
I(S11, i, j)
Figure 3: A multi-scale chart can be used to efficiently
compute inside/outside scores using productions of vary-
ing specificity.
above. We detect that we have split a production too
far when all child production features are driven to
zero under L1 regularization. In such cases, the chil-
dren are collapsed to their parent production, which
forms an entry in the multi-scale grammar.
5.2 Efficient Multi-Scale Inference
In order to compute the expected counts needed for
training, we need to parse the training set, score
all derivations and compute posteriors for all sub-
categories in the refinement hierarchy. The in-
side/outside algorithm (Lari and Young, 1990) is an
efficient dynamic program for summing over deriva-
tions under a context-free grammar. It is fairly
straightforward to adapt this algorithm to multi-
scale grammars, allowing us to sum over an expo-
nential number of derivations without explicitly re-
constructing the underlying fully split grammar.
For single-scale latent variable grammars, the in-
side score I(Ax, i, j) of a fully refined category Ax
spanning hi, ji is computed by summing over all
possible productions r = Ax → By Cz with weight
φr, spanning hi, ki and hk, ji respectively:4
I(Ax, i, j) =
X
r
φr
X
k
I(By, i, k)I(Cz, k, j)
Note that this involves summing over all relevant
fully refined grammar productions.
The key quantities we will need are marginals of
the form I(Ax, i, j), the sum of the scores of all fully
refined derivations rooted at any Ax dominated by
Ax and spanning hi, ji. We define these marginals
4
These scores lack any probabilistic interpretation, but can
be normalized to compute the necessary expectations for train-
ing (Petrov and Klein, 2008).
in terms of the standard inside scores of the most
refined subcategories Ax:
I(Ax, i, j) =
X
x≺x
I(Ax, i, j)
When working with multi-scale grammars, we
expand the standard three-dimensional chart over
spans and grammar categories to store the scores of
all subcategories of the refinement hierarchy, as il-
lustrated in Figure 3. This allows us to compute the
scores more efficiently by summing only over rules
r̂ = Ax̂ → Bŷ Cẑ ≻ r:
I(Ax, i, j) =
X
r̂
X
r≺r̂
φr
X
k
I(By, i, k)I(Cz, k, j)
=
X
r̂
φr̂
X
r≺r̂
X
k
I(By, i, k)I(Cz, k, j)
=
X
r̂
φr̂
X
y≺ŷ
X
z≺ẑ
X
k
I(By, i, k)I(Cz, k, j)
=
X
r̂
φr̂
X
k
X
y≺ŷ
I(By, i, k)
X
z≺ẑ
I(Cz, k, j)
=
X
r̂
φr̂
X
k
I(Bŷ, i, k)I(Cẑ, k, j)
Of course, some of the same quantities are computed
repeatedly in the above equation and can be cached
in order to obtain further efficiency gains. Due to
space constraints we omit these details, and also the
computation of the outside score, as well as the han-
dling of unary productions.
5.3 Feature Count Approximations
Estimating discriminative grammars is challenging,
as it requires repeatedly taking expectations over all
parses of all sentences in the training set. To make
this computation practical on large data sets, we
use the same approach as Petrov and Klein (2008).
Therein, the idea of coarse-to-fine parsing (Charniak
et al., 1998) is extended to handle the repeated pars-
ing of the same sentences. Rather than computing
the entire coarse-to-fine history in every round of
training, the pruning history is cached between train-
ing iterations, effectively avoiding the repeated cal-
culation of similar quantities and allowing the effi-
cient approximation of feature count expectations.
871
6 Additional Features
The discriminative framework gives us a convenient
way of incorporating additional, overlapping fea-
tures. We investigate two types of features: un-
known word features (for predicting the part-of-
speech tags of unknown or rare words) and span fea-
tures (for determining constituent boundaries based
on individual words and the overall sentence shape).
6.1 Unknown Word Features
Building a parser that can process arbitrary sen-
tences requires the handling of previously unseen
words. Typically, a classification of rare words into
word classes is used (Collins, 1999). In such an ap-
proach, the word classes need to be manually de-
fined a priori, for example based on discriminating
word shape features (suffixes, prefixes, digits, etc.).
While this component of the parsing system is
rarely talked about, its importance should not be un-
derestimated: when using only one unknown word
class, final parsing performance drops several per-
centage points. Some unknown word features are
universal (e.g. digits, dashes), but most of them
will be highly language dependent (prefixes, suf-
fixes), making additional human expertise necessary
for training a parser on a new language. It is there-
fore beneficial to automatically learn what the dis-
criminating word shape features for a language are.
The discriminative framework allows us to do that
with ease. In our experiments we extract prefixes
and suffixes of length ≤ 3 and add those features to
words that occur 25 times or less in the training set.
These unknown word features make the latent vari-
able grammar learning process more language inde-
pendent than in previous work.
6.2 Span Features
There are many features beyond local tree config-
urations which can enhance parsing discrimination;
Charniak and Johnson (2005) presents a varied list.
In reranking, one can incorporate any such features,
of course, but even in our dynamic programming ap-
proach it is possible to include features that decom-
pose along the dynamic program structure, as shown
by Taskar et al. (2004). We use non-local span fea-
tures, which condition on properties of input spans
(Taskar et al., 2004). We illustrate our span features
with the following example and the span h1, 4i:
0 “ 1 [ Yes 2 ” 3 , ] 4 he 5 said 6 . 7
We first added the following lexical features:
• the first (Yes), last (comma), preceding (“) and
following (he) words,
• the word pairs at the left edge h“,Yesi, right
edge hcomma,hei, inside border hYes,commai
and outside border h“,hei.
Lexical features were added for each span of length
three or more. We used two groups of span features,
one for natural constituents and one for synthetic
ones.5 We found this approach to work slightly
better than anchoring the span features to particular
constituent labels or having only one group.
We also added shape features, projecting the
sentence to abstract shapes to capture global sen-
tence structures. Punctuation shape replaces ev-
ery non-punctuation word with x and then further
collapses strings of x to x+. Our example be-
comes #‘‘x’’,x+.#, and the punctuation feature
for our span is ‘‘[x’’,]x. Capitalization shape
projects the example sentence to #.X..xx.#, and
.[X..]x for our span. Span features are a rich
source of information and our experiments should
be seen merely as an initial investigation of their ef-
fect in our system.
7 Experiments
We ran experiments on a variety of languages and
corpora using the standard training and test splits,
as described in Table 1. In each case, we start
with a completely unannotated X-bar grammar, ob-
tained from the raw treebank by a simple right-
branching binarization scheme. We then train multi-
scale grammars of increasing latent complexity as
described in Section 5, directly incorporating the
additional features from Section 6 into the training
procedure. Hierarchical training starting from a raw
treebank grammar and proceeding to our most re-
fined grammars took three days in a parallel im-
plementation using 8 CPUs. At testing time we
marginalize out the hidden structure and extract the
tree with the highest number of expected correct pro-
ductions, as in Petrov and Klein (2007).
5
Synthetic constituents are nodes that are introduced during
binarization.
872
Training Set Dev. Set Test Set
ENGLISH-WSJ Sections
Section 22 Section 23
(Marcus et al., 1993) 2-21
ENGLISH-BROWN see 10% of 10% of the
(Francis et al. 2002) ENGLISH-WSJ the data6
the data6
FRENCH7
Sentences Sentences Sentences
(Abeille et al., 2000) 1-18,609 18,610-19,609 19,609-20,610
GERMAN Sentences Sentences Sentences
(Skut et al., 1997) 1-18,602 18,603-19,602 19,603-20,602
Table 1: Corpora and standard experimental setups.
We compare to a baseline of discriminatively
trained latent variable grammars (Petrov and Klein,
2008). We also compare our discriminative multi-
scale grammars to their generative split-and-merge
cousins, which have been shown to produce the
state-of-the-art figures in terms of accuracy and effi-
ciency on many corpora. For those comparisons we
use the grammars from Petrov and Klein (2007).
7.1 Sparsity
One of the main motivations behind multi-scale
grammars was to create compact grammars. Fig-
ure 4 shows parsing accuracies vs. grammar sizes.
Focusing on the grammar size for now, we see that
multi-scale grammars are extremely compact - even
our most refined grammars have less than 50,000 ac-
tive productions. This is 20 times smaller than the
generative split-and-merge grammars, which use ex-
plicit category merging. The graph also shows that
this compactness is due to controlling production
sparsity, as the single-scale discriminative grammars
are two orders of magnitude larger.
7.2 Accuracy
Figure 4 shows development set results for En-
glish. In terms of parsing accuracy, multi-scale
grammars significantly outperform discriminatively
trained single-scale latent variable grammars and
perform on par with the generative split-and-merge
grammars. The graph also shows that the unknown
word and span features each add about 0.5% in final
parsing accuracy. Note that the span features im-
prove the performance of the unsplit baseline gram-
mar by 8%, but not surprisingly their contribution
6
See Gildea (2001) for the exact setup.
7
This setup contains only sentences without annotation er-
rors, as in (Arun and Keller, 2005).
90
85
80
75
1000000
100000
10000
Parsing
accuracy
(F1)
Number of grammar productions
Discriminative Multi-Scale Grammars
+ Lexical Features
+ Span Features
Generative Split-Merge Grammars
Flat Discriminative Grammars
Figure 4: Discriminative multi-scale grammars give sim-
ilar parsing accuracies as generative split-merge gram-
mars, while using an order of magnitude fewer rules.
gets smaller when the grammars get more refined.
Section 8 contains an analysis of some of the learned
features, as well as a comparison between discrimi-
natively and generatively trained grammars.
7.3 Efficiency
Petrov and Klein (2007) demonstrates how the idea
of coarse-to-fine parsing (Charniak et al., 1998;
Charniak et al., 2006) can be used in the context of
latent variable models. In coarse-to-fine parsing the
sentence is rapidly pre-parsed with increasingly re-
fined grammars, pruning away unlikely chart items
in each pass. In their work the grammar is pro-
jected onto coarser versions, which are then used
for pruning. Multi-scale grammars, in contrast, do
not require projections. The refinement hierarchy is
built in and can be used directly for coarse-to-fine
pruning. Each production in the grammar is associ-
ated with a set of hierarchical features. To obtain a
coarser version of a multi-scale grammar, one there-
fore simply limits which features in the refinement
hierarchy can be accessed. In our experiments, we
start by parsing with our coarsest grammar and al-
low an additional level of refinement at each stage of
the pre-parsing. Compared to the generative parser
of Petrov and Klein (2007), parsing with multi-scale
grammars requires the evaluation of 29% fewer pro-
ductions, decreasing the average parsing time per
sentence by 36% to 0.36 sec/sentence.
873
≤ 40 words all
Parser F1 EX F1 EX
ENGLISH-WSJ
Petrov and Klein (2008) 88.8 35.7 88.3 33.1
Charniak et al. (2005) 90.3 39.6 89.7 37.2
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
This work w/o span features 89.7 39.6 89.2 37.2
This work w/ span features 90.0 40.1 89.4 37.7
ENGLISH-WSJ (reranked)
Huang (2008) 92.3 46.2 91.7 43.5
ENGLISH-BROWN
Charniak et al. (2005) 84.5 34.8 82.9 31.7
Petrov and Klein (2007) 84.9 34.5 83.7 31.2
This work w/o span features 85.3 35.6 84.3 32.1
This work w/ span features 85.6 35.8 84.5 32.3
ENGLISH-BROWN (reranked)
Charniak et al. (2005) 86.8 39.9 85.2 37.8
FRENCH
Arun and Keller (2005) 79.2 21.2 75.6 16.4
This Paper 80.1 24.2 77.2 19.2
GERMAN
Petrov and Klein (2007) 80.8 40.8 80.1 39.1
This Paper 81.5 45.2 80.7 43.9
Table 2: Our final test set parsing accuracies compared to
the best previous work on English, French and German.
7.4 Final Results
For each corpus we selected the grammar that gave
the best performance on the development set to parse
the final test set. Table 2 summarizes our final test
set performance, showing that multi-scale grammars
achieve state-of-the-art performance on most tasks.
On WSJ-English, the discriminative grammars per-
form on par with the generative grammars of Petrov
et al. (2006), falling slightly short in terms of F1, but
having a higher exact match score. When trained
on WSJ-English but tested on the Brown corpus,
the discriminative grammars clearly outperform the
generative grammars, suggesting that the highly reg-
ularized and extremely compact multi-scale gram-
mars are less prone to overfitting. All those meth-
ods fall short of reranking parsers like Charniak and
Johnson (2005) and Huang (2008), which, however,
have access to many additional features, that cannot
be used in our dynamic program.
When trained on the French and German tree-
banks, our multi-scale grammars achieve the best
figures we are aware of, without any language spe-
cific modifications. This confirms that latent vari-
able models are well suited for capturing the syn-
tactic properties of a range of languages, and also
shows that discriminative grammars are still effec-
tive when trained on smaller corpora.
8 Analysis
It can be illuminating to see the subcategories that
are being learned by our discriminative multi-scale
grammars and to compare them to generatively es-
timated latent variable grammars. Compared to the
generative case, the lexical categories in the discrim-
inative grammars are substantially less refined. For
example, in the generative case, the nominal cate-
gories were fully refined, while in the discrimina-
tive case, fewer nominal clusters were heavily used.
One reason for this can be seen by inspecting the
first two-way split in the NNP tag. The genera-
tive model split into initial NNPs (San, Wall) and
final NNPs (Francisco, Street). In contrast, the dis-
criminative split was between organizational entities
(Stock, Exchange) and other entity types (September,
New, York). This constrast is unsurprising. Genera-
tive likelihood is advantaged by explaining lexical
choice – New and York occur in very different slots.
However, they convey the same information about
the syntactic context above their base NP and are
therefore treated the same, discriminatively, while
the systematic attachment distinctions between tem-
porals and named entities are more predictive.
Analyzing the syntactic and semantic patterns
learned by the grammars shows similar trends. In
Table 3 we compare the number of subcategories
in the generative split-and-merge grammars to the
average number of features per unsplit production
with that phrasal category as head in our multi-scale
grammars after 5 split (and merge) rounds. These
quantities are inherently different: the number of
features should be roughly cubic in the number of
subcategories. However, we observe that the num-
bers are very close, indicating that, due to the spar-
sity of our productions, and the efficient multi-scale
encoding, the number of grammar parameters grows
linearly in the number of subcategories. Further-
more, while most categories have similar complex-
ity in those two cases, the complexity of the two
most refined phrasal categories are flipped. Gener-
ative grammars split NPs most highly, discrimina-
874
NP
VP
PP
S
SBAR
ADJP
ADVP
QP
PRN
Generative
32 24 20 12 12 12 8 7 5
subcategories
Discriminative
19 32 20 14 14 8 7 9 6
production parameters
Table 3: Complexity of highly split phrasal categories in
generative and discriminative grammars. Note that sub-
categories are compared to production parameters, indi-
cating that the number of parameters grows cubicly in the
number of subcategories for generative grammars, while
growing linearly for multi-scale grammars.
tive grammars split the VP. This distinction seems
to be because the complexity of VPs is more syntac-
tic (e.g. complex subcategorization), while that of
NPs is more lexical (noun choice is generally higher
entropy than verb choice).
It is also interesting to examine the automatically
learned word class features. Table 4 shows the suf-
fixes with the highest weight for a few different cat-
egories across the three languages that we experi-
mented with. The learning algorithm has selected
discriminative suffixes that are typical derviational
or inflectional morphemes in their respective lan-
guages. Note that the highest weighted suffixes will
typically not correspond to the most common suffix
in the word class, but to the most discriminative.
Finally, the span features also exhibit clear pat-
terns. The highest scoring span features encourage
the words between the last two punctuation marks
to form a constituent (excluding the punctuation
marks), for example ,[x+]. and :[x+]. Words
between quotation marks are also encouraged to
form constituents: ‘‘[x+]’’ and x[‘‘x+’’]x.
Span features can also discourage grouping words
into constituents. The features with the highest neg-
ative weight involve single commas: x[x,x+],
and x[x+,x+]x and so on (indeed, such spans
were structurally disallowed by the Collins (1999)
parser).
9 Conclusions
Discriminatively trained multi-scale grammars give
state-of-the-art parsing performance on a variety of
languages and corpora. Grammar size is dramati-
cally reduced compared to the baseline, as well as to
ENGLISH GERMAN FRENCH
Adjectives
-ous -los -ien
-ble -bar -ble
-nth -ig -ive
Nouns
-ion -tät -té
-en -ung -eur
-cle -rei -ges
Verbs
-ed -st -ées
-s -eht -é
Adverbs -ly -mal -ent
Numbers -ty -zig —
Table 4: Automatically learned suffixes with the highest
weights for different languages and part-of-speech tags.
methods like split-and-merge (Petrov et al., 2006).
Because fewer parameters are estimated, multi-scale
grammars may also be less prone to overfitting, as
suggested by a cross-corpus evaluation experiment.
Furthermore, the discriminative framework enables
the seamless integration of additional, overlapping
features, such as span features and unknown word
features. Such features further improve parsing per-
formance and make the latent variable grammars
very language independent.
Our parser, along with trained grammars
for a variety of languages, is available at
http://nlp.cs.berkeley.edu.
References
A. Abeille, L. Clement, and A. Kinyon. 2000. Building a
treebank for French. In 2nd International Conference
on Language Resources and Evaluation.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In ICML ’07.
A. Arun and F. Keller. 2005. Lexicalization in crosslin-
guistic probabilistic parsing: the case of french. In
ACL ’05.
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
Best Parsing and MaxEnt Discriminative Reranking.
In ACL’05.
E. Charniak, S. Goldwater, and M. Johnson. 1998. Edge-
based best-first chart parsing. 6th
Workshop on Very
Large Corpora.
E. Charniak, M. Johnson, D. McClosky, et al. 2006.
Multi-level coarse-to-fine PCFG Parsing. In HLT-
NAACL ’06.
E. Charniak. 2000. A maximum–entropy–inspired
parser. In NAACL ’00.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In ACL ’04.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, UPenn.
875
J. Finkel, A. Kleeman, and C. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
In ACL ’08.
W. N. Francis and H. Kucera. 2002. Manual of infor-
mation to accompany a standard corpus of present-day
edited american english. In TR, Brown University.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. EMNLP ’01.
J. Henderson. 2004. Discriminative training of a neural
network statistical parser. In ACL ’04.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In ACL ’08.
M. Johnson. 1998. PCFG models of linguistic tree rep-
resentations. Computational Linguistics, 24:613–632.
M. Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In ACL ’01.
D. Klein and C. Manning. 2003. Accurate unlexicalized
parsing. In ACL ’03, pages 423–430.
T. Koo and M. Collins. 2005. Hidden-variable models
for discriminative reranking. In EMNLP ’05.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic models for seg-
menting and labeling sequence data. In ICML ’01.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In EMNLP ’07.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: The
Penn Treebank. In Computational Linguistics.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL ’05.
M. Mohri and B. Roark. 2006. Probabilistic context-free
grammar induction based on structural zeros. In HLT-
NAACL ’06.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL ’07.
S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In NIPS ’08.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree an-
notation. In ACL ’06.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Conf. on Applied Natural Language Processing.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Lingusitics.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In EMNLP ’04.
J. Turian, B. Wellington, and I. D. Melamed. 2007. Scal-
able discriminative learning for natural language pars-
ing and translation. In NIPS ’07.
876
