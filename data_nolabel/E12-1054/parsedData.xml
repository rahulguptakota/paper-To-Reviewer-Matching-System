<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.5986625">
b&apos;Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529538,
Avignon, France, April 23 - 27 2012. c
</bodyText>
<sectionHeader confidence="0.398119" genericHeader="abstract">
2012 Association for Computational Linguistics
</sectionHeader>
<title confidence="0.60506">
Measuring Contextual Fitness Using Error Contexts Extracted from the
Wikipedia Revision History
Torsten Zesch
Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information, Frankfurt
</title>
<author confidence="0.518668">
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
</author>
<affiliation confidence="0.969327">
Department of Computer Science, Technische Universitat Darmstadt
</affiliation>
<email confidence="0.873535">
http://www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.986053" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.998365842105263">
We evaluate measures of contextual fitness
on the task of detecting real-word spelling
errors. For that purpose, we extract nat-
urally occurring errors and their contexts
from the Wikipedia revision history. We
show that such natural errors are better
suited for evaluation than the previously
used artificially created errors. In partic-
ular, the precision of statistical methods
has been largely over-estimated, while the
precision of knowledge-based approaches
has been under-estimated. Additionally, we
show that knowledge-based approaches can
be improved by using semantic relatedness
measures that make use of knowledge be-
yond classical taxonomic relations. Finally,
we show that statistical and knowledge-
based methods can be combined for in-
creased performance.
</bodyText>
<sectionHeader confidence="0.996976" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998850673076923">
Measuring the contextual fitness of a term in its
context is a key component in different NLP ap-
plications like speech recognition (Inkpen and
Desilets, 2005), optical character recognition
(Wick et al., 2007), co-reference resolution (Bean
and Riloff, 2004), or malapropism detection (Bol-
shakov and Gelbukh, 2003). The main idea is al-
ways to test what fits better into the current con-
text: the actual term or a possible replacement that
is phonetically, structurally, or semantically simi-
lar. We are going to focus on malapropism detec-
tion as it allows evaluating measures of contex-
tual fitness in a more direct way than evaluating
in a complex application which always entails in-
fluence from other components, e.g. the quality of
the optical character recognition module (Walker
et al., 2010).
A malapropism or real-word spelling error oc-
curs when a word is replaced with another cor-
rectly spelled word which does not suit the con-
text, e.g. People with lots of honey usually
live in big houses., where money was replaced
with honey. Besides typing mistakes, a major
source of such errors is the failed attempt of au-
tomatic spelling correctors to correct a misspelled
word (Hirst and Budanitsky, 2005). A real-word
spelling error is hard to detect, as the erroneous
word is not misspelled and fits syntactically into
the sentence. Thus, measures of contextual fitness
are required to detect words that do not fit their
contexts.
Existing measures of contextual fitness can be
categorized into knowledge-based (Hirst and Bu-
danitsky, 2005) and statistical methods (Mays et
al., 1991; Wilcox-OHearn et al., 2008). Both
test the lexical cohesion of a word with its con-
text. For that purpose, knowledge-based ap-
proaches employ the structural knowledge en-
coded in lexical-semantic networks like WordNet
(Fellbaum, 1998), while statistical approaches
rely on co-occurrence counts collected from large
corpora, e.g. the Google Web1T corpus (Brants
and Franz, 2006).
So far, evaluation of contextual fitness mea-
sures relied on artificial datasets (Mays et al.,
1991; Hirst and Budanitsky, 2005) which are cre-
ated by taking a sentence that is known to be cor-
rect, and replacing a word with a similar word
from the vocabulary. This has a couple of dis-
advantages: (i) the replacement might be a syn-
onym of the original word and perfectly valid in
the given context, (ii) the generated error might
</bodyText>
<page confidence="0.995558">
529
</page>
<bodyText confidence="0.929729783783784">
\x0cbe very unlikely to be made by a human, and
(iii) inserting artificial errors often leads to un-
natural sentences that are quite easy to correct,
e.g. if the word class has changed. However,
even if the word class is unchanged, the origi-
nal word and its replacement might still be vari-
ants of the same lemma, e.g. a noun in singu-
lar and plural, or a verb in present and past form.
This usually leads to a sentence where the error
can be easily detected using syntactical or statis-
tical methods, but is almost impossible to detect
for knowledge-based measures of contextual fit-
ness, as the meaning of the word stays more or
less unchanged. To estimate the impact of this is-
sue, we randomly sampled 1,000 artificially cre-
ated real-word spelling errors1 and found 387 sin-
gular/plural pairs and 57 pairs which were in an-
other direct relation (e.g. adjective/adverb). This
means that almost half of the artificially created
errors are not suited for an evaluation targeted at
finding optimal measures of contextual fitness, as
they over-estimate the performance of statistical
measures while underestimating the potential of
semantic measures. In order to investigate this
issue, we present a framework for mining natu-
rally occurring errors and their contexts from the
Wikipedia revision history. We use the resulting
English and German datasets to evaluate statisti-
cal and knowledge-based measures.
We make the full experimental framework pub-
licly available2 which will allow reproducing our
experiments as well as conducting follow-up ex-
periments. The framework contains (i) methods
to extract natural errors from Wikipedia, (ii) ref-
erence implementations of the knowledge-based
and the statistical methods, and (iii) the evalua-
tion datasets described in this paper.
</bodyText>
<sectionHeader confidence="0.872722" genericHeader="method">
2 Mining Errors from Wikipedia
</sectionHeader>
<bodyText confidence="0.976654">
Measures of contextual fitness have previously
been evaluated using artificially created datasets,
as there are very few sources of sentences with
naturally occurring errors and their corrections.
Recently, the revision history of Wikipedia has
been introduced as a valuable knowledge source
for NLP (Nelken and Yamangil, 2008; Yatskar et
al., 2010). It is also a possible source of natural
errors, as it is likely that Wikipedia editors make
</bodyText>
<page confidence="0.931491">
1
</page>
<bodyText confidence="0.9867">
The same artificial data as described in Section 3.2.
</bodyText>
<page confidence="0.939983">
2
</page>
<bodyText confidence="0.9740305">
http://code.google.com/p/dkpro-spelling-asl/
real-word spelling errors at some point, which
are then corrected in subsequent revisions of the
same article. The challenge lies in discriminating
real-word spelling errors from all sorts of other
changes, including non-word spelling errors, re-
formulations, or the correction of wrong facts.
For that purpose, we apply a set of precision-
oriented heuristics narrowing down the number
of possible error candidates. Such an approach
is feasible, as the high number of revisions in
Wikipedia allows to be extremely selective.
</bodyText>
<subsectionHeader confidence="0.98838">
2.1 Accessing the Revision Data
</subsectionHeader>
<bodyText confidence="0.999356055555556">
We access the Wikipedia revision data using
the freely available Wikipedia Revision Toolkit
(Ferschke et al., 2011) together with the JWPL
Wikipedia API (Zesch et al., 2008a).3 The API
outputs plain text converted from Wiki-Markup,
but the text still contains a small portion of left-
over markup and other artifacts. Thus, we per-
form additional cleaning steps removing (i) to-
kens with more than 30 characters (often URLs),
(ii) sentences with less than 5 or more than 200
tokens, and (iii) sentences containing a high frac-
tion of special characters like : usually indicat-
ing Wikipedia-specific artifacts like lists of lan-
guage links. The remaining sentences are part-of-
speech tagged and lemmatized using TreeTagger
(Schmid, 2004). Using these cleaned and anno-
tated articles, we form pairs of adjacent article re-
visions (ri and ri+1).
</bodyText>
<subsectionHeader confidence="0.999129">
2.2 Sentence Alignment
</subsectionHeader>
<bodyText confidence="0.996291882352941">
Fully aligning all sentences of the adjacent revi-
sions is a quite costly operation, as sentences can
be split, joined, replaced, or moved in the arti-
cle. However, we are only looking for sentence
pairs which are almost identical except for the
real-word spelling error and its correction. Thus,
we form all sentence pairs and then apply an ag-
gressive but cheap filter that rules out all sentences
which (i) are equal, or (ii) whose lengths differ
more than a small number of characters. For the
resulting much smaller subset of sentence pairs,
we compute the Jaro distance (Jaro, 1995) be-
tween each pair. If the distance exceeds a cer-
tain threshold tsim (0.05 in this case), we do not
further consider the pair. The small amount of re-
maining sentence pairs is passed to the sentence
pair filter for in-depth inspection.
</bodyText>
<page confidence="0.942429">
3
</page>
<footnote confidence="0.266831">
http://code.google.com/p/jwpl/
</footnote>
<page confidence="0.899867">
530
</page>
<sectionHeader confidence="0.2805" genericHeader="method">
\x0c2.3 Sentence Pair Filtering
</sectionHeader>
<bodyText confidence="0.974507604166667">
The sentence pair filter further reduces the num-
ber of remaining sentence pairs by applying a set
of heuristics including surface level and semantic
level filters. Surface level filters include:
Replaced Token Sentences need to consist of
identical tokens, except for one replaced token.
No Numbers The replaced token may not be a
number.
UPPER CASE The replaced token may not be
in upper case.
Case Change The change should not only in-
volve case changes, e.g. changing english into
English.
Edit Distance The edit distance between the
replaced token and its correction need to be be-
low a certain threshold.
After applying the surface level filters, the re-
maining sentence pairs are well-formed and con-
tain exactly one changed token at the same posi-
tion in the sentence. However, the change does
not need to characterize a real-word spelling er-
ror, but could also be a normal spelling error or a
semantically motivated change. Thus, we apply a
set of semantic filters:
Vocabulary The replaced token needs to occur
in the vocabulary. We found that even quite com-
prehensive word lists discarded too many valid
errors as Wikipedia contains articles from a very
wide range of domains. Thus, we use a frequency
filter based on the Google Web1T n-gram counts
(Brants and Franz, 2006). We filter all sentences
where the replaced token has a very low unigram
count. We experimented with different values and
found 25,000 for English and 10,000 for German
to yield good results.
Same Lemma The original token and the re-
placed token may not have the same lemma, e.g.
car and cars would not pass this filter.
Stopwords The replaced token should not be in
a short list of stopwords (mostly function words).
Named Entity The replaced token should not
be part of a named entity. For this purpose, we
applied the Stanford NER (Finkel et al., 2005).
Normal Spelling Error We apply the Jazzy
spelling detector4 and rule out all cases in which
it is able to detect the error.
Semantic Relation If the original token and the
replaced token are in a close lexical-semantic rela-
</bodyText>
<page confidence="0.977464">
4
</page>
<bodyText confidence="0.94452225">
http://jazzy.sourceforge.net/
tions, the change is likely to be semantically mo-
tivated, e.g. if house was replaced with hut.
Thus, we do not consider cases, where we detect
a direct semantic relation between the original and
the replaced term. For this purpose, we use Word-
Net (Fellbaum, 1998) for English and GermaNet
(Lemnitzer and Kunze, 2002) for German.
</bodyText>
<sectionHeader confidence="0.98937" genericHeader="method">
3 Resulting Datasets
</sectionHeader>
<subsectionHeader confidence="0.998994">
3.1 Natural Error Datasets
</subsectionHeader>
<bodyText confidence="0.999225696969697">
Using our framework for mining real-word
spelling errors in context, we extracted an En-
glish dataset5, and a German dataset6. Although
the output generally was of high quality, man-
ual post-processing was necessary7, as (i) for
some pairs the available context did not provide
enough information to decide which form was
correct, and (ii) a problem that might be spe-
cific to Wikipedia vandalism. The revisions are
full of cases where words are replaced with simi-
lar sounding but greasy alternatives. A relatively
mild example is In romantic comedies, there is
a love story about a man and a woman who fall
in love, along with silly or funny comedy farts.,
where parts was replaced with farts only to be
changed back shortly afterwards by a Wikipedia
vandalism hunter. We removed all cases that re-
sulted from obvious vandalism. For further ex-
periments, a small list of offensive terms could be
added to the stopword list to facilitate this pro-
cess.
A connected problem is correct words that get
falsely corrected by Wikipedia editors (without
the malicious intend from the previous examples,
but with similar consequences). For example, the
initially correct sentence Dung beetles roll it into
a ball, sometimes being up to 50 times their own
weight. was corrected by exchanging weight
with wait. We manually removed such obvious
mistakes, but are still left with some borderline
cases. In the sentence By the 1780s the goals
of England were so full that convicts were often
chained up in rotting old ships. the obvious error
</bodyText>
<page confidence="0.95586">
5
</page>
<bodyText confidence="0.81727">
Using a revision dump from April 5, 2011.
</bodyText>
<page confidence="0.974785">
6
</page>
<bodyText confidence="0.824959">
Using a revision dump from August 13, 2010.
</bodyText>
<page confidence="0.996177">
7
</page>
<bodyText confidence="0.999573833333333">
The most efficient and precise way of finding real-word
spelling errors would of course be to apply measures of con-
textual fitness. However, the resulting dataset would then
only contain errors that are detectable by the measures we
want to evaluate a clearly unacceptable bias. Thus, a cer-
tain amount of manual validation is inevitable.
</bodyText>
<page confidence="0.989014">
531
</page>
<bodyText confidence="0.999128673913043">
\x0cgoal was changed by some Wikipedia editor to
jail. However, actually it should have been the
old English form for jail gaol which can be de-
duced when looking at the full context and later
versions of the article. We decided to not remove
these rare cases, because jail is a valid correction
in this context.
After manual inspection, we are left with 466
English and 200 German errors. Given that we
restricted our experiment to 5 million English and
German revisions, much larger datasets can be ex-
tracted if the whole revision history is taken into
account. Our snapshot of the English Wikipedia
contains 305106 revisions. Even if not all of them
correspond to article revisions, it is safe to assume
that more than 10,000 real-word spelling errors
can be extracted from this version of Wikipedia.
Using the same amount of source revisions, we
found significantly more English than German er-
rors. This might be due to (i) English having more
short nouns or verbs than German that are more
likely to be confused with each other, and (ii) the
English Wikipedia being known to attract a larger
amount of non-native editors which might lead to
higher rates of real-word spelling errors. How-
ever, this issue needs to be further investigated
e.g. based on comparable corpora build on the ba-
sis of different language editions of Wikipedia.
Further refining the identification of real-word er-
rors in Wikipedia would allow evaluating how fre-
quent such errors actually occur, and how long
it takes the Wikipedia editors to detect them. If
errors persist over a long time, using measures
of contextual fitness for detection would be even
more important.
Another interesting observation is that the av-
erage edit distance is around 1.4 for both datasets.
This means that a substantial proportion of errors
involve more than one edit operation. Given that
many measures of contextual fitness allow at most
one edit, many naturally occurring errors will not
be detected. However, allowing a larger edit dis-
tance enormously increases the search space re-
sulting in increased run-time and possibly de-
creased detection precision due to more false pos-
itives.
</bodyText>
<subsectionHeader confidence="0.999882">
3.2 Artificial Error Datasets
</subsectionHeader>
<bodyText confidence="0.9934007">
In contrast to the quite challenging process of
mining naturally occurring errors, creating artifi-
cial errors is relatively straightforward. From a
corpus that is known to be free of spelling errors,
sentences are randomly sampled. For each sen-
tence, a random word is selected and all strings
with edit distance smaller than a given threshold
(2 in our case) are generated. If one of those gen-
erated strings is a known word from the vocabu-
lary, it is picked as the artificial error.
Previous work on evaluating real-word spelling
correction (Hirst and Budanitsky, 2005; Wilcox-
OHearn et al., 2008; Islam and Inkpen, 2009)
used a dataset sampled from the Wall Street Jour-
nal corpus which is not freely available. Thus, we
created a comparable English dataset of 1,000 ar-
tificial errors based on the easily available Brown
corpus (Francis W. Nelson and Kucera, 1964).8
Additionally, we created a German dataset with
1,000 artificial errors based on the TIGER cor-
</bodyText>
<page confidence="0.40865">
pus.9
</page>
<sectionHeader confidence="0.922601" genericHeader="method">
4 Measuring Contextual Fitness
</sectionHeader>
<bodyText confidence="0.999532">
There are two main approaches for measuring the
contextual fitness of a word in its context: the
statistical (Mays et al., 1991) and the knowledge-
based approach (Hirst and Budanitsky, 2005).
</bodyText>
<subsectionHeader confidence="0.999763">
4.1 Statistical Approach
</subsectionHeader>
<bodyText confidence="0.980113785714286">
Mays et al. (1991) introduced an approach based
on the noisy-channel model. The model assumes
that the correct sentence s is transmitted through
a noisy channel adding noise which results in a
word w being replaced by an error e leading the
wrong sentence s0 which we observe. The prob-
ability of the correct word w given that we ob-
serve the error e can be computed as P(w|e) =
P(w) P(e|w). The channel model P(e|w) de-
scribes how likely the typist is to make an error.
This is modeled by the parameter .10 The re-
maining probability mass (1 ) is distributed
equally among all words in the vocabulary within
an edit distance of 1 (edits(w)):
</bodyText>
<equation confidence="0.997406">
P(e|w) =
(
if e = w
(1 )/|edits(w) |if e 6= w
</equation>
<bodyText confidence="0.971244">
The source model P(w) is estimated using a
trigram language model, i.e. the probability of the
</bodyText>
<page confidence="0.882542">
8
</page>
<equation confidence="0.402166333333333">
http://www.archive.org/details/BrownCorpus (CC-by-na).
9
http://www.ims.uni-stuttgart.de/projekte/TIGER/
</equation>
<bodyText confidence="0.984184">
The corpus contains 50,000 sentences of German newspaper
text, and is freely available under a non-commercial license.
</bodyText>
<page confidence="0.987744">
10
</page>
<bodyText confidence="0.98573">
We optimize on a held-out development set of errors.
</bodyText>
<page confidence="0.983153">
532
</page>
<bodyText confidence="0.75888225">
\x0cintended word wi is computed as the conditional
probability P(wi|wi1wi2). Hence, the proba-
bility of the correct sentence s = w1 . . . wn can
be estimated as
</bodyText>
<equation confidence="0.9997994">
P(s) =
n+2
Y
i=1
P(wi|wi1wi2)
</equation>
<bodyText confidence="0.906081625">
The set of candidate sentences Sc contains all ver-
sions of the observed sentence s0 derived by re-
placing one word with a word from edits(w),
while all other words in the sentence remain
unchanged. The correct sentence s is those
sentence from Sc that maximizes P(s|s0) =
arg maxsSc
P(s) P(s0|s).
</bodyText>
<subsectionHeader confidence="0.950451">
4.2 Knowledge Based Approach
</subsectionHeader>
<bodyText confidence="0.995962264705882">
Hirst and Budanitsky (2005) introduced a
knowledge-based approach that detects real-word
spelling errors by checking the semantic relations
of a target word with its context. For this pur-
pose, they apply WordNet as the source of lexical-
semantic knowledge.
The algorithm flags all words as error can-
didates and then applies filters to remove those
words from further consideration that are unlikely
to be errors. First, the algorithm removes all
closed-class word candidates as well as candi-
dates which cannot be found in the vocabulary.
Candidates are then tested for having lexical co-
hesion with their context, by (i) checking whether
the same surface form or lemma appears again in
the context, or (ii) a semantically related concept
is found in the context. In both cases, the candi-
date is removed from the list of candidates. For
each remaining possible real-word spelling error,
edits are generated by inserting, deleting, or re-
placing characters up to a certain edit distance
(usually 1). Each edit is then tested for lexical
cohesion with the context. If at least one of it fits
into the context, the candidate is selected as a real-
word error.
Hirst and Budanitsky (2005) use two additional
filters: First, they remove candidates that are
common non-topical words. It is unclear how
the list of such words was compiled. Their list
of examples contains words like find or world
which we consider to be perfectly valid candi-
dates. Second, they also applied a filter using a
list of known multi-words, as the probability for
words to accidentally form multi-words is low.
</bodyText>
<table confidence="0.9981516">
Dataset P R F
Artificial-English .77 .50 .60
Natural-English .54 .26 .35
Artificial-German .90 .49 .63
Natural-German .77 .20 .32
</table>
<tableCaption confidence="0.999352">
Table 1: Performance of the statistical approach using
</tableCaption>
<bodyText confidence="0.994870413793103">
a trigram model based on Google Web1T.
It is unclear which list was used. We could use
multi-words from WordNet, but coverage would
be rather limited. We decided not to use both fil-
ters in order to better assess the influence of the
underlying semantic relatedness measure on the
overall performance.
The knowledge based approach uses semantic
relatedness measures to determine the cohesion
between a candidate and its context. In the exper-
iments by Budanitsky and Hirst (2006), the mea-
sure by (Jiang and Conrath, 1997) yields the best
results. However, a wide range of other measures
have been proposed, cf. (Zesch and Gurevych,
2010). Some measures using a wider defini-
tion of semantic relatedness (Gabrilovich and
Markovitch, 2007; Zesch et al., 2008b) instead
of only using taxonomic relations in a knowledge
source.
As semantic relatedness measures usually re-
turn a numeric value, we need to determine a
threshold in order to come up with a binary
related/unrelated decision. Budanitsky and Hirst
(2006) used a characteristic gap in the stan-
dard evaluation dataset by Rubenstein and Good-
enough (1965) that separates unrelated from re-
lated word pairs. We do not follow this approach,
but optimize the threshold on a held-out develop-
ment set of real-word spelling errors.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="evaluation">
5 Results &amp; Discussion
</sectionHeader>
<bodyText confidence="0.98126475">
In this section, we report on the results obtained
in our evaluation of contextual fitness measures
using artificial and natural errors in English and
German.
</bodyText>
<subsectionHeader confidence="0.998537">
5.1 Statistical Approach
</subsectionHeader>
<bodyText confidence="0.9965396">
Table 1 summarizes the results obtained by the
statistical approach using a trigram model based
on the Google Web1T data (Brants and Franz,
2006). On the English artificial errors, we ob-
serve a quite high F-measure of .60 that drops to
</bodyText>
<page confidence="0.965842">
533
</page>
<figure confidence="0.995770073170732">
\x0cDataset N-gram model Size P R F
Art-En
Google Web
7 1011
.77 .50 .60
7 1010
.78 .48 .59
7 109
.76 .42 .54
Wikipedia 2 109
.72 .37 .49
Nat-En
Google Web
7 1011
.54 .26 .35
7 1010
.51 .23 .31
7 109
.46 .19 .27
Wikipedia 2 109
.49 .19 .27
Art-De
Google Web
8 1010
.90 .49 .63
8 109
.90 .47 .61
8 108
.88 .36 .51
Wikipedia 7 108
.90 .37 .52
Nat-De
Google Web
8 1010
.77 .20 .32
8 109
.68 .14 .23
8 108
.65 .10 .17
Wikipedia 7 108
.70 .13 .22
</figure>
<tableCaption confidence="0.976022">
Table 2: Influence of the n-gram model on the perfor-
</tableCaption>
<bodyText confidence="0.9943586">
mance of the statistical approach.
.35 when switching to the naturally occurring er-
rors which we extracted from Wikipedia. On the
German dataset, we observe almost the same per-
formance drop (from .63 to .32).
These observations correspond to our earlier
analysis where we showed that the artificial data
contains many cases that are quite easy to correct
using a statistical model, e.g. where a plural form
of a noun is replaced with its singular form (or
vice versa) as in I bought a car. vs. I bought
a cars.. The naturally occurring errors often con-
tain much harder contexts, as shown in the fol-
lowing example: Through the open window they
heard sounds below in the street: cartwheels, a
tired horses plodding step, vices. where vices
should be corrected to voices. While the lemma
voice is clearly semantically related to other
words in the context like hear or sound, the
position at the end of the sentence is especially
difficult for the trigram-based statistical approach.
The only trigram that connects the error to the
context is (step, ,, vices/voices) which will
probably yield a low frequency count even for
very large trigram models. Higher order n-gram
models would help, but suffer from the usual data-
sparseness problems.
Influence of the N-gram Model For building
the trigram model, we used the Google Web1T
data, which has some known quality issues and is
</bodyText>
<table confidence="0.9973294">
Dataset P R F
Artificial-English .26 .15 .19
Natural-English .29 .18 .23
Artificial-German .47 .16 .24
Natural-German .40 .13 .19
</table>
<tableCaption confidence="0.999559">
Table 3: Performance of the knowledge-based ap-
</tableCaption>
<bodyText confidence="0.999516766666667">
proach using the JiangConrath semantic relatedness
measure.
not targeted towards the Wikipedia articles from
which we sampled the natural errors. Thus, we
also tested a trigram model based on Wikipedia.
However, it is much smaller than the Web model,
which leads us to additionally testing smaller Web
models. Table 2 summarizes the results.
We observe that more data is better data still
holds, as the largest Web model always outper-
forms the Wikipedia model in terms of recall. If
we reduce the size of the Web model to the same
order of magnitude as the Wikipedia model, the
performance of the two models is comparable.
We would have expected to see better results for
the Wikipedia model in this setting, but its higher
quality does not lead to a significant difference.
Even if statistical approaches quite reliably de-
tect real-word spelling errors, the size of the re-
quired n-gram models remains a serious obstacle
for use in real-world applications. The English
Web1T trigram model is about 25GB, which cur-
rently is not suited for being applied in settings
with limited storage capacities e.g. for intelligent
input assistance in mobile devices. As we have
seen above, using smaller models will decrease
recall to a point where hardly any error will be de-
tected anymore. Thus, we will now have a look on
knowledge-based approaches which are less de-
manding in terms of the required resources.
</bodyText>
<subsectionHeader confidence="0.997678">
5.2 Knowledge-based Approach
</subsectionHeader>
<bodyText confidence="0.998565">
Table 3 shows the results for the knowledge-based
measure. In contrast to the statistical approach,
the results on the artificial errors are not higher
than on the natural errors, but almost equal for
German and even lower for English; another piece
of evidence supporting our view that the proper-
ties of artificial datasets over-estimate the perfor-
mance of statistical measures.
Influence of the Relatedness Measure As was
pointed out before, Budanitsky and Hirst (2006)
</bodyText>
<page confidence="0.999026">
534
</page>
<table confidence="0.998462">
\x0cDataset Measure P R F
Art-En
JiangConrath 0.5 .26 .15 .19
Lin 0.5 .22 .17 .19
Lesk 0.5 .19 .16 .17
ESA-Wikipedia 0.05 .43 .13 .20
ESA-Wiktionary 0.05 .35 .20 .25
ESA-Wordnet 0.05 .33 .15 .21
Nat-En
JiangConrath 0.5 .29 .18 .23
Lin 0.5 .26 .21 .23
Lesk 0.5 .19 .19 .19
ESA-Wikipedia 0.05 .48 .14 .22
ESA-Wiktionary 0.05 .39 .21 .27
ESA-Wordnet 0.05 .36 .15 .21
</table>
<tableCaption confidence="0.996495">
Table 4: Performance of knowledge-based approach
</tableCaption>
<bodyText confidence="0.985969772727273">
using different relatedness measures.
show that the measure by Jiang and Conrath
(1997) yields the best results in their experi-
ments on malapropism detection. In addition, we
test another path-based measure by Lin (1998),
the gloss-based measure by Lesk (1986), and
the ESA measure (Gabrilovich and Markovitch,
2007) based on concept vectors from Wikipedia,
Wiktionary, and WordNet. Table 4 summarizes
the results. In contrast to the findings of Budanit-
sky and Hirst (2006), JiangConrath is not the best
path-based measure, as Lin provides equal or bet-
ter performance. Even more importantly, other
(non path-based) measures yield better perfor-
mance than both path-based measures. Especially
ESA based on Wiktionary provides a good over-
all performance, while ESA based on Wikipedia
provides excellent precision. The advantage of
ESA over the other measure types can be ex-
plained with its ability to incorporate semantic re-
lationships beyond classical taxonomic relations
(as used by path-based measures).
</bodyText>
<subsectionHeader confidence="0.99692">
5.3 Combining the Approaches
</subsectionHeader>
<bodyText confidence="0.994914909090909">
The statistical and the knowledge-based approach
use quite different methods to assess the con-
textual fitness of a word in its context. This
makes it worthwhile trying to combine both ap-
proaches. We ran the statistical method (using the
full Wikipedia trigram model) and the knowledge-
based method (using the ESA-Wiktionary related-
ness measure) in parallel and then combined the
resulting detections using two strategies: (i) we
merge the detections of both approaches in order
to obtain higher recall (Union), and (ii) we only
</bodyText>
<table confidence="0.993635666666667">
Dataset Comb.-Strategy P R F
Artificial-English
Best-Single .77 .50 .60
Union .52 .55 .54
Intersection .91 .15 .25
Natural-English
Best-Single .54 .26 .35
Union .40 .36 .38
Intersection .82 .11 .19
</table>
<tableCaption confidence="0.982204">
Table 5: Results obtained by a combination of the best
statistical and knowledge-based configuration. Best-
</tableCaption>
<bodyText confidence="0.998576555555555">
Single is the best precision or recall obtained by a sin-
gle measure. Union merges the detections of both
approaches. Intersection only detects an error if both
methods agree on a detection.
count an error as detected if both methods agree
on a detection (Intersection). When compar-
ing the combined results in Table 5 with the best
precision or recall obtained by a single measure
(Best-Single), we observe that precision can be
significantly improved using the Union strategy,
while recall is only moderately improved using
the Intersect strategy. This means that (i) a large
subset of errors is detected by both approaches
that due to their different sources of knowledge
mutually reinforce the detection leading to in-
creased precision, and (ii) a small but otherwise
undetectable subset of errors requires considering
detections made by one approach only.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.99944715">
To our knowledge, we are the first to create a
dataset of naturally occurring errors based on the
revision history of Wikipedia. Max and Wis-
niewski (2010) used similar techniques to create
a dataset of errors from the French Wikipedia.
However, they target a wider class of errors in-
cluding non-word spelling errors, and their class
of real-word errors conflates malapropisms as
well as other types of changes like reformulations.
Thus, their dataset cannot be easily used for our
purposes and is only available in French, while
our framework allows creating datasets for all ma-
jor languages with minimal manual effort.
Another possible source of real-word spelling
errors are learner corpora (Granger, 2002), e.g.
the Cambridge Learner Corpus (Nicholls, 1999).
However, annotation of errors is difficult and
costly (Rozovskaya and Roth, 2010), only a small
fraction of observed errors will be real-word
spelling errors, and learners are likely to make dif-
</bodyText>
<page confidence="0.987815">
535
</page>
<bodyText confidence="0.999446818181818">
\x0cferent mistakes than proficient language users.
Islam and Inkpen (2009) presented another sta-
tistical approach using the Google Web1T data
(Brants and Franz, 2006) to create the n-gram
model. It slightly outperformed the approach by
Mays et al. (1991) when evaluated on a corpus of
artificial errors based on the WSJ corpus. How-
ever, the results are not directly comparable, as
Mays et al. (1991) used a much smaller n-gram
model and our results in Section 5.1 show that
the size of the n-gram model has a large influence
on the results. Eventually, we decided to use the
Mays et al. (1991) approach in our study, as it is
easier to adapt and augment.
In a re-evaluation of the statistical model by
Mays et al. (1991), Wilcox-OHearn et al. (2008)
found that it outperformed the knowledge-based
method by Hirst and Budanitsky (2005) when
evaluated on a corpus of artificial errors based on
the WSJ corpus. This is consistent with our find-
ings on the artificial errors based on the Brown
corpus, but - as we have seen in the previous sec-
tion - evaluation on the naturally occurring errors
shows a different picture. They also tried to im-
prove the model by permitting multiple correc-
tions and using fixed-length context windows in-
stead of sentences, but obtained discouraging re-
sults.
All previously discussed methods are unsuper-
vised in a way that they do not rely on any training
data with annotated errors. However, real-word
spelling correction has also been tackled by su-
pervised approaches (Golding and Schabes, 1996;
Jones and Martin, 1997; Carlson et al., 2001).
Those methods rely on predefined confusion-sets,
i.e. sets of words that are often confounded e.g.
{peace, piece} or {weather, whether}. For each
set, the methods learn a model of the context in
which one or the other alternative is more proba-
ble. This yields very high precision, but only for
the limited number of previously defined confu-
sion sets. Our framework for extracting natural
errors could be used to increase the number of
known confusion sets.
</bodyText>
<sectionHeader confidence="0.994826" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999921772727273">
In this paper, we evaluated two main approaches
for measuring the contextual fitness of terms: the
statistical approach by Mays et al. (1991) and
the knowledge-based approach by Hirst and Bu-
danitsky (2005) on the task of detecting real-
word spelling errors. For that purpose, we ex-
tracted a dataset with naturally occurring errors
and their contexts from the Wikipedia revision
history. We show that evaluating measures of con-
textual fitness on this dataset provides a more re-
alistic picture of task performance. In particular,
using artificial datasets over-estimates the perfor-
mance of the statistical approach, while it under-
estimates the performance of the knowledge-
based approach.
We show that n-gram models targeted towards
the domain from which the errors are sampled
do not improve the performance of the statisti-
cal approach if larger n-gram models are avail-
able. We further show that the performance of
the knowledge-based approach can be improved
by using semantic relatedness measures that in-
corporate knowledge beyond the taxonomic rela-
tions in a classical lexical-semantic resource like
WordNet. Finally, by combining both approaches,
significant increases in precision or recall can be
achieved.
In future work, we want to evaluate a wider
range of contextual fitness measures, and learn
how to combine them using more sophisticated
combination strategies. Both - the statistical as
well as the knowledge-based approach - will ben-
efit from a better model of the typist, as not all
edit operations are equally likely (Kernighan et
al., 1990). On the side of the error extraction, we
are going to further improve the extraction pro-
cess by incorporating more knowledge about the
revisions. For example, vandalism is often re-
verted very quickly, which can be detected when
looking at the full set of revisions of an article.
We hope that making the experimental frame-
work publicly available will foster future research
in this field, as our results on the natural errors
show that the problem is still quite challenging.
</bodyText>
<sectionHeader confidence="0.9669" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992655333333333">
This work has been supported by the Volk-
swagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
We Andreas Kellner and Tristan Miller for check-
ing the datasets, and the anonymous reviewers for
their helpful feedback.
</bodyText>
<page confidence="0.994916">
536
</page>
<reference confidence="0.989671336283187">
\x0cReferences
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Proc. of HLT/NAACL, pages
297304.
Igor A. Bolshakov and Alexander Gelbukh. 2003. On
Detection of Malapropisms by Multistage Colloca-
tion Testing. In Proceedings of NLDB-2003, 8th
International Workshop on Applications of Natural
Language to Information Systems, number Cic.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Version 1.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13
47.
Andrew J Carlson, Jeffrey Rosen, and Dan Roth.
2001. Scaling Up Context-Sensitive Text Correc-
tion. In Proceedings of IAAI.
C Fellbaum. 1998. WordNet An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Oliver Ferschke, Torsten Zesch, and Iryna Gurevych.
2011. Wikipedia Revision Toolkit: Efficiently
Accessing Wikipedias Edit History. In Proceed-
ings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. System Demonstrations, pages
97102, Portland, OR, USA.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics -
ACL 05, pages 363370, Morristown, NJ, USA.
Association for Computational Linguistics.
Francis W. Nelson and Henry Kucera. 1964. Manual
of information to accompany a standard corpus of
present-day edited American English, for use with
digital computers.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing Semantic Relatedness using Wikipedia-
based Explicit Semantic Analysis. In Proceedings
of the 20th International Joint Conference on Arti-
ficial Intelligence, pages 16061611.
Andrew R. Golding and Yves Schabes. 1996. Com-
bining Trigram-based and feature-based methods
for context-sensitive spelling correction. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics -, pages 7178, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Sylviane Granger, 2002. A birds-eye view of learner
corpus research, pages 333. John Benjamins Pub-
lishing Company.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lex-
ical cohesion. Natural Language Engineering,
11(1):87111, March.
Diana Inkpen and Alain Desilets. 2005. Semantic
similarity for detecting recognition errors in auto-
matic speech transcripts. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing -
HLT 05, number October, pages 4956, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using Google Web IT 3-grams.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing Vol-
ume 3 - EMNLP 09, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
M A Jaro. 1995. Probabilistic linkage of large public
health data file. Statistics in Medicine, 14:491498.
Jay J Jiang and David W Conrath. 1997. Seman-
tic Similarity Based on Corpus Statistics and Lex-
ical Taxonomy. In Proceedings of the 10th Inter-
national Conference on Research in Computational
Linguistics, Taipei, Taiwan.
Michael P Jones and James H Martin. 1997. Contex-
tual spelling correction using latent semantic analy-
sis. In Proceedings of the fifth conference on Ap-
plied natural language processing -, pages 166
173, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Mark D Kernighan, Kenneth W Church, and
William A Gale. 1990. A Spelling Correc-
tion Program Based on a Noisy Channel Model.
In Proceedings of the 13th International Confer-
ence on Computational Linguistics, pages 205210,
Helsinki, Finland.
Lothar Lemnitzer and Claudia Kunze. 2002. Ger-
maNet - Representation, Visualization, Application.
In Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC),
pages 14851491.
M Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: how to tell a pine
cone from an ice cream cone. Proceedings of the
5th annual international conference, pages 2426.
Dekang Lin. 1998. An Information-Theoretic Defini-
tion of Similarity. In Proceedings of International
Conference on Machine Learning, pages 296304,
Madison, Wisconsin.
Aurelien Max and Guillaume Wisniewski. 2010.
Mining Naturally-occurring Corrections and Para-
phrases from Wikipedias Revision History. In Pro-
ceedings of the Seventh conference on International
Language Resources and Evaluation (LREC10),
pages 31433148.
Eric Mays, Fred. J Damerau, and Robert L Mercer.
1991. Context based spelling correction. Informa-
tion Processing &amp; Management, 27(5):517522.
</reference>
<page confidence="0.946395">
537
</page>
<reference confidence="0.999708967213115">
\x0cRani Nelken and Elif Yamangil. 2008. Mining
Wikipedias Article Revision History for Train-
ing Computational Linguistics Algorithms. In
Proceedings of the AAAI Workshop on Wikipedia
and Artificial Intelligence: An Evolving Synergy
(WikiAI), WikiAI08.
Diane Nicholls. 1999. The Cambridge Learner Cor-
pus - Error Coding and Analysis for Lexicography
and ELT. In Summer Workshop on Learner Cor-
pora, Tokyo, Japan.
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL Errors: Challenges and Rewards. In The 5th
Workshop on Innovative Use of NLP for Building
Educational Applications (NAACL-HLT).
H Rubenstein and J B Goodenough. 1965. Contextual
Correlates of Synonymy. Communications of the
ACM, 8(10):627633.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of the 20th International
Conference on Computational Linguistics (COL-
ING 2004), Geneva, Switzerland.
Daniel D. Walker, William B. Lund, and Eric K. Ring-
ger. 2010. Evaluating Models of Latent Document
Semantics in the Presence of OCR Errors. Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, (October):240
250.
M. Wick, M. Ross, and E. Learned-Miller. 2007.
Context-sensitive error correction: Using topic
models to improve OCR. In Ninth International
Conference on Document Analysis and Recogni-
tion (ICDAR 2007) Vol 2, pages 11681172. Ieee,
September.
Amber Wilcox-OHearn, Graeme Hirst, and Alexander
Budanitsky. 2008. Real-word spelling correction
with trigrams: A reconsideration of the Mays, Dam-
erau, and Mercer model. In Proceedings of the 9th
international conference on Computational linguis-
tics and intelligent text processing (CICLing).
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of sim-
plicity: unsupervised extraction of lexical simplifi-
cations from Wikipedia. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT 10, pages 365368.
Torsten Zesch and Iryna Gurevych. 2010. Wisdom
of Crowds versus Wisdom of Linguists - Measur-
ing the Semantic Relatedness of Words. Journal of
Natural Language Engineering, 16(1):2559.
Torsten Zesch, Christof Muller, and Iryna Gurevych.
2008a. Extracting Lexical Semantic Knowledge
from Wikipedia and Wiktionary. In Proceedings of
the Conference on Language Resources and Evalu-
ation (LREC).
Torsten Zesch, Christof Muller, and Iryna Gurevych.
2008b. Using wiktionary for computing semantic
relatedness. In Proceedings of the 23rd AAAI Con-
ference on Artificial Intelligence, pages 861867,
Chicago, IL, USA, Jul.
</reference>
<page confidence="0.971741">
538
</page>
<figure confidence="0.254105">
\x0c&apos;
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.260695">
<note confidence="0.967621333333333">b&apos;Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 529538, Avignon, France, April 23 - 27 2012. c 2012 Association for Computational Linguistics</note>
<title confidence="0.95414">Measuring Contextual Fitness Using Error Contexts Extracted from the Wikipedia Revision History</title>
<author confidence="0.530778">Torsten Zesch</author>
<affiliation confidence="0.64115775">Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information, Frankfurt Ubiquitous Knowledge Processing Lab (UKP-TUDA) Department of Computer Science, Technische Universitat Darmstadt</affiliation>
<web confidence="0.955249">http://www.ukp.tu-darmstadt.de</web>
<abstract confidence="0.9954306">We evaluate measures of contextual fitness on the task of detecting real-word spelling errors. For that purpose, we extract naturally occurring errors and their contexts from the Wikipedia revision history. We show that such natural errors are better suited for evaluation than the previously used artificially created errors. In particular, the precision of statistical methods has been largely over-estimated, while the precision of knowledge-based approaches has been under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledgebased methods can be combined for increased performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>\x0cReferences David Bean</author>
<author>Ellen Riloff</author>
</authors>
<title>Unsupervised learning of contextual role knowledge for coreference resolution.</title>
<date>2004</date>
<booktitle>In Proc. of HLT/NAACL,</booktitle>
<pages>297304</pages>
<contexts>
<context position="1640" citStr="Bean and Riloff, 2004" startWordPosition="219" endWordPosition="222"> the precision of knowledge-based approaches has been under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledgebased methods can be combined for increased performance. 1 Introduction Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and Desilets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003). The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar. We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010). A malapropism or real-word spelling error occurs when a word i</context>
</contexts>
<marker>Bean, Riloff, 2004</marker>
<rawString>\x0cReferences David Bean and Ellen Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. In Proc. of HLT/NAACL, pages 297304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor A Bolshakov</author>
<author>Alexander Gelbukh</author>
</authors>
<title>On Detection of Malapropisms by Multistage Collocation Testing.</title>
<date>2003</date>
<booktitle>In Proceedings of NLDB-2003, 8th International Workshop on Applications of Natural Language to Information Systems, number Cic.</booktitle>
<contexts>
<context position="1696" citStr="Bolshakov and Gelbukh, 2003" startWordPosition="226" endWordPosition="230">een under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledgebased methods can be combined for increased performance. 1 Introduction Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and Desilets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003). The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar. We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010). A malapropism or real-word spelling error occurs when a word is replaced with another correctly spelled word which doe</context>
</contexts>
<marker>Bolshakov, Gelbukh, 2003</marker>
<rawString>Igor A. Bolshakov and Alexander Gelbukh. 2003. On Detection of Malapropisms by Multistage Collocation Testing. In Proceedings of NLDB-2003, 8th International Workshop on Applications of Natural Language to Information Systems, number Cic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2006</date>
<booktitle>Web 1T 5-gram Version 1.</booktitle>
<contexts>
<context position="3339" citStr="Brants and Franz, 2006" startWordPosition="492" endWordPosition="495">us, measures of contextual fitness are required to detect words that do not fit their contexts. Existing measures of contextual fitness can be categorized into knowledge-based (Hirst and Budanitsky, 2005) and statistical methods (Mays et al., 1991; Wilcox-OHearn et al., 2008). Both test the lexical cohesion of a word with its context. For that purpose, knowledge-based approaches employ the structural knowledge encoded in lexical-semantic networks like WordNet (Fellbaum, 1998), while statistical approaches rely on co-occurrence counts collected from large corpora, e.g. the Google Web1T corpus (Brants and Franz, 2006). So far, evaluation of contextual fitness measures relied on artificial datasets (Mays et al., 1991; Hirst and Budanitsky, 2005) which are created by taking a sentence that is known to be correct, and replacing a word with a similar word from the vocabulary. This has a couple of disadvantages: (i) the replacement might be a synonym of the original word and perfectly valid in the given context, (ii) the generated error might 529 \x0cbe very unlikely to be made by a human, and (iii) inserting artificial errors often leads to unnatural sentences that are quite easy to correct, e.g. if the word c</context>
<context position="9693" citStr="Brants and Franz, 2006" startWordPosition="1525" endWordPosition="1528">remaining sentence pairs are well-formed and contain exactly one changed token at the same position in the sentence. However, the change does not need to characterize a real-word spelling error, but could also be a normal spelling error or a semantically motivated change. Thus, we apply a set of semantic filters: Vocabulary The replaced token needs to occur in the vocabulary. We found that even quite comprehensive word lists discarded too many valid errors as Wikipedia contains articles from a very wide range of domains. Thus, we use a frequency filter based on the Google Web1T n-gram counts (Brants and Franz, 2006). We filter all sentences where the replaced token has a very low unigram count. We experimented with different values and found 25,000 for English and 10,000 for German to yield good results. Same Lemma The original token and the replaced token may not have the same lemma, e.g. car and cars would not pass this filter. Stopwords The replaced token should not be in a short list of stopwords (mostly function words). Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005). Normal Spelling Error We apply the Jazzy sp</context>
<context position="21196" citStr="Brants and Franz, 2006" startWordPosition="3438" endWordPosition="3441">006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs. We do not follow this approach, but optimize the threshold on a held-out development set of real-word spelling errors. 5 Results &amp; Discussion In this section, we report on the results obtained in our evaluation of contextual fitness measures using artificial and natural errors in English and German. 5.1 Statistical Approach Table 1 summarizes the results obtained by the statistical approach using a trigram model based on the Google Web1T data (Brants and Franz, 2006). On the English artificial errors, we observe a quite high F-measure of .60 that drops to 533 \x0cDataset N-gram model Size P R F Art-En Google Web 7 1011 .77 .50 .60 7 1010 .78 .48 .59 7 109 .76 .42 .54 Wikipedia 2 109 .72 .37 .49 Nat-En Google Web 7 1011 .54 .26 .35 7 1010 .51 .23 .31 7 109 .46 .19 .27 Wikipedia 2 109 .49 .19 .27 Art-De Google Web 8 1010 .90 .49 .63 8 109 .90 .47 .61 8 108 .88 .36 .51 Wikipedia 7 108 .90 .37 .52 Nat-De Google Web 8 1010 .77 .20 .32 8 109 .68 .14 .23 8 108 .65 .10 .17 Wikipedia 7 108 .70 .13 .22 Table 2: Influence of the n-gram model on the performance of th</context>
<context position="29516" citStr="Brants and Franz, 2006" startWordPosition="4815" endWordPosition="4818">ailable in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls, 1999). However, annotation of errors is difficult and costly (Rozovskaya and Roth, 2010), only a small fraction of observed errors will be real-word spelling errors, and learners are likely to make dif535 \x0cferent mistakes than proficient language users. Islam and Inkpen (2009) presented another statistical approach using the Google Web1T data (Brants and Franz, 2006) to create the n-gram model. It slightly outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the results are not directly comparable, as Mays et al. (1991) used a much smaller n-gram model and our results in Section 5.1 show that the size of the n-gram model has a large influence on the results. Eventually, we decided to use the Mays et al. (1991) approach in our study, as it is easier to adapt and augment. In a re-evaluation of the statistical model by Mays et al. (1991), Wilcox-OHearn et al. (2008) found that it ou</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="20046" citStr="Budanitsky and Hirst (2006)" startWordPosition="3253" endWordPosition="3256">7 .50 .60 Natural-English .54 .26 .35 Artificial-German .90 .49 .63 Natural-German .77 .20 .32 Table 1: Performance of the statistical approach using a trigram model based on Google Web1T. It is unclear which list was used. We could use multi-words from WordNet, but coverage would be rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rube</context>
<context position="25234" citStr="Budanitsky and Hirst (2006)" startWordPosition="4139" endWordPosition="4142">more. Thus, we will now have a look on knowledge-based approaches which are less demanding in terms of the required resources. 5.2 Knowledge-based Approach Table 3 shows the results for the knowledge-based measure. In contrast to the statistical approach, the results on the artificial errors are not higher than on the natural errors, but almost equal for German and even lower for English; another piece of evidence supporting our view that the properties of artificial datasets over-estimate the performance of statistical measures. Influence of the Relatedness Measure As was pointed out before, Budanitsky and Hirst (2006) 534 \x0cDataset Measure P R F Art-En JiangConrath 0.5 .26 .15 .19 Lin 0.5 .22 .17 .19 Lesk 0.5 .19 .16 .17 ESA-Wikipedia 0.05 .43 .13 .20 ESA-Wiktionary 0.05 .35 .20 .25 ESA-Wordnet 0.05 .33 .15 .21 Nat-En JiangConrath 0.5 .29 .18 .23 Lin 0.5 .26 .21 .23 Lesk 0.5 .19 .19 .19 ESA-Wikipedia 0.05 .48 .14 .22 ESA-Wiktionary 0.05 .39 .21 .27 ESA-Wordnet 0.05 .36 .15 .21 Table 4: Performance of knowledge-based approach using different relatedness measures. show that the measure by Jiang and Conrath (1997) yields the best results in their experiments on malapropism detection. In addition, we test an</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Carlson</author>
<author>Jeffrey Rosen</author>
<author>Dan Roth</author>
</authors>
<title>Scaling Up Context-Sensitive Text Correction.</title>
<date>2001</date>
<booktitle>In Proceedings of IAAI.</booktitle>
<contexts>
<context position="30919" citStr="Carlson et al., 2001" startWordPosition="5057" endWordPosition="5060">the artificial errors based on the Brown corpus, but - as we have seen in the previous section - evaluation on the naturally occurring errors shows a different picture. They also tried to improve the model by permitting multiple corrections and using fixed-length context windows instead of sentences, but obtained discouraging results. All previously discussed methods are unsupervised in a way that they do not rely on any training data with annotated errors. However, real-word spelling correction has also been tackled by supervised approaches (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). Those methods rely on predefined confusion-sets, i.e. sets of words that are often confounded e.g. {peace, piece} or {weather, whether}. For each set, the methods learn a model of the context in which one or the other alternative is more probable. This yields very high precision, but only for the limited number of previously defined confusion sets. Our framework for extracting natural errors could be used to increase the number of known confusion sets. 7 Conclusions and Future Work In this paper, we evaluated two main approaches for measuring the contextual fitness of terms: the statistical </context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>Andrew J Carlson, Jeffrey Rosen, and Dan Roth. 2001. Scaling Up Context-Sensitive Text Correction. In Proceedings of IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="3196" citStr="Fellbaum, 1998" startWordPosition="474" endWordPosition="475">05). A real-word spelling error is hard to detect, as the erroneous word is not misspelled and fits syntactically into the sentence. Thus, measures of contextual fitness are required to detect words that do not fit their contexts. Existing measures of contextual fitness can be categorized into knowledge-based (Hirst and Budanitsky, 2005) and statistical methods (Mays et al., 1991; Wilcox-OHearn et al., 2008). Both test the lexical cohesion of a word with its context. For that purpose, knowledge-based approaches employ the structural knowledge encoded in lexical-semantic networks like WordNet (Fellbaum, 1998), while statistical approaches rely on co-occurrence counts collected from large corpora, e.g. the Google Web1T corpus (Brants and Franz, 2006). So far, evaluation of contextual fitness measures relied on artificial datasets (Mays et al., 1991; Hirst and Budanitsky, 2005) which are created by taking a sentence that is known to be correct, and replacing a word with a similar word from the vocabulary. This has a couple of disadvantages: (i) the replacement might be a synonym of the original word and perfectly valid in the given context, (ii) the generated error might 529 \x0cbe very unlikely to </context>
<context position="10768" citStr="Fellbaum, 1998" startWordPosition="1712" endWordPosition="1713"> be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005). Normal Spelling Error We apply the Jazzy spelling detector4 and rule out all cases in which it is able to detect the error. Semantic Relation If the original token and the replaced token are in a close lexical-semantic rela4 http://jazzy.sourceforge.net/ tions, the change is likely to be semantically motivated, e.g. if house was replaced with hut. Thus, we do not consider cases, where we detect a direct semantic relation between the original and the replaced term. For this purpose, we use WordNet (Fellbaum, 1998) for English and GermaNet (Lemnitzer and Kunze, 2002) for German. 3 Resulting Datasets 3.1 Natural Error Datasets Using our framework for mining real-word spelling errors in context, we extracted an English dataset5, and a German dataset6. Although the output generally was of high quality, manual post-processing was necessary7, as (i) for some pairs the available context did not provide enough information to decide which form was correct, and (ii) a problem that might be specific to Wikipedia vandalism. The revisions are full of cases where words are replaced with similar sounding but greasy a</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C Fellbaum. 1998. WordNet An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Ferschke</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Wikipedia Revision Toolkit: Efficiently Accessing Wikipedias Edit History.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. System Demonstrations,</booktitle>
<pages>97102</pages>
<location>Portland, OR, USA.</location>
<contexts>
<context position="6786" citStr="Ferschke et al., 2011" startWordPosition="1041" endWordPosition="1044">re then corrected in subsequent revisions of the same article. The challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts. For that purpose, we apply a set of precisionoriented heuristics narrowing down the number of possible error candidates. Such an approach is feasible, as the high number of revisions in Wikipedia allows to be extremely selective. 2.1 Accessing the Revision Data We access the Wikipedia revision data using the freely available Wikipedia Revision Toolkit (Ferschke et al., 2011) together with the JWPL Wikipedia API (Zesch et al., 2008a).3 The API outputs plain text converted from Wiki-Markup, but the text still contains a small portion of leftover markup and other artifacts. Thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often URLs), (ii) sentences with less than 5 or more than 200 tokens, and (iii) sentences containing a high fraction of special characters like : usually indicating Wikipedia-specific artifacts like lists of language links. The remaining sentences are part-ofspeech tagged and lemmatized using TreeTagger (</context>
</contexts>
<marker>Ferschke, Zesch, Gurevych, 2011</marker>
<rawString>Oliver Ferschke, Torsten Zesch, and Iryna Gurevych. 2011. Wikipedia Revision Toolkit: Efficiently Accessing Wikipedias Edit History. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. System Demonstrations, pages 97102, Portland, OR, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics -ACL 05,</booktitle>
<pages>363370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10248" citStr="Finkel et al., 2005" startWordPosition="1623" endWordPosition="1626"> based on the Google Web1T n-gram counts (Brants and Franz, 2006). We filter all sentences where the replaced token has a very low unigram count. We experimented with different values and found 25,000 for English and 10,000 for German to yield good results. Same Lemma The original token and the replaced token may not have the same lemma, e.g. car and cars would not pass this filter. Stopwords The replaced token should not be in a short list of stopwords (mostly function words). Named Entity The replaced token should not be part of a named entity. For this purpose, we applied the Stanford NER (Finkel et al., 2005). Normal Spelling Error We apply the Jazzy spelling detector4 and rule out all cases in which it is able to detect the error. Semantic Relation If the original token and the replaced token are in a close lexical-semantic rela4 http://jazzy.sourceforge.net/ tions, the change is likely to be semantically motivated, e.g. if house was replaced with hut. Thus, we do not consider cases, where we detect a direct semantic relation between the original and the replaced term. For this purpose, we use WordNet (Fellbaum, 1998) for English and GermaNet (Lemnitzer and Kunze, 2002) for German. 3 Resulting Da</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics -ACL 05, pages 363370, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis W Nelson</author>
<author>Henry Kucera</author>
</authors>
<title>Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers.</title>
<date>1964</date>
<contexts>
<context position="15877" citStr="Nelson and Kucera, 1964" startWordPosition="2562" endWordPosition="2565">h sentence, a random word is selected and all strings with edit distance smaller than a given threshold (2 in our case) are generated. If one of those generated strings is a known word from the vocabulary, it is picked as the artificial error. Previous work on evaluating real-word spelling correction (Hirst and Budanitsky, 2005; WilcoxOHearn et al., 2008; Islam and Inkpen, 2009) used a dataset sampled from the Wall Street Journal corpus which is not freely available. Thus, we created a comparable English dataset of 1,000 artificial errors based on the easily available Brown corpus (Francis W. Nelson and Kucera, 1964).8 Additionally, we created a German dataset with 1,000 artificial errors based on the TIGER corpus.9 4 Measuring Contextual Fitness There are two main approaches for measuring the contextual fitness of a word in its context: the statistical (Mays et al., 1991) and the knowledgebased approach (Hirst and Budanitsky, 2005). 4.1 Statistical Approach Mays et al. (1991) introduced an approach based on the noisy-channel model. The model assumes that the correct sentence s is transmitted through a noisy channel adding noise which results in a word w being replaced by an error e leading the wrong sent</context>
</contexts>
<marker>Nelson, Kucera, 1964</marker>
<rawString>Francis W. Nelson and Henry Kucera. 1964. Manual of information to accompany a standard corpus of present-day edited American English, for use with digital computers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>16061611</pages>
<contexts>
<context position="20302" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3294" endWordPosition="3297">om WordNet, but coverage would be rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs. We do not follow this approach, but optimize the threshold on a held-out development set of real-word spelling errors. 5 Results &amp; Discussion In this section, we report on the r</context>
<context position="25968" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="4261" endWordPosition="4264">16 .17 ESA-Wikipedia 0.05 .43 .13 .20 ESA-Wiktionary 0.05 .35 .20 .25 ESA-Wordnet 0.05 .33 .15 .21 Nat-En JiangConrath 0.5 .29 .18 .23 Lin 0.5 .26 .21 .23 Lesk 0.5 .19 .19 .19 ESA-Wikipedia 0.05 .48 .14 .22 ESA-Wiktionary 0.05 .39 .21 .27 ESA-Wordnet 0.05 .36 .15 .21 Table 4: Performance of knowledge-based approach using different relatedness measures. show that the measure by Jiang and Conrath (1997) yields the best results in their experiments on malapropism detection. In addition, we test another path-based measure by Lin (1998), the gloss-based measure by Lesk (1986), and the ESA measure (Gabrilovich and Markovitch, 2007) based on concept vectors from Wikipedia, Wiktionary, and WordNet. Table 4 summarizes the results. In contrast to the findings of Budanitsky and Hirst (2006), JiangConrath is not the best path-based measure, as Lin provides equal or better performance. Even more importantly, other (non path-based) measures yield better performance than both path-based measures. Especially ESA based on Wiktionary provides a good overall performance, while ESA based on Wikipedia provides excellent precision. The advantage of ESA over the other measure types can be explained with its ability to incorporate semant</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipediabased Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 16061611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Yves Schabes</author>
</authors>
<title>Combining Trigram-based and feature-based methods for context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics -,</booktitle>
<pages>7178</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="30872" citStr="Golding and Schabes, 1996" startWordPosition="5049" endWordPosition="5052">SJ corpus. This is consistent with our findings on the artificial errors based on the Brown corpus, but - as we have seen in the previous section - evaluation on the naturally occurring errors shows a different picture. They also tried to improve the model by permitting multiple corrections and using fixed-length context windows instead of sentences, but obtained discouraging results. All previously discussed methods are unsupervised in a way that they do not rely on any training data with annotated errors. However, real-word spelling correction has also been tackled by supervised approaches (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). Those methods rely on predefined confusion-sets, i.e. sets of words that are often confounded e.g. {peace, piece} or {weather, whether}. For each set, the methods learn a model of the context in which one or the other alternative is more probable. This yields very high precision, but only for the limited number of previously defined confusion sets. Our framework for extracting natural errors could be used to increase the number of known confusion sets. 7 Conclusions and Future Work In this paper, we evaluated two main approaches for measuring th</context>
</contexts>
<marker>Golding, Schabes, 1996</marker>
<rawString>Andrew R. Golding and Yves Schabes. 1996. Combining Trigram-based and feature-based methods for context-sensitive spelling correction. In Proceedings of the 34th annual meeting on Association for Computational Linguistics -, pages 7178, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
</authors>
<title>A birds-eye view of learner corpus research,</title>
<date>2002</date>
<pages>333</pages>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="29097" citStr="Granger, 2002" startWordPosition="4754" endWordPosition="4755">the revision history of Wikipedia. Max and Wisniewski (2010) used similar techniques to create a dataset of errors from the French Wikipedia. However, they target a wider class of errors including non-word spelling errors, and their class of real-word errors conflates malapropisms as well as other types of changes like reformulations. Thus, their dataset cannot be easily used for our purposes and is only available in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls, 1999). However, annotation of errors is difficult and costly (Rozovskaya and Roth, 2010), only a small fraction of observed errors will be real-word spelling errors, and learners are likely to make dif535 \x0cferent mistakes than proficient language users. Islam and Inkpen (2009) presented another statistical approach using the Google Web1T data (Brants and Franz, 2006) to create the n-gram model. It slightly outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the result</context>
</contexts>
<marker>Granger, 2002</marker>
<rawString>Sylviane Granger, 2002. A birds-eye view of learner corpus research, pages 333. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Alexander Budanitsky</author>
</authors>
<title>Correcting real-word spelling errors by restoring lexical cohesion.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="2584" citStr="Hirst and Budanitsky, 2005" startWordPosition="378" endWordPosition="381">f contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010). A malapropism or real-word spelling error occurs when a word is replaced with another correctly spelled word which does not suit the context, e.g. People with lots of honey usually live in big houses., where money was replaced with honey. Besides typing mistakes, a major source of such errors is the failed attempt of automatic spelling correctors to correct a misspelled word (Hirst and Budanitsky, 2005). A real-word spelling error is hard to detect, as the erroneous word is not misspelled and fits syntactically into the sentence. Thus, measures of contextual fitness are required to detect words that do not fit their contexts. Existing measures of contextual fitness can be categorized into knowledge-based (Hirst and Budanitsky, 2005) and statistical methods (Mays et al., 1991; Wilcox-OHearn et al., 2008). Both test the lexical cohesion of a word with its context. For that purpose, knowledge-based approaches employ the structural knowledge encoded in lexical-semantic networks like WordNet (Fel</context>
<context position="15582" citStr="Hirst and Budanitsky, 2005" startWordPosition="2512" endWordPosition="2515">ue to more false positives. 3.2 Artificial Error Datasets In contrast to the quite challenging process of mining naturally occurring errors, creating artificial errors is relatively straightforward. From a corpus that is known to be free of spelling errors, sentences are randomly sampled. For each sentence, a random word is selected and all strings with edit distance smaller than a given threshold (2 in our case) are generated. If one of those generated strings is a known word from the vocabulary, it is picked as the artificial error. Previous work on evaluating real-word spelling correction (Hirst and Budanitsky, 2005; WilcoxOHearn et al., 2008; Islam and Inkpen, 2009) used a dataset sampled from the Wall Street Journal corpus which is not freely available. Thus, we created a comparable English dataset of 1,000 artificial errors based on the easily available Brown corpus (Francis W. Nelson and Kucera, 1964).8 Additionally, we created a German dataset with 1,000 artificial errors based on the TIGER corpus.9 4 Measuring Contextual Fitness There are two main approaches for measuring the contextual fitness of a word in its context: the statistical (Mays et al., 1991) and the knowledgebased approach (Hirst and </context>
<context position="17835" citStr="Hirst and Budanitsky (2005)" startWordPosition="2890" endWordPosition="2893">non-commercial license. 10 We optimize on a held-out development set of errors. 532 \x0cintended word wi is computed as the conditional probability P(wi|wi1wi2). Hence, the probability of the correct sentence s = w1 . . . wn can be estimated as P(s) = n+2 Y i=1 P(wi|wi1wi2) The set of candidate sentences Sc contains all versions of the observed sentence s0 derived by replacing one word with a word from edits(w), while all other words in the sentence remain unchanged. The correct sentence s is those sentence from Sc that maximizes P(s|s0) = arg maxsSc P(s) P(s0|s). 4.2 Knowledge Based Approach Hirst and Budanitsky (2005) introduced a knowledge-based approach that detects real-word spelling errors by checking the semantic relations of a target word with its context. For this purpose, they apply WordNet as the source of lexicalsemantic knowledge. The algorithm flags all words as error candidates and then applies filters to remove those words from further consideration that are unlikely to be errors. First, the algorithm removes all closed-class word candidates as well as candidates which cannot be found in the vocabulary. Candidates are then tested for having lexical cohesion with their context, by (i) checking</context>
<context position="30184" citStr="Hirst and Budanitsky (2005)" startWordPosition="4933" endWordPosition="4936">outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the results are not directly comparable, as Mays et al. (1991) used a much smaller n-gram model and our results in Section 5.1 show that the size of the n-gram model has a large influence on the results. Eventually, we decided to use the Mays et al. (1991) approach in our study, as it is easier to adapt and augment. In a re-evaluation of the statistical model by Mays et al. (1991), Wilcox-OHearn et al. (2008) found that it outperformed the knowledge-based method by Hirst and Budanitsky (2005) when evaluated on a corpus of artificial errors based on the WSJ corpus. This is consistent with our findings on the artificial errors based on the Brown corpus, but - as we have seen in the previous section - evaluation on the naturally occurring errors shows a different picture. They also tried to improve the model by permitting multiple corrections and using fixed-length context windows instead of sentences, but obtained discouraging results. All previously discussed methods are unsupervised in a way that they do not rely on any training data with annotated errors. However, real-word spell</context>
<context position="31613" citStr="Hirst and Budanitsky (2005)" startWordPosition="5170" endWordPosition="5174">ds that are often confounded e.g. {peace, piece} or {weather, whether}. For each set, the methods learn a model of the context in which one or the other alternative is more probable. This yields very high precision, but only for the limited number of previously defined confusion sets. Our framework for extracting natural errors could be used to increase the number of known confusion sets. 7 Conclusions and Future Work In this paper, we evaluated two main approaches for measuring the contextual fitness of terms: the statistical approach by Mays et al. (1991) and the knowledge-based approach by Hirst and Budanitsky (2005) on the task of detecting realword spelling errors. For that purpose, we extracted a dataset with naturally occurring errors and their contexts from the Wikipedia revision history. We show that evaluating measures of contextual fitness on this dataset provides a more realistic picture of task performance. In particular, using artificial datasets over-estimates the performance of the statistical approach, while it underestimates the performance of the knowledgebased approach. We show that n-gram models targeted towards the domain from which the errors are sampled do not improve the performance </context>
</contexts>
<marker>Hirst, Budanitsky, 2005</marker>
<rawString>Graeme Hirst and Alexander Budanitsky. 2005. Correcting real-word spelling errors by restoring lexical cohesion. Natural Language Engineering, 11(1):87111, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Inkpen</author>
<author>Alain Desilets</author>
</authors>
<title>Semantic similarity for detecting recognition errors in automatic speech transcripts.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing -HLT 05, number October,</booktitle>
<pages>4956</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1540" citStr="Inkpen and Desilets, 2005" startWordPosition="206" endWordPosition="209">eated errors. In particular, the precision of statistical methods has been largely over-estimated, while the precision of knowledge-based approaches has been under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledgebased methods can be combined for increased performance. 1 Introduction Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and Desilets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003). The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar. We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character reco</context>
</contexts>
<marker>Inkpen, Desilets, 2005</marker>
<rawString>Diana Inkpen and Alain Desilets. 2005. Semantic similarity for detecting recognition errors in automatic speech transcripts. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing -HLT 05, number October, pages 4956, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Real-word spelling correction using Google Web IT 3-grams.</title>
<date>2009</date>
<contexts>
<context position="15634" citStr="Islam and Inkpen, 2009" startWordPosition="2521" endWordPosition="2524">ts In contrast to the quite challenging process of mining naturally occurring errors, creating artificial errors is relatively straightforward. From a corpus that is known to be free of spelling errors, sentences are randomly sampled. For each sentence, a random word is selected and all strings with edit distance smaller than a given threshold (2 in our case) are generated. If one of those generated strings is a known word from the vocabulary, it is picked as the artificial error. Previous work on evaluating real-word spelling correction (Hirst and Budanitsky, 2005; WilcoxOHearn et al., 2008; Islam and Inkpen, 2009) used a dataset sampled from the Wall Street Journal corpus which is not freely available. Thus, we created a comparable English dataset of 1,000 artificial errors based on the easily available Brown corpus (Francis W. Nelson and Kucera, 1964).8 Additionally, we created a German dataset with 1,000 artificial errors based on the TIGER corpus.9 4 Measuring Contextual Fitness There are two main approaches for measuring the contextual fitness of a word in its context: the statistical (Mays et al., 1991) and the knowledgebased approach (Hirst and Budanitsky, 2005). 4.1 Statistical Approach Mays et </context>
<context position="29424" citStr="Islam and Inkpen (2009)" startWordPosition="4801" endWordPosition="4804">ke reformulations. Thus, their dataset cannot be easily used for our purposes and is only available in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls, 1999). However, annotation of errors is difficult and costly (Rozovskaya and Roth, 2010), only a small fraction of observed errors will be real-word spelling errors, and learners are likely to make dif535 \x0cferent mistakes than proficient language users. Islam and Inkpen (2009) presented another statistical approach using the Google Web1T data (Brants and Franz, 2006) to create the n-gram model. It slightly outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the results are not directly comparable, as Mays et al. (1991) used a much smaller n-gram model and our results in Section 5.1 show that the size of the n-gram model has a large influence on the results. Eventually, we decided to use the Mays et al. (1991) approach in our study, as it is easier to adapt and augment. In a re-evaluation </context>
</contexts>
<marker>Islam, Inkpen, 2009</marker>
<rawString>Aminul Islam and Diana Inkpen. 2009. Real-word spelling correction using Google Web IT 3-grams.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing Volume 3 - EMNLP 09,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker></marker>
<rawString>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing Volume 3 - EMNLP 09, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Jaro</author>
</authors>
<title>Probabilistic linkage of large public health data file. Statistics in Medicine,</title>
<date>1995</date>
<pages>14--491498</pages>
<contexts>
<context position="8112" citStr="Jaro, 1995" startWordPosition="1263" endWordPosition="1264">). 2.2 Sentence Alignment Fully aligning all sentences of the adjacent revisions is a quite costly operation, as sentences can be split, joined, replaced, or moved in the article. However, we are only looking for sentence pairs which are almost identical except for the real-word spelling error and its correction. Thus, we form all sentence pairs and then apply an aggressive but cheap filter that rules out all sentences which (i) are equal, or (ii) whose lengths differ more than a small number of characters. For the resulting much smaller subset of sentence pairs, we compute the Jaro distance (Jaro, 1995) between each pair. If the distance exceeds a certain threshold tsim (0.05 in this case), we do not further consider the pair. The small amount of remaining sentence pairs is passed to the sentence pair filter for in-depth inspection. 3 http://code.google.com/p/jwpl/ 530 \x0c2.3 Sentence Pair Filtering The sentence pair filter further reduces the number of remaining sentence pairs by applying a set of heuristics including surface level and semantic level filters. Surface level filters include: Replaced Token Sentences need to consist of identical tokens, except for one replaced token. No Numbe</context>
</contexts>
<marker>Jaro, 1995</marker>
<rawString>M A Jaro. 1995. Probabilistic linkage of large public health data file. Statistics in Medicine, 14:491498.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the 10th International Conference on Research in Computational Linguistics,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="20088" citStr="Jiang and Conrath, 1997" startWordPosition="3261" endWordPosition="3264">ial-German .90 .49 .63 Natural-German .77 .20 .32 Table 1: Performance of the statistical approach using a trigram model based on Google Web1T. It is unclear which list was used. We could use multi-words from WordNet, but coverage would be rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separate</context>
<context position="25739" citStr="Jiang and Conrath (1997)" startWordPosition="4226" endWordPosition="4229">ce of statistical measures. Influence of the Relatedness Measure As was pointed out before, Budanitsky and Hirst (2006) 534 \x0cDataset Measure P R F Art-En JiangConrath 0.5 .26 .15 .19 Lin 0.5 .22 .17 .19 Lesk 0.5 .19 .16 .17 ESA-Wikipedia 0.05 .43 .13 .20 ESA-Wiktionary 0.05 .35 .20 .25 ESA-Wordnet 0.05 .33 .15 .21 Nat-En JiangConrath 0.5 .29 .18 .23 Lin 0.5 .26 .21 .23 Lesk 0.5 .19 .19 .19 ESA-Wikipedia 0.05 .48 .14 .22 ESA-Wiktionary 0.05 .39 .21 .27 ESA-Wordnet 0.05 .36 .15 .21 Table 4: Performance of knowledge-based approach using different relatedness measures. show that the measure by Jiang and Conrath (1997) yields the best results in their experiments on malapropism detection. In addition, we test another path-based measure by Lin (1998), the gloss-based measure by Lesk (1986), and the ESA measure (Gabrilovich and Markovitch, 2007) based on concept vectors from Wikipedia, Wiktionary, and WordNet. Table 4 summarizes the results. In contrast to the findings of Budanitsky and Hirst (2006), JiangConrath is not the best path-based measure, as Lin provides equal or better performance. Even more importantly, other (non path-based) measures yield better performance than both path-based measures. Especia</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J Jiang and David W Conrath. 1997. Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P Jones</author>
<author>James H Martin</author>
</authors>
<title>Contextual spelling correction using latent semantic analysis.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied natural language processing -,</booktitle>
<pages>166--173</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="30896" citStr="Jones and Martin, 1997" startWordPosition="5053" endWordPosition="5056">nt with our findings on the artificial errors based on the Brown corpus, but - as we have seen in the previous section - evaluation on the naturally occurring errors shows a different picture. They also tried to improve the model by permitting multiple corrections and using fixed-length context windows instead of sentences, but obtained discouraging results. All previously discussed methods are unsupervised in a way that they do not rely on any training data with annotated errors. However, real-word spelling correction has also been tackled by supervised approaches (Golding and Schabes, 1996; Jones and Martin, 1997; Carlson et al., 2001). Those methods rely on predefined confusion-sets, i.e. sets of words that are often confounded e.g. {peace, piece} or {weather, whether}. For each set, the methods learn a model of the context in which one or the other alternative is more probable. This yields very high precision, but only for the limited number of previously defined confusion sets. Our framework for extracting natural errors could be used to increase the number of known confusion sets. 7 Conclusions and Future Work In this paper, we evaluated two main approaches for measuring the contextual fitness of </context>
</contexts>
<marker>Jones, Martin, 1997</marker>
<rawString>Michael P Jones and James H Martin. 1997. Contextual spelling correction using latent semantic analysis. In Proceedings of the fifth conference on Applied natural language processing -, pages 166 173, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark D Kernighan</author>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A Spelling Correction Program Based on a Noisy Channel Model.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<pages>205210</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="32962" citStr="Kernighan et al., 1990" startWordPosition="5383" endWordPosition="5386"> approach can be improved by using semantic relatedness measures that incorporate knowledge beyond the taxonomic relations in a classical lexical-semantic resource like WordNet. Finally, by combining both approaches, significant increases in precision or recall can be achieved. In future work, we want to evaluate a wider range of contextual fitness measures, and learn how to combine them using more sophisticated combination strategies. Both - the statistical as well as the knowledge-based approach - will benefit from a better model of the typist, as not all edit operations are equally likely (Kernighan et al., 1990). On the side of the error extraction, we are going to further improve the extraction process by incorporating more knowledge about the revisions. For example, vandalism is often reverted very quickly, which can be detected when looking at the full set of revisions of an article. We hope that making the experimental framework publicly available will foster future research in this field, as our results on the natural errors show that the problem is still quite challenging. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the LichtenbergProfessorship Program u</context>
</contexts>
<marker>Kernighan, Church, Gale, 1990</marker>
<rawString>Mark D Kernighan, Kenneth W Church, and William A Gale. 1990. A Spelling Correction Program Based on a Noisy Channel Model. In Proceedings of the 13th International Conference on Computational Linguistics, pages 205210, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lothar Lemnitzer</author>
<author>Claudia Kunze</author>
</authors>
<title>GermaNet - Representation, Visualization, Application.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>14851491</pages>
<contexts>
<context position="10821" citStr="Lemnitzer and Kunze, 2002" startWordPosition="1718" endWordPosition="1721">se, we applied the Stanford NER (Finkel et al., 2005). Normal Spelling Error We apply the Jazzy spelling detector4 and rule out all cases in which it is able to detect the error. Semantic Relation If the original token and the replaced token are in a close lexical-semantic rela4 http://jazzy.sourceforge.net/ tions, the change is likely to be semantically motivated, e.g. if house was replaced with hut. Thus, we do not consider cases, where we detect a direct semantic relation between the original and the replaced term. For this purpose, we use WordNet (Fellbaum, 1998) for English and GermaNet (Lemnitzer and Kunze, 2002) for German. 3 Resulting Datasets 3.1 Natural Error Datasets Using our framework for mining real-word spelling errors in context, we extracted an English dataset5, and a German dataset6. Although the output generally was of high quality, manual post-processing was necessary7, as (i) for some pairs the available context did not provide enough information to decide which form was correct, and (ii) a problem that might be specific to Wikipedia vandalism. The revisions are full of cases where words are replaced with similar sounding but greasy alternatives. A relatively mild example is In romantic</context>
</contexts>
<marker>Lemnitzer, Kunze, 2002</marker>
<rawString>Lothar Lemnitzer and Claudia Kunze. 2002. GermaNet - Representation, Visualization, Application. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), pages 14851491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>Proceedings of the 5th annual international conference,</booktitle>
<pages>2426</pages>
<contexts>
<context position="25912" citStr="Lesk (1986)" startWordPosition="4255" endWordPosition="4256">Lin 0.5 .22 .17 .19 Lesk 0.5 .19 .16 .17 ESA-Wikipedia 0.05 .43 .13 .20 ESA-Wiktionary 0.05 .35 .20 .25 ESA-Wordnet 0.05 .33 .15 .21 Nat-En JiangConrath 0.5 .29 .18 .23 Lin 0.5 .26 .21 .23 Lesk 0.5 .19 .19 .19 ESA-Wikipedia 0.05 .48 .14 .22 ESA-Wiktionary 0.05 .39 .21 .27 ESA-Wordnet 0.05 .36 .15 .21 Table 4: Performance of knowledge-based approach using different relatedness measures. show that the measure by Jiang and Conrath (1997) yields the best results in their experiments on malapropism detection. In addition, we test another path-based measure by Lin (1998), the gloss-based measure by Lesk (1986), and the ESA measure (Gabrilovich and Markovitch, 2007) based on concept vectors from Wikipedia, Wiktionary, and WordNet. Table 4 summarizes the results. In contrast to the findings of Budanitsky and Hirst (2006), JiangConrath is not the best path-based measure, as Lin provides equal or better performance. Even more importantly, other (non path-based) measures yield better performance than both path-based measures. Especially ESA based on Wiktionary provides a good overall performance, while ESA based on Wikipedia provides excellent precision. The advantage of ESA over the other measure types</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. Proceedings of the 5th annual international conference, pages 2426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity. In</title>
<date>1998</date>
<booktitle>Proceedings of International Conference on Machine Learning,</booktitle>
<pages>296304</pages>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="25872" citStr="Lin (1998)" startWordPosition="4249" endWordPosition="4250"> F Art-En JiangConrath 0.5 .26 .15 .19 Lin 0.5 .22 .17 .19 Lesk 0.5 .19 .16 .17 ESA-Wikipedia 0.05 .43 .13 .20 ESA-Wiktionary 0.05 .35 .20 .25 ESA-Wordnet 0.05 .33 .15 .21 Nat-En JiangConrath 0.5 .29 .18 .23 Lin 0.5 .26 .21 .23 Lesk 0.5 .19 .19 .19 ESA-Wikipedia 0.05 .48 .14 .22 ESA-Wiktionary 0.05 .39 .21 .27 ESA-Wordnet 0.05 .36 .15 .21 Table 4: Performance of knowledge-based approach using different relatedness measures. show that the measure by Jiang and Conrath (1997) yields the best results in their experiments on malapropism detection. In addition, we test another path-based measure by Lin (1998), the gloss-based measure by Lesk (1986), and the ESA measure (Gabrilovich and Markovitch, 2007) based on concept vectors from Wikipedia, Wiktionary, and WordNet. Table 4 summarizes the results. In contrast to the findings of Budanitsky and Hirst (2006), JiangConrath is not the best path-based measure, as Lin provides equal or better performance. Even more importantly, other (non path-based) measures yield better performance than both path-based measures. Especially ESA based on Wiktionary provides a good overall performance, while ESA based on Wikipedia provides excellent precision. The advan</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proceedings of International Conference on Machine Learning, pages 296304, Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aurelien Max</author>
<author>Guillaume Wisniewski</author>
</authors>
<title>Mining Naturally-occurring Corrections and Paraphrases from Wikipedias Revision History.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC10),</booktitle>
<pages>31433148</pages>
<contexts>
<context position="28543" citStr="Max and Wisniewski (2010)" startWordPosition="4665" endWordPosition="4669">rve that precision can be significantly improved using the Union strategy, while recall is only moderately improved using the Intersect strategy. This means that (i) a large subset of errors is detected by both approaches that due to their different sources of knowledge mutually reinforce the detection leading to increased precision, and (ii) a small but otherwise undetectable subset of errors requires considering detections made by one approach only. 6 Related Work To our knowledge, we are the first to create a dataset of naturally occurring errors based on the revision history of Wikipedia. Max and Wisniewski (2010) used similar techniques to create a dataset of errors from the French Wikipedia. However, they target a wider class of errors including non-word spelling errors, and their class of real-word errors conflates malapropisms as well as other types of changes like reformulations. Thus, their dataset cannot be easily used for our purposes and is only available in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls,</context>
</contexts>
<marker>Max, Wisniewski, 2010</marker>
<rawString>Aurelien Max and Guillaume Wisniewski. 2010. Mining Naturally-occurring Corrections and Paraphrases from Wikipedias Revision History. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC10), pages 31433148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Damerau</author>
<author>Robert L Mercer</author>
</authors>
<title>Context based spelling correction.</title>
<date>1991</date>
<journal>Information Processing &amp; Management,</journal>
<volume>27</volume>
<issue>5</issue>
<marker>Damerau, Mercer, 1991</marker>
<rawString>Eric Mays, Fred. J Damerau, and Robert L Mercer. 1991. Context based spelling correction. Information Processing &amp; Management, 27(5):517522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>\x0cRani Nelken</author>
<author>Elif Yamangil</author>
</authors>
<title>Mining Wikipedias Article Revision History for Training Computational Linguistics Algorithms.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy (WikiAI), WikiAI08.</booktitle>
<contexts>
<context position="5896" citStr="Nelken and Yamangil, 2008" startWordPosition="906" endWordPosition="909">nts as well as conducting follow-up experiments. The framework contains (i) methods to extract natural errors from Wikipedia, (ii) reference implementations of the knowledge-based and the statistical methods, and (iii) the evaluation datasets described in this paper. 2 Mining Errors from Wikipedia Measures of contextual fitness have previously been evaluated using artificially created datasets, as there are very few sources of sentences with naturally occurring errors and their corrections. Recently, the revision history of Wikipedia has been introduced as a valuable knowledge source for NLP (Nelken and Yamangil, 2008; Yatskar et al., 2010). It is also a possible source of natural errors, as it is likely that Wikipedia editors make 1 The same artificial data as described in Section 3.2. 2 http://code.google.com/p/dkpro-spelling-asl/ real-word spelling errors at some point, which are then corrected in subsequent revisions of the same article. The challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts. For that purpose, we apply a set of precisionoriented heuristics narrowing down the num</context>
</contexts>
<marker>Nelken, Yamangil, 2008</marker>
<rawString>\x0cRani Nelken and Elif Yamangil. 2008. Mining Wikipedias Article Revision History for Training Computational Linguistics Algorithms. In Proceedings of the AAAI Workshop on Wikipedia and Artificial Intelligence: An Evolving Synergy (WikiAI), WikiAI08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Nicholls</author>
</authors>
<title>The Cambridge Learner Corpus - Error Coding and Analysis for Lexicography and ELT.</title>
<date>1999</date>
<booktitle>In Summer Workshop on Learner Corpora,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="29149" citStr="Nicholls, 1999" startWordPosition="4761" endWordPosition="4762">ki (2010) used similar techniques to create a dataset of errors from the French Wikipedia. However, they target a wider class of errors including non-word spelling errors, and their class of real-word errors conflates malapropisms as well as other types of changes like reformulations. Thus, their dataset cannot be easily used for our purposes and is only available in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls, 1999). However, annotation of errors is difficult and costly (Rozovskaya and Roth, 2010), only a small fraction of observed errors will be real-word spelling errors, and learners are likely to make dif535 \x0cferent mistakes than proficient language users. Islam and Inkpen (2009) presented another statistical approach using the Google Web1T data (Brants and Franz, 2006) to create the n-gram model. It slightly outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the results are not directly comparable, as Mays et al. (1991)</context>
</contexts>
<marker>Nicholls, 1999</marker>
<rawString>Diane Nicholls. 1999. The Cambridge Learner Corpus - Error Coding and Analysis for Lexicography and ELT. In Summer Workshop on Learner Corpora, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Annotating ESL Errors: Challenges and Rewards.</title>
<date>2010</date>
<booktitle>In The 5th Workshop on Innovative Use of NLP for Building Educational Applications (NAACL-HLT).</booktitle>
<contexts>
<context position="29232" citStr="Rozovskaya and Roth, 2010" startWordPosition="4771" endWordPosition="4774"> French Wikipedia. However, they target a wider class of errors including non-word spelling errors, and their class of real-word errors conflates malapropisms as well as other types of changes like reformulations. Thus, their dataset cannot be easily used for our purposes and is only available in French, while our framework allows creating datasets for all major languages with minimal manual effort. Another possible source of real-word spelling errors are learner corpora (Granger, 2002), e.g. the Cambridge Learner Corpus (Nicholls, 1999). However, annotation of errors is difficult and costly (Rozovskaya and Roth, 2010), only a small fraction of observed errors will be real-word spelling errors, and learners are likely to make dif535 \x0cferent mistakes than proficient language users. Islam and Inkpen (2009) presented another statistical approach using the Google Web1T data (Brants and Franz, 2006) to create the n-gram model. It slightly outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the results are not directly comparable, as Mays et al. (1991) used a much smaller n-gram model and our results in Section 5.1 show that the size</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2010. Annotating ESL Errors: Challenges and Rewards. In The 5th Workshop on Innovative Use of NLP for Building Educational Applications (NAACL-HLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Rubenstein</author>
<author>J B Goodenough</author>
</authors>
<title>Contextual Correlates of Synonymy.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="20674" citStr="Rubenstein and Goodenough (1965)" startWordPosition="3353" endWordPosition="3357">006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs. We do not follow this approach, but optimize the threshold on a held-out development set of real-word spelling errors. 5 Results &amp; Discussion In this section, we report on the results obtained in our evaluation of contextual fitness measures using artificial and natural errors in English and German. 5.1 Statistical Approach Table 1 summarizes the results obtained by the statistical approach using a trigram model based on the Google Web1T data (Brants and Franz, 2006). On the English artificial errors, we observe a quite high F-measure of .60 t</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>H Rubenstein and J B Goodenough. 1965. Contextual Correlates of Synonymy. Communications of the ACM, 8(10):627633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="7399" citStr="Schmid, 2004" startWordPosition="1142" endWordPosition="1143"> together with the JWPL Wikipedia API (Zesch et al., 2008a).3 The API outputs plain text converted from Wiki-Markup, but the text still contains a small portion of leftover markup and other artifacts. Thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often URLs), (ii) sentences with less than 5 or more than 200 tokens, and (iii) sentences containing a high fraction of special characters like : usually indicating Wikipedia-specific artifacts like lists of language links. The remaining sentences are part-ofspeech tagged and lemmatized using TreeTagger (Schmid, 2004). Using these cleaned and annotated articles, we form pairs of adjacent article revisions (ri and ri+1). 2.2 Sentence Alignment Fully aligning all sentences of the adjacent revisions is a quite costly operation, as sentences can be split, joined, replaced, or moved in the article. However, we are only looking for sentence pairs which are almost identical except for the real-word spelling error and its correction. Thus, we form all sentence pairs and then apply an aggressive but cheap filter that rules out all sentences which (i) are equal, or (ii) whose lengths differ more than a small number </context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel D Walker</author>
<author>William B Lund</author>
<author>Eric K Ringger</author>
</authors>
<title>Evaluating Models of Latent Document Semantics in the Presence of OCR Errors.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, (October):240</booktitle>
<pages>250</pages>
<contexts>
<context position="2176" citStr="Walker et al., 2010" startWordPosition="308" endWordPosition="311">ter recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003). The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar. We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010). A malapropism or real-word spelling error occurs when a word is replaced with another correctly spelled word which does not suit the context, e.g. People with lots of honey usually live in big houses., where money was replaced with honey. Besides typing mistakes, a major source of such errors is the failed attempt of automatic spelling correctors to correct a misspelled word (Hirst and Budanitsky, 2005). A real-word spelling error is hard to detect, as the erroneous word is not misspelled and fits syntactically into the sentence. Thus, measures of contextual fitness are required to detect wo</context>
</contexts>
<marker>Walker, Lund, Ringger, 2010</marker>
<rawString>Daniel D. Walker, William B. Lund, and Eric K. Ringger. 2010. Evaluating Models of Latent Document Semantics in the Presence of OCR Errors. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, (October):240 250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wick</author>
<author>M Ross</author>
<author>E Learned-Miller</author>
</authors>
<date>2007</date>
<contexts>
<context position="1591" citStr="Wick et al., 2007" startWordPosition="213" endWordPosition="216">ethods has been largely over-estimated, while the precision of knowledge-based approaches has been under-estimated. Additionally, we show that knowledge-based approaches can be improved by using semantic relatedness measures that make use of knowledge beyond classical taxonomic relations. Finally, we show that statistical and knowledgebased methods can be combined for increased performance. 1 Introduction Measuring the contextual fitness of a term in its context is a key component in different NLP applications like speech recognition (Inkpen and Desilets, 2005), optical character recognition (Wick et al., 2007), co-reference resolution (Bean and Riloff, 2004), or malapropism detection (Bolshakov and Gelbukh, 2003). The main idea is always to test what fits better into the current context: the actual term or a possible replacement that is phonetically, structurally, or semantically similar. We are going to focus on malapropism detection as it allows evaluating measures of contextual fitness in a more direct way than evaluating in a complex application which always entails influence from other components, e.g. the quality of the optical character recognition module (Walker et al., 2010). A malapropism</context>
</contexts>
<marker>Wick, Ross, Learned-Miller, 2007</marker>
<rawString>M. Wick, M. Ross, and E. Learned-Miller. 2007.</rawString>
</citation>
<citation valid="true">
<title>Context-sensitive error correction: Using topic models to improve OCR.</title>
<date>2007</date>
<booktitle>In Ninth International Conference on Document Analysis and Recognition (ICDAR</booktitle>
<volume>2</volume>
<pages>11681172</pages>
<publisher>Ieee,</publisher>
<marker>2007</marker>
<rawString>Context-sensitive error correction: Using topic models to improve OCR. In Ninth International Conference on Document Analysis and Recognition (ICDAR 2007) Vol 2, pages 11681172. Ieee, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amber Wilcox-OHearn</author>
<author>Graeme Hirst</author>
<author>Alexander Budanitsky</author>
</authors>
<title>Real-word spelling correction with trigrams: A reconsideration of the Mays, Damerau, and Mercer model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th international conference on Computational linguistics and intelligent text processing (CICLing).</booktitle>
<contexts>
<context position="2992" citStr="Wilcox-OHearn et al., 2008" startWordPosition="441" endWordPosition="444">n big houses., where money was replaced with honey. Besides typing mistakes, a major source of such errors is the failed attempt of automatic spelling correctors to correct a misspelled word (Hirst and Budanitsky, 2005). A real-word spelling error is hard to detect, as the erroneous word is not misspelled and fits syntactically into the sentence. Thus, measures of contextual fitness are required to detect words that do not fit their contexts. Existing measures of contextual fitness can be categorized into knowledge-based (Hirst and Budanitsky, 2005) and statistical methods (Mays et al., 1991; Wilcox-OHearn et al., 2008). Both test the lexical cohesion of a word with its context. For that purpose, knowledge-based approaches employ the structural knowledge encoded in lexical-semantic networks like WordNet (Fellbaum, 1998), while statistical approaches rely on co-occurrence counts collected from large corpora, e.g. the Google Web1T corpus (Brants and Franz, 2006). So far, evaluation of contextual fitness measures relied on artificial datasets (Mays et al., 1991; Hirst and Budanitsky, 2005) which are created by taking a sentence that is known to be correct, and replacing a word with a similar word from the vocab</context>
<context position="30099" citStr="Wilcox-OHearn et al. (2008)" startWordPosition="4921" endWordPosition="4924">e Google Web1T data (Brants and Franz, 2006) to create the n-gram model. It slightly outperformed the approach by Mays et al. (1991) when evaluated on a corpus of artificial errors based on the WSJ corpus. However, the results are not directly comparable, as Mays et al. (1991) used a much smaller n-gram model and our results in Section 5.1 show that the size of the n-gram model has a large influence on the results. Eventually, we decided to use the Mays et al. (1991) approach in our study, as it is easier to adapt and augment. In a re-evaluation of the statistical model by Mays et al. (1991), Wilcox-OHearn et al. (2008) found that it outperformed the knowledge-based method by Hirst and Budanitsky (2005) when evaluated on a corpus of artificial errors based on the WSJ corpus. This is consistent with our findings on the artificial errors based on the Brown corpus, but - as we have seen in the previous section - evaluation on the naturally occurring errors shows a different picture. They also tried to improve the model by permitting multiple corrections and using fixed-length context windows instead of sentences, but obtained discouraging results. All previously discussed methods are unsupervised in a way that </context>
</contexts>
<marker>Wilcox-OHearn, Hirst, Budanitsky, 2008</marker>
<rawString>Amber Wilcox-OHearn, Graeme Hirst, and Alexander Budanitsky. 2008. Real-word spelling correction with trigrams: A reconsideration of the Mays, Damerau, and Mercer model. In Proceedings of the 9th international conference on Computational linguistics and intelligent text processing (CICLing).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Yatskar</author>
<author>Bo Pang</author>
<author>Cristian Danescu-NiculescuMizil</author>
<author>Lillian Lee</author>
</authors>
<title>For the sake of simplicity: unsupervised extraction of lexical simplifications from Wikipedia.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT 10,</booktitle>
<pages>365368</pages>
<contexts>
<context position="5919" citStr="Yatskar et al., 2010" startWordPosition="910" endWordPosition="913">ollow-up experiments. The framework contains (i) methods to extract natural errors from Wikipedia, (ii) reference implementations of the knowledge-based and the statistical methods, and (iii) the evaluation datasets described in this paper. 2 Mining Errors from Wikipedia Measures of contextual fitness have previously been evaluated using artificially created datasets, as there are very few sources of sentences with naturally occurring errors and their corrections. Recently, the revision history of Wikipedia has been introduced as a valuable knowledge source for NLP (Nelken and Yamangil, 2008; Yatskar et al., 2010). It is also a possible source of natural errors, as it is likely that Wikipedia editors make 1 The same artificial data as described in Section 3.2. 2 http://code.google.com/p/dkpro-spelling-asl/ real-word spelling errors at some point, which are then corrected in subsequent revisions of the same article. The challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts. For that purpose, we apply a set of precisionoriented heuristics narrowing down the number of possible error c</context>
</contexts>
<marker>Yatskar, Pang, Danescu-NiculescuMizil, Lee, 2010</marker>
<rawString>Mark Yatskar, Bo Pang, Cristian Danescu-NiculescuMizil, and Lillian Lee. 2010. For the sake of simplicity: unsupervised extraction of lexical simplifications from Wikipedia. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT 10, pages 365368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Wisdom of Crowds versus Wisdom of Linguists - Measuring the Semantic Relatedness of Words.</title>
<date>2010</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="20204" citStr="Zesch and Gurevych, 2010" startWordPosition="3280" endWordPosition="3283">model based on Google Web1T. It is unclear which list was used. We could use multi-words from WordNet, but coverage would be rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs. We do not follow this approach, but optimize the threshold on a held-out develo</context>
</contexts>
<marker>Zesch, Gurevych, 2010</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2010. Wisdom of Crowds versus Wisdom of Linguists - Measuring the Semantic Relatedness of Words. Journal of Natural Language Engineering, 16(1):2559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof Muller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="6843" citStr="Zesch et al., 2008" startWordPosition="1051" endWordPosition="1054">. The challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts. For that purpose, we apply a set of precisionoriented heuristics narrowing down the number of possible error candidates. Such an approach is feasible, as the high number of revisions in Wikipedia allows to be extremely selective. 2.1 Accessing the Revision Data We access the Wikipedia revision data using the freely available Wikipedia Revision Toolkit (Ferschke et al., 2011) together with the JWPL Wikipedia API (Zesch et al., 2008a).3 The API outputs plain text converted from Wiki-Markup, but the text still contains a small portion of leftover markup and other artifacts. Thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often URLs), (ii) sentences with less than 5 or more than 200 tokens, and (iii) sentences containing a high fraction of special characters like : usually indicating Wikipedia-specific artifacts like lists of language links. The remaining sentences are part-ofspeech tagged and lemmatized using TreeTagger (Schmid, 2004). Using these cleaned and annotated articles</context>
<context position="20322" citStr="Zesch et al., 2008" startWordPosition="3298" endWordPosition="3301">rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs. We do not follow this approach, but optimize the threshold on a held-out development set of real-word spelling errors. 5 Results &amp; Discussion In this section, we report on the results obtained in o</context>
</contexts>
<marker>Zesch, Muller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof Muller, and Iryna Gurevych. 2008a. Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary. In Proceedings of the Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Christof Muller</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using wiktionary for computing semantic relatedness.</title>
<date>2008</date>
<booktitle>In Proceedings of the 23rd AAAI Conference on Artificial Intelligence,</booktitle>
<pages>861867</pages>
<location>Chicago, IL, USA,</location>
<contexts>
<context position="6843" citStr="Zesch et al., 2008" startWordPosition="1051" endWordPosition="1054">. The challenge lies in discriminating real-word spelling errors from all sorts of other changes, including non-word spelling errors, reformulations, or the correction of wrong facts. For that purpose, we apply a set of precisionoriented heuristics narrowing down the number of possible error candidates. Such an approach is feasible, as the high number of revisions in Wikipedia allows to be extremely selective. 2.1 Accessing the Revision Data We access the Wikipedia revision data using the freely available Wikipedia Revision Toolkit (Ferschke et al., 2011) together with the JWPL Wikipedia API (Zesch et al., 2008a).3 The API outputs plain text converted from Wiki-Markup, but the text still contains a small portion of leftover markup and other artifacts. Thus, we perform additional cleaning steps removing (i) tokens with more than 30 characters (often URLs), (ii) sentences with less than 5 or more than 200 tokens, and (iii) sentences containing a high fraction of special characters like : usually indicating Wikipedia-specific artifacts like lists of language links. The remaining sentences are part-ofspeech tagged and lemmatized using TreeTagger (Schmid, 2004). Using these cleaned and annotated articles</context>
<context position="20322" citStr="Zesch et al., 2008" startWordPosition="3298" endWordPosition="3301">rather limited. We decided not to use both filters in order to better assess the influence of the underlying semantic relatedness measure on the overall performance. The knowledge based approach uses semantic relatedness measures to determine the cohesion between a candidate and its context. In the experiments by Budanitsky and Hirst (2006), the measure by (Jiang and Conrath, 1997) yields the best results. However, a wide range of other measures have been proposed, cf. (Zesch and Gurevych, 2010). Some measures using a wider definition of semantic relatedness (Gabrilovich and Markovitch, 2007; Zesch et al., 2008b) instead of only using taxonomic relations in a knowledge source. As semantic relatedness measures usually return a numeric value, we need to determine a threshold in order to come up with a binary related/unrelated decision. Budanitsky and Hirst (2006) used a characteristic gap in the standard evaluation dataset by Rubenstein and Goodenough (1965) that separates unrelated from related word pairs. We do not follow this approach, but optimize the threshold on a held-out development set of real-word spelling errors. 5 Results &amp; Discussion In this section, we report on the results obtained in o</context>
</contexts>
<marker>Zesch, Muller, Gurevych, 2008</marker>
<rawString>Torsten Zesch, Christof Muller, and Iryna Gurevych. 2008b. Using wiktionary for computing semantic relatedness. In Proceedings of the 23rd AAAI Conference on Artificial Intelligence, pages 861867, Chicago, IL, USA, Jul.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>