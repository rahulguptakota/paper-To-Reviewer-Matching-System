Corpus and Evaluation Measures for Multiple Document Summarization
with Multiple Sources
Tsutomu HIRAO
NTT Communication Science Laboratories
hirao@cslab.kecl.ntt.co.jp
Takahiro FUKUSIMA
Otemon Gakuin University
fukusima@res.otemon.ac.jp
Manabu OKUMURA
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Chikashi NOBATA
Communication Research Laboratories
nova@crl.go.jp
Hidetsugu NANBA
Hiroshima City University
nanba@its.hiroshima-cu.ac.jp
Abstract
In this paper, we introduce a large-scale test collec-
tion for multiple document summarization, the Text
Summarization Challenge 3 (TSC3) corpus. We
detail the corpus construction and evaluation mea-
sures. The significant feature of the corpus is that it
annotates not only the important sentences in a doc-
ument set, but also those among them that have the
same content. Moreover, we define new evaluation
metrics taking redundancy into account and discuss
the effectiveness of redundancy minimization.
1 Introduction
It has been said that we have too much informa-
tion on our hands, forcing us to read through a great
number of documents and extract relevant informa-
tion from them. With a view to coping with this situ-
ation, research on automatic text summarization has
attracted a lot of attention recently and there have
been many studies in this field. There is a particular
need to establish methods for the automatic sum-
marization of multiple documents rather than single
documents.
There have been several evaluation workshops
on text summarization. In 1998, TIPSTER SUM-
MAC (Mani et al., 2002) took place and the Doc-
ument Understanding Conference (DUC)1 has been
held annually since 2001. DUC has included multi-
ple document summarization among its tasks since
the first conference. The Text Summarization Chal-
lenge (TSC)2
has been held once in one and a half
years as part of the NTCIR (NII-NACSIS Test Col-
lection for IR Systems) project since 2001. Multiple
document summarization was included for the first
time as one of the tasks at TSC2 (in 2002) (Okumura
et al., 2003). Multiple document summarization is
now a central issue for text summarization research.
1
http://duc.nist.gov
2
http://www.lr.pi.titech.ac.jp/tsc
In this paper, we detail the corpus construction
and evaluation measures used at the Text Summa-
rization Challenge 3 (TSC3 hereafter), where multi-
ple document summarization is the main issue. We
also report the results of a preliminary experiment
on simple multiple document summarization sys-
tems.
2 TSC3 Corpus
2.1 Guidelines for Corpus Construction
Multiple document summarization from multiple
sources, i.e., several newspapers concerned with the
same topic but with different publishers, is more dif-
ficult than single document summarization since it
must deal with more text (in terms of numbers of
characters and sentences). Moreover, it is peculiar
to multiple document summarization that the sum-
marization system must decide how much redun-
dant information should be deleted3.
In a single document, there will be few sentences
with the same content. In contrast, in multiple doc-
uments with multiple sources, there will be many
sentences that convey the same content with differ-
ent words and phrases, or even identical sentences.
Thus, a text summarization system needs to recog-
nize such redundant sentences and reduce the redun-
dancy in the output summary.
However, we have no way of measuring the ef-
fectiveness of such redundancy in the corpora for
DUC and TSC2. Key data in TSC2 was given as
abstracts (free summaries) whose number of char-
acters was less than a fixed number and, thus, it
is difficult to use for repeated or automatic evalu-
ation, and for the extraction of important sentences.
Moreover, in DUC, where most of the key data were
abstracts whose number of words was less than a
3
It is true that we need other important techniques such as
those for maintaining the consistency of words and phrases that
refer to the same object, and for making the results more read-
able; however, they are not included here.
fixed number, the situation was the same as TSC2.
At DUC 2002, extracts (important sentences) were
used, and this allowed us to evaluate sentence ex-
traction. However, it is not possible to measure the
effectiveness of redundant sentences reduction since
the corpus was not annotated to show sentence with
same content. In addition, this is the same even if
we use the SummBank corpus (Radev et al., 2003).
In any case, because many of the current summa-
rization systems for multiple documents are based
on sentence extraction, we believe these corpora to
be unsuitable as sets of documents for evaluation.
On this basis, in TSC3, we assumed that the pro-
cess of multiple document summarization consists
of the following three steps, and we produce a cor-
pus for the evaluation of the system at each of the
three steps4
.
Step 1 Extract important sentences from a given set
of documents
Step 2 Minimize redundant sentences from the re-
sult of Step 1
Step 3 Rewrite the result of Step 2 to reduce the
size of the summary to the specified number of
characters or less.
We have annotated not only the important sen-
tences in the document set, but also those among
them that have the same content. These are the cor-
pora for steps 1 and 2. We have prepared human-
produced free summaries (abstracts) for step 3.
In TSC3, since we have key data (a set of cor-
rect important sentences) for steps 1 and 2, we con-
ducted automatic evaluation using a scoring pro-
gram. We adopted an intrinsic evaluation by human
judges for step 3, which is currently under evalu-
ation. We provide details of the extracts prepared
for steps 1 and 2 and their evaluation measures in
the following sections. We do not report the overall
evaluation results for TSC3.
2.2 Data Preparation for Sentence Extraction
We begin with guidelines for annotating important
sentences (extracts). We think that there are two
kinds of extract.
1. A set of sentences that human annotators
judge as being important in a document set
(Fukusima and Okumura, 2001; Zechner,
1996; Paice, 1990).
4
This is based on general ideas of a summarization system
and is not intended to impose any conditions on a summariza-
tion system.
Mainichi articles
Yomiuri articles
abstract
(a)
(b)
(c)
(d)
Doc. x
Doc. y
Figure 1: An example of an abstract and its sources.
2. A set of sentences that are suitable as a source
for producing an abstract, i.e., a set of sen-
tences in the original documents that corre-
spond to the sentences in the abstracts(Kupiec
et al., 1995; Teufel and Moens, 1997; Marcu,
1999; Jing and McKeown, 1999).
When we consider how summaries are produced,
it seems more natural to identify important seg-
ments in the document set and then produce sum-
maries by combining and rephrasing such informa-
tion than to select important sentences and revise
them as summaries. Therefore, we believe that sec-
ond type of extract is superior and thus we prepared
the extracts in that way.
However, as stated in the previous section, with
multiple document summarization, there may be
more than one sentence with the same content, and
thus we may have more than one set of sentences
in the original document that corresponds to a given
sentence in the abstract; that is to say, there may be
more than one key datum for a given sentence in the
abstract5
.
we have two sets of sentences that correspond to
sentence in the abstract.
(1)  of document  , or
(2) a combination of  and  of document 	
This means that  alone is able to produce , and
can also be produced by combining   and   (Fig-
ure 1).
We marked all the sentences in the original doc-
uments that were suitable sources for producing the
sentences of the abstract, and this made it possible
for us to determine whether or not a summariza-
tion system deleted redundant sentences correctly
at Step 2. If the system outputs the sentences in
the original documents that are annotated as cor-
responding to the same sentence in the abstract, it
5
We use â€˜set of sentencesâ€™ since we often find that more
than one sentence corresponds to a sentence in the abstract.
Table 1: Important Sentence Data.
Sentence ID of Abstract Set of Corresponding Sentences
1 
 

2 

3 
    
    
has redundancy. If not, it has no redundancy. Re-
turning to the above example, if the system outputs
  , ,and  , they all correspond to sentence in the
abstract, and thus it is redundant.
3 Evaluation Metrics
We use both intrinsic and extrinsic evaluation. The
intrinsic metrics are â€œPrecisionâ€, â€œCoverageâ€ and
â€œWeighted Coverage.â€ The extrinsic metric is
â€œPseudo Question-Answering.â€
3.1 Intrinsic Metrics
3.1.1 Number of Sentences System Should
Extract
Precision and Recall are generally used as evalua-
tion matrices for sentence extraction, and we used
the PR Breaking Point (Precision = Recall) for the
evaluation of extracts in TSC1 (Fukusima and Oku-
mura, 2001). This means that we evaluate systems
when the number of sentences in the correct ex-
tract is given. Moreover, in TSC3 we assume that
the number of sentences to be extracted is known
and we evaluate the system output that has the same
number of sentences.
However, it is not as easy to decide the number of
sentences to be extracted in TSC3 as in TSC1. We
assume that there are correspondences between sen-
tences in original documents and their abstract as in
Table 1. An ASCII space, â€ â€, is the delimiter for
the sets of corresponding sentences in the table. As
shown in the table, we often see several sets of sen-
tences that correspond to a sentence in the abstract
in multiple document summarization.
An â€˜extractâ€™ here is a set of sentences needed
to produce the abstract. For instance, we can ob-
tain â€˜extractsâ€™ such as â€œ  , ,! ," ,# ,"# â€, and
â€œ $# ,% , ,! ," ,# ,  , â€ from Table 1 6. Often
there are several â€˜extractsâ€™ and we must determine
which of these is the best. In such cases, we define
the â€˜correct extractâ€™ as the set with the least number
of sentences needed to produce the abstract because
it is desirable to convey the maximum amount of
information with the least number of sentences.
Finding the minimum set of sentences to produce
the abstract amounts to solving the constraint sat-
6
In fact, it is possible to produce the abstract with other sen-
tence combinations.
isfaction problem. In the example in Table 1, we
obtain the following constraints from each sentence
in the abstract:
&(' *)+-,/.#*0/ 1 ,
&(' 2)+30/!304" ,
&(' 2)+.#504%604718,4.-04#504"#71
With these conditions, we now find the minimum
set that makes all the conjunctions true. We need
to find the minimum set that makes ' 90 ' /0
' :)<;= >-? @ In this case, the minimum cover is
A
B CB%!7B%"CB%#DB%"#CE , and so the system should
extract six sentences.
In TSC3, we computed the number of sentences
that the system should extract and then evaluated the
system outputs, which must have the same number
of sentences, with the following precision and cov-
erage.
3.1.2 Precision
Precision is the ratio of how many sentences in the
system output are included in the set of the corre-
sponding sentences. It is defined by the following
equation.
Precision FHG IKJ (1)
where L is the least number of sentences needed
to produce the abstract by solving the constraint
satisfaction problem and M is the number of â€˜cor-
rectâ€™ sentences in the system output, i.e., the sen-
tences that are included in the set of correspond-
ing sentences. For example, the sentences listed
in Table 1 are â€˜correct.â€™ If the system output is
â€œ #CB  B%!B  NB%"#DB%"  â€, then the Precision is as
follows:
Precision FPOQRFRSUT
Q%QWV
T (2)
for â€œ XB  #DB%B CB%!B "# â€, the Precision is as fol-
lows:
Precision F
Q
Q FZY[T (3)
3.1.3 Coverage
Coverage is an evaluation metric for measuring how
close the system output is to the abstract taking into
account the redundancy found in the set of sentences
in the output.
The set of sentences in the original documents
that corresponds correctly to the \ -th sentence of
the human-produced abstract is denoted here as
]_^a`
B
]b^a`
CB%c%c cB
]_^a`d
B%c%c cB
]_^a`e
. In this case, we have
f
sets of corresponding sentences. Here,
]_^a`d
indi-
cates a set of elements each of which corresponds to
the sentence number in the original documents, de-
noted as
]_^a`d
)
A7g ^` d`
B
g ^a` d`
7B%c c%cB
g ^` d` h
B c%c c E . For
instance, from Table 1,
]

`
i)
g

`

`
XB
g

`

`
 and
g

`

`
3)+ $#CB
g

`

`
j)+  .
Then, we define the evaluation score ? .$\1 for the
\ -th sentence in the abstract as equation (1).
k[lnmo F:prqts
uvxwvUy
z {}|~ Â€z
ÂÂ‚ uÂ„Âƒ laÂ…Â†aÂ‡w Â‡ Â o
ÂˆÂŠÂ‰ Â†Â‡w Âˆ J (4)
where Â‹ÂŒ.ÂÂ1 is defined by the following equation.
Âƒ laÂÂo F
Y if the system outputs Â
S otherwise
(5)
Function ? returns 1 (one) when any
]b^a` d
is out-
puted completely. Otherwise it returns a partial
score according to the number of sentences Â‘
]_^nd
Â‘ .
Given function ? and the number of sentences in
the abstract Â’ , Coverage is defined as follows:
Coverage F
Â“
Â† Â‚ u k[lnmo
Â” T (6)
If the system extracts â€œ $# ,  ,! ,%N ,"# ,"  â€,
? .\1 is computed as follows:
k[l Y o F maxl SUJ[Y o F Y
k[lÂ•Wo F maxl SUT Â–tÂ– o F SUT Â–%Â–
k[l Â– o F maxl SUJSUT Â–%Â– o F SUT Â–%Â–
and its Coverage is 0.553. If the system extracts
â€œ  ,%$# ,% , ,! ,"# â€, then the Coverage is 0.780.
ktl Y o F maxl Y[JXY o FZY
ktlÂ•xo F maxl SUT
QWV o FÂ—SUT
QWV
ktl Â– o F maxl SUJSUT
QWV o FRSUT
QWV
3.1.4 Weighted Coverage
Now we define â€˜Weighted Coverageâ€™ since each
sentence in TSC3 is ranked A, B or C, where â€œAâ€ is
the best. This is similar to â€œRelative Utilityâ€ (Radev
et al., 2003). We only use three ranks in order to
limit the ranking cost. The definition is obtained by
modifying equation (6).
W.C. F
Â“Â† Â‚ uÂ˜ lnÂ™xlnmooÂÂšÂ›k[lnmo
Â˜ l Â‰ onÂ”Âœl Â‰ oÂÂ Â˜ lÂŸÂ onÂ”ÂlÂŸÂ oÂŒÂ Â˜ laÂ¡5onÂ”ÂlaÂ¡5o J (7)
where =Â.\1 denotes the ranking of the \ -th sentence
of the abstract and Â¢Â£.$=Â.$\11 is its weight. Â’Â¤.$= Â’Â¤Â¥Â1 is
the number of sentences whose ranking is = Â’Â¦Â¥ in
the abstract. Suppose the first sentence is ranked A,
the second B, and the third C in Table 1, and their
weights are given as Â¢9.
]
1Â›)Â¨Â§ ,Â¢Â£.Â©Âª1Â«)Â¨Â¬7@Â®Â­ and
Â¢9. ' 16)+Â¬7@Â®Â¯ 7.
As before, if the system extracts
â€œ # ,  , ! , N , "# , "  â€, then the Weighted
Coverage is computed as follows:
W.C. F
Y Âš Y Â S7T Â° Âš SUT Â–%Â– Â SUT Â– Âš SUT Â–%Â–
Y Âš Y Â SUT Â° Âš Y Â SUT Â– Âš Y
F/SUT
Q%Â±%Â²
T
(8)
3.2 Extrinsic Metrics
3.2.1 Pseudo Question-Answering
Sometimes question-answering (QA) by human
subjects is used for evaluation (Morris et al., 1992;
Hirao et al., 2001). That is, human subjects judge
whether predefined questions can be answered by
reading only a machine generated summary. How-
ever, the cost of this evaluation is huge. Therefore,
we employ a pseudo question-answering evaluation,
i.e., whether a summary has an â€˜answerâ€™ to the ques-
tion or not. The background to this evaluation is in-
spired by TIPSTER SUMMACâ€™s QA track (Mani et
al., 2002).
For each document set, there are about five ques-
tions for a short summary and about ten questions
for long summary. Note that the questions for the
short summary are included in the questions for the
long summary. Examples of questions for the topic
â€œRelease of SONYâ€™s AIBOâ€ are as follows: â€œHow
much is AIBO?â€, â€œWhen was AIBO sold?â€, and
â€œHow many AIBO are sold?â€.
Now, we evaluate the summary from the â€˜exact
matchâ€™ and â€˜edit distanceâ€™ for each question. â€˜Ex-
act matchâ€™ is a scoring function that returns one
when the summary includes the answer to the ques-
tion. â€˜Edit distanceâ€™ measures whether the systemâ€™s
summary has strings that are similar to the answer
strings. The score Â³ eÂ´ based on the edit distance is
normalized with the length of the sentence and the
answer string so that the range of the score is [0,1]:
Sed F
length of the sentence Âµ edit distance
length of the answer strings
T (9)
The score for a summary is the maximum value
of the scores for sentences in the summary. The
7Â¶Â¸Â·ÂºÂ¹ Â·nÂ»Â¼Â¼ may be computed differently. It is 1/rank (one
divided by rank) here.
Table 2: Description of TSC3 Corpus.
# of doc. sets 30
# of articles (The Mainichi) 175
# of articles (The Yomiuri) 177
Total 352
# of Sentences 3587
score is 1 if the summary has a sentence that in-
cludes the whole answer string.
It should be noted that the presence of answer
strings in the summary does not mean that a human
subject can necessarily answer the question.
4 Preliminary Experiment
In order to examine whether our corpus is suitable
for summarization evaluation, our evaluation mea-
sures significant information and redundancies in
the system summaries.
Below we provide the details of the corpus, eval-
uation results and effectiveness of the minimization
of redundant sentences.
4.1 Description of Corpus
According to the guidelines described in section
two, we constructed extracts and abstracts of thirty
sets of documents drawn from the Mainichi and
Yomiuri newspapers published between 1998 to
1999, each of which was related to a certain topic.
First, we prepared abstracts (their sizes were 5%
and 10% of the total number of the characters in
the document set), then produced extracts using the
abstracts. Table 2 shows the statistics.
One document set consists of about 10 articles
on average, and the almost same number of articles
were taken from the Mainichi newspaper and the
Yomiuri newspaper. Most of the topics are classified
into a single-event according to McKeown (2001).
The following list contains all the topics.
0310 Two-and-half-million-year old new hominid species
found in Ethiopia.
0320 Acquisition of IDC by NTT (and C&W).
0340 Remarketing of game software judged legal by Tokyo
District Court.
0350 Night landing practice of carrier-based aircrafts of the
Independence.
0360 Simultaneous bombing of the US Embassies in Tanzania
and Kenya.
0370 Resignation of President Suharto.
0380 Nomination of Mr. Putin as Russian prime minister.
0400 Osama bin Laden provided shelter by Taliban regime in
Afghanistan.
0410 Transfer of Nakata to A.C. Perugia.
0420 Release of Dreamcast.
0440 Existence of Japanese otter confirmed.
0450 Kyocera Corporation makes Mita Co. Ltd. its subsidiary.
0460 Five-story pagoda at Muroji Temple damaged by ty-
phoon.
0470 Retirement of aircraft YS-11.
0480 Test observation of astronomical telescope â€˜Subaruâ€™
started.
0500 Dolly the cloned sheep.
0510 Mass of neutrinos.
0520 Human Genome Project finishes decoding of the 22nd
chromosome.
0530 Peace talks in Northern Ireland at the end of 1999.
0540 Debut of new model of bullet train (700 family).
0550 Mr. Yukio Aoshima decides not to run for gubernatorial
election.
0560 Mistakes in entrance examination of Kansai University.
0570 Space shuttle Endeavour, from its launch to return.
0580 40 million-year-old fossil of new monkey species found
by research group at Kyoto University.
0590 Dead body of George Mallory found on Mt. Everest.
0600 Release of SONYâ€™s AIBO.
0610 e-one, look-alike of iMac.
0630 Research on Kitora tomb resumes.
0640 Tidal wave damage generated by earthquake in Papua
New Guinea.
0650 Mistaken bombing of the Chinese embassy by NATO.
4.2 Compared Extraction Methods
We used the lead-based method, the TFc IDF-based
method (Zechner, 1996) and the sequential pattern-
based method (Hirao et al., 2003), and compared
performance of these summarization methods on
the TSC3 corpus.
Lead-based Method
The documents in a test set were sorted in chrono-
logical and ascending order. Then, we extracted a
sentence at a time from the beginning of each docu-
ment and collected them to form a summary.
TFc IDF-based Method
The score of a sentence is the sum of the significant
scores of each content word in the sentence. We
therefore extracted sentences in descending order of
importance score. The sentence score Stfidf.
^
1 is
defined by the following.
Stfidf
laÂ½ Â† o F Â¾Â¿[Ã€
|
Â˜ lÂºÃ JÃ‚jÃƒ o J (10)
where Â¢9.;XB Ã„Â›Â³_1 is defined as follows:
Â˜ lnÃ JÃ‚Ã…Ãƒ o F ÃÃ†Ã‡lnÃ JÃ‚Ã…Ãƒ o}ÃˆÃ‰ÂºÃŠ%Ã‹
Âˆ
Ã‚ ÂŸ Âˆ
ÃŒ Ã†ÃlÂºÃo T (11)
;[ÃÃ.$;XB%Ã„Â«Â³b1 is the frequency of word ; in the docu-
ment set, ÃÃÂŒ.$;X1 is the document frequency of ; , and
Â‘ Ã„Â›Â©4Â‘ is the total number of documents in the set. In
fact, we computed these using all the articles pub-
lished in the Mainichi and Yomiuri newspapers for
the years 1998 and 1999.
Sequential Pattern-based Method
The score of a sentence is the sum of the signifi-
cant scores of each sequential pattern in the sen-
tence. The patterns used for scoring were decided
Table 3: Evaluation results for â€œPrecisionâ€, â€œCover-
ageâ€ and â€œWeighted Coverage.â€
Method Length Prec. Cov. W.C.
Lead
Short .426 .212 .326
Long .539 .259 .369
TFÃ‘ IDF
Short .497 .292 .397
Long .604 .325 .434
Pattern
Short .613 .305 .403
Long .665 .298 .418
Table 4: Evaluation results for â€œPseudo Question-
Answering.â€
Method Length Exact Edit
Lead
Short .300 .589
Long .275 .602
TFÃ‘ IDF
Short .375 .643
Long .393 .659
Pattern
Short .390 .644
Long .370 .640
by using a statistical significance test such as the Ã’

metric test and using 1,000 patterns. This is an ex-
tension of Linâ€™s method (Lin and Hovy, 2000). The
sentence score Spat.
^
1 is defined by the following.
Spat
laÂ½ Â† o F Ã“XÂ¿[Ã€
|
Â˜ lnÃ”Ão J (12)
where Â¢Â£.$Ã•Ã1 is defined as follows:
Â˜ lÂºÃ”Ão F
Ã‰ÂŠÃŠ%Ã‹WlaÃ†ÂlnÃ” JÃ‚Ã…Ãƒ o}Â Y oÃ‡ÃˆÃ‰ÂŠÃŠ%Ã‹Wl
z {ÃÃ–Ã‡z
Ã— Ã˜
Ã“
Â‡ {ÃÃ– Ã™ o
Ãš kÂ”ÂlÂºÃ”Ão T (13)
ÃÂŒ.Ã•ÃBÃ„Â›Â³_1 is the sentence frequency of pattern Ã• in
the document set and ÃÂŒ.Ã•ÃB
]
Â³b1 is the sentence fre-
quency of pattern Ã• in all topics. Â‘
]
Â³rÂ‘ is the number
of sentences in all topics and Ã›$?Â’Â¦.Ã•Ã1 is the pattern
length.
4.3 Evaluation Result
Table 3 shows the intrinsic evaluation result. All
methods have lower Coverage and Weighted Cov-
erage scores than Precision scores. This means that
the extracted sentences include redundant ones. In
particular, the difference between â€œPrecisionâ€ and
â€œCoverageâ€ is large in â€œPattern.â€
Although both â€œPatternâ€ and â€œTFc IDFâ€ outper-
form â€œLead,â€ the difference between them is small.
In addition, we know that â€œLeadâ€ is a good extrac-
tion method for newspaper articles; however, this is
not true for the TSC3 corpus.
Table 4 shows the extrinsic evaluation results.
Again, both â€œPatternâ€ and â€œTFc IDFâ€ outperform
â€œLeadâ€, but the difference between them is small.
We found a correlation between the intrinsic and ex-
trinsic measures.
Table 5: Effects of clustering (â€œPrecisionâ€, â€œCover-
ageâ€, â€œWeighted Coverageâ€).
Method Length Prec. Cov. W.C.
TFÃ‘ IDF
Short .430 .297 .377
Long .533 .345 .455
Pattern
Short .531 .289 .390
Long .620 .338 .456
Table 6: Effects of clustering (Pseudo Question-
Answering).
Method Length Exact Edit
TFÃ‘ IDF
Short .401 .650
Long .377 .648
Pattern
Short .392 .650
Long .380 .655
4.4 Effect of Redundant Sentence
Minimization
The experiment described in the previous section
shows that a group of sentences extracted in a sim-
ple way includes many redundant sentences. To
examine the effectiveness of minimizing redundant
sentences, we compare the Maximal Marginal Rele-
vance (MMR) based approach (Carbonell and Gold-
stein, 1998) with the clustering approach (Nomoto
and Matsumoto, 2001). We use â€˜cosine similarityâ€™
with a bag-of-words representation for the similar-
ity measure between sentences.
Clustering-based Approach
After computing importance scores using equations
(10) and (12), we conducted hierarchical clustering
using Wardâ€™s method until we reached L (see Sec-
tion 3.1.1) clusters for the first Â¯7L sentences. Then,
we extracted the sentence with the highest score
from each cluster.
Table 5 shows the results of the intrinsic evalu-
ation and Table 6 shows the results of the extrin-
sic evaluation. By comparison with Table 3, the
clustering-based approach resulted in TFc IDF and
Pattern scoring low in Precision, but high in Cov-
erage. When comparing Table 4 with Table 6, the
score is improved in most cases. These results im-
ply that redundancy minimization is effective for
improving the quality of summaries.
MMR-based Approach
After computing importance scores using equations
(10) and (12), we re-ranked the first Â¯DL sentences by
MMR and extracted the first L sentences.
Table 7 and 8 show the intrinsic and extrinsic
evaluation results, respectively. We can see the ef-
fectiveness of redundancy minimization by MMR.
Notably, in most cases, there is a large improvement
in both the intrinsic and extrinsic evaluation results
as compared with clustering.
Table 7: Effects of MMR (â€œPrecisionâ€, â€œCoverageâ€,
â€œWeighted Coverageâ€).
Method Length Prec. Cov. W.C.
TFÃ‘ IDF
Short .469 .306 .403
Long .565 .376 .475
Pattern
Short .469 .332 .429
Long .577 .377 .500
Table 8: Effects of MMR (Pseudo Question-
Answering).
Method Length Exact Edit
TFÃ‘ IDF
Short .386 .647
Long .405 .667
Pattern
Short .417 .663
Long .390 .656
These results show that redundancy minimization
has a significant effect on multiple document sum-
marization.
5 Conclusion
We described the details of a corpus constructed for
TSC3 and measures for its evaluation, focusing on
sentence extraction. We think that a corpus in which
important sentences and those with the same content
are annotated for multiple documents is a new and
significant feature for summarization corpora.
It is planned to make the TSC3 corpus available
(even if the recipient is not a TSC3 participant) by
exchanging memoranda with the National Institute
of Informatics in Japan. We sincerely hope that this
corpus will be useful to researchers who are inter-
ested in text summarization and serve to facilitate
further progress in this field.
References
J. Carbonell and J. Goldstein. 1998. The Use of
MMR, Diversity-Based Reranking for Reorder-
ing Documents and Producing Summaries. In
Proc. of the 21th ACM-SIGIR, pages 335â€“336.
T. Fukusima and M. Okumura. 2001. Text Summa-
rization Challenge: Text Summarization Evalua-
tion in Japan. In Proc. of the NAACL 2001 Work-
shop on Automatic summarization, pages 51â€“59.
T. Hirao, Y. Sasaki, and H. Isozaki. 2001. An
Extrinsic Evaluation for Question-Biased Text
Summarization on QA tasks. In Proc. of the
NAACL 2001 Workshop on Automatic Summa-
rization, pages 61â€“68.
T. Hirao, J. Suzuki, H. Isozaki, and E. Maeda.
2003. Multiple Document Summarization using
Sequential Pattern Mining (in Japanese). In The
Special Interest Group Notes of IPSJ (NL-158-6),
pages 31â€“38.
H. Jing and K. McKeown. 1999. The Decom-
position of Human-Written Summary Sentences.
Proc. of the 22nd ACM-SIGIR, pages 129â€“136.
J. Kupiec, J Petersen, and F. Chen. 1995. A Train-
able Document Summarizer. In Proc. of the 18th
SIGIR, pages 68â€“73.
C-Y. Lin and E. H. Hovy. 2000. The Automated
Acquisition of Topic Signatures for Text Sum-
marization. In Proc. of the 16th COLING, pages
495â€“501.
I. Mani, G. Klein, D. House, L. Hirschman, T. Fir-
man, and B. Sundheim. 2002. SUMMAC: a text
summarization evaluation. Natural Language
Engineering, 8(1):43â€“68.
D. Marcu. 1999. The Automatic Construction
of Large-scale Corpora for Summarization Re-
search. Proc. of the 22nd ACM-SIGIR, pages
137â€“144.
K. McKeown, R. Barzilay, D. Evans, V. Hatzivas-
silogou, M. Y. Kan, B. Schiffman, and S. Teufel.
2001. Columbia Multi-Document Summariza-
tion: Approach and Evaluation. In Proc. of the
Document Understanding Conference 2001.
A. H. Morris, G. M. Kasper, and D.A. Adams.
1992. The Effects and Limitations of Automatic
Text Condensing on Reading Comprehension.
Information System Research, 3(1):17â€“35.
T. Nomoto and M. Matsumoto. 2001. A New Ap-
proach to Unsupervised Text Summarization. In
Proc. of the 24th ACM-SIGIR, pages 26â€“34.
M. Okumura, T. Fukusima, and H. Nanba. 2003.
Text Summarization Challenge 2, Text Summa-
rization Evaluation at NTCIR Workshop 3. In
Proc. of the HLT/NAACL 2003 Text Summariza-
tion Workshop, pages 49â€“56.
C. Paice. 1990. Constructing Literature Abstracts
by Computer: Techniques and Prospects. Infor-
mation Processing and Management, 26(1):171â€“
186.
D. R. Radev, S. Teufel, H. Saggion, W. Lam,
J. Blitzer, H. Qi, A. Celebi, D. Liu, and
E. Drabek. 2003. Evaluation challenges in large-
scale document summarization. In Proc. of the
41st ACL, pages 375â€“382.
S. Teufel and M. Moens. 1997. Sentence Extrac-
tion as a Classification Task. In Proc. of the ACL
Workshop on Intelligent Scalable Text Summa-
rization, pages 58â€“65.
K. Zechner. 1996. Fast Generation of Abstracts
from General Domain Text Corpora by Extract-
ing Relevant Sentences. In Proc. of the 16th
COLING, pages 986â€“989.
